demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.03941016150020005, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0119,  0.0030,  0.0177, -0.0077, -0.1886, -0.8015]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.03445053590833595, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0088, -0.0051,  0.0129, -0.0172, -0.1928, -0.9796]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.028995667477474285, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.8944e-02,  3.1066e-03,  1.7270e-02, -1.0749e-03, -1.7316e-01,
         -1.1608e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.04011482553560853, val_acc 0.98
trigger times: 1
end of epoch 4: val_loss 0.016642679348475298, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0043, -0.0044,  0.0019, -0.0127, -0.3047, -1.4055]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.00782560371499656, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0448,  0.0280,  0.0094, -0.0106, -0.3921, -1.7658]],
       device='cuda:0'))])
end of epoch 6: val_loss 0.009816315159355184, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.02061782204813994, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.026916442979054125, val_acc 0.98
trigger times: 3
end of epoch 9: val_loss 0.010484902613992375, val_acc 1.0
trigger times: 4
end of epoch 10: val_loss 0.480900647487055, val_acc 0.88
trigger times: 5
end of epoch 11: val_loss 0.005261039019393649, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0176, -0.0266,  0.0507,  0.0131, -0.5267, -2.0602]],
       device='cuda:0'))])
end of epoch 12: val_loss 0.00797457669800167, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 0.016824501072602516, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 0.021153062948610142, val_acc 0.98
trigger times: 3
end of epoch 15: val_loss 0.023567659077140205, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 0.011545191470982985, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 0.023811113872760074, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 0.03770703666466772, val_acc 0.98
trigger times: 7
end of epoch 19: val_loss 0.26932132845145007, val_acc 0.9
trigger times: 8
end of epoch 20: val_loss 0.004224905945046373, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.0118e-03,  2.4389e-02,  1.2841e-05, -7.3024e-03, -4.4233e-01,
         -1.8580e+00]], device='cuda:0'))])
end of epoch 21: val_loss 0.04587807376502937, val_acc 0.98
trigger times: 1
end of epoch 22: val_loss 0.015494480193099064, val_acc 1.0
trigger times: 2
end of epoch 23: val_loss 0.0359329130262536, val_acc 0.98
trigger times: 3
end of epoch 24: val_loss 0.04461341737981257, val_acc 0.98
trigger times: 4
end of epoch 25: val_loss 0.012115959613791886, val_acc 1.0
trigger times: 5
end of epoch 26: val_loss 0.008876545214205294, val_acc 1.0
trigger times: 6
end of epoch 27: val_loss 0.03547721548177279, val_acc 0.98
trigger times: 7
end of epoch 28: val_loss 0.009707925434235988, val_acc 1.0
trigger times: 8
end of epoch 29: val_loss 0.018372705424584124, val_acc 1.0
trigger times: 9
end of epoch 30: val_loss 0.07624213314392961, val_acc 0.96
trigger times: 10
Early stopping.
0 -18.44079437851906 -50.00222059884506
1 -14.341390900313854 -48.823739140882175
2 -19.646634984761477 -47.15605336419176
3 -16.608453288674355 -46.19619104961985
4 -18.25340834259987 -45.594227093057754
5 -18.067465975880623 -45.36842966452394
6 -23.987934052944183 -45.19068756322445
7 -16.725930392742157 -44.16490839583478
8 -11.746523778885603 -44.078303125872196
9 -22.196180514991283 -43.993555290419714
10 -13.1458795722574 -43.86306534422809
11 -20.05235617607832 -43.84874807044028
12 -14.150983024388552 -43.84199129025074
13 -12.882704444229603 -43.806385671938365
14 -11.935954362154007 -43.80581985978556
15 -12.023088574409485 -42.7594767358323
16 -9.421451734378934 -42.66316468983175
17 -20.085926964879036 -42.33177224591743
18 -11.882301330566406 -41.774963389485094
19 -14.343451112508774 -41.410068073767725
20 -13.386151928454638 -41.17786296442943
21 -16.921403139829636 -40.723520424948155
22 -21.256194815039635 -40.527189756101116
23 -18.2464552372694 -40.49595848244517
24 -16.93179578334093 -40.429389880911344
25 -13.591289699543267 -40.05653450521898
26 -11.997674476355314 -39.59232357792555
27 -22.623962312936783 -39.54162101198148
28 -14.437094084918499 -39.195237471709476
29 -20.165647372603416 -39.172389579378766
30 -15.05457559786737 -38.51095963496708
31 -15.121304981410503 -38.447265769744824
32 -18.42453058436513 -38.392107037026264
33 -7.820462318137288 -38.00745349944469
34 -10.153916724026203 -37.46482488602393
35 -15.652972623705864 -37.10988160586883
36 -14.536313563585281 -34.27116723637227
37 -7.988313084002584 -34.14139118114101
38 -12.117093108594418 -33.263072731706835
39 -10.80628496594727 -33.13344797200536
40 -9.585941784083843 -33.07825234291984
41 -13.908080533146858 -33.0321314765637
42 -23.57189306616783 -32.44934973065406
43 -15.657638221979141 -32.4007978120153
44 -9.268338568508625 -32.40063925734975
45 -3.9211279451847076 -30.734403792103194
46 -6.769494975917041 -30.57151371770873
47 -10.224186170846224 -30.131236504472803
48 -12.368531659245491 -29.99326722619033
49 -8.87175297550857 -29.66908258985071
50 -17.445346914231777 -29.297233511513635
51 -12.489925414323807 -29.288890423975797
52 -6.179969429969788 -29.145878352769948
53 -9.28383501409553 -28.49601894351319
54 -7.688452176749706 -28.492023661124072
55 -13.684534803032875 -28.315961465855167
56 -6.44841461093165 -27.121110566589827
57 -1.489354345947504 -26.064532595535336
58 -13.559722753241658 -25.520524278341334
59 -12.971404284238815 -25.27101421179229
60 -7.379227831959724 -25.066644278800943
61 -14.820820458233356 -24.925849381327673
62 -8.432713586837053 -24.188105669766596
63 -11.685087531805038 -23.48479966198816
64 -1.4034402817487717 -23.153943559703283
65 -12.185110583901405 -22.954730295117237
66 -10.196023218333721 -22.74124885266394
67 -9.374682769179344 -22.739273544503753
68 -0.8416188005357981 -22.264945050603636
69 -11.87639919295907 -22.15569724300287
70 -5.302932197228074 -21.055920928583344
71 -0.6024170033633709 -20.543356562348553
72 -4.582050792872906 -20.33499633836848
73 -6.481761708855629 -20.18157658281111
74 -9.701255161315203 -19.58144410477429
75 -4.614679876714945 -19.377225745334304
76 -11.219164445996284 -19.243135617403095
77 -0.8971522152423859 -19.060620225371707
78 -8.087615373078734 -18.964124524696246
79 -11.251476749777794 -18.448962308005108
80 -10.898964330554008 -17.740722019993825
81 -0.29312023404054344 -16.85889369985028
82 -3.5198938930407166 -16.3381194095591
83 -5.673741929233074 -14.535892564189266
84 -1.809975117444992 -14.443670567499144
85 0.27607930824160576 -14.200413010108107
86 -10.464958399534225 -13.936976181618805
87 -3.2897990830242634 -13.862253042167257
88 -8.47552265971899 -13.483098530680483
89 -0.6546438075602055 -13.455892754889845
90 -0.9719302039593458 -13.355868275096913
91 -7.085456341505051 -12.278515244993585
92 1.9283539019525051 -12.227387460046547
93 0.8584425561130047 -12.020717825467683
94 -5.050629686564207 -11.910094799877324
95 -3.1006326973438263 -11.400284019256157
96 -4.952411957085133 -11.134618158086587
97 -1.441543941386044 -10.859166921158222
98 0.8130984087474644 -9.595137958067907
99 -0.23026109486818314 -9.289921608799773
100 -7.683963593095541 -8.230877068641124
101 -6.762027062475681 -7.882363241796725
102 -8.558593988418579 -7.6478984168416355
103 -6.117690462619066 -7.459623237418707
104 -8.389407597482204 -7.124357312750265
105 -4.640243887901306 -7.05379065585803
106 -5.726800039410591 -6.853091098326624
107 -7.206617992371321 -6.6211384471641495
108 -5.603591658174992 -6.494555224953677
109 -3.7828810960054398 -6.117354180737655
110 -5.755553647875786 -6.087055095509873
111 -9.049609780311584 -5.43500831968483
112 -0.9300979804247618 -5.105291741614599
113 0.43880467861890793 -4.628649413275992
114 2.4405886456370354 -4.471031187897325
115 -6.553541027009487 -4.455504779070034
116 -1.3547404315322638 -4.2805498188182405
117 -1.049306276254356 -3.7944735717969627
118 2.4756062664091587 -2.9512438456190186
119 2.1155217327177525 -2.541618164765197
train accuracy: 0.9866666666666667
validation accuracy: 0.96

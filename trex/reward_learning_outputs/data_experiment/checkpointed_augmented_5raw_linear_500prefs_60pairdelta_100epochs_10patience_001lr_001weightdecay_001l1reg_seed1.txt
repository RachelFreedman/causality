Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.027020018309412988, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0408,  0.0075,  0.0168, -0.0390,  0.1551, -0.5651]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.01444677185401531, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6653e-02, -3.3221e-05,  1.2946e-02, -4.1216e-02,  1.7767e-01,
         -7.1668e-01]], device='cuda:0'))])
end of epoch 2: val_loss 0.011239997020094136, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1626e-02,  1.4705e-02, -6.6562e-04, -3.3673e-02,  1.8693e-01,
         -8.1194e-01]], device='cuda:0'))])
end of epoch 3: val_loss 0.0243310735295357, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.009922815970821546, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0468,  0.0082,  0.0149, -0.0143,  0.1726, -0.9987]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.015069032456634943, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.013000055744921895, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 0.00562590087140336, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0131,  0.0274,  0.0091, -0.0204,  0.2334, -1.1025]],
       device='cuda:0'))])
end of epoch 8: val_loss 0.05942531898311842, val_acc 0.98
trigger times: 1
end of epoch 9: val_loss 0.0030539650361338035, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0579,  0.0064,  0.0210, -0.0416,  0.4065, -1.4248]],
       device='cuda:0'))])
end of epoch 10: val_loss 0.0032787538791934877, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 0.025631392146292314, val_acc 0.98
trigger times: 2
end of epoch 12: val_loss 0.22823921237865263, val_acc 0.94
trigger times: 3
end of epoch 13: val_loss 0.005724872676920114, val_acc 1.0
trigger times: 4
end of epoch 14: val_loss 0.05196839050110711, val_acc 0.96
trigger times: 5
end of epoch 15: val_loss 0.011358318940582706, val_acc 1.0
trigger times: 6
end of epoch 16: val_loss 0.0752981534001195, val_acc 0.98
trigger times: 7
end of epoch 17: val_loss 0.00584603985553457, val_acc 1.0
trigger times: 8
end of epoch 18: val_loss 0.0056843586059989095, val_acc 1.0
trigger times: 9
end of epoch 19: val_loss 0.06385806280256474, val_acc 0.96
trigger times: 10
Early stopping.
0 -34.67967739701271 -21.698217245906225
1 -22.94593968987465 -19.949715584796184
2 -12.717547416687012 -18.768323469697297
3 -19.521564096212387 -18.758804345874903
4 -16.14191633462906 -17.538967424246174
5 -14.632021188735962 -16.915411032748093
6 -20.320541247725487 -16.05718210808024
7 -15.820531651377678 -15.915815703668287
8 -8.507756426930428 -15.33402056686152
9 -12.9935064278543 -15.271252072853205
10 -6.1784892082214355 -15.208145749687734
11 -13.59387631714344 -14.050045254361477
12 -21.482522532343864 -13.75955413810145
13 -2.928588792681694 -13.396239081718472
14 1.6068132370710373 -12.467903050162427
15 -0.02866603434085846 -12.22796644484045
16 -3.521052699536085 -11.94379352386303
17 -7.117040681652725 -11.871155187939209
18 -2.2468416318297386 -10.822297454194436
19 3.61740180850029 -9.812197786666262
20 -21.165906071662903 -8.908673775575991
21 -16.73377615213394 -8.515160348153305
22 -21.93170338869095 -8.432900628058379
23 -11.371370255947113 -8.1574703320884
24 -12.469896495342255 -8.114623899357289
25 -12.952745348215103 -8.046310874369832
26 -13.912835851311684 -8.033434648498933
27 -12.46282148361206 -7.987663518560797
28 -20.120514512062073 -7.941312820022507
29 -11.178680464625359 -7.911792849843923
30 0.6355143934488297 -7.8403085472035166
31 -4.960206523537636 -7.6695346082072655
32 -20.28287309408188 -7.623631472187833
33 -22.112796634435654 -7.517092847713959
34 -14.269897386431694 -7.375773401152629
35 -5.410248219966888 -7.337722462601442
36 -14.418583378195763 -7.212595973698941
37 -16.42692133784294 -7.209372197564873
38 -10.739406017586589 -7.13670271909991
39 -4.750150218605995 -6.752736311974305
40 -7.725271046161652 -6.739358173065801
41 -11.421685948967934 -6.707093660958046
42 -4.018007066100836 -6.6943312910442465
43 -11.442045345902443 -6.689321861831577
44 -11.44123089313507 -6.686652102155316
45 -5.551749646663666 -6.6154467075651695
46 -2.8655930012464523 -6.524460542127663
47 -3.4720263620838523 -6.5047738194148765
48 -5.425118118524551 -6.437540250165604
49 -9.123996868729591 -6.430684532606243
50 -3.5581360161304474 -6.428909365437914
51 -4.518697664141655 -6.347921267036356
52 -1.6734751872718334 -6.268422498242901
53 -1.90885797329247 -6.103656113764034
54 0.10129141807556152 -6.102549838089866
55 -1.6115105599164963 -6.0542305827232905
56 -8.224868908524513 -6.021838489818482
57 -4.861719682812691 -5.960071728074796
58 -3.0963421394117177 -5.889090204198977
59 1.7984308004379272 -5.85687031698118
60 3.848089087754488 -5.832268177877353
61 -2.561444863677025 -5.758964645019952
62 -5.893957309424877 -5.725271574488492
63 -3.4661130867898464 -5.521965023222128
64 -2.0104392040520906 -5.482446099710484
65 0.048364728689193726 -5.402674139281961
66 0.30861303489655256 -5.37320826895953
67 0.34632085263729095 -5.327996155721218
68 -6.736572749912739 -5.319251163344653
69 0.123186856508255 -5.246628989103434
70 1.4024358373135328 -5.218349993849201
71 0.2209942489862442 -5.167763591366873
72 0.5874512083828449 -5.158536740652007
73 5.433491230010986 -5.15118490151008
74 1.0939589887857437 -5.087424824193218
75 8.089552715420723 -4.8945284164838885
76 6.948522441089153 -4.8012787804619315
77 2.496816039085388 -4.780550794817313
78 2.5024116709828377 -4.779959729642383
79 8.208408206701279 -4.684858983685201
80 10.335097417235374 -4.63376638301532
81 5.723211340606213 -4.544765918076566
82 0.16514167003333569 -4.541101762358445
83 8.09326958656311 -4.499705122960396
84 11.549404665827751 -4.46305722691507
85 5.057777985930443 -4.360082787577702
86 10.700378149747849 -4.347557790582064
87 6.765813626348972 -4.32717610371528
88 5.645570941269398 -4.293149032506209
89 8.762413784861565 -4.28328411059534
90 4.646436430513859 -4.261857985421295
91 12.189972534775734 -4.2556554048544175
92 2.411309067159891 -4.21892374827055
93 5.738801538944244 -4.212760778810066
94 11.686181396245956 -4.189471756120161
95 13.212453618645668 -4.077055383091453
96 4.868171393871307 -4.039226517389183
97 9.85136903822422 -4.022574133686165
98 0.4687473103404045 -4.000807316013546
99 9.906704366207123 -3.958259499152596
100 12.917167231440544 -3.9026108386832026
101 15.212879329919815 -3.898062481547635
102 13.845877051353455 -3.8736058235511592
103 13.596118479967117 -3.815576608227037
104 13.214760109782219 -3.7794012502260146
105 17.061201691627502 -3.7456394227913563
106 14.79664246737957 -3.58313093174521
107 16.792823016643524 -3.548328387478394
108 16.462836116552353 -3.4694240992184446
109 15.216833546757698 -3.467874494555462
110 17.135711908340454 -3.355536092072393
111 2.182895038276911 -3.2582378101026293
112 17.598499730229378 -3.2489250657416147
113 16.797627449035645 -3.2288126285346226
114 12.336222499608994 -3.1565816117673453
115 17.566755294799805 -3.098861944472417
116 5.341933764517307 -2.6651274851973605
117 6.700460582971573 -2.161582813943857
118 22.17834296822548 -1.9522217156632942
119 10.205594941973686 -1.8580213384140898
train accuracy: 0.9977777777777778
validation accuracy: 0.96

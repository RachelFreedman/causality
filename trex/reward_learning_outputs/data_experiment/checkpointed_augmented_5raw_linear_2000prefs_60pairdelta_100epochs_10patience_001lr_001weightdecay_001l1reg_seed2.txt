Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.004450576389089136, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0365, -0.0036,  0.0213, -0.0221,  0.1545, -1.0835]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.0076301689380157715, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.006613339356297736, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.0021499513987453867, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0434,  0.0083,  0.0257, -0.0228,  0.2284, -1.3779]],
       device='cuda:0'))])
end of epoch 4: val_loss 0.00674936570654598, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.03257392140044768, val_acc 0.99
trigger times: 2
end of epoch 6: val_loss 0.012268907114504657, val_acc 0.995
trigger times: 3
end of epoch 7: val_loss 0.03147749175137484, val_acc 0.985
trigger times: 4
end of epoch 8: val_loss 0.007561982684162309, val_acc 1.0
trigger times: 5
end of epoch 9: val_loss 0.009528075104657354, val_acc 0.995
trigger times: 6
end of epoch 10: val_loss 0.023234607969504745, val_acc 1.0
trigger times: 7
end of epoch 11: val_loss 0.003239247008835058, val_acc 1.0
trigger times: 8
end of epoch 12: val_loss 0.005771712580167332, val_acc 0.995
trigger times: 9
end of epoch 13: val_loss 0.010471860791043603, val_acc 0.99
trigger times: 10
Early stopping.
0 -19.275485426187515 -21.698217245906225
1 -17.03618198633194 -19.949715584796184
2 -9.305626228451729 -18.768323469697297
3 -15.358061820268631 -18.758804345874903
4 -10.848514005541801 -17.538967424246174
5 -11.188278660178185 -16.915411032748093
6 -14.12388151139021 -16.05718210808024
7 -10.136093467473984 -15.915815703668287
8 -7.405738145112991 -15.33402056686152
9 -9.706986989825964 -15.271252072853205
10 -4.143311470746994 -15.208145749687734
11 -10.530711717903614 -14.050045254361477
12 -14.60417465120554 -13.75955413810145
13 -1.553541325032711 -13.396239081718472
14 -2.637583903968334 -12.467903050162427
15 -3.0057139694690704 -12.22796644484045
16 -1.9865163192152977 -11.94379352386303
17 -3.663281463086605 -11.871155187939209
18 -2.4810489490628242 -10.822297454194436
19 2.1593582704663277 -9.812197786666262
20 -9.976098522543907 -8.908673775575991
21 -7.833213545382023 -8.515160348153305
22 -14.308589860796928 -8.432900628058379
23 -9.449107319116592 -8.1574703320884
24 -11.172617852687836 -8.114623899357289
25 -10.770786553621292 -8.046310874369832
26 -11.912313058972359 -8.033434648498933
27 -10.253243505954742 -7.987663518560797
28 -11.78549775481224 -7.941312820022507
29 -10.316401779651642 -7.911792849843923
30 0.6116044670343399 -7.8403085472035166
31 -7.578923679888248 -7.6695346082072655
32 -13.358274400234222 -7.623631472187833
33 -12.519212052226067 -7.517092847713959
34 -6.666957214474678 -7.375773401152629
35 -7.741527512669563 -7.337722462601442
36 -5.965423468500376 -7.212595973698941
37 -7.62911182641983 -7.209372197564873
38 -6.259942462667823 -7.13670271909991
39 -3.7767032496631145 -6.752736311974305
40 -6.601908579468727 -6.739358173065801
41 -4.0905223451554775 -6.707093660958046
42 -0.5458072684705257 -6.6943312910442465
43 -3.753864439204335 -6.689321861831577
44 -3.85056708753109 -6.686652102155316
45 -6.710150733590126 -6.6154467075651695
46 -5.250436250120401 -6.524460542127663
47 0.0544910803437233 -6.5047738194148765
48 -5.135788042098284 -6.437540250165604
49 -2.919834389089374 -6.430684532606243
50 -5.380783013999462 -6.428909365437914
51 -3.722268048673868 -6.347921267036356
52 1.0413259081542492 -6.268422498242901
53 0.4246945306658745 -6.103656113764034
54 0.2654929682612419 -6.102549838089866
55 -4.337266448885202 -6.0542305827232905
56 -2.3820003140717745 -6.021838489818482
57 -5.471503898501396 -5.960071728074796
58 0.5540321134030819 -5.889090204198977
59 1.75352131575346 -5.85687031698118
60 -2.098220329731703 -5.832268177877353
61 -1.6798014380037785 -5.758964645019952
62 -0.42942190170288086 -5.725271574488492
63 -2.935840342193842 -5.521965023222128
64 1.0332726128399372 -5.482446099710484
65 1.016207329928875 -5.402674139281961
66 -2.0156484725885093 -5.37320826895953
67 -0.8697908967733383 -5.327996155721218
68 -1.272693632170558 -5.319251163344653
69 -1.3155411928892136 -5.246628989103434
70 2.973718374967575 -5.218349993849201
71 -1.4318703263998032 -5.167763591366873
72 1.8031869642436504 -5.158536740652007
73 2.8815751895308495 -5.15118490151008
74 -1.063178725540638 -5.087424824193218
75 3.6134676188230515 -4.8945284164838885
76 -0.24130189651623368 -4.8012787804619315
77 2.1921044066548347 -4.780550794817313
78 0.5689058192074299 -4.779959729642383
79 2.075186936184764 -4.684858983685201
80 1.8728719120845199 -4.63376638301532
81 4.767054356634617 -4.544765918076566
82 1.524952169507742 -4.541101762358445
83 1.2920315731316805 -4.499705122960396
84 7.077996917068958 -4.46305722691507
85 2.8812417164444923 -4.360082787577702
86 6.793051950633526 -4.347557790582064
87 5.147826500236988 -4.32717610371528
88 4.5318333357572556 -4.293149032506209
89 6.17686040699482 -4.28328411059534
90 3.9600398018956184 -4.261857985421295
91 7.021448999643326 -4.2556554048544175
92 2.743638314306736 -4.21892374827055
93 4.133341088891029 -4.212760778810066
94 6.785444177687168 -4.189471756120161
95 3.673713542521 -4.077055383091453
96 4.101437158882618 -4.039226517389183
97 6.862313702702522 -4.022574133686165
98 2.7038142904639244 -4.000807316013546
99 3.344120427966118 -3.958259499152596
100 7.981239527463913 -3.9026108386832026
101 8.268799558281898 -3.898062481547635
102 7.360297575592995 -3.8736058235511592
103 8.410021275281906 -3.815576608227037
104 8.29530506581068 -3.7794012502260146
105 6.3213688507676125 -3.7456394227913563
106 6.690871320664883 -3.58313093174521
107 7.460824623703957 -3.548328387478394
108 6.8446497321128845 -3.4694240992184446
109 8.168977484107018 -3.467874494555462
110 9.122260481119156 -3.355536092072393
111 3.1032409705221653 -3.2582378101026293
112 8.219112537801266 -3.2489250657416147
113 9.0984413549304 -3.2288126285346226
114 7.892799898982048 -3.1565816117673453
115 10.282252095639706 -3.098861944472417
116 5.3011429607868195 -2.6651274851973605
117 6.102217607200146 -2.161582813943857
118 11.688962638378143 -1.9522217156632942
119 7.807957544922829 -1.8580213384140898
train accuracy: 0.9977777777777778
validation accuracy: 0.99

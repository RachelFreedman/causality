demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.02495298577239737, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0053,  0.0134,  0.0271,  0.0026, -0.1179, -0.8171]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.0154534237552798, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0032, -0.0081,  0.0338, -0.0014, -0.1621, -1.0130]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.008249381112473203, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.3810e-04, -8.7383e-03,  1.6918e-02, -8.1927e-03, -1.6370e-01,
         -1.1694e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.009488128070388483, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.006678879657106051, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0313, -0.0315,  0.0634, -0.0234, -0.4060, -1.7155]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.003159959080864496, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.8562e-04, -4.6240e-04,  3.3729e-02, -9.3153e-03, -3.2783e-01,
         -1.5697e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.013854018237948225, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0032913099688124704, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.006716013349430341, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.07743284066437496, val_acc 0.96
trigger times: 4
end of epoch 10: val_loss 0.0033718666892762882, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 0.005383406533278503, val_acc 1.0
trigger times: 6
end of epoch 12: val_loss 0.005415151362502684, val_acc 1.0
trigger times: 7
end of epoch 13: val_loss 0.034636483446213194, val_acc 0.98
trigger times: 8
end of epoch 14: val_loss 0.08187479481603205, val_acc 0.98
trigger times: 9
end of epoch 15: val_loss 0.0013810077838145674, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0038,  0.0101,  0.0373, -0.0065, -0.4232, -1.9484]],
       device='cuda:0'))])
end of epoch 16: val_loss 0.008082459165503764, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.003550729832544306, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.0035450220130893227, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.004796935788463088, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.12636525975077276, val_acc 0.94
trigger times: 5
end of epoch 21: val_loss 0.0016242497119321798, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.00414893883239742, val_acc 1.0
trigger times: 7
end of epoch 23: val_loss 0.006284588418141084, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 0.023956049590353815, val_acc 0.98
trigger times: 9
end of epoch 25: val_loss 0.0021972688662833663, val_acc 1.0
trigger times: 10
Early stopping.
0 -23.711895778775215 -50.00222059884506
1 -19.610033854842186 -48.823739140882175
2 -20.30062524229288 -47.15605336419176
3 -21.33986347913742 -46.19619104961985
4 -23.76050505042076 -45.594227093057754
5 -24.982039093971252 -45.36842966452394
6 -26.72022780776024 -45.19068756322445
7 -19.249804243445396 -44.16490839583478
8 -15.685374148190022 -44.078303125872196
9 -25.15497460961342 -43.993555290419714
10 -13.506022915244102 -43.86306534422809
11 -23.936784390360117 -43.84874807044028
12 -20.31933604925871 -43.84199129025074
13 -13.550660878419876 -43.806385671938365
14 -12.443411216139793 -43.80581985978556
15 -14.329498291015625 -42.7594767358323
16 -14.938922695815563 -42.66316468983175
17 -20.763386711478233 -42.33177224591743
18 -18.225181832909584 -41.774963389485094
19 -18.99045357108116 -41.410068073767725
20 -19.63186612725258 -41.17786296442943
21 -18.456424206495285 -40.723520424948155
22 -22.007145896553993 -40.527189756101116
23 -22.76838418841362 -40.49595848244517
24 -21.701501443982124 -40.429389880911344
25 -18.621302112936974 -40.05653450521898
26 -19.78730893880129 -39.59232357792555
27 -28.819710105657578 -39.54162101198148
28 -21.28049138188362 -39.195237471709476
29 -21.07876767218113 -39.172389579378766
30 -18.82266851514578 -38.51095963496708
31 -16.417779711075127 -38.447265769744824
32 -21.5155185200274 -38.392107037026264
33 -12.577843625098467 -38.00745349944469
34 -12.713757276535034 -37.46482488602393
35 -18.107649825513363 -37.10988160586883
36 -20.126126155257225 -34.27116723637227
37 -11.118665793910623 -34.14139118114101
38 -10.722405418753624 -33.263072731706835
39 -17.036811348050833 -33.13344797200536
40 -14.85548734664917 -33.07825234291984
41 -16.21978299319744 -33.0321314765637
42 -23.12331486865878 -32.44934973065406
43 -14.40937414765358 -32.4007978120153
44 -14.802572913467884 -32.40063925734975
45 -7.793043367564678 -30.734403792103194
46 -10.52812235057354 -30.57151371770873
47 -14.316080711781979 -30.131236504472803
48 -14.489365354180336 -29.99326722619033
49 -12.268450323492289 -29.66908258985071
50 -18.497420262545347 -29.297233511513635
51 -12.478764072060585 -29.288890423975797
52 -8.995864700525999 -29.145878352769948
53 -10.591588927432895 -28.49601894351319
54 -11.826954536139965 -28.492023661124072
55 -18.174535043537617 -28.315961465855167
56 -10.094947904348373 -27.121110566589827
57 -6.791667714715004 -26.064532595535336
58 -12.00184964388609 -25.520524278341334
59 -12.62626807950437 -25.27101421179229
60 -9.983403019607067 -25.066644278800943
61 -13.096788130700588 -24.925849381327673
62 -13.153727613389492 -24.188105669766596
63 -11.16302427276969 -23.48479966198816
64 -3.2116280905902386 -23.153943559703283
65 -11.217337615787983 -22.954730295117237
66 -7.990214923396707 -22.74124885266394
67 -7.632188472896814 -22.739273544503753
68 -5.613457020372152 -22.264945050603636
69 -10.175914976745844 -22.15569724300287
70 -9.863421708345413 -21.055920928583344
71 -5.973323084414005 -20.543356562348553
72 -7.578944142907858 -20.33499633836848
73 -6.084080222994089 -20.18157658281111
74 -7.709839692339301 -19.58144410477429
75 -6.499796248972416 -19.377225745334304
76 -11.090077757835388 -19.243135617403095
77 -5.9755193665623665 -19.060620225371707
78 -10.773421250283718 -18.964124524696246
79 -10.178551521152258 -18.448962308005108
80 -8.849556285887957 -17.740722019993825
81 -5.462201360613108 -16.85889369985028
82 -6.059333963319659 -16.3381194095591
83 -4.263654038310051 -14.535892564189266
84 -6.750971086323261 -14.443670567499144
85 -4.923215638846159 -14.200413010108107
86 -9.318980492651463 -13.936976181618805
87 -8.556000992655754 -13.862253042167257
88 -6.275978775694966 -13.483098530680483
89 -5.832493677735329 -13.455892754889845
90 -5.331380281597376 -13.355868275096913
91 -4.362978555262089 -12.278515244993585
92 -3.213383696973324 -12.227387460046547
93 -4.644729241728783 -12.020717825467683
94 -7.037545837461948 -11.910094799877324
95 -7.219357509166002 -11.400284019256157
96 -3.248971000313759 -11.134618158086587
97 -6.475765768438578 -10.859166921158222
98 -4.314661752432585 -9.595137958067907
99 -5.340599354356527 -9.289921608799773
100 -7.636411989107728 -8.230877068641124
101 -4.991567995399237 -7.882363241796725
102 -5.967426590621471 -7.6478984168416355
103 -4.571436025202274 -7.459623237418707
104 -5.481300115585327 -7.124357312750265
105 -10.095508061349392 -7.05379065585803
106 -3.5256663486361504 -6.853091098326624
107 -4.086232233792543 -6.6211384471641495
108 -10.810232020914555 -6.494555224953677
109 -2.7177924290299416 -6.117354180737655
110 -8.864043399691582 -6.087055095509873
111 -5.988015992566943 -5.43500831968483
112 -3.9047893919050694 -5.105291741614599
113 -4.74440348893404 -4.628649413275992
114 -2.8975960575044155 -4.471031187897325
115 -3.2778102960437536 -4.455504779070034
116 -5.908928256481886 -4.2805498188182405
117 -5.948483616113663 -3.7944735717969627
118 -2.0205428823828697 -2.9512438456190186
119 -2.4994564428925514 -2.541618164765197
train accuracy: 0.9977777777777778
validation accuracy: 1.0

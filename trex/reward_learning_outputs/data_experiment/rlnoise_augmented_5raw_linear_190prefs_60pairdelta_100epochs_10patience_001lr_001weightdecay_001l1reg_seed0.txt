demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 171
num training_labels 171
num val_obs 19
num val_labels 19
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.020175359884313748, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0314, -0.0098,  0.0393, -0.0007, -0.1781, -0.5379]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.02469280836751415, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.014703449952848084, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.3285e-02, -2.2799e-02,  1.9877e-02,  3.4961e-04, -1.6847e-01,
         -7.5099e-01]], device='cuda:0'))])
end of epoch 3: val_loss 0.05389911180124105, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.012483253252005517, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0233, -0.0154,  0.0087, -0.0017, -0.1802, -0.8909]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.006996769542494899, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0216, -0.0040,  0.0211, -0.0049, -0.1750, -0.9378]],
       device='cuda:0'))])
end of epoch 6: val_loss 0.06881017930768812, val_acc 0.9473684210526315
trigger times: 1
end of epoch 7: val_loss 0.010807615231415543, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.004295366706736259, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0127, -0.0297,  0.0363,  0.0026, -0.2649, -1.1299]],
       device='cuda:0'))])
end of epoch 9: val_loss 0.0027499941858704197, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0298,  0.0021,  0.0324, -0.0099, -0.2307, -1.1495]],
       device='cuda:0'))])
end of epoch 10: val_loss 0.0016501124954551845, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0365,  0.0038,  0.0282,  0.0017, -0.2395, -1.1441]],
       device='cuda:0'))])
end of epoch 11: val_loss 0.0126812712715934, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.003301157226384308, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.003054829346255199, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.005345150652637561, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.0014988488363466732, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0421, -0.0098,  0.0328, -0.0050, -0.2157, -1.2481]],
       device='cuda:0'))])
end of epoch 16: val_loss 0.001926042042930775, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.05481696959705124, val_acc 0.9473684210526315
trigger times: 2
end of epoch 18: val_loss 0.005876137352479897, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0027351705760721465, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.004317728793083494, val_acc 1.0
trigger times: 5
end of epoch 21: val_loss 0.001608645580885973, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.10865134604260245, val_acc 0.9473684210526315
trigger times: 7
end of epoch 23: val_loss 0.002937012940429596, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 2.986796811056461e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0444,  0.0025,  0.0239,  0.0348, -0.2822, -2.0463]],
       device='cuda:0'))])
end of epoch 25: val_loss 0.00022420133513858636, val_acc 1.0
trigger times: 1
end of epoch 26: val_loss 0.0004906076301447109, val_acc 1.0
trigger times: 2
end of epoch 27: val_loss 0.0007288407857805922, val_acc 1.0
trigger times: 3
end of epoch 28: val_loss 0.03768260273093466, val_acc 1.0
trigger times: 4
end of epoch 29: val_loss 0.001297619492184391, val_acc 1.0
trigger times: 5
end of epoch 30: val_loss 0.002337340703894896, val_acc 1.0
trigger times: 6
end of epoch 31: val_loss 0.00168109935767509, val_acc 1.0
trigger times: 7
end of epoch 32: val_loss 0.0040252679688672285, val_acc 1.0
trigger times: 8
end of epoch 33: val_loss 0.011367701655734962, val_acc 1.0
trigger times: 9
end of epoch 34: val_loss 0.0020587305500223345, val_acc 1.0
trigger times: 10
Early stopping.
0 -16.694087404757738 -50.00222059884506
1 -12.60061763972044 -48.823739140882175
2 -12.832270307466388 -47.15605336419176
3 -14.079742163419724 -46.19619104961985
4 -15.648711293935776 -45.594227093057754
5 -16.700950026512146 -45.36842966452394
6 -17.57998216152191 -45.19068756322445
7 -13.791763804852962 -44.16490839583478
8 -9.612689465284348 -44.078303125872196
9 -16.532878205180168 -43.993555290419714
10 -9.460202690213919 -43.86306534422809
11 -15.706514345481992 -43.84874807044028
12 -15.248549945652485 -43.84199129025074
13 -10.265259308740497 -43.806385671938365
14 -8.454999780282378 -43.80581985978556
15 -10.428412679582834 -42.7594767358323
16 -10.511226039379835 -42.66316468983175
17 -13.197400227189064 -42.33177224591743
18 -14.195564061403275 -41.774963389485094
19 -12.768321938812733 -41.410068073767725
20 -14.555838629603386 -41.17786296442943
21 -12.178793217986822 -40.723520424948155
22 -13.770296052098274 -40.527189756101116
23 -13.932496190071106 -40.49595848244517
24 -14.074386730790138 -40.429389880911344
25 -12.284952573478222 -40.05653450521898
26 -15.182269401848316 -39.59232357792555
27 -19.054519206285477 -39.54162101198148
28 -15.193458795547485 -39.195237471709476
29 -15.079262435436249 -39.172389579378766
30 -12.568272069096565 -38.51095963496708
31 -11.400533879175782 -38.447265769744824
32 -15.05529192276299 -38.392107037026264
33 -8.802570380270481 -38.00745349944469
34 -9.528410650789738 -37.46482488602393
35 -13.157449062913656 -37.10988160586883
36 -13.044302865862846 -34.27116723637227
37 -8.128293614834547 -34.14139118114101
38 -7.3757060803473 -33.263072731706835
39 -12.088917031884193 -33.13344797200536
40 -9.397476986050606 -33.07825234291984
41 -12.226000100374222 -33.0321314765637
42 -14.193430403247476 -32.44934973065406
43 -6.877094604074955 -32.4007978120153
44 -11.018336549401283 -32.40063925734975
45 -7.643756404519081 -30.734403792103194
46 -6.40109452418983 -30.57151371770873
47 -10.249159533530474 -30.131236504472803
48 -9.813152059912682 -29.99326722619033
49 -7.882862608879805 -29.66908258985071
50 -12.951771435327828 -29.297233511513635
51 -7.840184939093888 -29.288890423975797
52 -6.638042584061623 -29.145878352769948
53 -7.590709559619427 -28.49601894351319
54 -8.955668590962887 -28.492023661124072
55 -11.497537358663976 -28.315961465855167
56 -7.367065135389566 -27.121110566589827
57 -7.2243048176169395 -26.064532595535336
58 -6.550778245553374 -25.520524278341334
59 -8.035897289868444 -25.27101421179229
60 -7.032902844250202 -25.066644278800943
61 -7.059760129079223 -24.925849381327673
62 -8.580574337393045 -24.188105669766596
63 -7.6273703230544925 -23.48479966198816
64 -4.670637784525752 -23.153943559703283
65 -6.744641174562275 -22.954730295117237
66 -4.584014363586903 -22.74124885266394
67 -5.560989574529231 -22.739273544503753
68 -5.723701976239681 -22.264945050603636
69 -7.082403102889657 -22.15569724300287
70 -7.675338737666607 -21.055920928583344
71 -5.998164556920528 -20.543356562348553
72 -5.649915315210819 -20.33499633836848
73 -5.735524388961494 -20.18157658281111
74 -5.569914649706334 -19.58144410477429
75 -6.416600967291743 -19.377225745334304
76 -6.875366657972336 -19.243135617403095
77 -6.510750096291304 -19.060620225371707
78 -7.6437797248363495 -18.964124524696246
79 -5.4039224330335855 -18.448962308005108
80 -5.825185833498836 -17.740722019993825
81 -5.1597418412566185 -16.85889369985028
82 -4.61396336555481 -16.3381194095591
83 -4.310211173899006 -14.535892564189266
84 -6.623031936585903 -14.443670567499144
85 -5.733333999291062 -14.200413010108107
86 -4.6684636902064085 -13.936976181618805
87 -6.252908378839493 -13.862253042167257
88 -4.550337317399681 -13.483098530680483
89 -5.609801521524787 -13.455892754889845
90 -4.684059113264084 -13.355868275096913
91 -3.456242193467915 -12.278515244993585
92 -4.602841999381781 -12.227387460046547
93 -5.5984384045004845 -12.020717825467683
94 -4.844676245003939 -11.910094799877324
95 -4.238635363988578 -11.400284019256157
96 -3.6211694828234613 -11.134618158086587
97 -4.942301440984011 -10.859166921158222
98 -4.3277130499482155 -9.595137958067907
99 -5.303933659568429 -9.289921608799773
100 -5.828990340232849 -8.230877068641124
101 -4.524867321830243 -7.882363241796725
102 -4.2966529205441475 -7.6478984168416355
103 -4.4184575490653515 -7.459623237418707
104 -3.680150216445327 -7.124357312750265
105 -6.402407068759203 -7.05379065585803
106 -3.5248329383321106 -6.853091098326624
107 -2.93252638168633 -6.6211384471641495
108 -6.678072601556778 -6.494555224953677
109 -3.6852343131322414 -6.117354180737655
110 -4.644895110279322 -6.087055095509873
111 -2.9223665045574307 -5.43500831968483
112 -3.4087263327091932 -5.105291741614599
113 -5.649239595979452 -4.628649413275992
114 -4.4871965274214745 -4.471031187897325
115 -2.38869197294116 -4.455504779070034
116 -4.225799849256873 -4.2805498188182405
117 -4.654079344123602 -3.7944735717969627
118 -4.2645278461277485 -2.9512438456190186
119 -4.297633145004511 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

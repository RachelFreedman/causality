Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.005819916751311069, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0765, -0.0034,  0.0263, -0.0285,  0.1769, -0.6835]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.0049254144782156575, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0164,  0.0064,  0.0121, -0.0312,  0.1895, -0.8094]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.0015823408813365348, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0670,  0.0029,  0.0120, -0.0153,  0.1954, -0.9449]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.0011863282969623867, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0380,  0.0367,  0.0141, -0.0336,  0.1689, -1.0119]],
       device='cuda:0'))])
end of epoch 4: val_loss 0.0017004544812138534, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.00129732562464298, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 0.0005854002637350674, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0631,  0.0260,  0.0281, -0.0140,  0.1536, -1.1422]],
       device='cuda:0'))])
end of epoch 7: val_loss 0.0029387493144081133, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.00025247124830301003, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0088,  0.0223,  0.0319, -0.0317,  0.2690, -1.4704]],
       device='cuda:0'))])
end of epoch 9: val_loss 0.0009580361051052933, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.05613383888941669, val_acc 0.98
trigger times: 2
end of epoch 11: val_loss 5.187783397985868e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0642,  0.0493,  0.0196, -0.0567,  0.4412, -1.6164]],
       device='cuda:0'))])
end of epoch 12: val_loss 0.0004362108870584791, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 0.0018038447268666857, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 7.810833603855372e-05, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 0.0015829573102541872, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 0.003042842587154553, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 0.00011958831995457331, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 0.0007372579838397542, val_acc 1.0
trigger times: 7
end of epoch 19: val_loss 0.009316004323832203, val_acc 1.0
trigger times: 8
end of epoch 20: val_loss 0.001139931208590923, val_acc 1.0
trigger times: 9
end of epoch 21: val_loss 0.0006264710356393266, val_acc 1.0
trigger times: 10
Early stopping.
0 -21.344248861074448 -21.698217245906225
1 -20.448987901210785 -19.949715584796184
2 -15.827645719051361 -18.768323469697297
3 -18.803236320614815 -18.758804345874903
4 -14.733047008514404 -17.538967424246174
5 -15.551412671804428 -16.915411032748093
6 -14.028847344219685 -16.05718210808024
7 -11.437580631114542 -15.915815703668287
8 -12.396472223103046 -15.33402056686152
9 -11.846039524301887 -15.271252072853205
10 -10.749285608530045 -15.208145749687734
11 -11.661006672307849 -14.050045254361477
12 -14.309010200202465 -13.75955413810145
13 -7.982388466596603 -13.396239081718472
14 -8.599582836031914 -12.467903050162427
15 -7.903538927435875 -12.22796644484045
16 -8.135826669633389 -11.94379352386303
17 -7.27292531169951 -11.871155187939209
18 -6.609128348529339 -10.822297454194436
19 -3.9669316485524178 -9.812197786666262
20 -9.982662685215473 -8.908673775575991
21 -8.548281162977219 -8.515160348153305
22 -9.35838496685028 -8.432900628058379
23 -8.495438680052757 -8.1574703320884
24 -8.440451201051474 -8.114623899357289
25 -9.56575895473361 -8.046310874369832
26 -8.716875530779362 -8.033434648498933
27 -8.215308472514153 -7.987663518560797
28 -7.533168368041515 -7.941312820022507
29 -8.10453598946333 -7.911792849843923
30 -6.633687429130077 -7.8403085472035166
31 -7.335318602621555 -7.6695346082072655
32 -8.43687280267477 -7.623631472187833
33 -7.627493299543858 -7.517092847713959
34 -7.476733859628439 -7.375773401152629
35 -7.0923764407634735 -7.337722462601442
36 -7.351306457072496 -7.212595973698941
37 -7.7602563463151455 -7.209372197564873
38 -5.723530240356922 -7.13670271909991
39 -6.260383348912001 -6.752736311974305
40 -6.927560172975063 -6.739358173065801
41 -5.923946030437946 -6.707093660958046
42 -5.926243677735329 -6.6943312910442465
43 -5.518196906894445 -6.689321861831577
44 -5.972061190754175 -6.686652102155316
45 -6.560738742351532 -6.6154467075651695
46 -6.254968527704477 -6.524460542127663
47 -4.785234831273556 -6.5047738194148765
48 -5.9106706120073795 -6.437540250165604
49 -6.129297187551856 -6.430684532606243
50 -6.201051823794842 -6.428909365437914
51 -5.879236176609993 -6.347921267036356
52 -4.578956712037325 -6.268422498242901
53 -4.421295125037432 -6.103656113764034
54 -4.573031820356846 -6.102549838089866
55 -5.202418684959412 -6.0542305827232905
56 -4.939520571380854 -6.021838489818482
57 -5.621973350644112 -5.960071728074796
58 -4.496040532365441 -5.889090204198977
59 -3.5993131287395954 -5.85687031698118
60 -4.730802729725838 -5.832268177877353
61 -4.932509519159794 -5.758964645019952
62 -4.05852884426713 -5.725271574488492
63 -5.192677985876799 -5.521965023222128
64 -3.6669434094801545 -5.482446099710484
65 -3.374880526214838 -5.402674139281961
66 -3.598122764378786 -5.37320826895953
67 -3.8215517215430737 -5.327996155721218
68 -4.269749725237489 -5.319251163344653
69 -3.619932965375483 -5.246628989103434
70 -3.0330372639000416 -5.218349993849201
71 -3.7346974527463317 -5.167763591366873
72 -2.86127370595932 -5.158536740652007
73 -2.702888533473015 -5.15118490151008
74 -3.666361801326275 -5.087424824193218
75 -2.228849310427904 -4.8945284164838885
76 -3.0762951923534274 -4.8012787804619315
77 -2.219611255452037 -4.780550794817313
78 -2.8072937335819006 -4.779959729642383
79 -2.4724137391895056 -4.684858983685201
80 -2.183454346610233 -4.63376638301532
81 -1.8600788861513138 -4.544765918076566
82 -2.553864110261202 -4.541101762358445
83 -2.315479644574225 -4.499705122960396
84 -1.165854474529624 -4.46305722691507
85 -1.3949242560192943 -4.360082787577702
86 -0.9343064706772566 -4.347557790582064
87 -0.6460159905254841 -4.32717610371528
88 -1.1754777524620295 -4.293149032506209
89 -1.2107332982122898 -4.28328411059534
90 -1.4160007759928703 -4.261857985421295
91 -1.0483766049146652 -4.2556554048544175
92 -1.8161590863019228 -4.21892374827055
93 -0.8433592189103365 -4.212760778810066
94 -0.8962336480617523 -4.189471756120161
95 -1.2270871847867966 -4.077055383091453
96 -1.1805362068116665 -4.039226517389183
97 -0.5321332421153784 -4.022574133686165
98 -1.6313344221562147 -4.000807316013546
99 -1.0247449912130833 -3.958259499152596
100 -0.10685496591031551 -3.9026108386832026
101 0.003797566518187523 -3.898062481547635
102 -0.1735076904296875 -3.8736058235511592
103 0.2680818550288677 -3.815576608227037
104 0.16028079576790333 -3.7794012502260146
105 -0.3131611282005906 -3.7456394227913563
106 0.10877064615488052 -3.58313093174521
107 0.03827498946338892 -3.548328387478394
108 -0.062216130550950766 -3.4694240992184446
109 -0.003043215721845627 -3.467874494555462
110 0.5266457982361317 -3.355536092072393
111 -0.22526882402598858 -3.2582378101026293
112 0.7788387853652239 -3.2489250657416147
113 0.67752530798316 -3.2288126285346226
114 0.4723751191049814 -3.1565816117673453
115 1.6456742957234383 -3.098861944472417
116 1.0009903088212013 -2.6651274851973605
117 1.860420823097229 -2.161582813943857
118 2.8687911927700043 -1.9522217156632942
119 2.715753383934498 -1.8580213384140898
train accuracy: 1.0
validation accuracy: 1.0

Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 171
num training_labels 171
num val_obs 19
num val_labels 19
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.23129736041539523, val_acc 0.8947368421052632
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1424, -0.0054,  0.0712, -0.0701,  0.2113, -0.4173]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.13422086502953934, val_acc 0.9473684210526315
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.1309e-02,  1.1368e-02, -5.2626e-04, -3.6591e-02,  2.3788e-01,
         -5.7193e-01]], device='cuda:0'))])
end of epoch 2: val_loss 0.015710903890545438, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0130, -0.0274,  0.0258, -0.0181,  0.2473, -0.6682]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.03864777011078427, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.016390009519861786, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.0055519402034747145, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0218,  0.0014,  0.0199, -0.0146,  0.2682, -0.8642]],
       device='cuda:0'))])
end of epoch 6: val_loss 0.006583721306768845, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0061451368418943275, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.009526641738810775, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.0035253317620808837, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0421,  0.0218,  0.0093,  0.0038,  0.2629, -0.9647]],
       device='cuda:0'))])
end of epoch 10: val_loss 0.0027077223002297942, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0463, -0.0017,  0.0028, -0.0031,  0.2708, -0.9990]],
       device='cuda:0'))])
end of epoch 11: val_loss 0.004190121492752266, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.001550626045370058, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0466,  0.0027,  0.0266, -0.0088,  0.2920, -1.0874]],
       device='cuda:0'))])
end of epoch 13: val_loss 0.004036001551837312, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 0.0032502442103556086, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 0.003087484466408491, val_acc 1.0
trigger times: 3
end of epoch 16: val_loss 0.2794210156440721, val_acc 0.8947368421052632
trigger times: 4
end of epoch 17: val_loss 0.00010159949849103693, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.2366,  0.0559,  0.1568, -0.0385,  0.6813, -1.6456]],
       device='cuda:0'))])
end of epoch 18: val_loss 0.003523070432012937, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.00017810628129404606, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 0.00018692241872891575, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 0.0002881938458726477, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 0.0008713665944466548, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 0.0007563539770638266, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.0016521906564285498, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.0013633668715797853, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 0.02570252548708209, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 0.00019715009037624717, val_acc 1.0
trigger times: 10
Early stopping.
0 -28.42606270313263 -21.698217245906225
1 -23.883016914129257 -19.949715584796184
2 -16.991221398115158 -18.768323469697297
3 -23.9608174264431 -18.758804345874903
4 -14.331461444497108 -17.538967424246174
5 -17.684326469898224 -16.915411032748093
6 -17.279417004436255 -16.05718210808024
7 -13.16691379621625 -15.915815703668287
8 -8.890099093317986 -15.33402056686152
9 -11.901361647993326 -15.271252072853205
10 -10.089295700192451 -15.208145749687734
11 -12.262381885200739 -14.050045254361477
12 -19.91395051777363 -13.75955413810145
13 -4.9596047177910805 -13.396239081718472
14 -6.472289033234119 -12.467903050162427
15 -6.231513187289238 -12.22796644484045
16 -6.458817437291145 -11.94379352386303
17 -8.11942944675684 -11.871155187939209
18 -3.7209028154611588 -10.822297454194436
19 -0.6296974197030067 -9.812197786666262
20 -15.79443372786045 -8.908673775575991
21 -13.508107542991638 -8.515160348153305
22 -16.983161449432373 -8.432900628058379
23 -12.252316325902939 -8.1574703320884
24 -14.549631625413895 -8.114623899357289
25 -14.41332297027111 -8.046310874369832
26 -15.200514391064644 -8.033434648498933
27 -12.497756496071815 -7.987663518560797
28 -14.969125911593437 -7.941312820022507
29 -13.33499290049076 -7.911792849843923
30 -3.9044025652110577 -7.8403085472035166
31 -10.904846861958504 -7.6695346082072655
32 -16.740294620394707 -7.623631472187833
33 -13.567513853311539 -7.517092847713959
34 -12.156700491905212 -7.375773401152629
35 -10.887095004320145 -7.337722462601442
36 -11.151912584900856 -7.212595973698941
37 -12.784827500581741 -7.209372197564873
38 -6.2780739683657885 -7.13670271909991
39 -4.83179971948266 -6.752736311974305
40 -8.47825938463211 -6.739358173065801
41 -8.74703486263752 -6.707093660958046
42 -4.995988937094808 -6.6943312910442465
43 -8.032364293932915 -6.689321861831577
44 -8.365474537014961 -6.686652102155316
45 -9.214033521711826 -6.6154467075651695
46 -8.180970169603825 -6.524460542127663
47 -4.652727097272873 -6.5047738194148765
48 -7.354017037898302 -6.437540250165604
49 -7.514546126127243 -6.430684532606243
50 -7.8246535286307335 -6.428909365437914
51 -4.277086440473795 -6.347921267036356
52 -2.5462967567145824 -6.268422498242901
53 -3.9647647999227047 -6.103656113764034
54 -2.2590304762125015 -6.102549838089866
55 -6.93456144630909 -6.0542305827232905
56 -7.0131455063819885 -6.021838489818482
57 -7.5699620842933655 -5.960071728074796
58 -3.633593510836363 -5.889090204198977
59 -0.5700352266430855 -5.85687031698118
60 -3.226401187479496 -5.832268177877353
61 -3.077713944017887 -5.758964645019952
62 -2.3782990612089634 -5.725271574488492
63 -3.952774975448847 -5.521965023222128
64 -2.9587762746959925 -5.482446099710484
65 -0.8525671549141407 -5.402674139281961
66 -1.5373406242579222 -5.37320826895953
67 -3.0661868155002594 -5.327996155721218
68 -5.356841906905174 -5.319251163344653
69 -3.385866567492485 -5.246628989103434
70 -0.35448870062828064 -5.218349993849201
71 -3.5479234606027603 -5.167763591366873
72 -2.3477525478228927 -5.158536740652007
73 0.8442215397953987 -5.15118490151008
74 -3.1961848363280296 -5.087424824193218
75 1.6821944117546082 -4.8945284164838885
76 -0.7838410283438861 -4.8012787804619315
77 0.7572112753987312 -4.780550794817313
78 -1.3312007039785385 -4.779959729642383
79 1.8110044077038765 -4.684858983685201
80 1.3507324447855353 -4.63376638301532
81 1.1911291778087616 -4.544765918076566
82 -2.2342177787795663 -4.541101762358445
83 1.4181799553334713 -4.499705122960396
84 4.824505366384983 -4.46305722691507
85 1.4322834759950638 -4.360082787577702
86 4.10900280252099 -4.347557790582064
87 1.99910331889987 -4.32717610371528
88 1.175901498645544 -4.293149032506209
89 3.446304362267256 -4.28328411059534
90 0.5629501454532146 -4.261857985421295
91 4.873192343860865 -4.2556554048544175
92 -0.897340090945363 -4.21892374827055
93 3.0169211737811565 -4.212760778810066
94 4.778927896171808 -4.189471756120161
95 3.396930657327175 -4.077055383091453
96 2.837761837989092 -4.039226517389183
97 4.043060652911663 -4.022574133686165
98 1.5464356802403927 -4.000807316013546
99 1.9827184826135635 -3.958259499152596
100 5.5154686495661736 -3.9026108386832026
101 6.497555181384087 -3.898062481547635
102 5.863490395247936 -3.8736058235511592
103 6.117006093263626 -3.815576608227037
104 6.282727051526308 -3.7794012502260146
105 6.697235591709614 -3.7456394227913563
106 5.63235979527235 -3.58313093174521
107 7.564163967967033 -3.548328387478394
108 7.090473867952824 -3.4694240992184446
109 6.695799998939037 -3.467874494555462
110 7.692214947193861 -3.355536092072393
111 2.406474992632866 -3.2582378101026293
112 7.336028553545475 -3.2489250657416147
113 7.770228918641806 -3.2288126285346226
114 5.958689663559198 -3.1565816117673453
115 8.65054303407669 -3.098861944472417
116 5.209233596920967 -2.6651274851973605
117 6.433687970042229 -2.161582813943857
118 11.172946259379387 -1.9522217156632942
119 8.282638624310493 -1.8580213384140898
train accuracy: 1.0
validation accuracy: 1.0

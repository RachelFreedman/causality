demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.06274038876901614, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0025,  0.0059,  0.0202,  0.0014, -0.1255, -0.7305]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.04124030019737802, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0020,  0.0016,  0.0251,  0.0133, -0.0943, -0.9987]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.0404136173421, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0256,  0.0032,  0.0254,  0.0131, -0.1392, -1.1405]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.03871920871901509, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.4305e-02,  2.5443e-02,  2.2155e-02,  2.9316e-04, -2.2920e-01,
         -1.2574e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.026676411167976397, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0074, -0.0420, -0.0016, -0.0052, -0.1832, -1.3444]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.007475895279685574, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0123, -0.0053,  0.0196, -0.0035, -0.2699, -1.5293]],
       device='cuda:0'))])
end of epoch 6: val_loss 0.016686485084240844, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.01458404267988115, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.008143591621719537, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.019409012618226883, val_acc 1.0
trigger times: 4
end of epoch 10: val_loss 0.0827034070752812, val_acc 0.94
trigger times: 5
end of epoch 11: val_loss 0.03332684151379681, val_acc 0.98
trigger times: 6
end of epoch 12: val_loss 0.8490720029616966, val_acc 0.86
trigger times: 7
end of epoch 13: val_loss 0.04189130280162047, val_acc 0.98
trigger times: 8
end of epoch 14: val_loss 0.04189310723601409, val_acc 0.98
trigger times: 9
end of epoch 15: val_loss 0.025241860360728337, val_acc 0.98
trigger times: 10
Early stopping.
0 -24.945956133306026 -50.00222059884506
1 -19.919521123170853 -48.823739140882175
2 -20.542930522933602 -47.15605336419176
3 -21.19697594642639 -46.19619104961985
4 -26.33001273870468 -45.594227093057754
5 -24.442664176225662 -45.36842966452394
6 -28.92142364382744 -45.19068756322445
7 -20.557455375790596 -44.16490839583478
8 -16.37801208347082 -44.078303125872196
9 -25.863470908254385 -43.993555290419714
10 -15.624902185052633 -43.86306534422809
11 -23.637028593569994 -43.84874807044028
12 -20.36458346247673 -43.84199129025074
13 -14.589106135070324 -43.806385671938365
14 -14.099955199286342 -43.80581985978556
15 -15.533968403935432 -42.7594767358323
16 -14.766717407852411 -42.66316468983175
17 -21.34759210050106 -42.33177224591743
18 -19.19458371400833 -41.774963389485094
19 -19.837424367666245 -41.410068073767725
20 -19.521921411156654 -41.17786296442943
21 -18.055644690990448 -40.723520424948155
22 -23.200956851243973 -40.527189756101116
23 -23.035945788025856 -40.49595848244517
24 -22.85474418103695 -40.429389880911344
25 -20.177267134189606 -40.05653450521898
26 -19.740267492830753 -39.59232357792555
27 -28.56657975912094 -39.54162101198148
28 -21.067260514944792 -39.195237471709476
29 -23.004461720585823 -39.172389579378766
30 -17.991851553320885 -38.51095963496708
31 -16.198942301794887 -38.447265769744824
32 -21.380617251619697 -38.392107037026264
33 -13.579136498272419 -38.00745349944469
34 -13.551015295088291 -37.46482488602393
35 -20.288347259163857 -37.10988160586883
36 -19.076561838388443 -34.27116723637227
37 -12.063014619983733 -34.14139118114101
38 -12.849540295312181 -33.263072731706835
39 -16.907048039138317 -33.13344797200536
40 -13.553427517414093 -33.07825234291984
41 -17.381317749619484 -33.0321314765637
42 -24.319071225821972 -32.44934973065406
43 -14.203565776348114 -32.4007978120153
44 -16.782199680805206 -32.40063925734975
45 -8.235722117125988 -30.734403792103194
46 -10.641701132059097 -30.57151371770873
47 -15.532267078757286 -30.131236504472803
48 -13.081950202584267 -29.99326722619033
49 -13.747508570551872 -29.66908258985071
50 -19.983040116727352 -29.297233511513635
51 -12.963427558541298 -29.288890423975797
52 -10.729560010135174 -29.145878352769948
53 -10.226719809696078 -28.49601894351319
54 -13.585485681891441 -28.492023661124072
55 -16.92074767500162 -28.315961465855167
56 -10.740161187946796 -27.121110566589827
57 -7.514537926763296 -26.064532595535336
58 -12.7773989289999 -25.520524278341334
59 -11.781641639769077 -25.27101421179229
60 -12.439309522509575 -25.066644278800943
61 -14.388100109994411 -24.925849381327673
62 -12.822403457015753 -24.188105669766596
63 -11.273939918726683 -23.48479966198816
64 -4.173695936799049 -23.153943559703283
65 -10.870598692446947 -22.954730295117237
66 -9.885513626039028 -22.74124885266394
67 -9.551438913680613 -22.739273544503753
68 -6.549735356122255 -22.264945050603636
69 -11.978251684457064 -22.15569724300287
70 -8.60415631532669 -21.055920928583344
71 -7.412023700773716 -20.543356562348553
72 -9.62691006809473 -20.33499633836848
73 -7.563017936423421 -20.18157658281111
74 -9.175426632165909 -19.58144410477429
75 -7.67507023923099 -19.377225745334304
76 -9.294567160308361 -19.243135617403095
77 -7.000164240598679 -19.060620225371707
78 -11.33910121768713 -18.964124524696246
79 -11.473588936030865 -18.448962308005108
80 -11.536377236247063 -17.740722019993825
81 -5.388440292328596 -16.85889369985028
82 -7.9528172593563795 -16.3381194095591
83 -5.962869929149747 -14.535892564189266
84 -7.875661663711071 -14.443670567499144
85 -5.853246949613094 -14.200413010108107
86 -10.353334594517946 -13.936976181618805
87 -6.81149772927165 -13.862253042167257
88 -7.921710779890418 -13.483098530680483
89 -7.37225066870451 -13.455892754889845
90 -6.680845644325018 -13.355868275096913
91 -6.722170751541853 -12.278515244993585
92 -4.006946809589863 -12.227387460046547
93 -5.260297987610102 -12.020717825467683
94 -9.484974779188633 -11.910094799877324
95 -6.889777820557356 -11.400284019256157
96 -5.059270301833749 -11.134618158086587
97 -5.458016637712717 -10.859166921158222
98 -4.78770231641829 -9.595137958067907
99 -6.977828670293093 -9.289921608799773
100 -7.248657017946243 -8.230877068641124
101 -6.750588761642575 -7.882363241796725
102 -8.341478323563933 -7.6478984168416355
103 -6.237325098365545 -7.459623237418707
104 -8.00613908842206 -7.124357312750265
105 -8.53805560618639 -7.05379065585803
106 -5.448782578110695 -6.853091098326624
107 -6.59822498075664 -6.6211384471641495
108 -9.44653644412756 -6.494555224953677
109 -4.315089639276266 -6.117354180737655
110 -8.84338603913784 -6.087055095509873
111 -7.402441825717688 -5.43500831968483
112 -5.8583983685821295 -5.105291741614599
113 -5.647182669490576 -4.628649413275992
114 -3.9356421194970608 -4.471031187897325
115 -4.593838511034846 -4.455504779070034
116 -5.908123597502708 -4.2805498188182405
117 -5.687959898263216 -3.7944735717969627
118 -2.3746385388076305 -2.9512438456190186
119 -2.621506441384554 -2.541618164765197
train accuracy: 0.9955555555555555
validation accuracy: 0.98

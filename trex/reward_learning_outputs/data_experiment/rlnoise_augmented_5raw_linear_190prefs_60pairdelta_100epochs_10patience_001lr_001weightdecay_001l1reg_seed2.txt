demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 171
num training_labels 171
num val_obs 19
num val_labels 19
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.06656354717224051, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0070, -0.0011,  0.0258, -0.0049, -0.0725, -0.5293]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.03764570561102837, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0092, -0.0028,  0.0189, -0.0172, -0.1221, -0.7243]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.032281059562688154, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0123,  0.0128,  0.0244,  0.0024, -0.1556, -0.8402]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.025334536688253712, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.0542e-04,  9.5912e-03,  3.5145e-02, -1.6010e-03, -1.6275e-01,
         -9.1593e-01]], device='cuda:0'))])
end of epoch 4: val_loss 0.019847295554714754, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0116,  0.0023,  0.0336, -0.0028, -0.1789, -0.9737]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.019090930519987045, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0028,  0.0081,  0.0380,  0.0012, -0.1778, -1.0265]],
       device='cuda:0'))])
end of epoch 6: val_loss 0.015951027513055077, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0145,  0.0047,  0.0359,  0.0019, -0.1899, -1.0621]],
       device='cuda:0'))])
end of epoch 7: val_loss 0.016106453827169867, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.012994764292479408, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0265, -0.0137,  0.0215, -0.0182, -0.1994, -1.1598]],
       device='cuda:0'))])
end of epoch 9: val_loss 0.0133013152355584, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.023632826004454967, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.012054249323945873, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0145,  0.0063,  0.0308, -0.0036, -0.2149, -1.1672]],
       device='cuda:0'))])
end of epoch 12: val_loss 0.019074208615967637, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 0.018681274276751106, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 0.0011180032154893943, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0097, -0.0049, -0.0207,  0.0278, -0.3792, -1.9720]],
       device='cuda:0'))])
end of epoch 15: val_loss 0.0018338540115986522, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 0.0025387913133541637, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.003981898027135637, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 0.0059333914928653315, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 0.006334263209043313, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 0.008050645353585807, val_acc 1.0
trigger times: 6
end of epoch 21: val_loss 0.009573371986107304, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 0.15454865168208667, val_acc 0.9473684210526315
trigger times: 8
end of epoch 23: val_loss 0.01018869820128437, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.008886209390622474, val_acc 1.0
trigger times: 10
Early stopping.
0 -18.1961238887161 -50.00222059884506
1 -14.576271042227745 -48.823739140882175
2 -16.127245657145977 -47.15605336419176
3 -16.977511301636696 -46.19619104961985
4 -17.937203601002693 -45.594227093057754
5 -19.564951226115227 -45.36842966452394
6 -21.06089162826538 -45.19068756322445
7 -15.401331655681133 -44.16490839583478
8 -11.680286772549152 -44.078303125872196
9 -20.35898133739829 -43.993555290419714
10 -11.038807094097137 -43.86306534422809
11 -18.798419000580907 -43.84874807044028
12 -15.985296584665775 -43.84199129025074
13 -11.457225680351257 -43.806385671938365
14 -10.021224610507488 -43.80581985978556
15 -11.304584112018347 -42.7594767358323
16 -11.56565409898758 -42.66316468983175
17 -16.947793036699295 -42.33177224591743
18 -14.030235379934311 -41.774963389485094
19 -14.680728048086166 -41.410068073767725
20 -15.752071861177683 -41.17786296442943
21 -15.334758646786213 -40.723520424948155
22 -17.678620368242264 -40.527189756101116
23 -18.12388415634632 -40.49595848244517
24 -16.68693107366562 -40.429389880911344
25 -14.158311553299427 -40.05653450521898
26 -15.827464770525694 -39.59232357792555
27 -22.83871102333069 -39.54162101198148
28 -16.381121769547462 -39.195237471709476
29 -17.739743679761887 -39.172389579378766
30 -14.551960371434689 -38.51095963496708
31 -13.02503811288625 -38.447265769744824
32 -16.775950611568987 -38.392107037026264
33 -9.49540838599205 -38.00745349944469
34 -10.183741815388203 -37.46482488602393
35 -14.251625638455153 -37.10988160586883
36 -15.522429883480072 -34.27116723637227
37 -8.697081761434674 -34.14139118114101
38 -8.890058296732605 -33.263072731706835
39 -13.507980214431882 -33.13344797200536
40 -11.015796050429344 -33.07825234291984
41 -13.013363599777222 -33.0321314765637
42 -18.410285107791424 -32.44934973065406
43 -11.639383628964424 -32.4007978120153
44 -11.282512031495571 -32.40063925734975
45 -6.515392228960991 -30.734403792103194
46 -7.913295358419418 -30.57151371770873
47 -11.32280783727765 -30.131236504472803
48 -11.339962862432003 -29.99326722619033
49 -9.553179915994406 -29.66908258985071
50 -14.749268028885126 -29.297233511513635
51 -10.389136679470539 -29.288890423975797
52 -7.170485727488995 -29.145878352769948
53 -8.123532840749249 -28.49601894351319
54 -9.12699130922556 -28.492023661124072
55 -13.878989588469267 -28.315961465855167
56 -7.775079974904656 -27.121110566589827
57 -5.370148252695799 -26.064532595535336
58 -9.867682971060276 -25.520524278341334
59 -10.17822203785181 -25.27101421179229
60 -7.914873540401459 -25.066644278800943
61 -10.48144637979567 -24.925849381327673
62 -9.777070224285126 -24.188105669766596
63 -9.170320235192776 -23.48479966198816
64 -2.5821826718747616 -23.153943559703283
65 -9.198247909545898 -22.954730295117237
66 -6.583151508122683 -22.74124885266394
67 -6.481357857584953 -22.739273544503753
68 -4.447712410241365 -22.264945050603636
69 -8.376436436548829 -22.15569724300287
70 -7.6555614694952965 -21.055920928583344
71 -4.557704865932465 -20.543356562348553
72 -5.847902458161116 -20.33499633836848
73 -5.189678365364671 -20.18157658281111
74 -6.482517471536994 -19.58144410477429
75 -4.995431264862418 -19.377225745334304
76 -9.307502448558807 -19.243135617403095
77 -4.681066542863846 -19.060620225371707
78 -8.480026312172413 -18.964124524696246
79 -8.178733272477984 -18.448962308005108
80 -7.379698291420937 -17.740722019993825
81 -4.07904339954257 -16.85889369985028
82 -4.728183964267373 -16.3381194095591
83 -3.756415233016014 -14.535892564189266
84 -5.536688610911369 -14.443670567499144
85 -3.6262219697237015 -14.200413010108107
86 -7.515903849154711 -13.936976181618805
87 -6.028196979314089 -13.862253042167257
88 -5.438223982229829 -13.483098530680483
89 -4.25951043330133 -13.455892754889845
90 -3.945716606453061 -13.355868275096913
91 -3.846452746540308 -12.278515244993585
92 -2.5016666799783707 -12.227387460046547
93 -3.713613461703062 -12.020717825467683
94 -5.474101096391678 -11.910094799877324
95 -4.8701009675860405 -11.400284019256157
96 -2.916686223819852 -11.134618158086587
97 -4.528791334480047 -10.859166921158222
98 -3.0902405474334955 -9.595137958067907
99 -3.895790183916688 -9.289921608799773
100 -5.963528196327388 -8.230877068641124
101 -4.234234686940908 -7.882363241796725
102 -5.124139055609703 -7.6478984168416355
103 -3.8623918760567904 -7.459623237418707
104 -4.77626296505332 -7.124357312750265
105 -6.834600113332272 -7.05379065585803
106 -3.1557961255311966 -6.853091098326624
107 -3.6473282612860203 -6.6211384471641495
108 -7.503910217434168 -6.494555224953677
109 -2.4709963351488113 -6.117354180737655
110 -6.182966608554125 -6.087055095509873
111 -5.158289844170213 -5.43500831968483
112 -2.979757010936737 -5.105291741614599
113 -3.7795103155076504 -4.628649413275992
114 -2.1122674141079187 -4.471031187897325
115 -3.2663940861821175 -4.455504779070034
116 -4.022684210911393 -4.2805498188182405
117 -4.163368467241526 -3.7944735717969627
118 -1.7429150864481926 -2.9512438456190186
119 -2.143725525587797 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

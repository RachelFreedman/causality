[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.7139277  -21.70254077
 -19.72288085 -19.26334551 -18.56975936 -18.11146126 -17.71205165
 -17.6663674  -17.58971226 -16.59087236 -16.36275632 -15.99253949
 -15.33818265 -14.68824709 -14.05563167 -13.91164962 -13.42778136
 -12.61940694 -12.22562907 -11.73504365 -10.78101609 -10.72946334
  -9.78594151  -9.77591752  -9.56682446  -9.19326137  -7.57539849
  -7.36244313  -7.10832736  -7.07294326  -6.95906356  -6.77694649
  -6.72206384  -6.71997062  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 2.612734375, val_acc 0.98
trigger times: 1
end of epoch 2: val_loss 2.039840087890625, val_acc 0.985
trigger times: 2
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 6.4492919921875, val_acc 0.995
trigger times: 1
end of epoch 7: val_loss 1.44384765625, val_acc 0.995
trigger times: 2
end of epoch 8: val_loss 1.8511328125, val_acc 0.995
trigger times: 3
end of epoch 9: val_loss 1.733251953125, val_acc 0.995
trigger times: 4
end of epoch 10: val_loss 5.1745654296875, val_acc 0.995
trigger times: 5
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 3.8129931640625, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 13.3375537109375, val_acc 0.985
trigger times: 1
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 4.0658984375, val_acc 0.995
trigger times: 1
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.6234814453125, val_acc 0.995
trigger times: 1
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.360146484375, val_acc 0.995
trigger times: 1
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 3.0778125, val_acc 0.995
trigger times: 1
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.13561338901519776, val_acc 0.985
trigger times: 1
end of epoch 37: val_loss 1.1125390625, val_acc 0.995
trigger times: 2
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 2.20583984375, val_acc 0.995
trigger times: 1
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 4.02821533203125, val_acc 0.99
trigger times: 1
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.79450439453125, val_acc 0.995
trigger times: 1
end of epoch 48: val_loss 3.967955322265625, val_acc 0.99
trigger times: 2
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.9414599609375, val_acc 0.995
trigger times: 1
end of epoch 51: val_loss 29.7945361328125, val_acc 0.98
trigger times: 2
end of epoch 52: val_loss 37.37292051553726, val_acc 0.975
trigger times: 3
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 5.422880407422781e-05, val_acc 1.0
trigger times: 1
end of epoch 55: val_loss 0.1490185546875, val_acc 0.995
trigger times: 2
end of epoch 56: val_loss 2.861689453125, val_acc 0.995
trigger times: 3
end of epoch 57: val_loss 1.24451171875, val_acc 0.995
trigger times: 4
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 24.9425634765625, val_acc 0.975
trigger times: 1
end of epoch 64: val_loss 12.524072265625, val_acc 0.98
trigger times: 2
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 5.468349609375, val_acc 0.99
trigger times: 1
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 118.16920166015625, val_acc 0.955
trigger times: 1
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 1.53169921875, val_acc 0.995
trigger times: 1
end of epoch 83: val_loss 1.0823291015625, val_acc 0.995
trigger times: 2
end of epoch 84: val_loss 0.718740234375, val_acc 0.995
trigger times: 3
end of epoch 85: val_loss 0.7023876953125, val_acc 0.995
trigger times: 4
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 3.785693359375, val_acc 0.995
trigger times: 1
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -21586.0525932312 -51.11086407377863
1 -22485.974422454834 -49.11584688036212
2 -33655.89302062988 -48.97455066650668
3 -67830.90427398682 -48.87228569913625
4 -30469.61785888672 -48.723750706751915
5 -26241.727036601398 -47.242520801549375
6 -38459.46633911133 -44.969216444406115
7 -60768.55325317383 -44.85812386141036
8 -28432.316612243652 -44.465298807522004
9 -39513.49024963379 -44.438901626703945
10 -32547.992874145508 -44.1492199315879
11 -18490.781898498535 -43.750352447059115
12 -26589.517959594727 -43.58405711344297
13 -52150.838958740234 -43.39691284921414
14 -19188.291023254395 -43.22343612991455
15 -22657.366989135742 -42.92113389854083
16 -38323.72752380371 -42.65748859055198
17 -20078.063583374023 -42.60231490040316
18 -20503.63190460205 -42.3286380570152
19 -32063.59617614746 -42.30445768311566
20 -44626.25622558594 -41.12283699063089
21 -19103.891235351562 -40.758154070747366
22 -25161.77964782715 -40.70441036508885
23 -27305.694137573242 -40.037587376429016
24 -32709.57278191892 -39.90846007278091
25 -29584.809719085693 -39.57032589487153
26 -24541.16537475586 -39.31833047231749
27 -29293.451377868652 -39.27893976130787
28 -34933.36445617676 -39.11210375046212
29 -22705.9757232666 -39.07922820344388
30 -65799.18003845215 -38.73424350253481
31 -24645.465286254883 -38.46237505180561
32 -17343.51562309265 -37.86129653169397
33 -26357.3006439209 -37.308065525311996
34 -22603.262504577637 -36.41283474392047
35 -23248.38148498535 -36.307958906842664
36 -21752.971557617188 -36.12354062747346
37 -24053.03810119629 -36.012213433621724
38 -21446.425878830254 -35.76166081977253
39 -16224.711751937866 -35.558040948837615
40 -29324.22184753418 -34.83335159619024
41 -41130.28871154785 -34.24472332102743
42 -19481.969080924988 -33.78094921339959
43 -17125.747581481934 -32.949116337767926
44 -21951.96685218811 -32.774518645207735
45 -17796.896362304688 -32.11238423004613
46 -19169.23480987549 -31.534270745249735
47 -41173.40596008301 -30.667802091332018
48 -16258.128834724426 -30.402902212388465
49 -17926.846108436584 -30.233660870125824
50 -19283.948133218102 -29.87212515537898
51 -16054.600814819336 -29.40604134962864
52 -20113.7105178833 -29.13525869956722
53 -18422.364875793457 -28.84050982317397
54 -15267.669536590576 -28.578629026453186
55 -24232.299812316895 -28.438710074595054
56 -19169.121353149414 -28.433748774939595
57 -23489.633918762207 -28.16728517571421
58 -20336.34454345703 -27.920602937037046
59 -18162.2577137053 -27.739114797604373
60 -14353.946029663086 -27.29994968172231
61 -13921.109007835388 -26.752294794786035
62 -16235.351444244385 -25.939034905087084
63 -16538.551696777344 -25.929972112830644
64 -20582.080307006836 -25.901848378876174
65 -13272.99201965332 -24.374430568602165
66 -13749.013040542603 -24.180793753972072
67 -19159.766647338867 -24.11517186961488
68 -18412.95447921753 -23.991963674348654
69 -16620.313415527344 -23.291023207711152
70 -19401.487747192383 -22.595308335760205
71 -15428.19474697113 -22.402293560723127
72 -16222.072006225586 -22.256737260461836
73 -14726.031876354478 -21.713927702452338
74 -12503.399375915527 -21.702540774014164
75 -16778.54093170166 -19.722880852611173
76 -12515.819030761719 -19.26334550647668
77 -17587.261924743652 -18.569759364071594
78 -11462.919367504073 -18.111461264885516
79 -16512.71485900879 -17.712051650475324
80 -14239.214979171753 -17.66636739843018
81 -13859.72864251025 -17.589712263068826
82 -11032.36561216414 -16.590872364965673
83 -17087.26794052124 -16.362756319978686
84 -15560.991676330566 -15.992539490672064
85 -11435.765434265137 -15.338182650137867
86 -17766.75086929917 -14.688247089412398
87 -12355.695455577224 -14.055631672914624
88 -11504.121307373047 -13.911649622206243
89 -12062.382286071777 -13.427781363912512
90 -14951.205309394747 -12.619406941033082
91 -9879.010398089886 -12.225629065695466
92 -13099.230146408081 -11.735043647109059
93 -13247.053974151611 -10.781016093040188
94 -13943.735956598073 -10.729463335147118
95 -9783.418224334717 -9.785941512083282
96 -8381.825818514451 -9.775917521665287
97 -9640.250936520286 -9.56682446099046
98 -8961.921459197998 -9.193261368948118
99 -11957.510791063309 -7.57539849177145
100 -11121.939855337143 -7.362443126623615
101 -12735.561424880521 -7.108327355338034
102 -8405.393987621646 -7.072943263247867
103 -10409.744528182899 -6.959063561385431
104 -11932.947455011308 -6.776946485018116
105 -11194.572403788567 -6.7220638398623045
106 -11780.321038097143 -6.719970621583102
107 -10896.929187650036 -6.51820418055673
108 -10338.979657402728 -5.615796733870542
109 -9881.272396087646 -5.34720210027791
110 -8513.50814855867 -5.078485007852753
111 -9598.569537043106 -5.027957977402961
112 -8230.15726852417 -4.827572916892203
113 -7614.250423208112 -4.63049541560991
114 -8049.36149597168 -4.230832004686763
115 -8455.012565612793 -4.031048624093466
116 -7681.65892961435 -3.3844671463622564
117 -7648.414520263672 -3.3322555012187633
118 -6373.121306777 -2.6416623314910934
119 -6412.126739501953 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -23.25983131 -23.18216776 -22.59530834 -22.46012464 -22.40229356
 -22.33446619 -22.25673726 -21.85390446 -21.80181114 -21.7139277
 -21.70254077 -21.65652156 -21.54121021 -21.45033209 -21.29665786
 -21.1123623  -21.10394014 -21.07725225 -20.86567352 -20.81751689
 -20.59756139 -20.42910041 -20.21830275 -20.05680887 -20.02448568
 -19.99188335 -19.91294527 -19.72317042 -19.72288085 -19.67998592
 -19.67334465 -19.62785987 -19.57558883 -19.33286499 -19.29215256
 -19.26334551 -18.56975936 -18.53157795 -18.36776933 -18.32048608
 -18.28056766 -18.24596032 -18.1296814  -18.11146126 -18.03157237
 -17.98604931 -17.92821999 -17.89709525 -17.71205165 -17.6663674
 -17.6400464  -17.58971226 -17.52052107 -17.5163234  -17.34565801
 -17.31507509 -17.11270888 -17.06814743 -16.95884322 -16.92448838
 -16.91139886 -16.80093549 -16.66555987 -16.62171737 -16.59087236
 -16.55182544 -16.41323646 -16.37683999 -16.36275632 -16.26082515
 -16.23311195 -15.99253949 -15.82382906 -15.74782548 -15.47844355
 -15.45319442 -15.42512924 -15.41148028 -15.40471983 -15.33818265
 -15.24794117 -15.18047643 -15.07098851 -14.97754501 -14.68824709
 -14.54716901 -14.3412907  -14.2808412  -14.07316975 -14.05563167
 -13.99440709 -13.96667208 -13.91267391 -13.91164962 -13.89611176
 -13.70937707 -13.44138673 -13.42892048 -13.42778136 -13.24314154
 -13.00512726 -12.9736826  -12.93288364 -12.69584727 -12.62184173
 -12.61940694 -12.55635232 -12.46764183 -12.4147629  -12.22562907
 -12.20296896 -12.05094112 -11.8332835  -11.811794   -11.73504365
 -11.66958498 -11.25290519 -11.17072984 -10.78101609 -10.72946334
 -10.51911039 -10.39230164 -10.27293027  -9.80011094  -9.78594151
  -9.78495316  -9.77591752  -9.56682446  -9.37974853  -9.296719
  -9.19326137  -9.13250985  -9.03223409  -8.96289504  -8.94854838
  -8.58800559  -8.34862647  -7.88233495  -7.87256771  -7.83212014
  -7.76936259  -7.64665507  -7.57539849  -7.36244313  -7.35078227
  -7.17223122  -7.10832736  -7.07294326  -6.95906356  -6.86620619
  -6.77694649  -6.72206384  -6.71997062  -6.51820418  -6.28049573
  -6.0108133   -5.84434748  -5.61579673  -5.3472021   -5.33215005
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.002314575929911236, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.8989031982421875, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 1.414294792994042e-06, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.5410792011022022, val_acc 0.99
trigger times: 2
end of epoch 5: val_loss 0.008620876669883728, val_acc 0.995
trigger times: 3
end of epoch 6: val_loss 0.931788330078125, val_acc 0.995
trigger times: 4
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 2.5390934752067554e-07, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.8383827114105225, val_acc 0.99
trigger times: 2
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 3.713231490110047e-07, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 2.409078369140625, val_acc 0.985
trigger times: 2
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.9676861572265625, val_acc 0.99
trigger times: 1
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 1.790322265625, val_acc 0.995
trigger times: 1
end of epoch 20: val_loss 34.430379956662655, val_acc 0.92
trigger times: 2
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.07088013172149658, val_acc 0.995
trigger times: 1
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.06172365665435791, val_acc 0.995
trigger times: 1
end of epoch 37: val_loss 1.7410791015625, val_acc 0.995
trigger times: 2
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 1.3382763671875, val_acc 0.995
trigger times: 1
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.092462158203125, val_acc 0.995
trigger times: 1
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.559384765625, val_acc 0.995
trigger times: 1
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 4.614905449721809, val_acc 0.98
trigger times: 1
end of epoch 67: val_loss 1.35971923828125, val_acc 0.995
trigger times: 2
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 1.1510302734375, val_acc 0.99
trigger times: 1
end of epoch 71: val_loss 0.16166748046875, val_acc 0.995
trigger times: 2
end of epoch 72: val_loss 4.53008056640625, val_acc 0.985
trigger times: 3
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 1.7449609375, val_acc 0.995
trigger times: 1
end of epoch 94: val_loss 29.15328125, val_acc 0.99
trigger times: 2
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.23875109672546388, val_acc 0.99
trigger times: 1
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -1792.5122669963166 -51.11086407377863
1 -2564.5574426651 -48.97455066650668
2 -2047.1653938293457 -48.723750706751915
3 -2478.8097190856934 -44.969216444406115
4 -1879.5754654407501 -44.465298807522004
5 -2311.7580523490906 -44.1492199315879
6 -1640.64089012146 -43.58405711344297
7 -1595.0716772079468 -43.22343612991455
8 -2419.4685497283936 -42.65748859055198
9 -1511.207321882248 -42.3286380570152
10 -3133.947346687317 -41.12283699063089
11 -2117.953282356262 -40.70441036508885
12 -2828.3180632591248 -39.90846007278091
13 -2262.968632221222 -39.31833047231749
14 -2300.962728500366 -39.11210375046212
15 -6512.100206375122 -38.73424350253481
16 -1210.271773768589 -37.86129653169397
17 -1691.0120322247967 -36.41283474392047
18 -1392.6574039459229 -36.12354062747346
19 -1928.9698333740234 -35.76166081977253
20 -2086.4182777404785 -34.83335159619024
21 -1975.4393768310547 -33.78094921339959
22 -1450.8547773361206 -32.774518645207735
23 -1597.656826019287 -31.534270745249735
24 -1450.7968847574666 -30.402902212388465
25 -1287.4245162010193 -29.87212515537898
26 -1640.4075291156769 -29.13525869956722
27 -1407.679202079773 -28.578629026453186
28 -1835.7813618183136 -28.433748774939595
29 -1540.9142866134644 -27.920602937037046
30 -1261.3490419387817 -27.29994968172231
31 -1549.390613079071 -25.939034905087084
32 -1757.2947762012482 -25.901848378876174
33 -1202.2898191846907 -24.180793753972072
34 -1504.3026218414307 -23.991963674348654
35 -1207.1823043823242 -23.25983130699908
36 -1531.9396924972534 -22.595308335760205
37 -1136.342390537262 -22.402293560723127
38 -1374.829144001007 -22.256737260461836
39 -1229.7234363555908 -21.801811144701382
40 -1118.7896118164062 -21.702540774014164
41 -1230.522590637207 -21.54121020587753
42 -1243.5473499298096 -21.296657862598767
43 -1242.7222137451172 -21.10394014413595
44 -1204.036211013794 -20.865673524931356
45 -1215.6561279296875 -20.597561385027053
46 -1237.5744132995605 -20.21830275130273
47 -1175.2351760864258 -20.024485678852383
48 -1176.1128673553467 -19.912945268732383
49 -1141.0002717971802 -19.722880852611173
50 -1178.9910526275635 -19.673344645442132
51 -1139.444248199463 -19.575588828018667
52 -1161.1291255950928 -19.29215255520995
53 -1382.862446308136 -18.569759364071594
54 -1149.4924726486206 -18.367769326754775
55 -1188.0088024139404 -18.28056766443594
56 -1159.2305088043213 -18.129681401290675
57 -1161.4672946929932 -18.03157237123006
58 -1164.4757328033447 -17.928219988991017
59 -1459.674599647522 -17.712051650475324
60 -837.8631543377414 -17.589712263068826
61 -1185.983018875122 -17.516323398186362
62 -1107.29430103302 -17.315075092418684
63 -1109.3139171600342 -17.06814743225983
64 -1117.6957931518555 -16.924488375187856
65 -1123.850872039795 -16.80093549349091
66 -1122.1274433135986 -16.621717371289915
67 -1096.4859104156494 -16.551825437539033
68 -1120.7165718078613 -16.37683999173314
69 -1141.3073749542236 -16.260825150148627
70 -1382.8405806273222 -15.992539490672064
71 -1122.061237335205 -15.747825477780923
72 -1115.9242420196533 -15.453194424557724
73 -1108.4453716278076 -15.411480279764707
74 -1073.5320768356323 -15.338182650137867
75 -1139.2301120758057 -15.180476434279045
76 -1121.198787689209 -14.977545011404713
77 -1074.896987915039 -14.54716900609245
78 -1096.6380004882812 -14.280841199161697
79 -1026.2650085687637 -14.055631672914624
80 -1057.9668703079224 -13.966672078267072
81 -1027.4233589172363 -13.911649622206243
82 -1140.8772468566895 -13.709377067589873
83 -1118.2563915252686 -13.42892047593499
84 -1091.5697021484375 -13.243141543194646
85 -1107.1781635284424 -12.97368260428777
86 -1065.449091911316 -12.69584727345253
87 -980.6738806962967 -12.619406941033082
88 -1073.4339008331299 -12.467641830071226
89 -986.5837091207504 -12.225629065695466
90 -1121.8790950775146 -12.050941124213047
91 -1059.140027999878 -11.811793997665962
92 -1097.683406829834 -11.669584977416761
93 -1109.0116729736328 -11.170729842188573
94 -1085.586244652979 -10.729463335147118
95 -1049.2224617004395 -10.392301643248208
96 -1042.6974086761475 -9.800110940436982
97 -1058.8735675811768 -9.784953158386394
98 -919.9134998321533 -9.56682446099046
99 -1067.783763885498 -9.296719001882565
100 -1066.116828918457 -9.132509854664788
101 -1063.5700855255127 -8.962895036902655
102 -1037.3920783996582 -8.58800559068593
103 -1045.289330482483 -7.882334949169341
104 -1026.1692924499512 -7.8321201404928855
105 -1017.5781593322754 -7.646655073165271
106 -818.9368247976527 -7.362443126623615
107 -1021.0583219528198 -7.172231224265249
108 -861.1558294296265 -7.072943263247867
109 -1006.2791709899902 -6.866206190324398
110 -815.1608877303079 -6.7220638398623045
111 -599.2531510964036 -6.51820418055673
112 -1022.4841613769531 -6.010813304204734
113 -598.1494815349579 -5.615796733870542
114 -1014.0139675140381 -5.3321500505588375
115 -829.6086531877518 -5.027957977402961
116 -800.4688906669617 -4.63049541560991
117 -914.6817216873169 -4.031048624093466
118 -876.9521398544312 -3.3322555012187633
119 -894.8559837341309 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-63.80318709 -63.3366241  -62.32636225 -62.11891262 -62.09385094
 -60.81867582 -59.70279118 -59.22937913 -57.31000348 -56.40613711
 -56.00667963 -54.58281266 -53.49669433 -52.95249228 -52.44130672
 -51.49827622 -51.11086407 -50.30096989 -49.58899691 -49.40531233
 -49.11584688 -49.0308504  -48.97455067 -48.92057574 -48.8722857
 -48.72375071 -48.64352957 -48.37705553 -47.67489377 -47.42838367
 -47.25998585 -47.2425208  -47.23872936 -47.0078127  -46.95646515
 -46.57633633 -46.4945736  -46.32070559 -46.27584992 -45.89764164
 -45.83205788 -45.64929703 -45.11651962 -44.9775141  -44.96921644
 -44.93792323 -44.91874256 -44.85812386 -44.6942022  -44.57461633
 -44.46529881 -44.43890163 -44.34086798 -44.33222892 -44.30080477
 -44.14921993 -43.99737777 -43.76991783 -43.75035245 -43.58405711
 -43.583211   -43.57040515 -43.55484522 -43.47927697 -43.43739424
 -43.39691285 -43.32299468 -43.25038703 -43.22343613 -42.9211339
 -42.65748859 -42.6023149  -42.32863806 -42.30445768 -42.01922667
 -41.79397062 -41.76028168 -41.73236045 -41.12283699 -41.04740955
 -41.00233358 -40.75815407 -40.70441037 -40.6830923  -40.68198244
 -40.5368988  -40.45551398 -40.11647447 -40.03758738 -39.94940252
 -39.90846007 -39.57032589 -39.56432764 -39.51757874 -39.43200287
 -39.40560705 -39.31833047 -39.27893976 -39.17452637 -39.1467903
 -39.11210375 -39.0792282  -39.05727159 -39.01163027 -38.79806518
 -38.7342435  -38.46237505 -38.08627313 -37.94958159 -37.94295274
 -37.86129653 -37.30806553 -37.19161015 -37.11273927 -36.84565678
 -36.82607283 -36.79079441 -36.68197202 -36.66771719 -36.41661246
 -36.41407103 -36.41283474 -36.30795891 -36.25582613 -36.12759582
 -36.12354063 -36.03093985 -36.01221343 -35.8808435  -35.87850775
 -35.81241862 -35.76166082 -35.64630068 -35.55804095 -35.3916028
 -35.38181607 -35.26357048 -35.07068682 -34.97398625 -34.8333516
 -34.72103491 -34.24472332 -34.17113184 -33.9121592  -33.78186494
 -33.78094921 -33.68570214 -33.0430929  -32.94911634 -32.88474677
 -32.77451865 -32.73500825 -32.16692809 -32.11238423 -32.01609901
 -31.92157163 -31.83174162 -31.53427075 -30.88599155 -30.66780209
 -30.40290221 -30.23366087 -30.11697256 -30.06112903 -29.87212516
 -29.40604135 -29.1352587  -28.84050982 -28.57862903 -28.43871007
 -28.43374877 -28.16728518 -27.92060294 -27.7391148  -27.48520133
 -27.37044707 -27.29994968 -26.75229479 -25.93903491 -25.92997211
 -25.90184838 -25.67029819 -24.99453132 -24.75646406 -24.37443057
 -24.18079375 -24.11517187 -23.99196367 -23.91672487 -23.29102321
 -23.25983131 -23.18216776 -22.59530834 -22.46012464 -22.40229356
 -22.33446619 -22.25673726 -21.85390446 -21.80181114 -21.7139277
 -21.70254077 -21.65652156 -21.54121021 -21.45033209 -21.29665786
 -21.1123623  -21.10394014 -21.07725225 -20.86567352 -20.81751689
 -20.59756139 -20.42910041 -20.21830275 -20.05680887 -20.02448568
 -19.99188335 -19.91294527 -19.72317042 -19.72288085 -19.67998592
 -19.67334465 -19.62785987 -19.57558883 -19.33286499 -19.29215256
 -19.26334551 -18.56975936 -18.53157795 -18.36776933 -18.32048608
 -18.28056766 -18.24596032 -18.1296814  -18.11146126 -18.03157237
 -17.98604931 -17.92821999 -17.89709525 -17.71205165 -17.6663674
 -17.6400464  -17.58971226 -17.52052107 -17.5163234  -17.34565801
 -17.31507509 -17.11270888 -17.06814743 -16.95884322 -16.92448838
 -16.91139886 -16.80093549 -16.66555987 -16.62171737 -16.59087236
 -16.55182544 -16.41323646 -16.37683999 -16.36275632 -16.26082515
 -16.23311195 -15.99253949 -15.82382906 -15.74782548 -15.47844355
 -15.45319442 -15.42512924 -15.41148028 -15.40471983 -15.33818265
 -15.24794117 -15.18047643 -15.07098851 -14.97754501 -14.68824709
 -14.54716901 -14.3412907  -14.2808412  -14.07316975 -14.05563167
 -13.99440709 -13.96667208 -13.91267391 -13.91164962 -13.89611176
 -13.70937707 -13.44138673 -13.42892048 -13.42778136 -13.24314154
 -13.00512726 -12.9736826  -12.93288364 -12.69584727 -12.62184173
 -12.61940694 -12.55635232 -12.46764183 -12.4147629  -12.22562907
 -12.20296896 -12.05094112 -11.8332835  -11.811794   -11.73504365
 -11.66958498 -11.25290519 -11.17072984 -10.78101609 -10.72946334
 -10.51911039 -10.39230164 -10.27293027  -9.80011094  -9.78594151
  -9.78495316  -9.77591752  -9.56682446  -9.37974853  -9.296719
  -9.19326137  -9.13250985  -9.03223409  -8.96289504  -8.94854838
  -8.58800559  -8.34862647  -7.88233495  -7.87256771  -7.83212014
  -7.76936259  -7.64665507  -7.57539849  -7.36244313  -7.35078227
  -7.17223122  -7.10832736  -7.07294326  -6.95906356  -6.86620619
  -6.77694649  -6.72206384  -6.71997062  -6.51820418  -6.28049573
  -6.0108133   -5.84434748  -5.61579673  -5.3472021   -5.33215005
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.89060302734375, val_acc 0.995
trigger times: 1
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.97346435546875, val_acc 0.995
trigger times: 1
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.650076904296875, val_acc 0.995
trigger times: 1
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.612470703125, val_acc 0.995
trigger times: 1
end of epoch 24: val_loss 8.23614990234375, val_acc 0.985
trigger times: 2
end of epoch 25: val_loss 0.10745040893554687, val_acc 0.995
trigger times: 3
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.5177392578125, val_acc 0.995
trigger times: 1
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 3.0340234375, val_acc 0.995
trigger times: 1
end of epoch 37: val_loss 0.20412109375, val_acc 0.995
trigger times: 2
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 2.479698486328125, val_acc 0.985
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 1.3308203125, val_acc 0.995
trigger times: 1
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 1.3053296243015212e-07, val_acc 1.0
trigger times: 1
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.076376953125, val_acc 0.995
trigger times: 1
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.217001953125, val_acc 0.995
trigger times: 1
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 5.626361962640658e-07, val_acc 1.0
trigger times: 1
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.296611328125, val_acc 0.995
trigger times: 1
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 2.0839400531258433e-06, val_acc 1.0
trigger times: 1
end of epoch 67: val_loss 0.0006583264470100403, val_acc 1.0
trigger times: 2
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 54.95689483642578, val_acc 0.96
trigger times: 1
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.854375, val_acc 0.995
trigger times: 1
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 3.18486328125, val_acc 0.995
trigger times: 1
end of epoch 82: val_loss 9.194078426361084, val_acc 0.99
trigger times: 2
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 1.8620068359375, val_acc 0.995
trigger times: 1
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 7.1288720703125, val_acc 0.99
trigger times: 1
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -4189.628719329834 -63.80318709008168
1 -4160.939956665039 -62.1189126171533
2 -4098.976303100586 -59.70279118339014
3 -4069.9528312683105 -56.406137112527055
4 -4255.0877113342285 -53.49669433343748
5 -3826.9190254211426 -51.498276222031166
6 -4090.2599449157715 -49.5889969093609
7 -3811.9158248901367 -49.030850395941464
8 -11306.182318687439 -48.87228569913625
9 -3787.9417610168457 -48.37705553293593
10 -3843.423240661621 -47.2599858545806
11 -3756.4965171813965 -47.00781270180563
12 -4255.267532348633 -46.494573602860655
13 -3683.0374488830566 -45.89764163961389
14 -4025.1406784057617 -45.11651962392763
15 -4051.3195152282715 -44.937923231812896
16 -4224.075828552246 -44.694202198088234
17 -5929.465646743774 -44.438901626703945
18 -3822.7349853515625 -44.300804769361186
19 -4082.5772285461426 -43.76991782787556
20 -4170.465774536133 -43.583210996371044
21 -4064.5994300842285 -43.47927697390163
22 -3722.6574935913086 -43.32299468375923
23 -3298.859501361847 -42.92113389854083
24 -3123.1869974136353 -42.3286380570152
25 -4295.854785919189 -41.793970616278514
26 -7757.2111892700195 -41.12283699063089
27 -2875.9524993896484 -40.758154070747366
28 -3818.276351928711 -40.681982443749924
29 -4159.399921417236 -40.11647447046231
30 -4521.602848172188 -39.57032589487153
31 -3895.857955932617 -39.43200286546545
32 -5002.721862792969 -39.27893976130787
33 -5967.409173488617 -39.11210375046212
34 -4237.152038574219 -39.011630271808535
35 -4344.588088989258 -38.46237505180561
36 -4445.77587890625 -37.94295273950308
37 -4155.685691833496 -37.19161015441263
38 -3919.7336387634277 -36.826072831484076
39 -3927.8157348632812 -36.66771718728611
40 -3738.95876789093 -36.41283474392047
41 -3984.7466049194336 -36.12759581906656
42 -3696.9964740276337 -36.012213433621724
43 -3967.7875061035156 -35.812418623634045
44 -2350.1764697064646 -35.558040948837615
45 -4280.175048828125 -35.263570479911
46 -4619.851070404053 -34.83335159619024
47 -3904.114288330078 -34.17113183764528
48 -3557.28341037035 -33.78094921339959
49 -2272.0683357715607 -32.949116337767926
50 -3760.7044525146484 -32.73500825225695
51 -3937.8435440063477 -32.016099007020365
52 -3202.7158365249634 -31.534270745249735
53 -2921.5836304426193 -30.402902212388465
54 -3832.2892112731934 -30.06112902930644
55 -3078.2518216371536 -29.13525869956722
56 -3356.50682926178 -28.438710074595054
57 -3375.1327028274536 -27.920602937037046
58 -3694.99893951416 -27.37044706935712
59 -2817.1804466471076 -25.939034905087084
60 -3441.854564666748 -25.670298192095267
61 -2355.4073843955994 -24.374430568602165
62 -3513.1073932647705 -23.991963674348654
63 -1389.588168144226 -23.25983130699908
64 -1392.3455982208252 -22.460124641458123
65 -3136.300528526306 -22.256737260461836
66 -2294.600131392479 -21.713927702452338
67 -1422.3896598815918 -21.54121020587753
68 -1341.7414817810059 -21.112362300093853
69 -1380.8199300765991 -20.865673524931356
70 -1416.872046470642 -20.429100406889887
71 -1407.6824016571045 -20.024485678852383
72 -1422.9815378189087 -19.723170418262647
73 -1351.2951974868774 -19.673344645442132
74 -1399.8617506027222 -19.332864993187492
75 -2883.210837364197 -18.569759364071594
76 -1338.49068069458 -18.320486082826196
77 -1318.3392667770386 -18.129681401290675
78 -1358.6210889816284 -17.98604931010075
79 -3153.1217126846313 -17.712051650475324
80 -1560.8592197506223 -17.589712263068826
81 -1308.9055442810059 -17.345658014658007
82 -1280.0392417907715 -17.06814743225983
83 -1180.6128330230713 -16.911398857783805
84 -1255.2106552124023 -16.621717371289915
85 -1292.8035793304443 -16.41323646198552
86 -1305.4925050735474 -16.260825150148627
87 -1273.0475187301636 -15.823829057073402
88 -1269.377932548523 -15.453194424557724
89 -1248.327974319458 -15.404719825644449
90 -1317.8764114379883 -15.070988514268866
91 -1213.0137882232666 -14.54716900609245
92 -1279.095739364624 -14.073169748133829
93 -1182.8986568450928 -13.966672078267072
94 -1248.2877178192139 -13.896111761279693
95 -1268.860384941101 -13.42892047593499
96 -1203.4202823638916 -13.00512725714242
97 -1200.1550941467285 -12.69584727345253
98 -1243.2876195907593 -12.556352315089619
99 -1284.5897630490072 -12.225629065695466
100 -1228.8406829833984 -11.833283495902823
101 -1233.1095609664917 -11.669584977416761
102 -1954.0076045170426 -10.781016093040188
103 -1194.5848417282104 -10.392301643248208
104 -1628.7852997779846 -9.785941512083282
105 -1346.9242770671844 -9.56682446099046
106 -2303.3069937229156 -9.193261368948118
107 -1190.5190057754517 -8.962895036902655
108 -1157.0882658958435 -8.348626465467405
109 -1180.1213932037354 -7.8321201404928855
110 -1906.0918563455343 -7.57539849177145
111 -1126.2121815681458 -7.172231224265249
112 -1397.6709262281656 -6.959063561385431
113 -1776.238642132841 -6.7220638398623045
114 -1149.4087896347046 -6.280495733893076
115 -1089.8792438767268 -5.615796733870542
116 -854.5228419692285 -5.078485007852753
117 -863.5581079815165 -4.63049541560991
118 -1052.5858696699142 -3.3844671463622564
119 -1262.4101648330688 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-63.80318709 -63.3366241  -62.32636225 -62.11891262 -62.09385094
 -60.81867582 -59.70279118 -59.22937913 -57.31000348 -56.40613711
 -56.00667963 -54.58281266 -53.49669433 -52.95249228 -52.44130672
 -51.49827622 -51.11086407 -50.30096989 -49.58899691 -49.40531233
 -49.11584688 -49.0308504  -48.97455067 -48.92057574 -48.8722857
 -48.72375071 -48.64352957 -48.37705553 -47.67489377 -47.42838367
 -47.25998585 -47.2425208  -47.23872936 -47.0078127  -46.95646515
 -46.57633633 -46.4945736  -46.32070559 -46.27584992 -45.89764164
 -45.83205788 -45.64929703 -45.11651962 -44.9775141  -44.96921644
 -44.93792323 -44.91874256 -44.85812386 -44.6942022  -44.57461633
 -44.46529881 -44.43890163 -44.34086798 -44.33222892 -44.30080477
 -44.14921993 -43.99737777 -43.76991783 -43.75035245 -43.58405711
 -43.583211   -43.57040515 -43.55484522 -43.47927697 -43.43739424
 -43.39691285 -43.32299468 -43.25038703 -43.22343613 -42.9211339
 -42.65748859 -42.6023149  -42.32863806 -42.30445768 -42.01922667
 -41.79397062 -41.76028168 -41.73236045 -41.12283699 -41.04740955
 -41.00233358 -40.75815407 -40.70441037 -40.6830923  -40.68198244
 -40.5368988  -40.45551398 -40.11647447 -40.03758738 -39.94940252
 -39.90846007 -39.57032589 -39.56432764 -39.51757874 -39.43200287
 -39.40560705 -39.31833047 -39.27893976 -39.17452637 -39.1467903
 -39.11210375 -39.0792282  -39.05727159 -39.01163027 -38.79806518
 -38.7342435  -38.46237505 -38.08627313 -37.94958159 -37.94295274
 -37.86129653 -37.30806553 -37.19161015 -37.11273927 -36.84565678
 -36.82607283 -36.79079441 -36.68197202 -36.66771719 -36.41661246
 -36.41407103 -36.41283474 -36.30795891 -36.25582613 -36.12759582
 -36.12354063 -36.03093985 -36.01221343 -35.8808435  -35.87850775
 -35.81241862 -35.76166082 -35.64630068 -35.55804095 -35.3916028
 -35.38181607 -35.26357048 -35.07068682 -34.97398625 -34.8333516
 -34.72103491 -34.24472332 -34.17113184 -33.9121592  -33.78186494
 -33.78094921 -33.68570214 -33.0430929  -32.94911634 -32.88474677
 -32.77451865 -32.73500825 -32.16692809 -32.11238423 -32.01609901
 -31.92157163 -31.83174162 -31.53427075 -30.88599155 -30.66780209
 -30.40290221 -30.23366087 -30.11697256 -30.06112903 -29.87212516
 -29.40604135 -29.1352587  -28.84050982 -28.57862903 -28.43871007
 -28.43374877 -28.16728518 -27.92060294 -27.7391148  -27.48520133
 -27.37044707 -27.29994968 -26.75229479 -25.93903491 -25.92997211
 -25.90184838 -25.67029819 -24.99453132 -24.75646406 -24.37443057
 -24.18079375 -24.11517187 -23.99196367 -23.91672487 -23.29102321
 -23.26732263 -23.25983131 -23.18216776 -23.14081539 -23.10346457
 -22.99351351 -22.99145195 -22.70462412 -22.59530834 -22.54180655
 -22.46012464 -22.40229356 -22.33446619 -22.25673726 -21.96148228
 -21.95298519 -21.85390446 -21.82040774 -21.80181114 -21.76498217
 -21.7139277  -21.70254077 -21.65652156 -21.54121021 -21.51024177
 -21.45033209 -21.29665786 -21.16195827 -21.16139316 -21.1123623
 -21.10394014 -21.09906841 -21.07725225 -21.01442037 -20.86567352
 -20.85062394 -20.81751689 -20.74823789 -20.67630351 -20.62237898
 -20.59756139 -20.52537316 -20.47438567 -20.42910041 -20.31647497
 -20.22059391 -20.21830275 -20.13612291 -20.05680887 -20.02448568
 -19.99188335 -19.92396275 -19.91294527 -19.72317042 -19.72288085
 -19.67998592 -19.67334465 -19.64378475 -19.63125909 -19.62785987
 -19.57558883 -19.56488968 -19.47623704 -19.46087966 -19.37117322
 -19.36139425 -19.33286499 -19.29215256 -19.27283462 -19.26334551
 -18.88847791 -18.84069557 -18.71901476 -18.69393092 -18.64457019
 -18.60398172 -18.56975936 -18.54098765 -18.53157795 -18.47925607
 -18.44493788 -18.36776933 -18.34306645 -18.34174785 -18.32048608
 -18.28056766 -18.24596032 -18.1296814  -18.11146126 -18.03157237
 -18.02173735 -17.98604931 -17.97883409 -17.92821999 -17.89709525
 -17.83710124 -17.71205165 -17.6663674  -17.6400464  -17.58971226
 -17.52052107 -17.5163234  -17.5074629  -17.42140315 -17.41105127
 -17.34565801 -17.31507509 -17.28107356 -17.12891885 -17.11270888
 -17.0853136  -17.06814743 -17.00358108 -16.95884322 -16.92448838
 -16.91139886 -16.9056968  -16.9023109  -16.80093549 -16.66555987
 -16.62171737 -16.59087236 -16.55182544 -16.52871441 -16.52184618
 -16.48862972 -16.41323646 -16.37683999 -16.36275632 -16.26082515
 -16.23311195 -16.2219063  -15.99253949 -15.88654317 -15.82382906
 -15.76723781 -15.7603865  -15.74782548 -15.67500454 -15.6039325
 -15.56885626 -15.47844355 -15.45319442 -15.43622815 -15.42512924
 -15.41148028 -15.40471983 -15.33818265 -15.24794117 -15.23320986
 -15.18047643 -15.0725541  -15.07098851 -14.97754501 -14.93353996
 -14.88080703 -14.87705014 -14.81510392 -14.68824709 -14.65847502
 -14.54716901 -14.42808736 -14.42405808 -14.3412907  -14.2808412
 -14.26718087 -14.25836096 -14.07316975 -14.05563167 -13.99440709
 -13.96667208 -13.94485075 -13.91267391 -13.91164962 -13.89611176
 -13.8111931  -13.7950522  -13.72483683 -13.70937707 -13.66012516
 -13.64297292 -13.48360994 -13.44138673 -13.42892048 -13.42778136
 -13.24314154 -13.10148167 -13.00512726 -12.9736826  -12.94759223
 -12.93288364 -12.78326283 -12.69584727 -12.62184173 -12.61940694
 -12.55635232 -12.46764183 -12.4147629  -12.38426645 -12.2282058
 -12.22562907 -12.20296896 -12.05094112 -11.93281394 -11.91644396
 -11.89735871 -11.84758642 -11.8332835  -11.82645136 -11.811794
 -11.73504365 -11.72950901 -11.66958498 -11.5614898  -11.25290519
 -11.20952246 -11.17072984 -10.87914684 -10.78101609 -10.73370335
 -10.72946334 -10.51911039 -10.45257015 -10.39230164 -10.27293027
 -10.26123795 -10.12971059 -10.12327662 -10.03405912  -9.93487679
  -9.80011094  -9.78594151  -9.78495316  -9.77591752  -9.76513328
  -9.74924816  -9.56682446  -9.55600806  -9.37974853  -9.296719
  -9.22081195  -9.19564184  -9.19326137  -9.17879346  -9.13250985
  -9.12051037  -9.11447385  -9.03223409  -8.96289504  -8.94854838
  -8.88311311  -8.69330803  -8.66363489  -8.58800559  -8.57908631
  -8.34862647  -8.22646041  -7.88233495  -7.87256771  -7.83212014
  -7.76936259  -7.64665507  -7.57539849  -7.36244313  -7.35078227
  -7.17223122  -7.10832736  -7.07294326  -6.95906356  -6.86620619
  -6.77694649  -6.72206384  -6.71997062  -6.51820418  -6.28049573
  -6.0108133   -5.84434748  -5.61579673  -5.3472021   -5.33215005
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.43425537109375, val_acc 0.995
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.202445068359375, val_acc 0.995
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.04430368959903713, val_acc 0.995
trigger times: 1
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.9792138671875, val_acc 0.995
trigger times: 1
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 4.550341186523437, val_acc 0.98
trigger times: 1
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.52691162109375, val_acc 0.995
trigger times: 1
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.837684326171875, val_acc 0.995
trigger times: 1
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 4.69866656055674, val_acc 0.98
trigger times: 1
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -8881.250610351562 -63.80318709008168
1 -8642.12149810791 -62.0938509436313
2 -8875.598442077637 -57.310003480801775
3 -9156.19017791748 -53.49669433343748
4 -6497.184931136668 -51.11086407377863
5 -5647.062312841415 -49.11584688036212
6 -18388.649633407593 -48.87228569913625
7 -7835.962020874023 -47.67489376922608
8 -9301.09538269043 -47.238729359504866
9 -9007.871620178223 -46.494573602860655
10 -7521.826530456543 -45.83205788373689
11 -10244.873378515244 -44.969216444406115
12 -8838.932426452637 -44.694202198088234
13 -8350.754043579102 -44.34086798236464
14 -8397.822776794434 -43.997377765029896
15 -8827.129112243652 -43.583210996371044
16 -9254.960258483887 -43.43739423714021
17 -6008.344848513603 -43.22343612991455
18 -6636.870809316635 -42.3286380570152
19 -7070.66414642334 -41.760281676658025
20 -5214.054881751537 -40.758154070747366
21 -8346.589881896973 -40.536898802842494
22 -8991.07763671875 -39.94940252261085
23 -8175.783241271973 -39.51757874044001
24 -8508.15531384945 -39.27893976130787
25 -6554.035507798195 -39.07922820344388
26 -21572.731172561646 -38.73424350253481
27 -9024.156188964844 -37.94295273950308
28 -8241.14289855957 -37.11273927403557
29 -7820.143829345703 -36.68197202349702
30 -6225.418547689915 -36.41283474392047
31 -5131.643740415573 -36.12354062747346
32 -8258.802993774414 -35.87850775313792
33 -3846.867077112198 -35.558040948837615
34 -8334.467430114746 -35.07068682037155
35 -14336.238405227661 -34.24472332102743
36 -6505.502539057285 -33.78094921339959
37 -8063.628456115723 -32.884746767663
38 -6652.894307971001 -32.11238423004613
39 -5604.798938393593 -31.534270745249735
40 -5383.524097442627 -30.233660870125824
41 -4887.171227216721 -29.40604134962864
42 -6205.7085609436035 -28.438710074595054
43 -5642.742769271135 -27.739114797604373
44 -3325.588452875614 -26.752294794786035
45 -7152.982963562012 -25.670298192095267
46 -4502.94601303339 -24.180793753972072
47 -5450.977741122246 -23.291023207711152
48 -3516.589756011963 -23.14081538622166
49 -3612.8854446411133 -22.704624120014362
50 -3278.960600912571 -22.402293560723127
51 -3591.9322624206543 -21.95298518637714
52 -3419.7044944763184 -21.764982170977934
53 -3501.9932594299316 -21.54121020587753
54 -3423.050308227539 -21.16195826952961
55 -3280.656276702881 -21.09906840532338
56 -3174.893585205078 -20.85062394088777
57 -3377.7670860290527 -20.622378976422425
58 -3482.2569885253906 -20.429100406889887
59 -3392.912380218506 -20.136122905141207
60 -3578.874725341797 -19.912945268732383
61 -3426.819179534912 -19.673344645442132
62 -3493.180244445801 -19.575588828018667
63 -3262.8191146850586 -19.371173219149373
64 -3067.626075744629 -19.27283461908258
65 -3248.268539428711 -18.719014759752064
66 -5289.332653999329 -18.569759364071594
67 -2939.051082611084 -18.444937879266227
68 -2990.6301193237305 -18.320486082826196
69 -2439.8641346096992 -18.111461264885516
70 -3116.0618438720703 -17.97883408563241
71 -4733.607089996338 -17.712051650475324
72 -2980.7767333984375 -17.520521069255306
73 -3034.7359466552734 -17.411051265630903
74 -2926.1791954040527 -17.128918849981133
75 -3018.2668991088867 -17.00358108195218
76 -2959.8060340881348 -16.905696797222333
77 -2675.152545928955 -16.621717371289915
78 -2643.8762168884277 -16.521846183497114
79 -5093.577561020851 -16.362756319978686
80 -4374.397008135915 -15.992539490672064
81 -2905.608877182007 -15.760386496482925
82 -2745.8486766815186 -15.568856258395003
83 -2836.771713256836 -15.425129236594607
84 -2799.9911727905273 -15.247941165092728
85 -2818.4152393341064 -15.070988514268866
86 -2871.984722137451 -14.877050142067818
87 -2631.960723876953 -14.54716900609245
88 -2592.667486190796 -14.280841199161697
89 -1975.1875040531158 -14.055631672914624
90 -2563.9446659088135 -13.912673913490881
91 -2453.6776332855225 -13.795052204151288
92 -2704.508813858032 -13.642972916797163
93 -3020.020994849503 -13.427781363912512
94 -2481.3660793304443 -12.97368260428777
95 -2337.439109802246 -12.69584727345253
96 -2359.8737354278564 -12.467641830071226
97 -2043.4705370664597 -12.225629065695466
98 -2369.196153640747 -11.916443960831456
99 -2387.9040718078613 -11.826451362122935
100 -2210.348735809326 -11.561489797853922
101 -2226.337152481079 -10.879146841354652
102 -2069.2336616516113 -10.519110388253992
103 -2242.490379333496 -10.261237947975806
104 -2186.0262994766235 -9.93487679192007
105 -1981.1283635795116 -9.775917521665287
106 -2288.864809036255 -9.55600806299503
107 -1988.0680837631226 -9.195641835826349
108 -2021.0461587905884 -9.120510366599436
109 -1995.723687171936 -8.948548379154271
110 -1943.300880432129 -8.58800559068593
111 -1793.642409324646 -7.882334949169341
112 -1768.9913730621338 -7.646655073165271
113 -1804.931001663208 -7.172231224265249
114 -1594.2990188598633 -6.866206190324398
115 -1366.884315609932 -6.51820418055673
116 -1233.3131118416786 -5.615796733870542
117 -1787.8076683729887 -5.027957977402961
118 -3156.8332862854004 -4.031048624093466
119 -1866.365596294403 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-63.80318709 -63.3366241  -62.32636225 -62.11891262 -62.09385094
 -60.81867582 -59.70279118 -59.22937913 -57.31000348 -56.40613711
 -56.00667963 -54.58281266 -53.49669433 -52.95249228 -52.44130672
 -51.49827622 -51.11086407 -50.30096989 -49.58899691 -49.40531233
 -49.11584688 -49.0308504  -48.97455067 -48.92057574 -48.8722857
 -48.72375071 -48.64352957 -48.37705553 -47.67489377 -47.42838367
 -47.25998585 -47.2425208  -47.23872936 -47.0078127  -46.95646515
 -46.57633633 -46.4945736  -46.32070559 -46.27584992 -45.89764164
 -45.83205788 -45.64929703 -45.11651962 -44.9775141  -44.96921644
 -44.93792323 -44.91874256 -44.85812386 -44.6942022  -44.57461633
 -44.46529881 -44.43890163 -44.34086798 -44.33222892 -44.30080477
 -44.14921993 -43.99737777 -43.76991783 -43.75035245 -43.58405711
 -43.583211   -43.57040515 -43.55484522 -43.47927697 -43.43739424
 -43.39691285 -43.32299468 -43.25038703 -43.22343613 -42.9211339
 -42.65748859 -42.6023149  -42.32863806 -42.30445768 -42.01922667
 -41.79397062 -41.76028168 -41.73236045 -41.12283699 -41.04740955
 -41.00233358 -40.75815407 -40.70441037 -40.6830923  -40.68198244
 -40.5368988  -40.45551398 -40.11647447 -40.03758738 -39.94940252
 -39.90846007 -39.57032589 -39.56432764 -39.51757874 -39.43200287
 -39.40560705 -39.31833047 -39.27893976 -39.17452637 -39.1467903
 -39.11210375 -39.0792282  -39.05727159 -39.01163027 -38.79806518
 -38.7342435  -38.46237505 -38.08627313 -37.94958159 -37.94295274
 -37.86129653 -37.30806553 -37.19161015 -37.11273927 -36.84565678
 -36.82607283 -36.79079441 -36.68197202 -36.66771719 -36.41661246
 -36.41407103 -36.41283474 -36.30795891 -36.25582613 -36.12759582
 -36.12354063 -36.03093985 -36.01221343 -35.8808435  -35.87850775
 -35.81241862 -35.76166082 -35.64630068 -35.55804095 -35.3916028
 -35.38181607 -35.26357048 -35.07068682 -34.97398625 -34.89002282
 -34.8333516  -34.72103491 -34.24472332 -34.2245602  -34.17113184
 -33.9121592  -33.78186494 -33.78094921 -33.68570214 -33.60031179
 -33.15069538 -33.05692584 -33.0430929  -32.94911634 -32.88474677
 -32.77451865 -32.73500825 -32.6375153  -32.54424446 -32.16692809
 -32.15852825 -32.11238423 -32.02667899 -32.01609901 -31.92157163
 -31.88097311 -31.83174162 -31.81015275 -31.77899769 -31.62183989
 -31.60878327 -31.54002065 -31.53427075 -31.48448158 -31.42615961
 -31.41405385 -31.39120229 -31.25348519 -31.24532504 -31.19074683
 -31.08237497 -31.03928417 -30.97108246 -30.96292656 -30.88599155
 -30.88274601 -30.66780209 -30.64673851 -30.63041943 -30.58615446
 -30.57769466 -30.47942104 -30.46740271 -30.44448455 -30.40290221
 -30.34087904 -30.28444942 -30.27355271 -30.23366087 -30.11697256
 -30.06112903 -30.04991568 -29.94285212 -29.89896946 -29.87212516
 -29.6257794  -29.44330333 -29.40604135 -29.25427735 -29.24266682
 -29.1352587  -29.04163947 -29.03147699 -28.88654216 -28.84050982
 -28.75264262 -28.71883348 -28.57862903 -28.52158527 -28.46381266
 -28.43871007 -28.43374877 -28.423883   -28.41403653 -28.40476165
 -28.21725614 -28.18256687 -28.16728518 -28.07394302 -28.05778299
 -27.98313886 -27.92485943 -27.92060294 -27.84124324 -27.7391148
 -27.71074732 -27.52346362 -27.48520133 -27.44520839 -27.43290873
 -27.39613093 -27.37044707 -27.29994968 -26.75229479 -26.5241416
 -26.12486254 -26.08804441 -26.0371964  -26.02593909 -25.95629251
 -25.93903491 -25.92997211 -25.90184838 -25.83954036 -25.80850534
 -25.67029819 -25.63290698 -25.48010655 -25.46109543 -25.35315466
 -25.27552177 -25.05212069 -24.99805283 -24.99453132 -24.93527205
 -24.76028834 -24.75646406 -24.39784273 -24.37443057 -24.31834004
 -24.18079375 -24.11517187 -23.99196367 -23.91672487 -23.90690057
 -23.40339926 -23.29102321 -23.26732263 -23.25983131 -23.18216776
 -23.14081539 -23.10346457 -22.99351351 -22.99145195 -22.70462412
 -22.67324122 -22.59530834 -22.54180655 -22.46012464 -22.40229356
 -22.33446619 -22.30606268 -22.25673726 -21.96148228 -21.95298519
 -21.85390446 -21.82040774 -21.80181114 -21.76498217 -21.7139277
 -21.70254077 -21.65652156 -21.55535273 -21.54121021 -21.51024177
 -21.45033209 -21.29665786 -21.16195827 -21.16139316 -21.1123623
 -21.10394014 -21.09906841 -21.07725225 -21.01442037 -20.86567352
 -20.85062394 -20.81751689 -20.74823789 -20.67630351 -20.62237898
 -20.59756139 -20.54556347 -20.52537316 -20.47438567 -20.42910041
 -20.31647497 -20.22059391 -20.21830275 -20.13612291 -20.05680887
 -20.02448568 -19.99188335 -19.92396275 -19.91294527 -19.72317042
 -19.72288085 -19.67998592 -19.67334465 -19.64378475 -19.63125909
 -19.62785987 -19.57558883 -19.56488968 -19.47623704 -19.46087966
 -19.37117322 -19.36139425 -19.33286499 -19.29215256 -19.27283462
 -19.26334551 -18.88847791 -18.84069557 -18.71901476 -18.69393092
 -18.64457019 -18.60398172 -18.56975936 -18.54098765 -18.53157795
 -18.47925607 -18.44493788 -18.36776933 -18.34306645 -18.34174785
 -18.32048608 -18.28056766 -18.24596032 -18.1296814  -18.11146126
 -18.03157237 -18.02173735 -17.98604931 -17.97883409 -17.92821999
 -17.89709525 -17.83710124 -17.71205165 -17.6663674  -17.6400464
 -17.58971226 -17.52052107 -17.5163234  -17.5074629  -17.42140315
 -17.41105127 -17.34565801 -17.31507509 -17.28107356 -17.12891885
 -17.11270888 -17.0853136  -17.06814743 -17.00358108 -16.95884322
 -16.92448838 -16.91139886 -16.9056968  -16.9023109  -16.80093549
 -16.66555987 -16.62171737 -16.59087236 -16.55182544 -16.52871441
 -16.52184618 -16.48862972 -16.41323646 -16.37683999 -16.36275632
 -16.26082515 -16.23311195 -16.2219063  -15.99253949 -15.88654317
 -15.82382906 -15.76723781 -15.7603865  -15.74782548 -15.74248829
 -15.67500454 -15.6039325  -15.56885626 -15.47844355 -15.45319442
 -15.43622815 -15.42512924 -15.41148028 -15.40471983 -15.33818265
 -15.24986438 -15.24794117 -15.23320986 -15.18047643 -15.0731901
 -15.0725541  -15.07098851 -14.97754501 -14.93353996 -14.88080703
 -14.87705014 -14.81510392 -14.68824709 -14.65847502 -14.58546977
 -14.54716901 -14.42808736 -14.42405808 -14.3412907  -14.2808412
 -14.26718087 -14.25836096 -14.07316975 -14.05563167 -13.99440709
 -13.96667208 -13.94485075 -13.91267391 -13.91164962 -13.89611176
 -13.8111931  -13.7950522  -13.72483683 -13.70937707 -13.69922275
 -13.66012516 -13.64297292 -13.48360994 -13.44138673 -13.42892048
 -13.42778136 -13.24314154 -13.17377374 -13.10148167 -13.00512726
 -12.9736826  -12.94759223 -12.93288364 -12.78326283 -12.69584727
 -12.62184173 -12.61940694 -12.55635232 -12.46764183 -12.4147629
 -12.39389755 -12.38426645 -12.2282058  -12.22562907 -12.20296896
 -12.05094112 -11.93281394 -11.91644396 -11.89735871 -11.84758642
 -11.8332835  -11.82645136 -11.811794   -11.73504365 -11.72950901
 -11.66958498 -11.5614898  -11.25290519 -11.20952246 -11.17072984
 -11.08984382 -11.00076037 -10.87914684 -10.78101609 -10.73370335
 -10.72946334 -10.51911039 -10.45257015 -10.39230164 -10.27293027
 -10.26123795 -10.12971059 -10.12327662 -10.03405912  -9.93487679
  -9.80011094  -9.78594151  -9.78495316  -9.77591752  -9.76513328
  -9.75576082  -9.74924816  -9.56682446  -9.55600806  -9.37974853
  -9.296719    -9.22081195  -9.19564184  -9.19326137  -9.17879346
  -9.13250985  -9.12051037  -9.11447385  -9.03223409  -8.96289504
  -8.94854838  -8.88311311  -8.70903202  -8.69330803  -8.66363489
  -8.58800559  -8.57908631  -8.34862647  -8.22646041  -8.15452418
  -7.88233495  -7.87256771  -7.84511562  -7.83212014  -7.76936259
  -7.64665507  -7.57539849  -7.48190562  -7.36244313  -7.35078227
  -7.17223122  -7.10832736  -7.07294326  -7.01704492  -6.96994429
  -6.95906356  -6.86620619  -6.77694649  -6.72206384  -6.71997062
  -6.67824961  -6.51820418  -6.28049573  -6.16980121  -6.0108133
  -5.97448073  -5.96957533  -5.84434748  -5.78773192  -5.61579673
  -5.38069736  -5.3472021   -5.33215005  -5.27395642  -5.07848501
  -5.02795798  -5.02782257  -4.93574335  -4.83888504  -4.82757292
  -4.63049542  -4.230832    -4.14612012  -4.12444636  -4.03104862
  -3.84288471  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 1.6531595611013471e-06, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.04715372085571289, val_acc 0.995
trigger times: 1
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 1.5134423828125, val_acc 0.995
trigger times: 1
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 11.1087841796875, val_acc 0.99
trigger times: 1
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 29.1504345703125, val_acc 0.98
trigger times: 1
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 2.2385986328125, val_acc 0.995
trigger times: 1
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.207890625, val_acc 0.995
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 4.9939404296875, val_acc 0.99
trigger times: 1
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 1.18334716796875, val_acc 0.995
trigger times: 1
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 9.52528564453125, val_acc 0.98
trigger times: 1
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 9.92112548828125, val_acc 0.98
trigger times: 1
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 2.134647216796875, val_acc 0.99
trigger times: 1
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.3913623046875, val_acc 0.995
trigger times: 1
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.00102457195520401, val_acc 1.0
trigger times: 1
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.1445074462890625, val_acc 0.995
trigger times: 1
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -3148.732925415039 -63.80318709008168
1 -3034.40287399292 -60.818675818191004
2 -2789.68861579895 -56.00667963477602
3 -2767.2742443084717 -51.498276222031166
4 -2610.9603416249156 -49.11584688036212
5 -4688.522193908691 -48.723750706751915
6 -2606.4477519989014 -47.2599858545806
7 -2733.5747604370117 -46.57633633290048
8 -2679.250150680542 -45.83205788373689
9 -2798.833240509033 -44.937923231812896
10 -4520.868691444397 -44.465298807522004
11 -4579.23736000061 -44.1492199315879
12 -2963.7659873962402 -43.583210996371044
13 -8667.29794549942 -43.39691284921414
14 -6477.098936080933 -42.65748859055198
15 -2468.748966217041 -41.760281676658025
16 -2332.011383011937 -40.758154070747366
17 -2814.291410446167 -40.45551397834559
18 -4430.166079998016 -39.57032589487153
19 -3038.8208698034286 -39.31833047231749
20 -3670.680353142321 -39.07922820344388
21 -3257.8588531017303 -38.46237505180561
22 -4191.227638721466 -37.308065525311996
23 -2690.8930587768555 -36.790794408728836
24 -2437.930333942175 -36.41283474392047
25 -2728.699993133545 -36.030939852349846
26 -3455.530808802694 -35.76166081977253
27 -3129.2606811523438 -35.263570479911
28 -2579.2284049987793 -34.72103490913417
29 -2694.418462753296 -33.781864939824544
30 -3075.5093307495117 -33.05692584182202
31 -2604.4744262695312 -32.73500825225695
32 -2811.9851482510567 -32.11238423004613
33 -2560.9701175689697 -31.831741617185198
34 -3022.7235231399536 -31.540020653059496
35 -3031.47859954834 -31.39120229078913
36 -2911.818745613098 -31.039284174982367
37 -4725.847343444824 -30.667802091332018
38 -2886.0183625221252 -30.47942104302843
39 -2975.9724979400635 -30.284449418850397
40 -2978.5099177360535 -30.04991568297847
41 -2953.911570072174 -29.44330333462347
42 -3004.638635635376 -29.041639465950247
43 -2934.278386592865 -28.718833479138844
44 -2052.512595653534 -28.433748774939595
45 -3241.9408016204834 -28.16728517571421
46 -3164.0018762499094 -27.920602937037046
47 -2438.575298309326 -27.48520132700644
48 -2145.2161517441273 -27.29994968172231
49 -2845.2040758132935 -26.037196400263454
50 -2879.0782623291016 -25.901848378876174
51 -2924.9603395462036 -25.48010655168999
52 -2733.1208860874176 -24.99805282601126
53 -2784.2337946891785 -24.397842726000693
54 -2872.919183731079 -23.991963674348654
55 -2076.556520462036 -23.267322625813648
56 -1964.8691520690918 -22.993513513255376
57 -1963.9352293014526 -22.541806546310404
58 -1898.1769678369164 -22.256737260461836
59 -1662.8234281539917 -21.801811144701382
60 -2705.7137978076935 -21.555352728681846
61 -1936.251009941101 -21.16195826952961
62 -1768.1024131774902 -21.077252246384205
63 -1920.8896036148071 -20.748237888399508
64 -1952.674892425537 -20.525373160240413
65 -1652.347583770752 -20.21830275130273
66 -1935.1397113800049 -19.923962746409735
67 -1658.1744585037231 -19.673344645442132
68 -1831.7700967788696 -19.56488968023022
69 -1660.5076923370361 -19.332864993187492
70 -1886.8985862731934 -18.84069557346339
71 -2059.888072013855 -18.569759364071594
72 -1599.2969808578491 -18.367769326754775
73 -1598.1113214492798 -18.2459603241156
74 -1584.1961936950684 -17.98604931010075
75 -1757.899114370346 -17.66636739843018
76 -1726.992696762085 -17.5074629010064
77 -1760.1977968215942 -17.281073564217284
78 -1757.8458185195923 -17.00358108195218
79 -1727.2499017715454 -16.902310903596288
80 -1429.633578300476 -16.551825437539033
81 -1342.1804609298706 -16.37683999173314
82 -1875.2902711629868 -15.992539490672064
83 -1440.16468334198 -15.747825477780923
84 -1496.670142173767 -15.478443549369333
85 -1412.6759538650513 -15.404719825644449
86 -1456.7860078811646 -15.180476434279045
87 -1716.0014657974243 -14.933539963174686
88 -1661.0917358398438 -14.658475017559288
89 -1184.4803385734558 -14.341290704560633
90 -1837.7606697678566 -14.055631672914624
91 -1858.53049325943 -13.911649622206243
92 -1331.7865800857544 -13.709377067589873
93 -1305.3644771575928 -13.441386734957018
94 -1735.381736755371 -13.101481669276232
95 -1621.8556337356567 -12.78326283489492
96 -1343.4473433494568 -12.467641830071226
97 -1494.3187520299107 -12.225629065695466
98 -1679.4030027389526 -11.897358709196734
99 -1400.940602183342 -11.735043647109059
100 -1576.7155723571777 -11.209522460705639
101 -1462.551454514265 -10.781016093040188
102 -1263.1826691627502 -10.392301643248208
103 -1505.0446367263794 -10.034059115444888
104 -1422.8381670383242 -9.775917521665287
105 -1225.4545872211456 -9.379748533508021
106 -1499.7003135681152 -9.178793461886615
107 -1129.671662569046 -8.962895036902655
108 -1566.3446636199951 -8.663634887347657
109 -1096.3350343937054 -8.154524177846461
110 -1161.2589993476868 -7.769362590670558
111 -1120.1250545978546 -7.35078226692049
112 -1072.1192948431708 -6.9699442883711376
113 -1195.8925516605377 -6.719970621583102
114 -1149.4201955795288 -6.010813304204734
115 -1990.9562351107597 -5.615796733870542
116 -1377.939462736249 -5.078485007852753
117 -1117.9594860166544 -4.827572916892203
118 -1319.6583204492927 -4.031048624093466
119 -851.0052730590105 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-63.80318709 -63.3366241  -62.32636225 -62.11891262 -62.09385094
 -60.81867582 -59.70279118 -59.22937913 -57.31000348 -56.40613711
 -56.00667963 -54.58281266 -53.49669433 -52.95249228 -52.44130672
 -51.49827622 -51.11086407 -50.30096989 -49.58899691 -49.40531233
 -49.11584688 -49.0308504  -48.97455067 -48.92057574 -48.8722857
 -48.72375071 -48.64352957 -48.37705553 -47.67489377 -47.42838367
 -47.25998585 -47.2425208  -47.23872936 -47.0078127  -46.95646515
 -46.57633633 -46.4945736  -46.32070559 -46.27584992 -45.89764164
 -45.83205788 -45.64929703 -45.11651962 -44.9775141  -44.96921644
 -44.93792323 -44.91874256 -44.85812386 -44.6942022  -44.57461633
 -44.46529881 -44.43890163 -44.34086798 -44.33222892 -44.30080477
 -44.14921993 -43.99737777 -43.76991783 -43.75035245 -43.58405711
 -43.583211   -43.57040515 -43.55484522 -43.47927697 -43.43739424
 -43.39691285 -43.32299468 -43.25038703 -43.22343613 -42.9211339
 -42.65748859 -42.6023149  -42.32863806 -42.30445768 -42.01922667
 -41.79397062 -41.76028168 -41.73236045 -41.12283699 -41.04740955
 -41.00233358 -40.75815407 -40.70441037 -40.6830923  -40.68198244
 -40.5368988  -40.45551398 -40.11647447 -40.03758738 -39.94940252
 -39.90846007 -39.57032589 -39.56432764 -39.51757874 -39.43200287
 -39.40560705 -39.31833047 -39.27893976 -39.17452637 -39.1467903
 -39.11210375 -39.0792282  -39.05727159 -39.01163027 -38.79806518
 -38.7342435  -38.46237505 -38.08627313 -37.94958159 -37.94295274
 -37.86129653 -37.30806553 -37.19161015 -37.11273927 -36.84565678
 -36.82607283 -36.79079441 -36.68197202 -36.66771719 -36.41661246
 -36.41407103 -36.41283474 -36.30795891 -36.25582613 -36.12759582
 -36.12354063 -36.03093985 -36.01221343 -35.8808435  -35.87850775
 -35.81241862 -35.76166082 -35.64630068 -35.55804095 -35.3916028
 -35.38181607 -35.26357048 -35.07068682 -34.97398625 -34.89002282
 -34.8333516  -34.72103491 -34.24472332 -34.2245602  -34.17113184
 -33.9121592  -33.78186494 -33.78094921 -33.68570214 -33.60031179
 -33.15069538 -33.05692584 -33.0430929  -32.94911634 -32.88474677
 -32.77451865 -32.73500825 -32.6375153  -32.54424446 -32.16692809
 -32.15852825 -32.11238423 -32.02667899 -32.01609901 -31.92157163
 -31.88097311 -31.83174162 -31.81015275 -31.77899769 -31.62183989
 -31.60878327 -31.54002065 -31.53427075 -31.48448158 -31.42615961
 -31.41405385 -31.39120229 -31.25348519 -31.24532504 -31.19074683
 -31.08237497 -31.03928417 -30.97108246 -30.96292656 -30.88599155
 -30.88274601 -30.66780209 -30.64673851 -30.63041943 -30.58615446
 -30.57769466 -30.47942104 -30.46740271 -30.44448455 -30.40290221
 -30.34087904 -30.28444942 -30.27355271 -30.23366087 -30.11697256
 -30.06112903 -30.04991568 -29.94285212 -29.89896946 -29.87212516
 -29.6257794  -29.44330333 -29.40604135 -29.25427735 -29.24266682
 -29.1352587  -29.04163947 -29.03147699 -28.88654216 -28.84050982
 -28.75264262 -28.71883348 -28.57862903 -28.52158527 -28.46381266
 -28.43871007 -28.43374877 -28.423883   -28.41403653 -28.40476165
 -28.21725614 -28.18256687 -28.16728518 -28.07394302 -28.05778299
 -27.98313886 -27.92485943 -27.92060294 -27.84124324 -27.7391148
 -27.71074732 -27.52346362 -27.48520133 -27.44520839 -27.43290873
 -27.39613093 -27.37044707 -27.29994968 -26.75229479 -26.5241416
 -26.12486254 -26.08804441 -26.0371964  -26.02593909 -25.95629251
 -25.93903491 -25.92997211 -25.90184838 -25.83954036 -25.80850534
 -25.67029819 -25.63290698 -25.48010655 -25.46109543 -25.35315466
 -25.27552177 -25.05212069 -24.99805283 -24.99453132 -24.93527205
 -24.76028834 -24.75646406 -24.39784273 -24.37443057 -24.31834004
 -24.18079375 -24.11517187 -23.99196367 -23.91672487 -23.90690057
 -23.40339926 -23.29102321 -23.26732263 -23.25983131 -23.18216776
 -23.14081539 -23.10346457 -22.99351351 -22.99145195 -22.70462412
 -22.67324122 -22.59530834 -22.54180655 -22.46012464 -22.40229356
 -22.33446619 -22.30606268 -22.25673726 -21.96148228 -21.95298519
 -21.89291965 -21.85390446 -21.82040774 -21.80181114 -21.76498217
 -21.73172368 -21.7139277  -21.70254077 -21.6626587  -21.65652156
 -21.55535273 -21.54121021 -21.51024177 -21.45033209 -21.37876346
 -21.29665786 -21.21894581 -21.16195827 -21.16139316 -21.1123623
 -21.10394014 -21.09906841 -21.07725225 -21.01442037 -21.0127672
 -20.86567352 -20.85062394 -20.84841269 -20.81751689 -20.74823789
 -20.73520359 -20.67630351 -20.62237898 -20.59756139 -20.54556347
 -20.52537316 -20.47438567 -20.42910041 -20.31647497 -20.29996303
 -20.22059391 -20.21830275 -20.13612291 -20.05680887 -20.02448568
 -19.99188335 -19.92396275 -19.91294527 -19.87718738 -19.78268322
 -19.72317042 -19.72288085 -19.67998592 -19.67334465 -19.64378475
 -19.63125909 -19.6291884  -19.62785987 -19.5890299  -19.57558883
 -19.56488968 -19.48929253 -19.47623704 -19.46087966 -19.37117322
 -19.36139425 -19.33286499 -19.29215256 -19.27574854 -19.27283462
 -19.26334551 -19.13705707 -19.04564203 -18.88847791 -18.84069557
 -18.74021155 -18.73855242 -18.71901476 -18.71455163 -18.69393092
 -18.64457019 -18.60398172 -18.56975936 -18.54098765 -18.53157795
 -18.47925607 -18.44493788 -18.37899113 -18.36776933 -18.34306645
 -18.34174785 -18.32048608 -18.28056766 -18.24596032 -18.22025421
 -18.19943377 -18.1296814  -18.11146126 -18.03157237 -18.02173735
 -17.98604931 -17.97883409 -17.92821999 -17.89709525 -17.83710124
 -17.71205165 -17.6663674  -17.6400464  -17.61253133 -17.58971226
 -17.52052107 -17.5163234  -17.5074629  -17.46790236 -17.42140315
 -17.41105127 -17.34779774 -17.34565801 -17.31507509 -17.28107356
 -17.24389616 -17.12891885 -17.11270888 -17.09611456 -17.09182935
 -17.0853136  -17.06814743 -17.00701061 -17.00358108 -16.96495757
 -16.95884322 -16.92448838 -16.91139886 -16.9056968  -16.9023109
 -16.84614175 -16.80093549 -16.77325927 -16.76784757 -16.73878665
 -16.73761805 -16.70934981 -16.69565572 -16.66555987 -16.62171737
 -16.59087236 -16.57932864 -16.55182544 -16.52871441 -16.52184618
 -16.48862972 -16.41323646 -16.37683999 -16.3653141  -16.36275632
 -16.33499579 -16.32444958 -16.26082515 -16.23311195 -16.2219063
 -16.03771155 -16.00060067 -15.99253949 -15.95139926 -15.88654317
 -15.82382906 -15.76723781 -15.7603865  -15.74782548 -15.74248829
 -15.70165283 -15.67500454 -15.66804236 -15.64901575 -15.6039325
 -15.56885626 -15.554956   -15.47844355 -15.45319442 -15.43622815
 -15.42512924 -15.41148028 -15.40471983 -15.34779365 -15.33818265
 -15.33233931 -15.24986438 -15.24794117 -15.23320986 -15.22904472
 -15.18047643 -15.0731901  -15.0725541  -15.07098851 -15.05828745
 -14.97754501 -14.93353996 -14.88080703 -14.87705014 -14.81510392
 -14.68824709 -14.65847502 -14.58546977 -14.54716901 -14.42808736
 -14.42405808 -14.3412907  -14.30576953 -14.2808412  -14.26718087
 -14.25836096 -14.25411501 -14.07316975 -14.05563167 -14.0453968
 -13.99440709 -13.96667208 -13.95333014 -13.94485075 -13.91267391
 -13.91164962 -13.89611176 -13.8111931  -13.7950522  -13.77550193
 -13.72483683 -13.70937707 -13.70429182 -13.69922275 -13.66012516
 -13.64297292 -13.62201779 -13.49451183 -13.48360994 -13.44138673
 -13.42892048 -13.42778136 -13.24314154 -13.21686412 -13.19160775
 -13.17377374 -13.15144815 -13.14819899 -13.10148167 -13.00512726
 -12.9736826  -12.94759223 -12.93288364 -12.82623163 -12.78326283
 -12.71043333 -12.69584727 -12.68244355 -12.62184173 -12.61940694
 -12.55635232 -12.53263328 -12.46764183 -12.45807201 -12.45734581
 -12.43286961 -12.4147629  -12.407759   -12.39850695 -12.39389755
 -12.38426645 -12.35917023 -12.32567677 -12.28878779 -12.2282058
 -12.22562907 -12.20296896 -12.05094112 -11.93281394 -11.91644396
 -11.89735871 -11.84758642 -11.8332835  -11.82645136 -11.82576782
 -11.811794   -11.75183538 -11.73504365 -11.72950901 -11.66958498
 -11.5614898  -11.25290519 -11.20952246 -11.19582105 -11.18329788
 -11.17072984 -11.08984382 -11.00076037 -10.87914684 -10.78101609
 -10.73370335 -10.72946334 -10.70077592 -10.51911039 -10.45257015
 -10.44405057 -10.39230164 -10.38156234 -10.3398939  -10.29953265
 -10.27293027 -10.26123795 -10.24344303 -10.12971059 -10.12327662
 -10.08398225 -10.03405912  -9.93487679  -9.80011094  -9.79156397
  -9.78594151  -9.78495316  -9.77591752  -9.77399931  -9.76513328
  -9.75576082  -9.74924816  -9.60699104  -9.56682446  -9.55600806
  -9.51793734  -9.37974853  -9.296719    -9.2631713   -9.22081195
  -9.19564184  -9.19326137  -9.17879346  -9.13250985  -9.13135323
  -9.12051037  -9.11447385  -9.03223409  -8.96289504  -8.94854838
  -8.91147673  -8.88311311  -8.87834848  -8.82133542  -8.70903202
  -8.69330803  -8.66363489  -8.58800559  -8.57908631  -8.5130687
  -8.3981916   -8.35849704  -8.34862647  -8.30436036  -8.22646041
  -8.15452418  -7.92036573  -7.88233495  -7.87256771  -7.84511562
  -7.83212014  -7.76936259  -7.64665507  -7.57539849  -7.48190562
  -7.36244313  -7.35078227  -7.25609682  -7.17223122  -7.10832736
  -7.09668492  -7.07294326  -7.01704492  -6.9870487   -6.98558297
  -6.96994429  -6.95906356  -6.86620619  -6.77694649  -6.72206384
  -6.71997062  -6.67824961  -6.62348143  -6.58618666  -6.54175437
  -6.51820418  -6.44628034  -6.31547248  -6.28049573  -6.19797146
  -6.16980121  -6.0108133   -5.97448073  -5.96957533  -5.84589661
  -5.84434748  -5.78773192  -5.61579673  -5.38069736  -5.3472021
  -5.33215005  -5.27395642  -5.19827638  -5.07848501  -5.02795798
  -5.02782257  -4.99259565  -4.93574335  -4.84277448  -4.83888504
  -4.82757292  -4.63049542  -4.57112457  -4.53236601  -4.40979513
  -4.36957434  -4.230832    -4.14612012  -4.12444636  -4.03104862
  -3.84288471  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 2.1277035120874643e-05, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 4.767885839100927e-06, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.133372802734375, val_acc 0.995
trigger times: 1
end of epoch 12: val_loss 0.025985989570617676, val_acc 0.995
trigger times: 2
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 14.93690673828125, val_acc 0.975
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 17.65917724609375, val_acc 0.975
trigger times: 1
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 2.128189697265625, val_acc 0.995
trigger times: 1
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.8016134614318737, val_acc 0.91
trigger times: 1
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 8.9966015625, val_acc 0.985
trigger times: 1
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 1.34401611328125, val_acc 0.995
trigger times: 1
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.06239625930786133, val_acc 0.995
trigger times: 1
end of epoch 68: val_loss 0.7875927734375, val_acc 0.995
trigger times: 2
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 1.359415283203125, val_acc 0.99
trigger times: 1
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.08496475219726562, val_acc 0.995
trigger times: 1
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 4.623782348632813, val_acc 0.99
trigger times: 1
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 1.375595703125, val_acc 0.995
trigger times: 1
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.25397552490234376, val_acc 0.995
trigger times: 1
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -3181.8324675559998 -63.80318709008168
1 -2945.034034729004 -59.70279118339014
2 -4982.574502944946 -53.49669433343748
3 -4624.79939365387 -49.5889969093609
4 -1377.418846487999 -48.87228569913625
5 -2923.333099126816 -47.2599858545806
6 -5214.255376815796 -46.494573602860655
7 -4268.552065849304 -45.11651962392763
8 -5431.217723846436 -44.694202198088234
9 -3448.962439060211 -44.300804769361186
10 -5257.985876083374 -43.583210996371044
11 -4011.0017709732056 -43.32299468375923
12 -9364.31389439106 -42.30445768311566
13 -5624.384368896484 -41.04740954562004
14 -4832.522829055786 -40.536898802842494
15 -3437.9230236262083 -39.57032589487153
16 -7869.087317347527 -39.27893976130787
17 -6130.453762054443 -39.011630271808535
18 -6368.250957489014 -37.94295273950308
19 -4861.273246765137 -36.826072831484076
20 -2706.0287780165672 -36.41283474392047
21 -1310.937568962574 -36.012213433621724
22 -1199.9176995754242 -35.558040948837615
23 -909.7038238048553 -34.89002281849731
24 -4528.655641555786 -33.91215919744923
25 -1125.712299823761 -33.05692584182202
26 -942.2301578521729 -32.63751529675296
27 -5250.72815322876 -32.016099007020365
28 -969.8463335037231 -31.62183989395988
29 -657.1394457221031 -31.41405385246766
30 -550.5583798289299 -31.039284174982367
31 -1007.0194829702377 -30.646738514472947
32 -977.134527683258 -30.444484547903063
33 -5147.76215171814 -30.11697256458485
34 -732.1502218842506 -29.625779396565527
35 -881.5224497318268 -29.041639465950247
36 -668.8408617973328 -28.521585268361733
37 -797.8536511659622 -28.404761654728503
38 -898.0253267288208 -27.983138860935668
39 -898.1978738307953 -27.523463615306014
40 -949.4213083982468 -27.29994968172231
41 -789.5308481454849 -26.02593908725448
42 -698.9754315018654 -25.80850534220451
43 -606.241245508194 -25.27552177125189
44 -4671.847864151001 -24.756464064783835
45 -3544.9997828006744 -23.991963674348654
46 -424.13972568511963 -23.25983130699908
47 -426.28534841537476 -22.704624120014362
48 -377.48114132881165 -22.334466186168438
49 -347.5875691175461 -21.85390446342082
50 -656.0085118412971 -21.702540774014164
51 -332.8607156276703 -21.450332094804114
52 -327.00708091259 -21.112362300093853
53 -327.1457278728485 -20.865673524931356
54 -377.24221873283386 -20.676303506871673
55 -287.22910141944885 -20.429100406889887
56 -279.6841368675232 -20.056808871320076
57 -311.3172342777252 -19.782683224876678
58 -261.5011547803879 -19.63125908816738
59 -347.6545341014862 -19.489292528168903
60 -320.2151782512665 -19.27574853629232
61 -305.20593428611755 -18.84069557346339
62 -281.54967308044434 -18.64457019144472
63 -259.4521563053131 -18.444937879266227
64 -230.8978841304779 -18.28056766443594
65 -198.4093052148819 -18.03157237123006
66 -240.5706194639206 -17.837101235057677
67 -205.9642938375473 -17.520521069255306
68 -270.51812958717346 -17.347797738164388
69 -203.7282919883728 -17.112708877338502
70 -197.36161708831787 -17.00358108195218
71 -241.55086648464203 -16.902310903596288
72 -215.53447544574738 -16.737618047565153
73 -282.3132643699646 -16.57932864373118
74 -172.1701157093048 -16.37683999173314
75 -171.83515226840973 -16.233111952108505
76 -245.020268201828 -15.886543171985064
77 -233.2606840133667 -15.701652832074387
78 -185.13869726657867 -15.554956003655427
79 -168.08493435382843 -15.404719825644449
80 -197.41747879981995 -15.23320986201785
81 -148.22952151298523 -15.058287448894124
82 -712.5446935892105 -14.688247089412398
83 -119.96355497837067 -14.341290704560633
84 -615.9522094726562 -14.055631672914624
85 -132.99556398391724 -13.912673913490881
86 -182.6410449743271 -13.724836828602397
87 -214.73424816131592 -13.622017789496521
88 -131.00408101081848 -13.243141543194646
89 -175.15894854068756 -13.101481669276232
90 -133.5542265176773 -12.78326283489492
91 -123.57294702529907 -12.556352315089619
92 -143.51117777824402 -12.414762899143213
93 -154.40758228302002 -12.32567676564739
94 -159.83357179164886 -11.932813944092088
95 -183.5795156955719 -11.825767816759772
96 -125.01364934444427 -11.561489797853922
97 -152.3332941532135 -11.089843818371676
98 -157.92408990859985 -10.700775915056283
99 -125.38117957115173 -10.33989390013824
100 -119.52441674470901 -10.12327661956338
101 -405.9381224513054 -9.785941512083282
102 -105.57918804883957 -9.74924816112743
103 -110.65731728076935 -9.296719001882565
104 -107.9147617816925 -9.132509854664788
105 -108.54140973091125 -8.948548379154271
106 -116.42174351215363 -8.693308028234105
107 -113.92955207824707 -8.358497042085745
108 -118.45517539978027 -7.87256771417772
109 -135.63271975517273 -7.481905622161368
110 -109.669753074646 -7.096684915925878
111 -176.16110545396805 -6.959063561385431
112 -106.7551738023758 -6.623481432290237
113 -95.13400590419769 -6.280495733893076
114 -112.49855661392212 -5.845896608304209
115 -96.72133600711823 -5.3321500505588375
116 -106.17689716815948 -4.992595648859268
117 -104.63450336456299 -4.571124567477852
118 -109.59335565567017 -4.124446362432144
119 -144.58505475521088 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

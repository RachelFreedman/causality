[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.7139277  -21.70254077
 -19.72288085 -19.26334551 -18.56975936 -18.11146126 -17.71205165
 -17.6663674  -17.58971226 -16.59087236 -16.36275632 -15.99253949
 -15.33818265 -14.68824709 -14.05563167 -13.91164962 -13.42778136
 -12.61940694 -12.22562907 -11.73504365 -10.78101609 -10.72946334
  -9.78594151  -9.77591752  -9.56682446  -9.19326137  -7.57539849
  -7.36244313  -7.10832736  -7.07294326  -6.95906356  -6.77694649
  -6.72206384  -6.71997062  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 3.768319396972656, val_acc 0.99
trigger times: 0
saving model weights...
end of epoch 1: val_loss 1.3511459110304712e-06, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 4.02843994140625, val_acc 0.995
trigger times: 1
end of epoch 3: val_loss 1.187244873046875, val_acc 0.99
trigger times: 2
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 1.22584716796875, val_acc 0.995
trigger times: 1
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 36.35847900390625, val_acc 0.975
trigger times: 1
end of epoch 12: val_loss 2.385361328125, val_acc 0.99
trigger times: 2
end of epoch 13: val_loss 2.3657275390625, val_acc 0.99
trigger times: 3
end of epoch 14: val_loss 3.522510451148264e-07, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0033762195706367494, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 17.255419921875, val_acc 0.99
trigger times: 2
end of epoch 18: val_loss 0.78340087890625, val_acc 0.995
trigger times: 3
end of epoch 19: val_loss 0.652529296875, val_acc 0.995
trigger times: 4
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.1316845703125, val_acc 0.995
trigger times: 1
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 9.795439453125, val_acc 0.995
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 6.792587890625, val_acc 0.99
trigger times: 1
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 2.6099365234375, val_acc 0.995
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.5593505859375, val_acc 0.995
trigger times: 1
end of epoch 50: val_loss 1.5626806640625, val_acc 0.995
trigger times: 2
end of epoch 51: val_loss 1.15087158203125, val_acc 0.995
trigger times: 3
end of epoch 52: val_loss 0.501181640625, val_acc 0.995
trigger times: 4
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 1.19989501953125, val_acc 0.995
trigger times: 1
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.10483642578125, val_acc 0.995
trigger times: 1
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 2.38469482421875, val_acc 0.995
trigger times: 1
end of epoch 62: val_loss 3.3050048828125, val_acc 0.995
trigger times: 2
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 12.726279296875, val_acc 0.98
trigger times: 1
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 1.7043359375, val_acc 0.995
trigger times: 1
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 2.391669921875, val_acc 0.995
trigger times: 1
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 3.347412109375, val_acc 0.995
trigger times: 1
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 11.269658203125, val_acc 0.995
trigger times: 1
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 8.298798828125, val_acc 0.995
trigger times: 1
end of epoch 85: val_loss 2.09345703125, val_acc 0.995
trigger times: 2
end of epoch 86: val_loss 1.6495068359375, val_acc 0.995
trigger times: 3
end of epoch 87: val_loss 7.105187396518886e-06, val_acc 1.0
trigger times: 4
end of epoch 88: val_loss 0.899111328125, val_acc 0.995
trigger times: 5
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 10.055595703125, val_acc 0.98
trigger times: 1
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 21.45252197265625, val_acc 0.975
trigger times: 1
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -9701.454798698425 -51.11086407377863
1 -10425.246047973633 -49.11584688036212
2 -14188.529258728027 -48.97455066650668
3 -26181.392150878906 -48.87228569913625
4 -12280.073570251465 -48.723750706751915
5 -9352.881769016385 -47.242520801549375
6 -14636.651725769043 -44.969216444406115
7 -23795.70656967163 -44.85812386141036
8 -10649.770330429077 -44.465298807522004
9 -17244.336059570312 -44.438901626703945
10 -13080.681694030762 -44.1492199315879
11 -7541.269342422485 -43.750352447059115
12 -10200.118850708008 -43.58405711344297
13 -18745.69968032837 -43.39691284921414
14 -8534.018283843994 -43.22343612991455
15 -9446.575954437256 -42.92113389854083
16 -14187.717681884766 -42.65748859055198
17 -8395.353942871094 -42.60231490040316
18 -8068.993999481201 -42.3286380570152
19 -13562.394760131836 -42.30445768311566
20 -17195.295471191406 -41.12283699063089
21 -7992.651664733887 -40.758154070747366
22 -10702.029098510742 -40.70441036508885
23 -11146.997428894043 -40.037587376429016
24 -13688.981914388016 -39.90846007278091
25 -11431.792469024658 -39.57032589487153
26 -10092.979919433594 -39.31833047231749
27 -13190.587982177734 -39.27893976130787
28 -13913.938468933105 -39.11210375046212
29 -9443.579532623291 -39.07922820344388
30 -24944.212593078613 -38.73424350253481
31 -9621.53396987915 -38.46237505180561
32 -6603.46008014679 -37.86129653169397
33 -9734.626815795898 -37.308065525311996
34 -9476.593978881836 -36.41283474392047
35 -9205.088485717773 -36.307958906842664
36 -7978.626497268677 -36.12354062747346
37 -9419.645133972168 -36.012213433621724
38 -8118.415601506829 -35.76166081977253
39 -6199.874401092529 -35.558040948837615
40 -11110.38858795166 -34.83335159619024
41 -15781.222591400146 -34.24472332102743
42 -7505.06270888506 -33.78094921339959
43 -6294.442531585693 -32.949116337767926
44 -8258.049830198288 -32.774518645207735
45 -7457.613475799561 -32.11238423004613
46 -8291.860065460205 -31.534270745249735
47 -17294.752040863037 -30.667802091332018
48 -6882.137184381485 -30.402902212388465
49 -8056.744348526001 -30.233660870125824
50 -7554.954924823134 -29.87212515537898
51 -7087.308151245117 -29.40604134962864
52 -9042.459190368652 -29.13525869956722
53 -7014.246133804321 -28.84050982317397
54 -6704.2058753967285 -28.578629026453186
55 -10223.822620391846 -28.438710074595054
56 -8097.164968490601 -28.433748774939595
57 -9031.578929901123 -28.16728517571421
58 -7885.824478149414 -27.920602937037046
59 -7149.3926409538835 -27.739114797604373
60 -6115.6147384643555 -27.29994968172231
61 -5379.587709836662 -26.752294794786035
62 -7252.86075592041 -25.939034905087084
63 -6944.959826469421 -25.929972112830644
64 -8080.685219697654 -25.901848378876174
65 -5597.17004776001 -24.374430568602165
66 -5496.105017662048 -24.180793753972072
67 -8206.840789794922 -24.11517186961488
68 -7463.579593658447 -23.991963674348654
69 -6681.653953552246 -23.291023207711152
70 -8208.641681671143 -22.595308335760205
71 -6047.782547235489 -22.402293560723127
72 -6841.300617218018 -22.256737260461836
73 -5531.335740875453 -21.713927702452338
74 -5231.143287658691 -21.702540774014164
75 -6490.731404781342 -19.722880852611173
76 -5389.050178527832 -19.26334550647668
77 -7409.841701507568 -18.569759364071594
78 -4427.743787433952 -18.111461264885516
79 -7197.047760009766 -17.712051650475324
80 -5548.154240384698 -17.66636739843018
81 -5138.0316768065095 -17.589712263068826
82 -4483.558006644249 -16.590872364965673
83 -7556.437913894653 -16.362756319978686
84 -6575.934131249785 -15.992539490672064
85 -4924.660526275635 -15.338182650137867
86 -6366.567371688783 -14.688247089412398
87 -4929.122516902164 -14.055631672914624
88 -4866.523281097412 -13.911649622206243
89 -5100.277872085571 -13.427781363912512
90 -5661.606394276023 -12.619406941033082
91 -3849.382046174258 -12.225629065695466
92 -5515.313436985016 -11.735043647109059
93 -5348.987051010132 -10.781016093040188
94 -5831.869179163361 -10.729463335147118
95 -4347.2879276275635 -9.785941512083282
96 -3292.4687776230276 -9.775917521665287
97 -3907.7498009204865 -9.56682446099046
98 -3784.436779022217 -9.193261368948118
99 -4537.966490093502 -7.57539849177145
100 -4126.222017565247 -7.362443126623615
101 -5402.6555085629225 -7.108327355338034
102 -3387.8840498924255 -7.072943263247867
103 -3833.565908243414 -6.959063561385431
104 -5037.743781269062 -6.776946485018116
105 -4294.601332036429 -6.7220638398623045
106 -5014.14189982973 -6.719970621583102
107 -4069.570559556596 -6.51820418055673
108 -3918.0387278683484 -5.615796733870542
109 -4420.722724914551 -5.34720210027791
110 -3238.49447105499 -5.078485007852753
111 -3581.35914817499 -5.027957977402961
112 -3650.4125213623047 -4.827572916892203
113 -2916.766743024811 -4.63049541560991
114 -3494.474853515625 -4.230832004686763
115 -3973.8372077941895 -4.031048624093466
116 -3158.0144481658936 -3.3844671463622564
117 -3678.649669647217 -3.3322555012187633
118 -2888.1642055511475 -2.6416623314910934
119 -3245.203052520752 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.9069465  -21.86797946
 -21.7139277  -21.70254077 -21.32057624 -20.8667639  -20.59517611
 -20.50682666 -20.41630544 -20.31414165 -20.11746241 -20.10285427
 -19.97336864 -19.81651812 -19.80835739 -19.77955428 -19.72288085
 -19.52602929 -19.26334551 -19.25924556 -19.16219437 -19.00306686
 -18.97010249 -18.60063345 -18.56975936 -18.45932994 -18.45352357
 -18.42398901 -18.11146126 -18.07407101 -17.97085933 -17.95472992
 -17.85723948 -17.75662413 -17.74546736 -17.71205165 -17.6663674
 -17.63387173 -17.58971226 -17.35168639 -16.97734474 -16.95820405
 -16.8515998  -16.75809937 -16.60817421 -16.59087236 -16.57896001
 -16.54174682 -16.53255878 -16.44469859 -16.36275632 -16.33256004
 -16.15526479 -16.13255686 -16.02801337 -15.99253949 -15.95238892
 -15.85583061 -15.79448428 -15.77894493 -15.7756701  -15.60668786
 -15.52907632 -15.49679341 -15.43704736 -15.33818265 -15.19473948
 -14.98784175 -14.92610381 -14.87437472 -14.80977971 -14.78839874
 -14.74223806 -14.68824709 -14.47499777 -14.34019064 -14.18738651
 -14.09599742 -14.05563167 -14.04039701 -13.91164962 -13.82783661
 -13.82353566 -13.79997294 -13.59728494 -13.42778136 -13.35178405
 -13.33215494 -13.04834322 -12.88490566 -12.62088822 -12.61940694
 -12.39464435 -12.38250073 -12.3770637  -12.23072325 -12.22844555
 -12.22562907 -12.13672894 -12.13075436 -12.00890873 -11.9663894
 -11.80006188 -11.76184543 -11.73504365 -11.7033745  -11.62039073
 -11.56850502 -11.35871475 -11.31023635 -11.23362438 -10.78101609
 -10.76731367 -10.72946334 -10.58452923 -10.36663524 -10.213012
 -10.20807029 -10.10570998  -9.78594151  -9.77591752  -9.76647909
  -9.69718478  -9.56682446  -9.51177255  -9.36231491  -9.35840558
  -9.19326137  -9.09080694  -8.84291363  -8.63986884  -8.45273098
  -8.32171026  -8.27993494  -8.10693589  -8.09172333  -8.02526319
  -7.79278439  -7.76570574  -7.72198686  -7.57539849  -7.36244313
  -7.10832736  -7.07294326  -6.98041139  -6.95906356  -6.77694649
  -6.72206384  -6.71997062  -6.51820418  -6.26984272  -6.01511038
  -5.96733015  -5.67537604  -5.61579673  -5.59092715  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.0327778007866409, val_acc 0.985
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 2.9567081946879626e-06, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.04497200906219405, val_acc 0.995
trigger times: 2
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 6.885560216221673, val_acc 0.955
trigger times: 1
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 13.676402587890625, val_acc 0.96
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 2.1585652139037847e-05, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 2.3972648195922376e-05, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 2.667979118905496, val_acc 0.985
trigger times: 2
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 138.7639013671875, val_acc 0.945
trigger times: 1
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 81.82239990234375, val_acc 0.955
trigger times: 1
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.930341796875, val_acc 0.99
trigger times: 1
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.18063232421875, val_acc 0.995
trigger times: 1
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 2.781990967988968, val_acc 0.985
trigger times: 1
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.31805419921875, val_acc 0.995
trigger times: 1
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 4.07736328125, val_acc 0.985
trigger times: 1
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 1.26413330078125, val_acc 0.995
trigger times: 1
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.649619140625, val_acc 0.995
trigger times: 1
end of epoch 83: val_loss 2.980223371196189e-08, val_acc 1.0
trigger times: 2
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.55895751953125, val_acc 0.995
trigger times: 1
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0764404296875, val_acc 0.995
trigger times: 1
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -8564.92198753357 -51.11086407377863
1 -14743.941040039062 -48.97455066650668
2 -10999.655990600586 -48.723750706751915
3 -13501.96295928955 -44.969216444406115
4 -12632.160838127136 -44.465298807522004
5 -11586.58639717102 -44.1492199315879
6 -9194.478183746338 -43.58405711344297
7 -8251.011459350586 -43.22343612991455
8 -14499.740135192871 -42.65748859055198
9 -8468.867809772491 -42.3286380570152
10 -17389.599853515625 -41.12283699063089
11 -10548.877235412598 -40.70441036508885
12 -12119.886512756348 -39.90846007278091
13 -11631.250354766846 -39.31833047231749
14 -13515.473190307617 -39.11210375046212
15 -30679.73999786377 -38.73424350253481
16 -8093.708027124405 -37.86129653169397
17 -8464.026676177979 -36.41283474392047
18 -8789.181289672852 -36.12354062747346
19 -10732.89344406128 -35.76166081977253
20 -9707.172409057617 -34.83335159619024
21 -10125.124542236328 -33.78094921339959
22 -7690.013967514038 -32.774518645207735
23 -8539.886337280273 -31.534270745249735
24 -7535.496873855591 -30.402902212388465
25 -7849.315837860107 -29.87212515537898
26 -7417.457855224609 -29.13525869956722
27 -7008.043830871582 -28.578629026453186
28 -7873.654438972473 -28.433748774939595
29 -8906.174198150635 -27.920602937037046
30 -6628.542392730713 -27.29994968172231
31 -6638.153936386108 -25.939034905087084
32 -9909.87883758545 -25.901848378876174
33 -6938.056306838989 -24.180793753972072
34 -8913.463897705078 -23.991963674348654
35 -8989.181800842285 -22.595308335760205
36 -7992.184700012207 -22.256737260461836
37 -5619.361129760742 -21.86797945995538
38 -5624.098163604736 -21.702540774014164
39 -5659.260726928711 -20.86676390075848
40 -5679.435981750488 -20.50682665824434
41 -5600.1025314331055 -20.314141646730853
42 -5501.492263793945 -20.102854270660917
43 -5581.877716064453 -19.816518116204254
44 -5491.916259765625 -19.779554275647623
45 -5405.641639709473 -19.526029288091493
46 -5594.73299407959 -19.259245561683045
47 -5344.4243240356445 -19.003066858288907
48 -5221.63809967041 -18.60063345054153
49 -5301.879600524902 -18.459329942103203
50 -5333.818519592285 -18.42398901423023
51 -5249.690162658691 -18.07407101264685
52 -5308.634567260742 -17.95472992323132
53 -5289.496604919434 -17.756624131992723
54 -8093.679443359375 -17.712051650475324
55 -5311.8090896606445 -17.6338717345421
56 -5212.328430175781 -17.351686389035557
57 -4903.406608581543 -16.95820404850849
58 -5247.386779785156 -16.758099366130438
59 -4889.23761844635 -16.590872364965673
60 -4978.615425109863 -16.53255877874835
61 -7052.397357940674 -16.362756319978686
62 -5059.687065124512 -16.155264790882434
63 -5014.420997619629 -16.028013366630972
64 -4968.61319732666 -15.952388916875135
65 -5123.185417175293 -15.794484275424995
66 -4965.960739135742 -15.775670101943286
67 -5052.1784591674805 -15.529076323160664
68 -4912.71728515625 -15.437047356205234
69 -4999.500106811523 -15.194739479491817
70 -4984.049278259277 -14.92610381152729
71 -4979.219131469727 -14.809779706441942
72 -4794.992340087891 -14.742238063929952
73 -4927.479736328125 -14.474997769495515
74 -4924.257621765137 -14.187386511754596
75 -4873.862481802702 -14.055631672914624
76 -5477.945083618164 -13.911649622206243
77 -4831.055976867676 -13.82353565658536
78 -4854.6872634887695 -13.597284942464915
79 -4823.93350982666 -13.35178405275221
80 -4361.409484863281 -13.048343217028776
81 -4836.853248596191 -12.620888218946986
82 -4549.474212646484 -12.39464435310091
83 -4477.705207824707 -12.37706370336157
84 -4577.005966186523 -12.22844555198374
85 -4620.829940795898 -12.136728942442508
86 -4404.0103759765625 -12.008908729799574
87 -4463.474761962891 -11.800061882506126
88 -5404.142153739929 -11.735043647109059
89 -4615.327056884766 -11.62039072542062
90 -4460.862846374512 -11.358714751405106
91 -4515.667755126953 -11.233624375822926
92 -4449.511932373047 -10.767313672844105
93 -4401.798469543457 -10.584529226200601
94 -4431.346008300781 -10.213011996049739
95 -4492.781051635742 -10.105709980738714
96 -4116.111678481102 -9.775917521665287
97 -4420.125907897949 -9.69718477806092
98 -4328.118057250977 -9.51177255431409
99 -4123.44775390625 -9.35840558254668
100 -4244.624649047852 -9.0908069415119
101 -4201.296585083008 -8.6398688419274
102 -4298.682159423828 -8.321710264146693
103 -4088.9292373657227 -8.106935890697521
104 -4001.7439002990723 -8.025263186762157
105 -4197.375118255615 -7.765705738616163
106 -5032.814303398132 -7.57539849177145
107 -5136.088435982354 -7.108327355338034
108 -4101.199901580811 -6.980411388513233
109 -4860.192993234843 -6.776946485018116
110 -4353.789985626936 -6.719970621583102
111 -3989.074447631836 -6.269842719972043
112 -4012.069969177246 -5.96733015294676
113 -3825.7509257793427 -5.615796733870542
114 -5173.11173248291 -5.34720210027791
115 -4065.985668182373 -5.027957977402961
116 -3401.5492453575134 -4.63049541560991
117 -4660.437511444092 -4.031048624093466
118 -4350.327434539795 -3.3322555012187633
119 -4009.598476409912 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.9069465  -21.86797946
 -21.7139277  -21.70254077 -21.32057624 -20.8667639  -20.59517611
 -20.50682666 -20.41630544 -20.31414165 -20.11746241 -20.10285427
 -19.97336864 -19.81651812 -19.80835739 -19.77955428 -19.72288085
 -19.63683111 -19.52602929 -19.3506066  -19.33513915 -19.26334551
 -19.26007337 -19.25924556 -19.16219437 -19.00306686 -18.97010249
 -18.60063345 -18.56975936 -18.45932994 -18.45352357 -18.42398901
 -18.28628407 -18.2258585  -18.13547102 -18.11146126 -18.07407101
 -17.97085933 -17.95472992 -17.95166537 -17.9290503  -17.85723948
 -17.84699757 -17.8054813  -17.75662413 -17.74546736 -17.71205165
 -17.6663674  -17.65512118 -17.63387173 -17.61161357 -17.58971226
 -17.51183757 -17.43849542 -17.36487772 -17.36073492 -17.35168639
 -17.27716509 -17.22153202 -17.09325654 -17.08109624 -16.97734474
 -16.95820405 -16.95457477 -16.89730134 -16.8515998  -16.80943948
 -16.75809937 -16.73677859 -16.73269513 -16.63455796 -16.60817421
 -16.59087236 -16.57896001 -16.57299835 -16.54174682 -16.53255878
 -16.45120857 -16.44469859 -16.36275632 -16.35707908 -16.33256004
 -16.29947135 -16.27447854 -16.15526479 -16.13255686 -16.02801337
 -15.99253949 -15.95238892 -15.9057733  -15.89234139 -15.85583061
 -15.79477206 -15.79448428 -15.77894493 -15.7756701  -15.72688415
 -15.67440178 -15.66051624 -15.60668786 -15.59814931 -15.52907632
 -15.51057028 -15.49679341 -15.43704736 -15.33818265 -15.19473948
 -15.1483704  -15.13060768 -15.01530004 -14.98784175 -14.95174185
 -14.92610381 -14.87437472 -14.81118416 -14.80977971 -14.78839874
 -14.77221158 -14.74223806 -14.72090711 -14.70351335 -14.68824709
 -14.64107649 -14.47499777 -14.47108145 -14.42243042 -14.39557876
 -14.36950992 -14.34019064 -14.28564088 -14.18738651 -14.09599742
 -14.08914645 -14.05563167 -14.04039701 -13.97778476 -13.91164962
 -13.89945791 -13.82783661 -13.82353566 -13.79997294 -13.59728494
 -13.42778136 -13.38481489 -13.35178405 -13.33215494 -13.19059064
 -13.17086184 -13.04834322 -13.04449829 -12.98689216 -12.89291261
 -12.88490566 -12.74490672 -12.7026712  -12.62088822 -12.61940694
 -12.59390135 -12.58790292 -12.55134176 -12.54459409 -12.52178592
 -12.39464435 -12.38250073 -12.3770637  -12.30865572 -12.23072325
 -12.22844555 -12.22562907 -12.13672894 -12.13075436 -12.00890873
 -11.9663894  -11.95948182 -11.90645051 -11.86989777 -11.80006188
 -11.77250563 -11.76184543 -11.73504365 -11.7033745  -11.66688687
 -11.62039073 -11.60782381 -11.56850502 -11.4765726  -11.46455069
 -11.38864946 -11.35871475 -11.31023635 -11.308883   -11.25315641
 -11.23362438 -11.20694824 -11.1059805  -10.81789026 -10.81155238
 -10.80765915 -10.78101609 -10.76731367 -10.73979539 -10.72946334
 -10.60067834 -10.58452923 -10.56027642 -10.52194514 -10.43566972
 -10.39217958 -10.36663524 -10.213012   -10.20807029 -10.13265498
 -10.10570998 -10.01008383  -9.89743194  -9.78594151  -9.77591752
  -9.76647909  -9.70516528  -9.69718478  -9.60602623  -9.56682446
  -9.53068747  -9.51177255  -9.36231491  -9.36038573  -9.35840558
  -9.35323225  -9.34517996  -9.19326137  -9.09080694  -9.07707119
  -9.01925355  -8.88025075  -8.84291363  -8.74871517  -8.68061664
  -8.67079082  -8.63986884  -8.45273098  -8.32171026  -8.27993494
  -8.26120838  -8.10693589  -8.09172333  -8.02526319  -7.89490715
  -7.79278439  -7.76570574  -7.72198686  -7.71555367  -7.57539849
  -7.55474513  -7.36244313  -7.22922322  -7.10832736  -7.07294326
  -6.98041139  -6.95906356  -6.82221482  -6.77694649  -6.72206384
  -6.71997062  -6.7153285   -6.71190737  -6.60652501  -6.51820418
  -6.45405044  -6.28787088  -6.26984272  -6.25832818  -6.01511038
  -5.96733015  -5.67537604  -5.61579673  -5.59092715  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:0
end of epoch 0: val_loss 3.2338427734375, val_acc 0.98
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.18247040152143426, val_acc 0.995
trigger times: 0
saving model weights...
end of epoch 2: val_loss 1.89321044921875, val_acc 0.99
trigger times: 1
end of epoch 3: val_loss 8.318552221115679, val_acc 0.97
trigger times: 2
end of epoch 4: val_loss 0.19160727739334107, val_acc 0.985
trigger times: 3
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.345189208984375, val_acc 0.995
trigger times: 1
end of epoch 7: val_loss 2.38245326499964e-06, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 1.715177001953125, val_acc 0.995
trigger times: 3
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 3.866746826171875, val_acc 0.985
trigger times: 1
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 20.1668092405796, val_acc 0.965
trigger times: 1
end of epoch 14: val_loss 0.012203117609024048, val_acc 0.995
trigger times: 2
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.62406005859375, val_acc 0.995
trigger times: 1
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 1.1106964111328126, val_acc 0.995
trigger times: 1
end of epoch 20: val_loss 2.0352285790443414, val_acc 0.99
trigger times: 2
end of epoch 21: val_loss 0.551602783203125, val_acc 0.995
trigger times: 3
end of epoch 22: val_loss 0.339420166015625, val_acc 0.995
trigger times: 4
end of epoch 23: val_loss 4.745997314453125, val_acc 0.985
trigger times: 5
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.5223103332519531, val_acc 0.995
trigger times: 1
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.843321533203125, val_acc 0.99
trigger times: 1
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 1.159600830078125, val_acc 0.99
trigger times: 1
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 35.397855068743226, val_acc 0.96
trigger times: 1
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.34810791015625, val_acc 0.995
trigger times: 1
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 2.152315673828125, val_acc 0.99
trigger times: 1
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 3.8562717236345634e-07, val_acc 1.0
trigger times: 1
end of epoch 45: val_loss 4.649709604978561, val_acc 0.975
trigger times: 2
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.386927490234375, val_acc 0.995
trigger times: 1
end of epoch 49: val_loss 0.4100469970703125, val_acc 0.995
trigger times: 2
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 1.1505098396503126, val_acc 0.99
trigger times: 1
end of epoch 53: val_loss 1.8648779296875, val_acc 0.985
trigger times: 2
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.5503961181640625, val_acc 0.99
trigger times: 1
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.005983790755271912, val_acc 0.995
trigger times: 1
end of epoch 65: val_loss 3.226033935546875, val_acc 0.99
trigger times: 2
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 29.456988525390624, val_acc 0.97
trigger times: 1
end of epoch 69: val_loss 0.3540704345703125, val_acc 0.995
trigger times: 2
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.1274578857421875, val_acc 0.995
trigger times: 1
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 4.35588134765625, val_acc 0.985
trigger times: 1
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.15166748046875, val_acc 0.995
trigger times: 1
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.259013671875, val_acc 0.995
trigger times: 1
end of epoch 85: val_loss 0.4655859375, val_acc 0.995
trigger times: 2
end of epoch 86: val_loss 0.038067655563354494, val_acc 0.995
trigger times: 3
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 2.7854931640625, val_acc 0.985
trigger times: 1
end of epoch 89: val_loss 0.0050827527046203615, val_acc 0.995
trigger times: 2
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 9.184233203640702e-07, val_acc 1.0
trigger times: 1
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.09412200927734375, val_acc 0.995
trigger times: 1
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.76058837890625, val_acc 0.99
trigger times: 1
end of epoch 98: val_loss 0.018695746660232545, val_acc 0.995
trigger times: 2
end of epoch 99: val_loss 0.0002439812384545803, val_acc 1.0
trigger times: 3
Finished training.
0 -3391.3656935691833 -51.11086407377863
1 -9152.101845741272 -48.87228569913625
2 -5576.179210662842 -44.969216444406115
3 -5473.919567108154 -44.438901626703945
4 -3419.439663812518 -43.58405711344297
5 -2820.0704311430454 -42.92113389854083
6 -3103.1484268307686 -42.3286380570152
7 -3037.4016633033752 -40.758154070747366
8 -4490.3011112213135 -39.90846007278091
9 -4610.681478500366 -39.27893976130787
10 -9463.617004394531 -38.73424350253481
11 -3542.459056854248 -37.308065525311996
12 -3519.1430530548096 -36.12354062747346
13 -3015.5069391429424 -35.558040948837615
14 -3490.278516769409 -33.78094921339959
15 -3182.693067073822 -32.11238423004613
16 -2568.9513325691223 -30.402902212388465
17 -2753.4712285995483 -29.40604134962864
18 -2450.548405647278 -28.578629026453186
19 -3091.8966817855835 -28.16728517571421
20 -2383.8812217712402 -27.29994968172231
21 -2449.2840032577515 -25.929972112830644
22 -2445.8720483779907 -24.180793753972072
23 -2535.087791442871 -23.291023207711152
24 -2672.4675331115723 -22.256737260461836
25 -2140.517037421465 -21.713927702452338
26 -2392.445037841797 -20.86676390075848
27 -2522.270809173584 -20.41630543980729
28 -2423.6761474609375 -20.102854270660917
29 -2411.685157775879 -19.808357391846588
30 -2329.5194778442383 -19.526029288091493
31 -2014.868688583374 -19.26334550647668
32 -2427.70751953125 -19.162194366882535
33 -2317.6769523620605 -18.60063345054153
34 -2309.7880477905273 -18.453523571383435
35 -2070.4703674316406 -18.22585849570374
36 -2302.6375732421875 -18.07407101264685
37 -2165.516893386841 -17.951665370461264
38 -2064.9197635650635 -17.846997570106016
39 -2264.722942352295 -17.745467358284394
40 -2097.4400386810303 -17.655121176249942
41 -1920.380528561771 -17.589712263068826
42 -1932.12278175354 -17.36487771675097
43 -2121.689281463623 -17.277165086981885
44 -1917.7812967300415 -17.081096237891547
45 -2007.4409999847412 -16.954574768343313
46 -2093.911102294922 -16.80943947850167
47 -2007.235647201538 -16.73269513027031
48 -1798.1522694826126 -16.590872364965673
49 -1826.5894184112549 -16.541746819096872
50 -1883.7350730895996 -16.44469859328402
51 -2164.4418411254883 -16.33256004047405
52 -2160.8999042510986 -16.155264790882434
53 -2864.2392587661743 -15.992539490672064
54 -1923.7296228408813 -15.89234138521727
55 -2097.4741172790527 -15.794484275424995
56 -1935.9844341278076 -15.726884145287103
57 -2087.0197410583496 -15.606687858313204
58 -2098.17276763916 -15.510570275370258
59 -1948.1305508613586 -15.338182650137867
60 -1956.511637687683 -15.130607678261642
61 -1789.3757486343384 -14.951741849737582
62 -1698.2932844161987 -14.811184164342883
63 -1716.996787071228 -14.772211578938265
64 -1603.4673099517822 -14.70351334892693
65 -2025.589361190796 -14.474997769495515
66 -1876.9240369796753 -14.395578763946805
67 -1899.937388420105 -14.285640879315661
68 -1733.236304283142 -14.089146448568828
69 -1586.7650241851807 -13.97778476213838
70 -1558.6327533721924 -13.827836612264356
71 -1894.332757949829 -13.597284942464915
72 -1883.8714408874512 -13.35178405275221
73 -1590.650616645813 -13.17086184281371
74 -1662.9900569915771 -12.986892158662183
75 -1786.2928171157837 -12.744906717364657
76 -2323.1807126253843 -12.619406941033082
77 -1653.6031646728516 -12.551341763077097
78 -1705.2595310211182 -12.39464435310091
79 -1631.2062978744507 -12.308655722203584
80 -1428.6909370571375 -12.225629065695466
81 -1545.8218727111816 -12.008908729799574
82 -1454.350796699524 -11.906450509346199
83 -1726.076397895813 -11.772505627580154
84 -1781.9887390136719 -11.703374496894819
85 -1344.9573917388916 -11.607823812782787
86 -1611.83651638031 -11.464550686521155
87 -1660.5635452270508 -11.310236348499787
88 -1756.2280158996582 -11.233624375822926
89 -1309.830493927002 -10.817890258651689
90 -1543.1950283050537 -10.767313672844105
91 -1470.4457941055298 -10.600678341650587
92 -1421.7564706802368 -10.521945141749367
93 -1712.905330657959 -10.366635237242699
94 -1556.4822101593018 -10.132654979517474
95 -1279.4786295890808 -9.897431939219627
96 -1638.2446842193604 -9.766479093273167
97 -1300.5677280426025 -9.606026227744174
98 -1506.727560043335 -9.51177255431409
99 -1375.669412612915 -9.35840558254668
100 -1541.4096789360046 -9.193261368948118
101 -1394.3134722709656 -9.019253546901933
102 -1417.1775531768799 -8.748715170157773
103 -1507.5316162109375 -8.6398688419274
104 -1419.8007354736328 -8.279934943515096
105 -1484.6631450653076 -8.091723328984825
106 -1265.406509399414 -7.792784389250493
107 -1270.2503509521484 -7.71555366542547
108 -1914.7856450080872 -7.362443126623615
109 -1400.1374368667603 -7.072943263247867
110 -1219.9790558815002 -6.82221482433696
111 -1603.1100294739008 -6.719970621583102
112 -1340.4768795967102 -6.60652501043313
113 -1090.6437566280365 -6.2878708805222
114 -1327.3210792541504 -6.015110378159633
115 -1539.35872207582 -5.615796733870542
116 -1130.6208655238152 -5.078485007852753
117 -1199.306917130947 -4.63049541560991
118 -1459.4473538398743 -3.3844671463622564
119 -1176.2191791534424 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.51100961 -22.40229356 -22.33400514 -22.25673726
 -22.08161462 -21.94265454 -21.93735247 -21.92768849 -21.9069465
 -21.86797946 -21.7139277  -21.70254077 -21.4067322  -21.32057624
 -21.2464276  -21.14941217 -21.03603085 -21.03115105 -20.97173814
 -20.90036032 -20.8667639  -20.59517611 -20.50682666 -20.41630544
 -20.40828658 -20.32742659 -20.31414165 -20.11746241 -20.10285427
 -20.00687126 -19.97336864 -19.92405461 -19.81651812 -19.80835739
 -19.77955428 -19.72288085 -19.67515505 -19.64626823 -19.63683111
 -19.52602929 -19.43766143 -19.3506066  -19.33513915 -19.28467606
 -19.26435053 -19.26334551 -19.26007337 -19.25924556 -19.2094934
 -19.16219437 -19.08540861 -19.00306686 -18.97010249 -18.87256489
 -18.69792003 -18.60063345 -18.57086412 -18.56975936 -18.45932994
 -18.45352357 -18.42398901 -18.28628407 -18.2258585  -18.1813197
 -18.13919154 -18.13547102 -18.11146126 -18.09192293 -18.07407101
 -18.07220724 -18.07042286 -17.97085933 -17.95472992 -17.95166537
 -17.9290503  -17.85723948 -17.84867392 -17.84699757 -17.8054813
 -17.75662413 -17.74546736 -17.71205165 -17.6663674  -17.65512118
 -17.63387173 -17.63315314 -17.61161357 -17.58971226 -17.51183757
 -17.46980974 -17.43849542 -17.36487772 -17.36073492 -17.35168639
 -17.31415305 -17.27716509 -17.23406021 -17.22153202 -17.09325654
 -17.08109624 -17.04948064 -16.97734474 -16.95820405 -16.95457477
 -16.9355018  -16.92892101 -16.89730134 -16.8515998  -16.80943948
 -16.78480578 -16.76607391 -16.76154135 -16.75809937 -16.73677859
 -16.73269513 -16.71053435 -16.63455796 -16.60817421 -16.59652088
 -16.59087236 -16.57896001 -16.57299835 -16.54174682 -16.53255878
 -16.45120857 -16.44469859 -16.39596754 -16.38892163 -16.36275632
 -16.35707908 -16.33256004 -16.29947135 -16.27447854 -16.22647587
 -16.20466954 -16.15526479 -16.14328286 -16.13255686 -16.11031245
 -16.10524544 -16.04500137 -16.03003414 -16.02801337 -15.99253949
 -15.98152899 -15.95238892 -15.9057733  -15.89234139 -15.85583061
 -15.79477206 -15.79448428 -15.77894493 -15.7756701  -15.72688415
 -15.71908909 -15.67440178 -15.66051624 -15.60668786 -15.59814931
 -15.52907632 -15.51057028 -15.49679341 -15.43704736 -15.33818265
 -15.33019099 -15.21196832 -15.20639553 -15.19473948 -15.16894398
 -15.1483704  -15.13060768 -15.01530004 -14.98784175 -14.95248677
 -14.95174185 -14.93935265 -14.92610381 -14.87437472 -14.81118416
 -14.80977971 -14.78839874 -14.77221158 -14.7464275  -14.74223806
 -14.72090711 -14.70351335 -14.68824709 -14.64107649 -14.64007261
 -14.5606485  -14.49209662 -14.47499777 -14.47108145 -14.42243042
 -14.40267741 -14.39557876 -14.37365552 -14.36950992 -14.34019064
 -14.28564088 -14.18738651 -14.13953753 -14.09599742 -14.08914645
 -14.05563167 -14.04039701 -14.03525871 -13.97778476 -13.95728957
 -13.91164962 -13.89945791 -13.88488723 -13.82994682 -13.82783661
 -13.82353566 -13.79997294 -13.59728494 -13.50654844 -13.42778136
 -13.38481489 -13.36905377 -13.35178405 -13.33215494 -13.25653241
 -13.19059064 -13.17086184 -13.11986316 -13.04834322 -13.04449829
 -12.98689216 -12.95570391 -12.94982701 -12.89291261 -12.88490566
 -12.74490672 -12.7026712  -12.62088822 -12.61940694 -12.59390135
 -12.58790292 -12.55134176 -12.54459409 -12.52178592 -12.39464435
 -12.38250073 -12.3770637  -12.30865572 -12.23072325 -12.22844555
 -12.22562907 -12.18827725 -12.17212884 -12.13672894 -12.13075436
 -12.00890873 -11.9663894  -11.95948182 -11.90645051 -11.86989777
 -11.80006188 -11.79967056 -11.77250563 -11.76184543 -11.73504365
 -11.7033745  -11.66688687 -11.63931645 -11.62039073 -11.60782381
 -11.56850502 -11.4765726  -11.46455069 -11.46331699 -11.38864946
 -11.35871475 -11.31023635 -11.308883   -11.2657018  -11.26340152
 -11.26038305 -11.25315641 -11.23362438 -11.20694824 -11.11829851
 -11.1059805  -11.07882539 -11.00879851 -10.927208   -10.92002237
 -10.81789026 -10.81155238 -10.80765915 -10.80765783 -10.78101609
 -10.76731367 -10.73979539 -10.72946334 -10.60067834 -10.58452923
 -10.56027642 -10.52194514 -10.48932899 -10.43566972 -10.39506987
 -10.39217958 -10.38609058 -10.36663524 -10.3190131  -10.213012
 -10.20807029 -10.13265498 -10.10570998 -10.01008383  -9.89743194
  -9.88930142  -9.82030819  -9.78594151  -9.77591752  -9.76647909
  -9.70516528  -9.69718478  -9.6918277   -9.62812948  -9.60602623
  -9.56682446  -9.53068747  -9.51177255  -9.36231491  -9.36038573
  -9.35840558  -9.35323225  -9.34517996  -9.30394436  -9.19326137
  -9.15676681  -9.09080694  -9.07707119  -9.01925355  -8.88025075
  -8.84291363  -8.74871517  -8.68061664  -8.67079082  -8.63986884
  -8.57475366  -8.52444084  -8.45273098  -8.32171026  -8.27993494
  -8.26120838  -8.10693589  -8.09172333  -8.02526319  -7.99589318
  -7.90386891  -7.89490715  -7.79278439  -7.76570574  -7.76403803
  -7.72198686  -7.71555367  -7.57539849  -7.55474513  -7.42216562
  -7.36244313  -7.22922322  -7.22073483  -7.12642433  -7.10832736
  -7.07294326  -6.98041139  -6.95906356  -6.92967281  -6.82221482
  -6.77694649  -6.72206384  -6.71997062  -6.7153285   -6.71190737
  -6.67069707  -6.60652501  -6.56331532  -6.51820418  -6.45405044
  -6.28787088  -6.26984272  -6.25832818  -6.24466509  -6.05204268
  -6.04891947  -6.01511038  -6.01005289  -5.96733015  -5.67537604
  -5.61579673  -5.59092715  -5.56224787  -5.42273788  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.12321225618308958, val_acc 0.97
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.045503945350646974, val_acc 0.995
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 39.38848213024438, val_acc 0.845
trigger times: 1
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 1.25766845703125, val_acc 0.99
trigger times: 1
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.5330813598632812, val_acc 0.99
trigger times: 1
end of epoch 14: val_loss 0.0712558525800705, val_acc 0.99
trigger times: 2
end of epoch 15: val_loss 3.3058096875293574, val_acc 0.925
trigger times: 3
end of epoch 16: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 4
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.722801513671875, val_acc 0.995
trigger times: 1
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.5050951373572979, val_acc 0.995
trigger times: 1
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 1.2516966876319202e-08, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 0.36704010009765625, val_acc 0.995
trigger times: 2
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 6.689219408035278, val_acc 0.925
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 2.391985384747386e-05, val_acc 1.0
trigger times: 1
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.2416131591796875, val_acc 0.995
trigger times: 1
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 3.1507040248811244, val_acc 0.955
trigger times: 1
end of epoch 42: val_loss 4.649140464607626e-08, val_acc 1.0
trigger times: 2
end of epoch 43: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 3
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 3.772831769310869e-07, val_acc 1.0
trigger times: 1
end of epoch 46: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 1
end of epoch 49: val_loss 0.03629009184078285, val_acc 0.995
trigger times: 2
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.18637809718260542, val_acc 0.99
trigger times: 1
end of epoch 53: val_loss 0.0845574951171875, val_acc 0.995
trigger times: 2
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.01055328369140625, val_acc 0.995
trigger times: 1
end of epoch 59: val_loss 2.1457626644405536e-08, val_acc 1.0
trigger times: 2
end of epoch 60: val_loss 0.00015946093944307904, val_acc 1.0
trigger times: 3
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 6.119627685546875, val_acc 0.965
trigger times: 1
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.5709402465820312, val_acc 0.99
trigger times: 1
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 1.0906463283788525e-06, val_acc 1.0
trigger times: 1
end of epoch 74: val_loss 3.7155646714381874e-06, val_acc 1.0
trigger times: 2
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 9.568230421844958e-05, val_acc 1.0
trigger times: 1
end of epoch 77: val_loss 0.06773882464535032, val_acc 0.99
trigger times: 2
end of epoch 78: val_loss 0.0003645306921838554, val_acc 1.0
trigger times: 3
end of epoch 79: val_loss 0.18303228408348335, val_acc 0.97
trigger times: 4
end of epoch 80: val_loss 0.07628950525219949, val_acc 0.995
trigger times: 5
end of epoch 81: val_loss 0.012548636198043823, val_acc 0.995
trigger times: 6
end of epoch 82: val_loss 0.06014468193054199, val_acc 0.995
trigger times: 7
end of epoch 83: val_loss 1.666284128951645e-06, val_acc 1.0
trigger times: 8
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.00011068601161241531, val_acc 1.0
trigger times: 1
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 7.748597568024706e-09, val_acc 1.0
trigger times: 1
end of epoch 88: val_loss 0.030570151805877684, val_acc 0.995
trigger times: 2
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 1.4130264753475786e-06, val_acc 1.0
trigger times: 1
end of epoch 91: val_loss 2.7174847567081453, val_acc 0.95
trigger times: 2
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.008405190706253052, val_acc 0.995
trigger times: 1
end of epoch 96: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 97: val_loss 58.71854582060443, val_acc 0.675
trigger times: 3
end of epoch 98: val_loss 1.6689022231730632e-07, val_acc 1.0
trigger times: 4
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -307.27947658102494 -51.11086407377863
1 -281.9276401773095 -48.723750706751915
2 -523.8989649226423 -44.465298807522004
3 -262.8574673146941 -43.58405711344297
4 -477.3809544444084 -42.65748859055198
5 -564.3441414952977 -41.12283699063089
6 -347.652341830777 -39.90846007278091
7 -356.28457793500274 -39.11210375046212
8 -487.8421722762437 -37.86129653169397
9 -338.8336869071936 -36.12354062747346
10 -227.24548280239105 -34.83335159619024
11 -214.96597624284914 -32.774518645207735
12 -263.324699515033 -30.402902212388465
13 -213.967234786076 -29.13525869956722
14 -284.84131497796625 -28.433748774939595
15 -168.9899005512707 -27.29994968172231
16 -372.15389196254546 -25.901848378876174
17 -380.35516128015297 -23.991963674348654
18 -224.61037287604995 -22.402293560723127
19 -278.15902185440063 -21.942654540956863
20 -158.50126973481383 -21.713927702452338
21 -266.5116629600525 -21.246427597470223
22 -232.32986974716187 -20.97173813932376
23 -188.1000895500183 -20.50682665824434
24 -194.91751313209534 -20.314141646730853
25 -183.84064888954163 -19.97336863738377
26 -168.9581801891327 -19.779554275647623
27 -229.4421284198761 -19.63683110819533
28 -158.16376304626465 -19.335139152637694
29 -250.74508476257324 -19.260073374596132
30 -239.9593005180359 -19.08540860569798
31 -231.35388851165771 -18.69792003140269
32 -145.68247389793396 -18.459329942103203
33 -221.24239420890808 -18.22585849570374
34 -133.08690248735365 -18.111461264885516
35 -224.51449298858643 -18.070422855285532
36 -225.1037585735321 -17.929050301190845
37 -224.81075382232666 -17.805481303119382
38 -147.03226374753285 -17.66636739843018
39 -171.687002658844 -17.611613567199534
40 -195.41032600402832 -17.438495417077682
41 -175.94334411621094 -17.314153049324972
42 -198.2757921218872 -17.093256542723946
43 -108.67658078670502 -16.95820404850849
44 -195.31687343120575 -16.897301337425482
45 -184.5955991744995 -16.766073911391917
46 -164.05637669563293 -16.73269513027031
47 -191.36962175369263 -16.596520884444924
48 -108.7500125169754 -16.541746819096872
49 -170.84754753112793 -16.395967536800466
50 -152.28592801094055 -16.33256004047405
51 -167.98262882232666 -16.20466954479457
52 -185.58323526382446 -16.110312449161487
53 -157.87948966026306 -16.028013366630972
54 -115.513722717762 -15.905773304028493
55 -145.19761848449707 -15.794484275424995
56 -164.28129863739014 -15.719089090727318
57 -120.73968148231506 -15.59814931452164
58 -118.40520453453064 -15.437047356205234
59 -159.71642208099365 -15.206395525953795
60 -155.48750519752502 -15.015300037334518
61 -167.37973046302795 -14.939352654791714
62 -134.20058035850525 -14.809779706441942
63 -105.58706212043762 -14.742238063929952
64 -150.5184190273285 -14.641076494945048
65 -140.64000391960144 -14.474997769495515
66 -120.67095285654068 -14.395578763946805
67 -138.6928664445877 -14.285640879315661
68 -68.51639318466187 -14.089146448568828
69 -61.59580296278 -13.97778476213838
70 -145.85723733901978 -13.884887234984905
71 -91.51045501232147 -13.79997294305851
72 -58.569848716259 -13.384814891662144
73 -133.44517278671265 -13.256532411228832
74 -60.373318284749985 -13.048343217028776
75 -135.1857807636261 -12.949827009633701
76 -106.27080059051514 -12.702671199083033
77 -78.52310556173325 -12.58790291699515
78 -78.22587609291077 -12.39464435310091
79 -103.49687361717224 -12.230723248639054
80 -134.55537283420563 -12.172128840519285
81 -74.33413517475128 -11.966389399510833
82 -96.72522282600403 -11.800061882506126
83 -114.06512451567687 -11.735043647109059
84 -96.06637835502625 -11.62039072542062
85 -78.01311752200127 -11.464550686521155
86 -92.03309088945389 -11.310236348499787
87 -114.44421720504761 -11.260383048532884
88 -107.16566789150238 -11.11829851420953
89 -114.86556684970856 -10.92720800345217
90 -78.36923733353615 -10.807659150350398
91 -62.05757859349251 -10.739795387115782
92 -80.26260969042778 -10.560276424784254
93 -101.74693930149078 -10.395069873410131
94 -100.41373467445374 -10.319013099441191
95 -69.70617485046387 -10.105709980738714
96 -86.33544325828552 -9.820308192562994
97 -53.76622430793941 -9.705165283613164
98 -45.38657922483981 -9.606026227744174
99 -68.5258417725563 -9.36231491277634
100 -78.91921281814575 -9.30394435771646
101 -31.114819543356134 -9.077071187517834
102 -58.40979831665754 -8.748715170157773
103 -67.72181963920593 -8.574753660594855
104 -45.527294248342514 -8.279934943515096
105 -38.42512699961662 -8.025263186762157
106 -27.317773818969727 -7.792784389250493
107 -37.94417032801721 -7.71555366542547
108 -92.50695296563208 -7.362443126623615
109 -103.46643095664331 -7.108327355338034
110 -65.12987086176872 -6.92967281083551
111 -103.96913051141019 -6.719970621583102
112 -33.808892535904306 -6.60652501043313
113 -27.542872999809333 -6.2878708805222
114 -50.078269958496094 -6.052042679930768
115 -40.398849299002904 -5.96733015294676
116 -53.61240495741367 -5.562247874537251
117 -71.61498714372283 -5.027957977402961
118 -86.5639460615348 -4.031048624093466
119 -15.804873207525816 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.51100961 -22.40229356 -22.33400514 -22.25673726
 -22.08161462 -21.94265454 -21.93735247 -21.92768849 -21.9069465
 -21.86797946 -21.7139277  -21.70254077 -21.4067322  -21.32057624
 -21.2464276  -21.14941217 -21.03603085 -21.03115105 -20.97173814
 -20.90036032 -20.87581145 -20.8667639  -20.59517611 -20.50682666
 -20.44547878 -20.41630544 -20.40828658 -20.33610377 -20.32742659
 -20.31414165 -20.11746241 -20.10285427 -20.05984663 -20.01204933
 -20.00687126 -19.97336864 -19.92405461 -19.81651812 -19.80835739
 -19.79447455 -19.77955428 -19.76592981 -19.75885787 -19.72824227
 -19.72288085 -19.67515505 -19.64626823 -19.6417073  -19.63683111
 -19.52602929 -19.43766143 -19.3506066  -19.33513915 -19.33192714
 -19.28467606 -19.26435053 -19.26334551 -19.26007337 -19.25987288
 -19.25924556 -19.2094934  -19.16219437 -19.08540861 -19.00684048
 -19.00306686 -18.98540767 -18.97010249 -18.87256489 -18.78710226
 -18.69792003 -18.60063345 -18.57086412 -18.56975936 -18.45932994
 -18.45352357 -18.42398901 -18.36799156 -18.28628407 -18.24381623
 -18.2258585  -18.1813197  -18.13919154 -18.13547102 -18.11891132
 -18.11146126 -18.09192293 -18.07407101 -18.07220724 -18.07042286
 -18.00085788 -17.97085933 -17.95472992 -17.95166537 -17.9290503
 -17.85723948 -17.84867392 -17.84699757 -17.84125662 -17.83623468
 -17.8054813  -17.75662413 -17.74546736 -17.73569893 -17.71205165
 -17.6663674  -17.65512118 -17.63387173 -17.63315314 -17.61161357
 -17.60162324 -17.58971226 -17.51183757 -17.48152448 -17.46980974
 -17.43849542 -17.37186326 -17.36487772 -17.36433732 -17.36073492
 -17.35168639 -17.31415305 -17.27716509 -17.27614874 -17.26271858
 -17.23406021 -17.22153202 -17.09325654 -17.08109624 -17.04948064
 -17.01915882 -16.97734474 -16.95820405 -16.95457477 -16.94236341
 -16.9355018  -16.92892101 -16.89730134 -16.85495683 -16.8515998
 -16.84639624 -16.80943948 -16.78480578 -16.76607391 -16.76154135
 -16.75809937 -16.73677859 -16.73269513 -16.72397641 -16.71053435
 -16.63455796 -16.60817421 -16.59652088 -16.59087236 -16.57896001
 -16.57299835 -16.54174682 -16.53255878 -16.45120857 -16.44469859
 -16.39596754 -16.38892163 -16.36275632 -16.35707908 -16.33256004
 -16.31772189 -16.29947135 -16.27447854 -16.22647587 -16.20466954
 -16.15526479 -16.14328286 -16.13255686 -16.11031245 -16.10524544
 -16.04500137 -16.03003414 -16.02801337 -16.02562225 -15.99253949
 -15.98152899 -15.95238892 -15.9057733  -15.89234139 -15.85583061
 -15.79477206 -15.79448428 -15.77894493 -15.7756701  -15.74938854
 -15.72688415 -15.71908909 -15.69484526 -15.67440178 -15.66051624
 -15.62682543 -15.60668786 -15.59814931 -15.52907632 -15.51057028
 -15.49679341 -15.46311091 -15.43704736 -15.33818265 -15.33019099
 -15.30645033 -15.23973399 -15.21196832 -15.20639553 -15.19473948
 -15.16894398 -15.15709267 -15.1483704  -15.13060768 -15.01891808
 -15.01530004 -14.99206627 -14.98784175 -14.95248677 -14.95174185
 -14.93935265 -14.92610381 -14.90632666 -14.87437472 -14.81275067
 -14.81118416 -14.80977971 -14.78839874 -14.77221158 -14.75843888
 -14.7464275  -14.74223806 -14.72090711 -14.70351335 -14.68824709
 -14.64862569 -14.64107649 -14.64007261 -14.5606485  -14.55935123
 -14.49209662 -14.47499777 -14.47108145 -14.42243042 -14.40267741
 -14.40240076 -14.39557876 -14.37365552 -14.36950992 -14.34019064
 -14.30835533 -14.28564088 -14.26311154 -14.24978098 -14.21818048
 -14.18738651 -14.13953753 -14.12019815 -14.09599742 -14.08914645
 -14.05563167 -14.04039701 -14.03525871 -13.97778476 -13.95728957
 -13.91164962 -13.89945791 -13.88488723 -13.87921509 -13.82994682
 -13.82783661 -13.82770192 -13.82353566 -13.82298128 -13.79997294
 -13.7726489  -13.70587319 -13.67902684 -13.59728494 -13.59481596
 -13.57290544 -13.50654844 -13.48795723 -13.42778136 -13.38481489
 -13.36905377 -13.35860743 -13.35178405 -13.34567384 -13.33215494
 -13.25653241 -13.19059064 -13.17086184 -13.17045554 -13.11986316
 -13.10606339 -13.04834322 -13.04449829 -13.03614982 -12.98689216
 -12.95570391 -12.94982701 -12.89291261 -12.88490566 -12.78007439
 -12.74726098 -12.74490672 -12.7026712  -12.69641239 -12.69235985
 -12.63730135 -12.62088822 -12.61940694 -12.59390135 -12.58790292
 -12.57049911 -12.55134176 -12.54459409 -12.52178592 -12.39464435
 -12.39393459 -12.38250073 -12.3770637  -12.30865572 -12.23072325
 -12.22844555 -12.22562907 -12.18827725 -12.17212884 -12.13672894
 -12.13075436 -12.09628093 -12.00890873 -11.96981535 -11.9663894
 -11.95948182 -11.90645051 -11.86989777 -11.86175134 -11.80680685
 -11.80006188 -11.79967056 -11.77250563 -11.76184543 -11.73504365
 -11.7033745  -11.69713876 -11.66688687 -11.66049138 -11.63931645
 -11.62039073 -11.60782381 -11.56850502 -11.49208025 -11.4765726
 -11.46455069 -11.46331699 -11.38968666 -11.38864946 -11.35871475
 -11.31023635 -11.308883   -11.27627605 -11.2657018  -11.26340152
 -11.26038305 -11.25315641 -11.23362438 -11.20694824 -11.14594933
 -11.13943565 -11.11829851 -11.1059805  -11.08975794 -11.07882539
 -11.04279438 -11.00879851 -10.96940348 -10.927208   -10.92002237
 -10.84688235 -10.81789026 -10.81155238 -10.80765915 -10.80765783
 -10.80726086 -10.78101609 -10.76731367 -10.73979539 -10.72946334
 -10.62129143 -10.61712938 -10.60067834 -10.58452923 -10.56027642
 -10.52194514 -10.50677681 -10.50385518 -10.48932899 -10.43566972
 -10.40175726 -10.39506987 -10.39217958 -10.38609058 -10.36663524
 -10.34186853 -10.32103188 -10.3190131  -10.31820751 -10.213012
 -10.20807029 -10.13497612 -10.13265498 -10.10570998 -10.01008383
  -9.92613788  -9.89743194  -9.88930142  -9.82030819  -9.78594151
  -9.77591752  -9.76647909  -9.70516528  -9.69718478  -9.6918277
  -9.67932815  -9.62812948  -9.62062436  -9.60602623  -9.56682446
  -9.55033818  -9.53068747  -9.52408863  -9.51177255  -9.4907881
  -9.43590733  -9.36231491  -9.36038573  -9.35840558  -9.35323225
  -9.34517996  -9.33937152  -9.30394436  -9.19326137  -9.15676681
  -9.09080694  -9.07707119  -9.04536328  -9.01925355  -8.88025075
  -8.84291363  -8.83302112  -8.74871517  -8.68061664  -8.67079082
  -8.66776455  -8.63986884  -8.61427442  -8.57475366  -8.52444084
  -8.45577588  -8.45273098  -8.32171026  -8.28527612  -8.27993494
  -8.26120838  -8.1348138   -8.10693589  -8.09172333  -8.02526319
  -7.99589318  -7.90386891  -7.89490715  -7.83745298  -7.79278439
  -7.76570574  -7.76403803  -7.72198686  -7.71555367  -7.64814832
  -7.57539849  -7.55474513  -7.45344773  -7.42216562  -7.36244313
  -7.22922322  -7.22073483  -7.12642433  -7.10832736  -7.08699748
  -7.07294326  -6.98041139  -6.95906356  -6.92967281  -6.82221482
  -6.77694649  -6.72206384  -6.71997062  -6.7153285   -6.71190737
  -6.67069707  -6.60652501  -6.56331532  -6.51820418  -6.45405044
  -6.28787088  -6.26984272  -6.25832818  -6.24466509  -6.05204268
  -6.04891947  -6.01511038  -6.01005289  -5.96733015  -5.67537604
  -5.61579673  -5.59092715  -5.56224787  -5.42273788  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.0021888248376014372, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.030437461956462356, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 9.000261339764393e-08, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.06360968472436067, val_acc 0.99
trigger times: 1
end of epoch 4: val_loss 1.0743214416503906, val_acc 0.99
trigger times: 2
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.00023057410132253152, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.6463282775878906, val_acc 0.985
trigger times: 2
end of epoch 8: val_loss 2.026555876000202e-08, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.014775586128234864, val_acc 0.99
trigger times: 4
end of epoch 10: val_loss 5.292752393870615e-07, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0256451139645651, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 0.3984544372558594, val_acc 0.99
trigger times: 2
end of epoch 14: val_loss 0.808570556640625, val_acc 0.99
trigger times: 3
end of epoch 15: val_loss 0.734205322265625, val_acc 0.99
trigger times: 4
end of epoch 16: val_loss 10.263445834142622, val_acc 0.95
trigger times: 5
end of epoch 17: val_loss 2.0904031372070313, val_acc 0.985
trigger times: 6
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0002285546046800846, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 4.621395783033222e-06, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 7.277938394546509, val_acc 0.94
trigger times: 3
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.08354173421859741, val_acc 0.99
trigger times: 1
end of epoch 25: val_loss 1.877100830078125, val_acc 0.99
trigger times: 2
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 1.3424007920264102e-05, val_acc 1.0
trigger times: 1
end of epoch 28: val_loss 0.16669434428214686, val_acc 0.995
trigger times: 2
end of epoch 29: val_loss 1.7732252246141433, val_acc 0.975
trigger times: 3
end of epoch 30: val_loss 0.06746468603610709, val_acc 0.995
trigger times: 4
end of epoch 31: val_loss 9.679376307758502e-07, val_acc 1.0
trigger times: 5
end of epoch 32: val_loss 4.2199458995639814e-07, val_acc 1.0
trigger times: 6
end of epoch 33: val_loss 2.2649712718703087e-08, val_acc 1.0
trigger times: 7
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 1.9643464283761443, val_acc 0.95
trigger times: 1
end of epoch 37: val_loss 0.4732679015398017, val_acc 0.99
trigger times: 2
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.005593629909569984, val_acc 0.995
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.3515178632736206, val_acc 0.985
trigger times: 1
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 2.14753088445363e-05, val_acc 1.0
trigger times: 1
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 1
end of epoch 53: val_loss 0.013270387649536133, val_acc 0.995
trigger times: 2
end of epoch 54: val_loss 0.3837574785362813, val_acc 0.985
trigger times: 3
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.04749087333679199, val_acc 0.99
trigger times: 1
end of epoch 57: val_loss 0.8585848999023438, val_acc 0.99
trigger times: 2
end of epoch 58: val_loss 0.1826605224609375, val_acc 0.99
trigger times: 3
end of epoch 59: val_loss 0.5621484375, val_acc 0.99
trigger times: 4
end of epoch 60: val_loss 0.8898776811361313, val_acc 0.995
trigger times: 5
end of epoch 61: val_loss 7.509621354984119e-07, val_acc 1.0
trigger times: 6
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 1.918305497383699e-06, val_acc 1.0
trigger times: 1
end of epoch 65: val_loss 0.04859466075897217, val_acc 0.995
trigger times: 2
end of epoch 66: val_loss 0.6774507141113282, val_acc 0.985
trigger times: 3
end of epoch 67: val_loss 0.6028125, val_acc 0.99
trigger times: 4
end of epoch 68: val_loss 6.020032742526383e-08, val_acc 1.0
trigger times: 5
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.45299468994140624, val_acc 0.98
trigger times: 1
end of epoch 72: val_loss 7.758302944332827, val_acc 0.95
trigger times: 2
end of epoch 73: val_loss 0.08573827743530274, val_acc 0.99
trigger times: 3
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.12484513461498636, val_acc 0.99
trigger times: 1
end of epoch 77: val_loss 0.0005995016388964203, val_acc 1.0
trigger times: 2
end of epoch 78: val_loss 0.0013721294378762749, val_acc 1.0
trigger times: 3
end of epoch 79: val_loss 14.07235036987811, val_acc 0.81
trigger times: 4
end of epoch 80: val_loss 3.2722386094974354e-07, val_acc 1.0
trigger times: 5
end of epoch 81: val_loss 3.4862408973621495, val_acc 0.95
trigger times: 6
end of epoch 82: val_loss 0.0001459517861974291, val_acc 1.0
trigger times: 7
end of epoch 83: val_loss 3.8720467424665013, val_acc 0.87
trigger times: 8
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 1.311300479756028e-08, val_acc 1.0
trigger times: 1
end of epoch 86: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 2
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 1.9513954377174378, val_acc 0.95
trigger times: 1
end of epoch 89: val_loss 0.11062878051772713, val_acc 0.995
trigger times: 2
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0005391996404631882, val_acc 1.0
trigger times: 1
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.970794677734375, val_acc 0.99
trigger times: 1
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 1.8358061424805782e-07, val_acc 1.0
trigger times: 1
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.35013900756835936, val_acc 0.985
trigger times: 1
Finished training.
0 -2102.4826209545135 -51.11086407377863
1 -934.7477769851685 -47.242520801549375
2 -1047.84253808856 -44.1492199315879
3 -997.8863138854504 -42.92113389854083
4 -2101.70043900609 -41.12283699063089
5 -1892.7246808111668 -39.57032589487153
6 -9434.599086113274 -38.73424350253481
7 -1262.6361355632544 -36.307958906842664
8 -653.629158616066 -34.83335159619024
9 -1807.4646604210138 -32.11238423004613
10 -1107.6470400094986 -29.87212515537898
11 -1172.5234011411667 -28.438710074595054
12 -599.1890637874603 -27.29994968172231
13 -619.3511665016413 -24.374430568602165
14 -944.7819364666939 -22.595308335760205
15 -824.7640099525452 -21.942654540956863
16 -580.9309548586607 -21.713927702452338
17 -824.6485757827759 -21.149412168934415
18 -823.887131690979 -20.875811445955815
19 -716.4823131561279 -20.41630543980729
20 -608.7145810127258 -20.11746241375952
21 -629.1548295021057 -19.97336863738377
22 -586.7801456451416 -19.779554275647623
23 -793.9765477180481 -19.675155052352824
24 -721.473521232605 -19.43766143008292
25 -576.850528717041 -19.264350532666864
26 -756.6692383289337 -19.209493395775965
27 -703.9752655029297 -18.985407670965735
28 -667.4739103317261 -18.60063345054153
29 -532.115903377533 -18.42398901423023
30 -514.748082190752 -18.181319701428954
31 -725.8235642910004 -18.09192292676653
32 -555.4751658439636 -17.970859330881126
33 -717.0432356595993 -17.848673915376665
34 -572.5005941390991 -17.756624131992723
35 -548.9646363258362 -17.655121176249942
36 -835.6615018248558 -17.589712263068826
37 -657.2810316085815 -17.371863257606183
38 -529.0686630606651 -17.314153049324972
39 -537.5553088188171 -17.221532020401778
40 -591.9075055122375 -16.97734473514058
41 -683.6935430765152 -16.928921005773564
42 -521.6429982185364 -16.80943947850167
43 -563.5885806083679 -16.736778588028123
44 -581.2037630081177 -16.60817421173863
45 -431.9443082809448 -16.53255877874835
46 -813.9571760762483 -16.362756319978686
47 -467.40124344825745 -16.274478538806633
48 -536.3936252593994 -16.13255685827815
49 -537.1959848403931 -16.028013366630972
50 -422.7902216911316 -15.905773304028493
51 -436.62090730667114 -15.778944932846587
52 -741.8050584793091 -15.694845255022246
53 -454.19537830352783 -15.59814931452164
54 -418.34387731552124 -15.437047356205234
55 -637.21615254879 -15.211968321763651
56 -453.7802131175995 -15.148370395042228
57 -502.59531116485596 -14.987841748426487
58 -663.4786329269409 -14.906326663504613
59 -418.1394233703613 -14.788398738623297
60 -498.0761008262634 -14.72090710603378
61 -560.5532546341419 -14.64007261000646
62 -453.49940299987793 -14.47108144616139
63 -556.3617025315762 -14.373655519668311
64 -708.8157606124878 -14.263111541873613
65 -570.2287130355835 -14.120198145791594
66 -452.2176408432424 -14.035258706555712
67 -547.7354129850864 -13.884887234984905
68 -418.55917978286743 -13.82353565658536
69 -609.809045791626 -13.679026842468675
70 -643.6328325271606 -13.48795722819429
71 -400.3528881072998 -13.35178405275221
72 -345.2798843383789 -13.17086184281371
73 -481.4770669937134 -13.044498285724444
74 -388.0619034767151 -12.892912609639513
75 -473.6604685783386 -12.696412386709799
76 -392.2035458087921 -12.593901353194546
77 -385.74643754959106 -12.521785915400786
78 -403.0929057598114 -12.308655722203584
79 -540.9854079931974 -12.172128840519285
80 -623.734495639801 -11.96981534933715
81 -547.0472555160522 -11.861751336829725
82 -406.081307888031 -11.761845425700502
83 -376.684965133667 -11.66049138338989
84 -575.1547719240189 -11.492080254522232
85 -326.91143913567066 -11.388649464628998
86 -382.8419524729252 -11.265701798697854
87 -284.0790533274412 -11.206948239092318
88 -514.8311023712158 -11.089757941075062
89 -466.89326705783606 -10.92720800345217
90 -364.7745486795902 -10.807659150350398
91 -324.9445050954819 -10.739795387115782
92 -356.76902770996094 -10.584529226200601
93 -415.335867010057 -10.489328989261931
94 -453.52447880990803 -10.38609057633489
95 -424.25715684890747 -10.318207509682265
96 -317.405996799469 -10.105709980738714
97 -369.55361488834023 -9.820308192562994
98 -325.62761783599854 -9.69718477806092
99 -268.8640079535544 -9.606026227744174
100 -249.18101525306702 -9.51177255431409
101 -193.79028391838074 -9.35840558254668
102 -392.72473326325417 -9.193261368948118
103 -351.5982822701335 -9.019253546901933
104 -303.7626441512257 -8.680616640102857
105 -422.34369323402643 -8.52444084072109
106 -215.1084394454956 -8.279934943515096
107 -199.93224966526031 -8.025263186762157
108 -183.53932398557663 -7.792784389250493
109 -423.0432367324829 -7.648148315378313
110 -248.21382774412632 -7.362443126623615
111 -359.7705236673355 -7.086997479112215
112 -234.63979156315327 -6.82221482433696
113 -220.92461113631725 -6.711907367004333
114 -246.09877302125096 -6.454050442643692
115 -344.0092271976173 -6.052042679930768
116 -214.64403247833252 -5.6753760441877805
117 -447.8852998614311 -5.34720210027791
118 -318.48524735867977 -4.230832004686763
119 -270.6815494298935 -1.9136196540088464
train accuracy: 0.9972222222222222
validation accuracy: 0.985
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.51100961 -22.40229356 -22.33400514 -22.25673726
 -22.08161462 -21.94265454 -21.93735247 -21.92768849 -21.9069465
 -21.86797946 -21.7139277  -21.70254077 -21.59133659 -21.54389932
 -21.4067322  -21.32057624 -21.27354827 -21.2464276  -21.14941217
 -21.13273976 -21.03603085 -21.03115105 -20.97173814 -20.9212325
 -20.90036032 -20.87581145 -20.8667639  -20.85305072 -20.60987063
 -20.59517611 -20.51747063 -20.50682666 -20.47266386 -20.44547878
 -20.41630544 -20.40828658 -20.36043964 -20.34189776 -20.33610377
 -20.32742659 -20.31414165 -20.19957433 -20.11746241 -20.10285427
 -20.05984663 -20.04940617 -20.02381688 -20.01204933 -20.00687126
 -19.97336864 -19.92405461 -19.82695173 -19.81651812 -19.80835739
 -19.79447455 -19.77955428 -19.76592981 -19.75885787 -19.74765609
 -19.72824227 -19.72288085 -19.68428296 -19.67515505 -19.64626823
 -19.6417073  -19.63760684 -19.63683111 -19.56432087 -19.52602929
 -19.48900185 -19.48691443 -19.43766143 -19.3506066  -19.33513915
 -19.33192714 -19.28467606 -19.26435053 -19.26334551 -19.26007337
 -19.25987288 -19.25978421 -19.25924556 -19.25158155 -19.2094934
 -19.17471204 -19.16219437 -19.08540861 -19.00684048 -19.00306686
 -18.98636403 -18.98540767 -18.97010249 -18.87256489 -18.82081764
 -18.78710226 -18.76995651 -18.71587695 -18.69792003 -18.63496039
 -18.60063345 -18.58309964 -18.57086412 -18.56975936 -18.56691045
 -18.55262859 -18.52000088 -18.49438719 -18.46366296 -18.45932994
 -18.45352357 -18.42398901 -18.36799156 -18.35411076 -18.28628407
 -18.27701381 -18.27322038 -18.24381623 -18.23266423 -18.2258585
 -18.19826315 -18.1816213  -18.1813197  -18.16888766 -18.13919154
 -18.13547102 -18.11891132 -18.11146126 -18.09192293 -18.07407101
 -18.07220724 -18.07042286 -18.00506241 -18.00085788 -17.97085933
 -17.95472992 -17.95166537 -17.9290503  -17.85723948 -17.84867392
 -17.84699757 -17.84125662 -17.83623468 -17.8054813  -17.79284815
 -17.75662413 -17.74546736 -17.74276389 -17.73569893 -17.71205165
 -17.6663674  -17.65512118 -17.64540115 -17.63387173 -17.63315314
 -17.61161357 -17.60162324 -17.58971226 -17.51183757 -17.48152448
 -17.46980974 -17.43849542 -17.41489332 -17.37186326 -17.36487772
 -17.36433732 -17.36073492 -17.35524783 -17.35168639 -17.35019562
 -17.31415305 -17.30977295 -17.27716509 -17.27614874 -17.26271858
 -17.23406021 -17.22153202 -17.20095174 -17.19886158 -17.1148048
 -17.09325654 -17.08109624 -17.04948064 -17.01915882 -17.01876853
 -16.97734474 -16.95820405 -16.95457477 -16.95134668 -16.94236341
 -16.9355018  -16.92892101 -16.91499542 -16.89730134 -16.85495683
 -16.8515998  -16.84639624 -16.80943948 -16.7887845  -16.78480578
 -16.76607391 -16.76154135 -16.75809937 -16.73677859 -16.73269513
 -16.72468342 -16.72397641 -16.71053435 -16.63455796 -16.60817421
 -16.59652088 -16.59087236 -16.57896001 -16.57299835 -16.54174682
 -16.53255878 -16.51345817 -16.49007905 -16.45120857 -16.44469859
 -16.39596754 -16.38892163 -16.36275632 -16.35707908 -16.35192561
 -16.33256004 -16.31772189 -16.31615436 -16.29947135 -16.28379132
 -16.27447854 -16.26123764 -16.22647587 -16.20466954 -16.18857486
 -16.16305946 -16.15526479 -16.14328286 -16.13255686 -16.11031245
 -16.10524544 -16.07577257 -16.04500137 -16.03003414 -16.02801337
 -16.02562225 -16.01465856 -15.99253949 -15.98152899 -15.95238892
 -15.9057733  -15.89234139 -15.88087956 -15.87955329 -15.85583061
 -15.79477206 -15.79448428 -15.77894493 -15.7756701  -15.76956401
 -15.74938854 -15.72927943 -15.72688415 -15.71908909 -15.69484526
 -15.67440178 -15.67178013 -15.66051624 -15.65893394 -15.62682543
 -15.60668786 -15.59814931 -15.52907632 -15.51057028 -15.49679341
 -15.46311091 -15.43704736 -15.39858401 -15.33818265 -15.33019099
 -15.30645033 -15.30457939 -15.23973399 -15.21196832 -15.20639553
 -15.19473948 -15.16894398 -15.15709267 -15.1483704  -15.14192495
 -15.13773676 -15.13060768 -15.10709341 -15.01891808 -15.01530004
 -15.00745698 -14.99206627 -14.98784175 -14.95248677 -14.95174185
 -14.93935265 -14.92610381 -14.90632666 -14.87437472 -14.82361185
 -14.81275067 -14.81118416 -14.80977971 -14.79061506 -14.78839874
 -14.77221158 -14.75843888 -14.75637695 -14.7464275  -14.74223806
 -14.73491936 -14.72090711 -14.70351335 -14.68824709 -14.64862569
 -14.64107649 -14.64007261 -14.5606485  -14.55935123 -14.53716026
 -14.49209662 -14.47997736 -14.47499777 -14.47108145 -14.42243042
 -14.41466271 -14.40267741 -14.40240076 -14.39557876 -14.37365552
 -14.36950992 -14.34019064 -14.33757452 -14.30835533 -14.28564088
 -14.26311154 -14.24978098 -14.21818048 -14.18738651 -14.18046524
 -14.13953753 -14.125827   -14.12019815 -14.09599742 -14.09189666
 -14.08914645 -14.07269242 -14.05563167 -14.04039701 -14.03525871
 -14.00221391 -13.97778476 -13.95728957 -13.93144119 -13.91164962
 -13.89945791 -13.89277717 -13.88488723 -13.87921509 -13.85279749
 -13.82994682 -13.82783661 -13.82770192 -13.82353566 -13.82298128
 -13.79997294 -13.7726489  -13.74233731 -13.73911553 -13.72926053
 -13.72615789 -13.70587319 -13.67902684 -13.61049516 -13.59728494
 -13.59481596 -13.58758295 -13.57290544 -13.53568293 -13.50654844
 -13.48795723 -13.48117195 -13.42778136 -13.38481489 -13.36905377
 -13.36098949 -13.36019247 -13.35860743 -13.35178405 -13.34567384
 -13.33215494 -13.25653241 -13.24373676 -13.21658949 -13.19059064
 -13.17086184 -13.17045554 -13.11986316 -13.10941244 -13.10606339
 -13.04834322 -13.04449829 -13.03614982 -12.98689216 -12.95570391
 -12.94982701 -12.90317968 -12.89291261 -12.88490566 -12.83647828
 -12.78764138 -12.78007439 -12.74726098 -12.74490672 -12.7026712
 -12.69641239 -12.69235985 -12.63730135 -12.62088822 -12.61940694
 -12.59390135 -12.58790292 -12.57049911 -12.55134176 -12.54459409
 -12.52178592 -12.50623631 -12.47800508 -12.40509802 -12.39464435
 -12.39393459 -12.38250073 -12.3770637  -12.30865572 -12.23072325
 -12.22844555 -12.22562907 -12.18827725 -12.17212884 -12.13951384
 -12.13672894 -12.13075436 -12.09628093 -12.00890873 -12.00648553
 -11.96981535 -11.9663894  -11.95948182 -11.90645051 -11.86989777
 -11.86175134 -11.82075571 -11.80680685 -11.80006188 -11.79967056
 -11.77250563 -11.76184543 -11.73504365 -11.7033745  -11.69713876
 -11.66688687 -11.66049138 -11.63931645 -11.62039073 -11.60782381
 -11.56850502 -11.49368833 -11.49208025 -11.4765726  -11.46455069
 -11.46331699 -11.38968666 -11.38864946 -11.35871475 -11.31023635
 -11.308883   -11.28833296 -11.27627605 -11.2657018  -11.26340152
 -11.26038305 -11.25315641 -11.23362438 -11.20694824 -11.14594933
 -11.13943565 -11.11829851 -11.1059805  -11.08975794 -11.07882539
 -11.04279438 -11.00879851 -10.96940348 -10.927208   -10.92002237
 -10.84688235 -10.81789026 -10.81155238 -10.80765915 -10.80765783
 -10.80726086 -10.78101609 -10.76731367 -10.73979539 -10.72946334
 -10.62129143 -10.61712938 -10.60067834 -10.58452923 -10.56027642
 -10.52194514 -10.50677681 -10.50385518 -10.48932899 -10.43566972
 -10.40175726 -10.39506987 -10.39217958 -10.38609058 -10.36663524
 -10.34186853 -10.32103188 -10.3190131  -10.31820751 -10.213012
 -10.20807029 -10.13497612 -10.13265498 -10.10570998 -10.01008383
  -9.92613788  -9.89743194  -9.88930142  -9.82030819  -9.78594151
  -9.77591752  -9.76647909  -9.70516528  -9.69718478  -9.6918277
  -9.67932815  -9.62812948  -9.62062436  -9.60602623  -9.56682446
  -9.55033818  -9.53068747  -9.52408863  -9.51177255  -9.4907881
  -9.43590733  -9.36231491  -9.36038573  -9.35840558  -9.35323225
  -9.34517996  -9.33937152  -9.30394436  -9.19326137  -9.15676681
  -9.09080694  -9.07707119  -9.04536328  -9.01925355  -8.88025075
  -8.84291363  -8.83302112  -8.74871517  -8.68061664  -8.67079082
  -8.66776455  -8.63986884  -8.61427442  -8.57475366  -8.52444084
  -8.45577588  -8.45273098  -8.32171026  -8.28527612  -8.27993494
  -8.26120838  -8.1348138   -8.10693589  -8.09172333  -8.02526319
  -7.99589318  -7.90386891  -7.89490715  -7.83745298  -7.79278439
  -7.76570574  -7.76403803  -7.72198686  -7.71555367  -7.64814832
  -7.57539849  -7.55474513  -7.45344773  -7.42216562  -7.36244313
  -7.22922322  -7.22073483  -7.12642433  -7.10832736  -7.08699748
  -7.07294326  -6.98041139  -6.95906356  -6.92967281  -6.82221482
  -6.77694649  -6.72206384  -6.71997062  -6.7153285   -6.71190737
  -6.67069707  -6.60652501  -6.56331532  -6.51820418  -6.45405044
  -6.28787088  -6.26984272  -6.25832818  -6.24466509  -6.05204268
  -6.04891947  -6.01511038  -6.01005289  -5.96733015  -5.67537604
  -5.61579673  -5.59092715  -5.56224787  -5.42273788  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.05949475322055811, val_acc 0.985
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1395451001535239, val_acc 0.985
trigger times: 1
end of epoch 2: val_loss 5.200199752607659e-05, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 8.600216813192674e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.021837285182414377, val_acc 0.995
trigger times: 1
end of epoch 5: val_loss 0.22962894255680993, val_acc 0.985
trigger times: 2
end of epoch 6: val_loss 0.0002352560177109808, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 2.808100737386076, val_acc 0.925
trigger times: 4
end of epoch 8: val_loss 0.01384785792052753, val_acc 0.995
trigger times: 5
end of epoch 9: val_loss 0.03153720011143715, val_acc 0.99
trigger times: 6
end of epoch 10: val_loss 0.0030888235568727394, val_acc 1.0
trigger times: 7
end of epoch 11: val_loss 1.430509428246296e-08, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 11.627928421514662, val_acc 0.925
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.007391677265259169, val_acc 0.995
trigger times: 1
end of epoch 15: val_loss 0.5216377038042992, val_acc 0.995
trigger times: 2
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 25.674003522098065, val_acc 0.86
trigger times: 1
end of epoch 18: val_loss 0.08053573608398437, val_acc 0.995
trigger times: 2
end of epoch 19: val_loss 0.00010652576573193073, val_acc 1.0
trigger times: 3
end of epoch 20: val_loss 3.4594486351124942e-06, val_acc 1.0
trigger times: 4
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.3586892718076703, val_acc 0.995
trigger times: 1
end of epoch 24: val_loss 0.10437850952148438, val_acc 0.995
trigger times: 2
end of epoch 25: val_loss 0.033668304085723404, val_acc 0.995
trigger times: 3
end of epoch 26: val_loss 4.1723234289747776e-09, val_acc 1.0
trigger times: 4
end of epoch 27: val_loss 0.14158523559570313, val_acc 0.995
trigger times: 5
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.1856011962890625, val_acc 0.995
trigger times: 1
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.4895440673828125, val_acc 0.995
trigger times: 1
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.25302661791443826, val_acc 0.99
trigger times: 1
end of epoch 37: val_loss 8.106166205834598e-08, val_acc 1.0
trigger times: 2
end of epoch 38: val_loss 0.052582836151123045, val_acc 0.995
trigger times: 3
end of epoch 39: val_loss 2.153587646484375, val_acc 0.995
trigger times: 4
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.10868194580078125, val_acc 0.995
trigger times: 1
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.11372746592969633, val_acc 0.995
trigger times: 1
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 1
end of epoch 49: val_loss 0.5471526102527104, val_acc 0.985
trigger times: 2
end of epoch 50: val_loss 0.19363931071594692, val_acc 0.99
trigger times: 3
end of epoch 51: val_loss 0.04479237436935591, val_acc 0.995
trigger times: 4
end of epoch 52: val_loss 1.6735460303607397e-06, val_acc 1.0
trigger times: 5
end of epoch 53: val_loss 0.13982190802693364, val_acc 0.995
trigger times: 6
end of epoch 54: val_loss 0.15452158509036962, val_acc 0.995
trigger times: 7
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.3305295324325562, val_acc 0.99
trigger times: 1
end of epoch 58: val_loss 8.808790153125301e-07, val_acc 1.0
trigger times: 2
end of epoch 59: val_loss 0.008517352647995722, val_acc 0.995
trigger times: 3
end of epoch 60: val_loss 0.1444455498456955, val_acc 0.995
trigger times: 4
end of epoch 61: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 5
end of epoch 62: val_loss 0.9732589721679688, val_acc 0.995
trigger times: 6
end of epoch 63: val_loss 0.06625612348318043, val_acc 0.995
trigger times: 7
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.8023913604021063, val_acc 0.995
trigger times: 1
end of epoch 67: val_loss 0.167044677734375, val_acc 0.995
trigger times: 2
end of epoch 68: val_loss 0.054852991104125975, val_acc 0.995
trigger times: 3
end of epoch 69: val_loss 0.50673828125, val_acc 0.985
trigger times: 4
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 3.5762775496550603e-09, val_acc 1.0
trigger times: 1
end of epoch 72: val_loss 0.02974435329437256, val_acc 0.995
trigger times: 2
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.04551399044226855, val_acc 0.995
trigger times: 1
end of epoch 75: val_loss 0.3197503662109375, val_acc 0.995
trigger times: 2
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0005653617893686303, val_acc 1.0
trigger times: 1
end of epoch 79: val_loss 2.2673381026834248e-05, val_acc 1.0
trigger times: 2
end of epoch 80: val_loss 0.1716262912750244, val_acc 0.99
trigger times: 3
end of epoch 81: val_loss 0.0008174496144047794, val_acc 1.0
trigger times: 4
end of epoch 82: val_loss 21.44588882446289, val_acc 0.91
trigger times: 5
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 3.6595913115888835e-07, val_acc 1.0
trigger times: 1
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 1.847740577431978e-08, val_acc 1.0
trigger times: 1
end of epoch 89: val_loss 0.08397552490234375, val_acc 0.995
trigger times: 2
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 31.142916595935823, val_acc 0.905
trigger times: 1
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.03752960443496704, val_acc 0.995
trigger times: 1
end of epoch 96: val_loss 0.6819426447139812, val_acc 0.985
trigger times: 2
end of epoch 97: val_loss 3.49573335647583, val_acc 0.975
trigger times: 3
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -951.5906349313445 -51.11086407377863
1 -1399.0498359203339 -44.969216444406115
2 -1077.569823358499 -43.58405711344297
3 -830.1134296350647 -42.3286380570152
4 -1701.4717519283295 -39.90846007278091
5 -3225.038168042898 -38.73424350253481
6 -929.8473463058472 -36.12354062747346
7 -1197.5668728351593 -33.78094921339959
8 -850.4045962438686 -30.402902212388465
9 -712.1881140470505 -28.578629026453186
10 -637.3523109257221 -27.29994968172231
11 -736.0780856286001 -24.180793753972072
12 -681.2279028892517 -22.334005141485896
13 -644.6753005981445 -21.906946500025814
14 -719.2217109203339 -21.406732197706983
15 -625.4540762901306 -21.036030847803215
16 -609.5510396957397 -20.86676390075848
17 -660.8983358731057 -20.472663862274185
18 -706.923282623291 -20.336103765299747
19 -681.332407951355 -20.059846634692384
20 -680.2902936935425 -19.924054612721346
21 -664.5190954208374 -19.76592980702272
22 -679.599668264389 -19.675155052352824
23 -581.685299873352 -19.526029288091493
24 -714.2127847671509 -19.331927143116648
25 -619.1997910626305 -19.25978421072938
26 -658.788135766983 -19.08540860569798
27 -610.2298078536987 -18.872564889750354
28 -597.1332948139461 -18.634960394147807
29 -664.1988093999862 -18.552628587401966
30 -535.8407859802246 -18.42398901423023
31 -691.1808772087097 -18.243816233462507
32 -628.6337811946869 -18.168887655347984
33 -574.9274892807007 -18.07407101264685
34 -584.5732383728027 -17.95472992323132
35 -576.8864612579346 -17.84125662133664
36 -627.129316329956 -17.735698933713266
37 -580.9605135917664 -17.633153136983214
38 -614.9832105636597 -17.46980974244427
39 -537.5757160186768 -17.360734923884614
40 -515.8007526397705 -17.277165086981885
41 -609.1943209757083 -17.198861581998848
42 -610.2987564900768 -17.018768531238404
43 -566.8305380344391 -16.935501798907445
44 -633.6189012527466 -16.84639623800167
45 -528.6832637786865 -16.758099366130438
46 -503.09655809402466 -16.63455795541301
47 -456.8525667190552 -16.541746819096872
48 -536.978511095047 -16.395967536800466
49 -553.9152965545654 -16.317721889505922
50 -562.2989925146103 -16.22647586873857
51 -516.0474014282227 -16.13255685827815
52 -516.3301005363464 -16.028013366630972
53 -451.64526534080505 -15.905773304028493
54 -507.90040493011475 -15.794484275424995
55 -478.2850651741028 -15.726884145287103
56 -573.8733897522397 -15.658933944647996
57 -457.15271520614624 -15.496793413937699
58 -620.7821426391602 -15.306450325069301
59 -537.9995707273483 -15.168943975018234
60 -570.7788281440735 -15.018918084349849
61 -476.4406898021698 -14.951741849737582
62 -578.7341871261597 -14.812750672152166
63 -543.8874797821045 -14.758438882347857
64 -455.65593338012695 -14.70351334892693
65 -615.8942937850952 -14.559351232747705
66 -473.8742530345917 -14.422430415074533
67 -444.98618841171265 -14.369509922552446
68 -579.0958299636841 -14.249780982734455
69 -541.4577627182007 -14.120198145791594
70 -466.69665908813477 -14.040397014475062
71 -415.49778154984233 -13.911649622206243
72 -517.0881499052048 -13.829946818029187
73 -572.7856702804565 -13.772648900663555
74 -553.2451920509338 -13.679026842468675
75 -559.8696296059097 -13.535682926297456
76 -480.9055725932121 -13.369053769163
77 -463.64803314208984 -13.332154938401493
78 -477.74651622772217 -13.170455540977969
79 -526.4263262748718 -13.036149817563755
80 -406.00509691238403 -12.884905658544039
81 -417.7225842475891 -12.702671199083033
82 -427.22905135154724 -12.593901353194546
83 -565.3022553951814 -12.506236311578217
84 -418.5639533996582 -12.308655722203584
85 -563.1740134882202 -12.139513837153816
86 -526.7855641841888 -11.96981534933715
87 -569.489088660187 -11.820755707016403
88 -409.14386743307114 -11.735043647109059
89 -394.37977504730225 -11.62039072542062
90 -413.0247890949249 -11.464550686521155
91 -402.03743743896484 -11.308882999248118
92 -388.167848110199 -11.253156413841888
93 -390.7989912033081 -11.105980498728915
94 -425.27538150548935 -10.92720800345217
95 -420.59626309576834 -10.807657829418003
96 -452.7257790565491 -10.62129142769806
97 -324.7104526758194 -10.506776806979948
98 -394.16877698898315 -10.392179581703525
99 -407.80521512031555 -10.318207509682265
100 -372.68027544021606 -10.010083830055333
101 -378.1535878228897 -9.775917521665287
102 -363.82575574216935 -9.628129484008644
103 -357.0193130970001 -9.524088630208611
104 -312.5201654434204 -9.35840558254668
105 -402.4396028793417 -9.156766806254744
106 -326.7707884311676 -8.842913629134763
107 -315.6224594116211 -8.6398688419274
108 -315.709991812706 -8.285276121868762
109 -299.23033690452576 -8.025263186762157
110 -306.7241966724396 -7.765705738616163
111 -350.3326843380928 -7.554745134379745
112 -300.3037356508812 -7.126424331397133
113 -357.1390806884219 -6.92967281083551
114 -299.7803127169609 -6.711907367004333
115 -310.9421525001526 -6.2878708805222
116 -261.2948285341263 -6.015110378159633
117 -322.31963814709616 -5.562247874537251
118 -416.3395903585624 -4.63049541560991
119 -222.44425708055496 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

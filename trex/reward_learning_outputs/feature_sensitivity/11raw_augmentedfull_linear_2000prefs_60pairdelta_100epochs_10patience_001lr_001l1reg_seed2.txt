demos: (120, 50, 13)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 5.107909921875375e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.2950e-06,  1.8994e-01,  7.7284e-02, -8.1489e-02,  4.6850e-02,
         -6.3119e-02, -1.3375e-02,  1.9081e-02, -6.5094e-06, -8.8839e-05,
         -2.3655e-03, -6.7444e-01, -1.0353e+00]], device='cuda:0'))])
end of epoch 1: val_loss 2.3066179008424113e-06, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.000167150431402483, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 2.9802313861182485e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.7879e-06,  1.8929e-01,  2.1044e-01,  3.6837e-02,  9.1119e-06,
         -7.4247e-06, -9.9020e-07, -1.7843e-02,  6.8070e-05, -5.7346e-05,
          7.5657e-06, -8.7533e-01, -1.5855e+00]], device='cuda:0'))])
end of epoch 4: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1403e-05,  2.4706e-02,  5.2755e-02,  8.8430e-06,  3.1848e-06,
         -2.0461e-05,  3.7285e-06, -2.4551e-06, -6.2593e-05, -1.2717e-04,
         -2.2278e-05,  3.4484e-06, -1.3649e+00]], device='cuda:0'))])
end of epoch 5: val_loss 7.456473435496491e-07, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 6.616087944166793e-08, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 6.258451264784525e-08, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 1.24573455799748e-07, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.1205e-02,  2.2322e-01,  1.1867e-01, -2.0020e-01, -5.7804e-06,
         -3.7850e-02, -2.1852e-02,  5.1363e-02, -2.7723e-02,  5.2207e-04,
         -5.2126e-04, -1.6445e+00, -2.0499e+00]], device='cuda:0'))])
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.8336e-05,  2.0997e-02, -7.3651e-05, -1.0723e-01, -5.0558e-05,
         -6.7622e-05, -1.2235e-02,  2.9254e-02,  5.1695e-05, -4.2581e-06,
          8.1412e-04, -9.8011e-01, -1.8591e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4021e-05, -5.1910e-06,  1.7537e-05, -1.9230e-05, -2.5692e-04,
         -1.6388e-04,  3.4308e-07,  8.8699e-07,  1.4241e-04, -2.4430e-05,
         -2.3633e-03,  3.6094e-05, -1.3896e+00]], device='cuda:0'))])
end of epoch 12: val_loss 4.758714105079776e-06, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 2.026554053458085e-08, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 1.1086443652885692e-07, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 9.549402540866225e-05, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.2637e-05,  1.2121e-02,  1.2115e-01, -1.1117e-01,  7.2282e-06,
         -1.1996e-01, -2.0392e-02,  6.2601e-02,  1.4479e-05,  2.6808e-05,
          1.5774e-03, -1.3635e+00, -2.1650e+00]], device='cuda:0'))])
end of epoch 17: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 18: val_loss 1.0490399535711958e-07, val_acc 1.0
trigger times: 2
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0632e-04,  9.7004e-03,  9.2756e-02, -9.4149e-03,  1.6454e-05,
         -4.5052e-01, -2.2675e-03, -1.8762e-02, -1.0297e-04,  5.5078e-02,
         -3.1727e-04, -9.7936e-01, -1.6087e+00]], device='cuda:0'))])
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.5141e-05, -9.7565e-06, -1.7293e-05,  4.3529e-07,  9.5387e-05,
         -1.0835e-05, -1.0691e-06,  8.9221e-06, -2.3518e-04, -3.3195e-05,
         -5.1906e-04, -4.4531e-02, -1.3609e+00]], device='cuda:0'))])
end of epoch 21: val_loss 2.0139991330125893e-06, val_acc 1.0
trigger times: 1
end of epoch 22: val_loss 0.02782276805289712, val_acc 0.995
trigger times: 2
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.5393e-03,  2.1801e-01,  1.9644e-01, -9.2709e-02,  3.7379e-05,
         -6.6011e-03,  3.7551e-02,  5.5372e-02,  3.7120e-06, -1.5307e-05,
          3.8004e-03, -1.3033e+00, -2.7890e+00]], device='cuda:0'))])
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.5527e-06, -4.0157e-05,  1.8565e-05,  7.8129e-06, -1.0705e-04,
          1.6449e-04,  2.7881e-02,  1.7358e-02,  3.3562e-05,  3.3064e-05,
         -2.5045e-03,  5.1611e-05, -2.4578e+00]], device='cuda:0'))])
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.8391e-06,  2.3083e-05,  4.2079e-05,  7.6048e-05,  1.6237e-04,
          4.3044e-04,  4.1199e-03, -3.1300e-06, -4.5336e-05,  3.0674e-04,
          1.1983e-05, -1.4506e-04, -1.6437e+00]], device='cuda:0'))])
end of epoch 26: val_loss 6.616095149070134e-08, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 5.7816235425889316e-08, val_acc 1.0
trigger times: 2
end of epoch 28: val_loss 8.558941447844859e-07, val_acc 1.0
trigger times: 3
end of epoch 29: val_loss 2.618475236495499e-05, val_acc 1.0
trigger times: 4
end of epoch 30: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 5
end of epoch 31: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 6
end of epoch 32: val_loss 1.2806046016180516e-05, val_acc 1.0
trigger times: 7
end of epoch 33: val_loss 0.0035155880451196706, val_acc 1.0
trigger times: 8
end of epoch 34: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 9
end of epoch 35: val_loss 1.758306098054163e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -101.46890442073345 -54.98547503240923
1 -113.93836849927902 -50.492268601198035
2 -104.99682968854904 -50.03933801517046
3 -98.51756846904755 -49.75347184620696
4 -98.96340875327587 -49.72654640753777
5 -93.62649500370026 -46.98011874490918
6 -89.90130913257599 -45.7351542845057
7 -81.75995305925608 -45.670579884154705
8 -86.85104206204414 -44.99030608142343
9 -86.80198329687119 -44.14602409201361
10 -86.95910674333572 -43.81326882122305
11 -92.14784318208694 -43.18878399086166
12 -105.75496929883957 -42.29180714825394
13 -81.5894054621458 -42.00401746161006
14 -104.7774338722229 -41.6910044370425
15 -90.33608227968216 -41.68588229294918
16 -86.23125463724136 -41.281777102712205
17 -78.44213992357254 -40.44278203413966
18 -78.4883158877492 -40.34838365523108
19 -80.8111457824707 -39.599701153458774
20 -84.93532198667526 -39.57586365327889
21 -97.40605962276459 -39.31972693233231
22 -83.56431244313717 -39.024610555047154
23 -87.84081780910492 -38.45534493538269
24 -87.97253966331482 -38.41270390343083
25 -79.24843740463257 -38.35634328077039
26 -90.05923634767532 -37.79713616772368
27 -78.20651736855507 -37.741528994987384
28 -88.42725211381912 -37.66475323879293
29 -65.0931222140789 -37.513139380385574
30 -83.15036532282829 -37.1809993033689
31 -82.82896439731121 -37.100703136010694
32 -90.962561622262 -37.00630588930485
33 -73.1987875699997 -36.821916772458344
34 -83.93900787830353 -36.48799015296732
35 -85.76578776538372 -36.20965269874363
36 -86.72205185890198 -36.19207561676116
37 -86.04252150654793 -36.114459029559086
38 -88.92605996131897 -35.78149902167743
39 -66.29291382431984 -35.394503873250635
40 -79.40897014737129 -35.26282499693737
41 -65.2765920907259 -35.24303541418371
42 -85.98432029783726 -35.209705244501436
43 -71.05981531739235 -35.0654408505187
44 -80.50525960326195 -34.80241747531743
45 -74.93300205469131 -34.64469044638467
46 -84.44162839651108 -33.84284985953318
47 -68.6718417853117 -32.70706485357069
48 -63.82476019859314 -31.969099402548657
49 -72.6620322316885 -31.7109134007892
50 -60.062284499406815 -31.64414355845032
51 -75.8138218820095 -31.392382758954444
52 -76.67808537185192 -31.223196019713853
53 -63.14175570011139 -31.12953085092458
54 -77.42178571224213 -29.39157139549552
55 -63.5197132229805 -29.340125609942326
56 -56.39856444299221 -29.106189988903285
57 -70.0431474596262 -27.41102349748205
58 -69.06935628503561 -27.343722362182305
59 -63.83816226571798 -27.196681629483837
60 -75.24868015944958 -27.07399028854534
61 -61.560593985021114 -26.7047217556024
62 -66.74310505390167 -26.244794902859052
63 -64.36714291572571 -25.548365085275513
64 -52.791027158498764 -25.45878528601009
65 -50.973027132451534 -24.879106999799365
66 -65.26719819754362 -24.828695359328833
67 -71.50312343239784 -24.592745144504722
68 -66.85721235722303 -23.978745577896312
69 -49.577331990003586 -23.57262108435893
70 -55.91588041186333 -23.44970807952351
71 -55.17103932797909 -22.745309160183492
72 -54.06617560982704 -22.60679894414887
73 -55.11591889709234 -22.19891031871716
74 -46.070177003741264 -20.656863763892378
75 -47.479290917515755 -20.444472560731253
76 -41.767022117972374 -20.19699010077007
77 -59.02379283308983 -20.13839114930498
78 -49.96981590986252 -19.63760343800059
79 -63.94497090578079 -19.515598718228343
80 -50.10230261832476 -18.92838809611677
81 -41.71607343107462 -17.994774057192853
82 -50.25284841656685 -17.55742370467821
83 -38.821484580636024 -16.823073927842348
84 -46.530445735901594 -14.855082803515382
85 -47.672560438513756 -14.531424598833084
86 -31.106657408177853 -14.442420089224363
87 -43.86351731419563 -13.596012850960644
88 -21.020788937807083 -12.68135972540495
89 -42.757513765245676 -12.66418205637357
90 -39.12668351829052 -12.30017947419658
91 -28.90139576047659 -12.151904772081672
92 -45.03883841633797 -11.788852141676486
93 -24.342560902237892 -10.869989101210326
94 -35.71949554234743 -10.327681503524177
95 -33.41818252205849 -9.8572159761571
96 -30.43997447937727 -8.330116995310416
97 -30.083201624453068 -8.133195842510668
98 -30.562335968017578 -8.108197691178031
99 -17.307279102504253 -7.57539849177145
100 -17.391482669860125 -7.362443126623615
101 -10.732632659375668 -7.108327355338034
102 -23.76910065487027 -6.959063561385431
103 -17.441063478589058 -6.776946485018116
104 -13.726762063801289 -6.7220638398623045
105 -18.406811006367207 -6.719970621583102
106 -30.362872667610645 -6.535447341844848
107 -29.326144069433212 -6.51820418055673
108 -27.902567770332098 -5.615796733870542
109 -27.05594439432025 -5.34720210027791
110 -22.055071856826544 -5.078485007852753
111 -21.81050330400467 -5.027957977402961
112 -24.140990562736988 -4.827572916892203
113 -21.412037007510662 -4.63049541560991
114 -23.297570154070854 -4.230832004686763
115 -23.874938279390335 -4.031048624093466
116 -20.464766703546047 -3.3844671463622564
117 -21.46119112893939 -3.3322555012187633
118 -19.348041862249374 -2.6416623314910934
119 -16.60374542698264 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

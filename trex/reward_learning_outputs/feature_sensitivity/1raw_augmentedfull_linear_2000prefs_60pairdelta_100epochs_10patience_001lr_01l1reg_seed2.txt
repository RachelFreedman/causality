demos: (120, 50, 3)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=3, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 3
Number of trainable paramters: 3
device: cuda:2
end of epoch 0: val_loss 0.0050320926863787465, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.005660631905993796, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.00973564541694941, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.006090822244223091, val_acc 1.0
trigger times: 3
end of epoch 4: val_loss 0.007003606291982578, val_acc 1.0
trigger times: 4
end of epoch 5: val_loss 0.0074565827140759215, val_acc 1.0
trigger times: 5
end of epoch 6: val_loss 0.004627626028159284, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0031077628701837055, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.005481478070578305, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.0050903583854233145, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.004302926150048733, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.0038785123588968417, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 0.008800819759526348, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.00452403494498867, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.006176371157343965, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.002476839172268228, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.005141648254575557, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.003987572496371285, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.005446055104312108, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0023796879511064615, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.004433243616513209, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 0.004973743828304578, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 0.003075246387379593, val_acc 1.0
trigger times: 3
end of epoch 23: val_loss 0.003908757111421437, val_acc 1.0
trigger times: 4
end of epoch 24: val_loss 0.004147384019561286, val_acc 1.0
trigger times: 5
end of epoch 25: val_loss 0.007319752305047587, val_acc 1.0
trigger times: 6
end of epoch 26: val_loss 0.005250548431795323, val_acc 1.0
trigger times: 7
end of epoch 27: val_loss 0.005735501133895014, val_acc 1.0
trigger times: 8
end of epoch 28: val_loss 0.0028734238990909943, val_acc 1.0
trigger times: 9
end of epoch 29: val_loss 0.004530152238803566, val_acc 1.0
trigger times: 10
Early stopping.
0 -10.997713044285774 -54.98547503240923
1 -11.196703020483255 -50.492268601198035
2 -10.210376251488924 -50.03933801517046
3 -10.442002534866333 -49.75347184620696
4 -10.431006632745266 -49.72654640753777
5 -10.880280189216137 -46.98011874490918
6 -10.564516831189394 -45.7351542845057
7 -10.200164418667555 -45.670579884154705
8 -10.330311633646488 -44.99030608142343
9 -9.66501795873046 -44.14602409201361
10 -9.781806625425816 -43.81326882122305
11 -9.800848526880145 -43.18878399086166
12 -10.062587264925241 -42.29180714825394
13 -9.4578248411417 -42.00401746161006
14 -10.55053660646081 -41.6910044370425
15 -9.549459578469396 -41.68588229294918
16 -9.972902063280344 -41.281777102712205
17 -9.365040481090546 -40.44278203413966
18 -9.551102893892676 -40.34838365523108
19 -9.338716614991426 -39.599701153458774
20 -9.20836160890758 -39.57586365327889
21 -8.868180427700281 -39.31972693233231
22 -8.598003707826138 -39.024610555047154
23 -9.032318331301212 -38.45534493538269
24 -8.687640821561217 -38.41270390343083
25 -9.047182446345687 -38.35634328077039
26 -8.631377978250384 -37.79713616772368
27 -8.162892878055573 -37.741528994987384
28 -9.577478229999542 -37.66475323879293
29 -8.790745185688138 -37.513139380385574
30 -9.747581537812948 -37.1809993033689
31 -8.763784231152385 -37.100703136010694
32 -8.827769357711077 -37.00630588930485
33 -9.15514551103115 -36.821916772458344
34 -8.986755579710007 -36.48799015296732
35 -8.55849215388298 -36.20965269874363
36 -8.845517065608874 -36.19207561676116
37 -9.279567666351795 -36.114459029559086
38 -8.641828287392855 -35.78149902167743
39 -7.957318658940494 -35.394503873250635
40 -8.786615770310163 -35.26282499693737
41 -8.661316243931651 -35.24303541418371
42 -9.126331094652414 -35.209705244501436
43 -8.82988503575325 -35.0654408505187
44 -8.583078821189702 -34.80241747531743
45 -8.444401107728481 -34.64469044638467
46 -7.957310338271782 -33.84284985953318
47 -7.459034746512771 -32.70706485357069
48 -7.511442709714174 -31.969099402548657
49 -7.578806629404426 -31.7109134007892
50 -8.086447497829795 -31.64414355845032
51 -8.064995711669326 -31.392382758954444
52 -8.535744482651353 -31.223196019713853
53 -7.6725988034158945 -31.12953085092458
54 -7.524003483355045 -29.39157139549552
55 -8.085766170173883 -29.340125609942326
56 -6.457205735146999 -29.106189988903285
57 -7.104106524959207 -27.41102349748205
58 -7.280353559181094 -27.343722362182305
59 -7.489690005779266 -27.196681629483837
60 -7.038724794285372 -27.07399028854534
61 -6.321406098082662 -26.7047217556024
62 -6.955491475760937 -26.244794902859052
63 -7.150830093771219 -25.548365085275513
64 -6.1877740900963545 -25.45878528601009
65 -6.998084891587496 -24.879106999799365
66 -6.815972376614809 -24.828695359328833
67 -6.87775394320488 -24.592745144504722
68 -6.689438907429576 -23.978745577896312
69 -6.386662373319268 -23.57262108435893
70 -5.918024209793657 -23.44970807952351
71 -6.145885455422103 -22.745309160183492
72 -5.971009776927531 -22.60679894414887
73 -6.271669493988156 -22.19891031871716
74 -5.850228643044829 -20.656863763892378
75 -5.37866434501484 -20.444472560731253
76 -5.659463619813323 -20.19699010077007
77 -6.321869596838951 -20.13839114930498
78 -6.002499129623175 -19.63760343800059
79 -5.874450539238751 -19.515598718228343
80 -5.401754258200526 -18.92838809611677
81 -5.648359416984022 -17.994774057192853
82 -4.974386224057525 -17.55742370467821
83 -4.734584549907595 -16.823073927842348
84 -4.543328834755812 -14.855082803515382
85 -4.736277393065393 -14.531424598833084
86 -4.079301109071821 -14.442420089224363
87 -4.599784381687641 -13.596012850960644
88 -3.4510012213140726 -12.68135972540495
89 -4.6934422589838505 -12.66418205637357
90 -4.218757268739864 -12.30017947419658
91 -3.9592444552108645 -12.151904772081672
92 -3.8502428022402455 -11.788852141676486
93 -3.8559916922822595 -10.869989101210326
94 -3.8204314910108224 -10.327681503524177
95 -3.385466090403497 -9.8572159761571
96 -3.0983410296030343 -8.330116995310416
97 -3.4518029680475593 -8.133195842510668
98 -3.5837469482794404 -8.108197691178031
99 -2.2810247116722167 -7.57539849177145
100 -2.16570690786466 -7.362443126623615
101 -2.1491549706552178 -7.108327355338034
102 -2.1714031958254054 -6.959063561385431
103 -2.1238365147728473 -6.776946485018116
104 -2.054421527252998 -6.7220638398623045
105 -2.1886423643445596 -6.719970621583102
106 -3.348400494083762 -6.535447341844848
107 -2.58335882530082 -6.51820418055673
108 -2.546295173233375 -5.615796733870542
109 -2.4804105181246996 -5.34720210027791
110 -2.110882422653958 -5.078485007852753
111 -2.1525584753835574 -5.027957977402961
112 -2.2202001195400953 -4.827572916892203
113 -2.1236005413811654 -4.63049541560991
114 -2.224703344516456 -4.230832004686763
115 -2.3849474508315325 -4.031048624093466
116 -2.1086086235009134 -3.3844671463622564
117 -2.266268069855869 -3.3322555012187633
118 -2.229419973678887 -2.6416623314910934
119 -1.9517760025337338 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

demos: (120, 50, 12)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.0005660220112854475, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.9712e-04, -4.6377e-04,  8.0563e-02,  3.9929e-05, -1.3641e-01,
         -1.3940e-01, -1.1377e-03, -1.2933e-02,  5.2232e-06,  1.3407e-04,
          1.7065e-03, -2.4958e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.0005391114643515138, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0198e-03, -9.8248e-03,  7.3772e-02, -2.9152e-02, -9.3101e-02,
          1.3344e-04,  1.5265e-03,  1.5931e-02,  2.2980e-04, -8.1146e-02,
          1.7064e-03, -3.0002e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.1092794309071227, val_acc 0.975
trigger times: 1
end of epoch 3: val_loss 0.012640327831710358, val_acc 0.99
trigger times: 2
end of epoch 4: val_loss 0.04467311350357654, val_acc 0.98
trigger times: 3
end of epoch 5: val_loss 0.0062411678260646045, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 0.0003799934689068607, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.8645e-02, -8.6581e-03,  3.6129e-02,  8.0113e-03, -2.4013e-02,
         -1.1988e-04, -1.6263e-04,  1.0945e-03, -2.6196e-04, -5.9274e-06,
          1.7061e-03, -3.0385e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.000711672271792203, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.9137928372679174, val_acc 0.845
trigger times: 2
end of epoch 9: val_loss 0.8504812512149263, val_acc 0.88
trigger times: 3
end of epoch 10: val_loss 0.0008500840635835872, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.7978707767939983, val_acc 0.87
trigger times: 5
end of epoch 12: val_loss 0.013580514201770697, val_acc 0.995
trigger times: 6
end of epoch 13: val_loss 0.052353501354035575, val_acc 0.985
trigger times: 7
end of epoch 14: val_loss 0.03444049425336665, val_acc 0.985
trigger times: 8
end of epoch 15: val_loss 0.0012270387006507377, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 0.10495921254742012, val_acc 0.965
trigger times: 10
Early stopping.
0 -32.41223372519016 -50.00222059884506
1 -37.68812698125839 -48.823739140882175
2 -52.42358499765396 -47.15605336419176
3 -44.27116617560387 -46.19619104961985
4 -31.50716343522072 -45.594227093057754
5 -35.83003482222557 -45.36842966452394
6 -56.218703627586365 -45.19068756322445
7 -33.93276399374008 -44.16490839583478
8 -33.316431634128094 -44.078303125872196
9 -48.11895167827606 -43.993555290419714
10 -23.85098508745432 -43.86306534422809
11 -49.81122583150864 -43.84874807044028
12 -29.66128334403038 -43.84199129025074
13 -28.673495024442673 -43.806385671938365
14 -18.569391787052155 -43.80581985978556
15 -15.236397355794907 -42.7594767358323
16 -27.82661384344101 -42.66316468983175
17 -33.880367666482925 -42.33177224591743
18 -35.03240865468979 -41.774963389485094
19 -21.094666250050068 -41.410068073767725
20 -22.646666198968887 -41.17786296442943
21 -19.220022156834602 -40.723520424948155
22 -49.03486940264702 -40.527189756101116
23 -27.26604028046131 -40.49595848244517
24 -31.54511694610119 -40.429389880911344
25 -16.354574501514435 -40.05653450521898
26 -27.420368619263172 -39.59232357792555
27 -42.93617233633995 -39.54162101198148
28 -37.97020784020424 -39.195237471709476
29 -41.28779521584511 -39.172389579378766
30 -42.08758062124252 -38.51095963496708
31 -34.101839765906334 -38.447265769744824
32 -46.58282056450844 -38.392107037026264
33 -12.644008994102478 -38.00745349944469
34 -23.918143928050995 -37.46482488602393
35 -28.721089899539948 -37.10988160586883
36 -26.770717322826385 -34.27116723637227
37 -11.588572978973389 -34.14139118114101
38 -26.388786993920803 -33.263072731706835
39 -15.614744290709496 -33.13344797200536
40 -18.874285638332367 -33.07825234291984
41 -30.352912694215775 -33.0321314765637
42 -38.548849165439606 -32.44934973065406
43 -29.18296456336975 -32.4007978120153
44 -19.424865171313286 -32.40063925734975
45 -19.43263817578554 -30.734403792103194
46 -8.074515908956528 -30.57151371770873
47 -14.980413526296616 -30.131236504472803
48 -23.78263209760189 -29.99326722619033
49 -6.780398443341255 -29.66908258985071
50 -41.67760789394379 -29.297233511513635
51 -21.57531549036503 -29.288890423975797
52 -7.6469855308532715 -29.145878352769948
53 -26.94879512116313 -28.49601894351319
54 -11.699242755770683 -28.492023661124072
55 -22.92247150838375 -28.315961465855167
56 -14.654488127678633 -27.121110566589827
57 -13.088276334106922 -26.064532595535336
58 -19.054437093436718 -25.520524278341334
59 -28.512537986040115 -25.27101421179229
60 -9.143799647688866 -25.066644278800943
61 -31.816811621189117 -24.925849381327673
62 -15.08983363211155 -24.188105669766596
63 -28.682186663150787 -23.48479966198816
64 -11.193120818585157 -23.153943559703283
65 -26.18968316912651 -22.954730295117237
66 -18.40860229730606 -22.74124885266394
67 -23.57909170538187 -22.739273544503753
68 -7.505367822945118 -22.264945050603636
69 -21.706584312021732 -22.15569724300287
70 -23.79303666949272 -21.055920928583344
71 -7.921790011227131 -20.543356562348553
72 -5.16718614846468 -20.33499633836848
73 -19.296407863497734 -20.18157658281111
74 -17.6722968891263 -19.58144410477429
75 -16.644514119252563 -19.377225745334304
76 -24.71186798810959 -19.243135617403095
77 -15.1368003860116 -19.060620225371707
78 -12.60639251023531 -18.964124524696246
79 -18.282903388142586 -18.448962308005108
80 -23.68687552958727 -17.740722019993825
81 -9.27661545574665 -16.85889369985028
82 -1.8008963465690613 -16.3381194095591
83 -16.191471338272095 -14.535892564189266
84 -8.910713791847229 -14.443670567499144
85 -7.399253435432911 -14.200413010108107
86 -16.032192163169384 -13.936976181618805
87 -13.734541863203049 -13.862253042167257
88 -21.783850787207484 -13.483098530680483
89 -4.078713960945606 -13.455892754889845
90 -5.468742407858372 -13.355868275096913
91 -17.681120235472918 -12.278515244993585
92 -7.003917785361409 -12.227387460046547
93 -9.24495179951191 -12.020717825467683
94 -3.865086778998375 -11.910094799877324
95 -6.945936277508736 -11.400284019256157
96 -16.328948787413538 -11.134618158086587
97 -9.09891002997756 -10.859166921158222
98 -6.114232625812292 -9.595137958067907
99 -4.031027317047119 -9.289921608799773
100 -22.297299372032285 -8.230877068641124
101 -17.243256527930498 -7.882363241796725
102 -19.298681687563658 -7.6478984168416355
103 -16.550406012684107 -7.459623237418707
104 -17.4308180809021 -7.124357312750265
105 -13.083840601146221 -7.05379065585803
106 -15.592139724642038 -6.853091098326624
107 -16.726506754755974 -6.6211384471641495
108 -13.233449187129736 -6.494555224953677
109 -14.335331607609987 -6.117354180737655
110 -7.52927010692656 -6.087055095509873
111 -18.9953250028193 -5.43500831968483
112 2.068781055510044 -5.105291741614599
113 -6.281859301030636 -4.628649413275992
114 -3.356775052845478 -4.471031187897325
115 -15.210173036903143 -4.455504779070034
116 -6.649977073073387 -4.2805498188182405
117 -7.756246194243431 -3.7944735717969627
118 -8.066939536482096 -2.9512438456190186
119 -6.546789268963039 -2.541618164765197
train accuracy: 0.9405555555555556
validation accuracy: 0.965

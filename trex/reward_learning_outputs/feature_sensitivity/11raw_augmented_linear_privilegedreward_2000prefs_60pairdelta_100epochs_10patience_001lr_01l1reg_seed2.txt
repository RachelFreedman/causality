Using reward based purely on privileged features...
demos: (120, 50, 12)
demo_rewards: (120,)
[-14.85791774 -13.76094503 -13.66535661 -13.42155585 -12.79752535
 -12.27695738 -12.1291282  -11.72643822 -11.57570342 -11.4560068
 -11.42866947 -11.22058305 -11.19541909 -10.99446173 -10.98210088
 -10.8571586  -10.63711923 -10.58125298 -10.57318568 -10.3663351
 -10.20167178 -10.11221097  -9.98959279  -9.63633162  -9.6011403
  -9.57875874  -9.31410649  -9.30546471  -9.22470903  -9.10966233
  -8.97686902  -8.63508006  -8.59927311  -8.30647364  -8.13122117
  -8.05033441  -8.01118418  -7.69056555  -7.64306483  -7.4153496
  -7.36787644  -7.36495659  -7.29057777  -7.17760929  -7.17153839
  -6.98241694  -6.93519891  -6.76900782  -6.64725897  -6.52709835
  -6.52152749  -6.41487851  -6.41037209  -6.36940409  -6.23328883
  -6.09453464  -5.99716727  -5.95074116  -5.93230188  -5.89948025
  -5.83692643  -5.80246552  -5.54279998  -5.53970939  -5.44046594
  -5.42333419  -5.41391022  -5.37107565  -5.32325185  -5.27134888
  -5.26824041  -5.1794402   -5.11163938  -5.10276504  -5.00834401
  -4.94983944  -4.7295443   -4.70425209  -4.65421193  -4.61602401
  -4.61436506  -4.57205556  -4.51428412  -4.39516432  -4.38817518
  -4.37430519  -4.35568938  -4.31449272  -4.26200003  -4.22302916
  -4.1333606   -4.08321706  -4.02633649  -3.92623516  -3.90981626
  -3.89942827  -3.89032645  -3.8872099   -3.88489119  -3.75465691
  -3.74416519  -3.71317916  -3.34717703  -3.29453532  -3.12907393
  -3.08291211  -3.04161692  -2.85755847  -2.81526423  -2.62322513
  -2.54733891  -2.52538476  -2.44036713  -2.34252113  -2.26615554
  -2.25698833  -2.08748768  -2.00991353  -1.53424024  -1.27345687]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.009800524140355265, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2495e-03, -3.0597e-04,  9.4042e-03, -1.1523e-02,  6.8954e-04,
         -5.5286e-04,  2.2943e-03, -1.4546e-03, -1.2010e-03,  4.9461e-05,
          1.7065e-03, -7.5123e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.003756150447129585, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.2100e-05, -1.7450e-04,  8.1729e-04, -1.2182e-03,  1.2122e-04,
         -2.5156e-04, -3.7501e-04, -5.8996e-04, -3.1707e-04, -1.5178e-04,
          1.7064e-03, -9.4017e-01]], device='cuda:0'))])
end of epoch 2: val_loss 0.00133177332118521, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4176e-05, -1.1181e-04,  1.1108e-04, -2.4900e-04, -6.2173e-05,
          1.0034e-04,  6.8144e-05, -6.9750e-05,  7.3475e-05,  2.0327e-04,
          1.7063e-03, -1.1511e+00]], device='cuda:0'))])
end of epoch 3: val_loss 7.773737510738954e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.7544e-06,  8.6118e-07,  1.1774e-05,  1.3259e-05,  1.7469e-04,
         -9.9986e-05,  6.5709e-06,  2.4452e-06, -3.2588e-04,  1.7443e-05,
          1.7063e-03, -1.7579e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.0002538020882922609, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 6.033869722649854e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.4461e-05, -8.8262e-06,  3.1368e-02,  4.0184e-05, -2.1001e-04,
          5.6383e-05, -7.3458e-03, -1.4104e-06,  4.7629e-05, -1.1485e-04,
          1.7061e-03, -1.9002e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.00018479251705841193, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0001265711239603462, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.0015397860292118537, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.002853669044540368, val_acc 1.0
trigger times: 4
end of epoch 10: val_loss 0.018742068194596905, val_acc 0.99
trigger times: 5
end of epoch 11: val_loss 8.353926366261533e-05, val_acc 1.0
trigger times: 6
end of epoch 12: val_loss 0.10767507340916513, val_acc 0.975
trigger times: 7
end of epoch 13: val_loss 0.04200766530129105, val_acc 0.995
trigger times: 8
end of epoch 14: val_loss 0.10284205588335688, val_acc 0.965
trigger times: 9
end of epoch 15: val_loss 1.1109072594040166, val_acc 0.86
trigger times: 10
Early stopping.
0 -4.24297259747982 -14.857917737169885
1 -1.222345307469368 -13.760945032136553
2 0.9292898327112198 -13.665356611697247
3 -25.188920691609383 -13.421555850023143
4 -10.79614321142435 -12.79752535131424
5 -35.012022256851196 -12.276957383174551
6 -28.34396255016327 -12.129128199666564
7 -13.99872200936079 -11.726438221774067
8 -4.5842466205358505 -11.575703421057804
9 -1.957964301109314 -11.45600679926644
10 -19.47333315014839 -11.428669465994592
11 -17.323214881122112 -11.220583049581975
12 -5.9189498052001 -11.195419085153274
13 -5.8866203017532825 -10.994461725751476
14 -8.802103959023952 -10.982100883253427
15 -20.96829779446125 -10.857158604065402
16 -0.17043416947126389 -10.637119226889629
17 -8.360465131700039 -10.581252977120647
18 -28.705865681171417 -10.57318568221164
19 -17.383072309195995 -10.366335101082012
20 12.268590457737446 -10.201671783331733
21 10.011453866958618 -10.112210969210379
22 2.443480871617794 -9.989592788267442
23 -18.253643169999123 -9.636331624790365
24 -2.2977904826402664 -9.601140304879115
25 -7.502909928560257 -9.578758736670897
26 0.006139442324638367 -9.314106491939148
27 21.385755065828562 -9.305464709502179
28 -25.360547736287117 -9.224709027836628
29 -9.28939052671194 -9.109662326462507
30 -3.4869934245944023 -8.9768690196357
31 -6.465251572430134 -8.635080059050962
32 9.924377590417862 -8.599273111315565
33 -2.794185910373926 -8.306473643465596
34 -21.289493277668953 -8.131221165134493
35 1.3368007317185402 -8.050334414267054
36 -12.323560234159231 -8.011184177045752
37 -6.960230313241482 -7.690565550918285
38 4.1782776564359665 -7.643064825505027
39 -3.973366999067366 -7.415349598732486
40 -4.278237532824278 -7.367876442206603
41 7.7024375684559345 -7.364956588829062
42 -5.259167019277811 -7.290577772068284
43 4.679358847439289 -7.177609292647348
44 -19.343466445803642 -7.171538386651344
45 -1.2740606926381588 -6.9824169445409625
46 -8.641974572092295 -6.9351989118904225
47 -15.54127512872219 -6.7690078175369015
48 13.881234250962734 -6.647258965381362
49 0.5948125924915075 -6.52709834517458
50 -22.134017065167427 -6.521527485301631
51 -19.939321406185627 -6.414878510216887
52 -19.58636886999011 -6.410372091550173
53 -10.360081106424332 -6.3694040946839525
54 10.831202685832977 -6.233288826291319
55 14.196345426142216 -6.094534642524499
56 5.104586813598871 -5.997167272764618
57 -5.098410900682211 -5.950741155645245
58 1.6889034751802683 -5.93230188156937
59 2.0389673486351967 -5.899480249325287
60 -4.63831901922822 -5.836926428931782
61 7.296033933758736 -5.80246551779649
62 -0.31853696145117283 -5.542799983276438
63 -17.65648240596056 -5.539709393523854
64 2.178777627646923 -5.440465935277435
65 5.713608369231224 -5.423334187907614
66 -3.5396772883832455 -5.4139102154677
67 -1.3004765966907144 -5.371075648457043
68 -21.632039174437523 -5.323251845363989
69 -4.274169959127903 -5.271348878324376
70 -3.0240075886249542 -5.268240406327687
71 1.4457937106490135 -5.179440204599235
72 -4.361268328502774 -5.111639376062079
73 4.049716137349606 -5.102765035513635
74 4.289027724415064 -5.008344009609225
75 0.5494820028543472 -4.949839435408552
76 6.5265214294195175 -4.729544301417749
77 2.5476048216223717 -4.704252094305983
78 -0.2671901825815439 -4.654211925967799
79 1.2691154479980469 -4.616024012787141
80 -2.692862303927541 -4.614365057638494
81 -8.093039490282536 -4.572055555427089
82 8.458855241537094 -4.514284121276057
83 -9.750219449400902 -4.395164317798481
84 -1.146269541233778 -4.388175179034372
85 1.3039471432566643 -4.3743051941816775
86 3.737096633762121 -4.355689380234473
87 6.1291914358735085 -4.3144927222222025
88 1.8892398122698069 -4.2620000260882644
89 -1.5345506891608238 -4.2230291643481825
90 3.3607395151630044 -4.133360599164207
91 0.16570373624563217 -4.083217057852922
92 -2.3256331644952297 -4.026336493873877
93 -3.638810943812132 -3.9262351613387976
94 4.777350626885891 -3.9098162592153867
95 -0.9580662734806538 -3.8994282747402877
96 -0.5493422895669937 -3.8903264513768816
97 3.152758188545704 -3.8872099001190463
98 4.869171757251024 -3.8848911864576006
99 -7.08184652402997 -3.7546569093221462
100 4.532964207231998 -3.744165186620453
101 3.246550217270851 -3.713179158085272
102 5.161440182477236 -3.3471770256080786
103 -2.0286727799102664 -3.294535322484194
104 7.5588162913918495 -3.129073931650425
105 8.208912905305624 -3.082912105420874
106 4.490375615656376 -3.041616924495897
107 6.611788529902697 -2.8575584743648856
108 3.8965674210339785 -2.815264229940162
109 3.7084443867206573 -2.62322513096209
110 3.213718742132187 -2.547338912767599
111 5.757734069600701 -2.5253847629391624
112 7.242835909128189 -2.4403671323608287
113 6.930057123303413 -2.342521127844057
114 4.136706151068211 -2.266155535426123
115 7.511823244392872 -2.256988325062527
116 6.418040165677667 -2.0874876766280326
117 9.029648706316948 -2.0099135282821634
118 5.320804566144943 -1.5342402374776334
119 7.023145779967308 -1.2734568732176126
train accuracy: 0.8305555555555556
validation accuracy: 0.86

Using reward based purely on privileged features...
demos: (120, 50, 12)
demo_rewards: (120,)
[-14.85791774 -13.76094503 -13.66535661 -13.42155585 -12.79752535
 -12.27695738 -12.1291282  -11.72643822 -11.57570342 -11.4560068
 -11.42866947 -11.22058305 -11.19541909 -10.99446173 -10.98210088
 -10.8571586  -10.63711923 -10.58125298 -10.57318568 -10.3663351
 -10.20167178 -10.11221097  -9.98959279  -9.63633162  -9.6011403
  -9.57875874  -9.31410649  -9.30546471  -9.22470903  -9.10966233
  -8.97686902  -8.63508006  -8.59927311  -8.30647364  -8.13122117
  -8.05033441  -8.01118418  -7.69056555  -7.64306483  -7.4153496
  -7.36787644  -7.36495659  -7.29057777  -7.17760929  -7.17153839
  -6.98241694  -6.93519891  -6.76900782  -6.64725897  -6.52709835
  -6.52152749  -6.41487851  -6.41037209  -6.36940409  -6.23328883
  -6.09453464  -5.99716727  -5.95074116  -5.93230188  -5.89948025
  -5.83692643  -5.80246552  -5.54279998  -5.53970939  -5.44046594
  -5.42333419  -5.41391022  -5.37107565  -5.32325185  -5.27134888
  -5.26824041  -5.1794402   -5.11163938  -5.10276504  -5.00834401
  -4.94983944  -4.7295443   -4.70425209  -4.65421193  -4.61602401
  -4.61436506  -4.57205556  -4.51428412  -4.39516432  -4.38817518
  -4.37430519  -4.35568938  -4.31449272  -4.26200003  -4.22302916
  -4.1333606   -4.08321706  -4.02633649  -3.92623516  -3.90981626
  -3.89942827  -3.89032645  -3.8872099   -3.88489119  -3.75465691
  -3.74416519  -3.71317916  -3.34717703  -3.29453532  -3.12907393
  -3.08291211  -3.04161692  -2.85755847  -2.81526423  -2.62322513
  -2.54733891  -2.52538476  -2.44036713  -2.34252113  -2.26615554
  -2.25698833  -2.08748768  -2.00991353  -1.53424024  -1.27345687]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.03724430569981216, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2973e-01, -2.1902e-02,  1.4575e-01, -4.2756e-02,  4.8549e-03,
          2.4744e-02, -1.5795e-02,  2.3502e-02,  1.1341e-03,  6.5132e-03,
         -2.6687e-03, -1.9883e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.1605911540824831, val_acc 0.985
trigger times: 1
end of epoch 2: val_loss 0.014059437556461489, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0415e-02, -8.0493e-05,  3.2762e-03, -2.3021e-03,  4.6983e-04,
         -4.7792e-03, -4.6320e-04,  5.9856e-03,  3.7720e-04, -1.7574e-04,
         -2.6691e-03, -7.3188e-01]], device='cuda:0'))])
end of epoch 3: val_loss 0.0018499459077494862, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.9487e-05,  1.4444e-04,  8.5719e-05, -3.6775e-04, -4.7006e-04,
         -9.8918e-05, -5.8495e-04, -2.4132e-04, -5.1901e-05, -2.9232e-05,
         -2.6693e-03, -1.1578e+00]], device='cuda:0'))])
end of epoch 4: val_loss 6.21636949334814e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.2450e-04,  6.2049e-06, -4.6210e-05, -2.4842e-05,  1.4450e-05,
         -6.7749e-07, -1.2037e-06, -1.8192e-05, -8.8566e-05,  1.2073e-05,
         -2.6695e-03, -1.9300e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.25774827722660915, val_acc 0.94
trigger times: 1
end of epoch 6: val_loss 0.0006222648383092278, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 0.00039845718259151397, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 0.0012614282827885504, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 0.00012453572216976737, val_acc 1.0
trigger times: 5
end of epoch 10: val_loss 9.328524115982617e-05, val_acc 1.0
trigger times: 6
end of epoch 11: val_loss 0.03949275481269247, val_acc 0.98
trigger times: 7
end of epoch 12: val_loss 0.11685299735697789, val_acc 0.985
trigger times: 8
end of epoch 13: val_loss 0.8934500169856358, val_acc 0.91
trigger times: 9
end of epoch 14: val_loss 0.0029885278852884766, val_acc 1.0
trigger times: 10
Early stopping.
0 -15.557569354772568 -14.857917737169885
1 -14.235237643122673 -13.760945032136553
2 -14.320427253842354 -13.665356611697247
3 -13.704783886671066 -13.421555850023143
4 -13.554307658225298 -12.79752535131424
5 -12.322073301300406 -12.276957383174551
6 -12.53323994204402 -12.129128199666564
7 -12.304480709135532 -11.726438221774067
8 -12.05050789564848 -11.575703421057804
9 -12.040788471698761 -11.45600679926644
10 -11.804339781403542 -11.428669465994592
11 -11.36801166832447 -11.220583049581975
12 -11.401914358139038 -11.195419085153274
13 -11.518448118120432 -10.994461725751476
14 -11.282697755843401 -10.982100883253427
15 -11.010278917849064 -10.857158604065402
16 -11.186382584273815 -10.637119226889629
17 -10.843860283493996 -10.581252977120647
18 -10.869390090927482 -10.57318568221164
19 -10.621809616684914 -10.366335101082012
20 -11.112614661455154 -10.201671783331733
21 -10.652272902429104 -10.112210969210379
22 -10.439377691596746 -9.989592788267442
23 -9.911341853439808 -9.636331624790365
24 -9.952853620052338 -9.601140304879115
25 -9.740044243633747 -9.578758736670897
26 -9.683382822200656 -9.314106491939148
27 -10.180137537419796 -9.305464709502179
28 -9.481491312384605 -9.224709027836628
29 -9.501726580783725 -9.109662326462507
30 -9.280362129211426 -8.9768690196357
31 -8.795539997518063 -8.635080059050962
32 -9.114154480397701 -8.599273111315565
33 -8.59811913408339 -8.306473643465596
34 -8.14358084462583 -8.131221165134493
35 -8.4305828679353 -8.050334414267054
36 -8.125130357220769 -8.011184177045752
37 -8.032378900796175 -7.690565550918285
38 -7.953229255974293 -7.643064825505027
39 -7.946019629947841 -7.415349598732486
40 -7.513048013672233 -7.367876442206603
41 -7.619919705903158 -7.364956588829062
42 -7.77664412278682 -7.290577772068284
43 -7.412370063364506 -7.177609292647348
44 -7.412545770406723 -7.171538386651344
45 -7.139853350818157 -6.9824169445409625
46 -7.232965175062418 -6.9351989118904225
47 -6.634014477021992 -6.7690078175369015
48 -7.028588090091944 -6.647258965381362
49 -6.965243387967348 -6.52709834517458
50 -6.782305343076587 -6.521527485301631
51 -6.712917193770409 -6.414878510216887
52 -6.739326551556587 -6.410372091550173
53 -6.439718194305897 -6.3694040946839525
54 -6.659712794236839 -6.233288826291319
55 -6.401659153401852 -6.094534642524499
56 -6.08594847843051 -5.997167272764618
57 -6.436320504173636 -5.950741155645245
58 -5.946843484416604 -5.93230188156937
59 -6.40644646435976 -5.899480249325287
60 -6.1600538259372115 -5.836926428931782
61 -6.300323203206062 -5.80246551779649
62 -5.869941103854217 -5.542799983276438
63 -5.82496696151793 -5.539709393523854
64 -5.898185890167952 -5.440465935277435
65 -5.499662520363927 -5.423334187907614
66 -5.690665751695633 -5.4139102154677
67 -5.784510478377342 -5.371075648457043
68 -5.563292809762061 -5.323251845363989
69 -5.673459073528647 -5.271348878324376
70 -5.585441919974983 -5.268240406327687
71 -5.2310581766068935 -5.179440204599235
72 -5.492844484746456 -5.111639376062079
73 -5.113924380391836 -5.102765035513635
74 -5.240256521850824 -5.008344009609225
75 -5.351612871512771 -4.949839435408552
76 -4.768605433404446 -4.729544301417749
77 -5.160327680408955 -4.704252094305983
78 -4.996661759912968 -4.654211925967799
79 -4.903772001620382 -4.616024012787141
80 -4.930672481190413 -4.614365057638494
81 -4.554782750084996 -4.572055555427089
82 -4.543903570622206 -4.514284121276057
83 -4.387741398066282 -4.395164317798481
84 -4.710175854153931 -4.388175179034372
85 -4.715659519657493 -4.3743051941816775
86 -4.522792674601078 -4.355689380234473
87 -4.315776901319623 -4.3144927222222025
88 -4.313625844195485 -4.2620000260882644
89 -4.516757163684815 -4.2230291643481825
90 -4.209604447707534 -4.133360599164207
91 -4.19744773581624 -4.083217057852922
92 -4.302287321537733 -4.026336493873877
93 -4.137341531924903 -3.9262351613387976
94 -3.953108999412507 -3.9098162592153867
95 -4.027901920489967 -3.8994282747402877
96 -3.9507953971624374 -3.8903264513768816
97 -3.9362282771617174 -3.8872099001190463
98 -3.9326038397848606 -3.8848911864576006
99 -3.92192754894495 -3.7546569093221462
100 -3.7854213900864124 -3.744165186620453
101 -3.637756137177348 -3.713179158085272
102 -3.5674126110970974 -3.3471770256080786
103 -3.2411675276234746 -3.294535322484194
104 -3.0966470018029213 -3.129073931650425
105 -3.026705975178629 -3.082912105420874
106 -3.0632268022745848 -3.041616924495897
107 -2.88102662214078 -2.8575584743648856
108 -2.6864298065192997 -2.815264229940162
109 -2.8110210923478007 -2.62322513096209
110 -2.564912864472717 -2.547338912767599
111 -2.4747316278517246 -2.5253847629391624
112 -2.309854223858565 -2.4403671323608287
113 -2.216746254824102 -2.342521127844057
114 -2.1442718245089054 -2.266155535426123
115 -2.150728323031217 -2.256988325062527
116 -2.0241369823925197 -2.0874876766280326
117 -1.886434217914939 -2.0099135282821634
118 -1.4784178780391812 -1.5342402374776334
119 -1.1841833260841668 -1.2734568732176126
train accuracy: 1.0
validation accuracy: 1.0

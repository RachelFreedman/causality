demos: (360, 50, 3)
demo_rewards: (360,)
sorted_train_rewards: [-51.15845655 -50.95730303 -48.93380144 -48.88659405 -48.81199543
 -47.73632204 -47.63792498 -47.09890202 -47.07471735 -45.58205756
 -45.55641869 -45.3781584  -45.10455724 -45.09649042 -45.06090831
 -44.90717655 -44.66710015 -44.58646443 -44.40276741 -44.29353296
 -44.26132764 -44.205686   -44.18844659 -44.18649198 -43.97817017
 -43.90835803 -43.90233576 -43.67337234 -43.63795834 -43.57867127
 -43.41725811 -43.07917355 -42.78812793 -42.73280043 -42.37660312
 -42.35156982 -42.35130146 -42.2741431  -42.27178642 -42.06976011
 -42.02441567 -41.9208162  -41.85021898 -41.83003384 -41.71300333
 -41.60676811 -41.5158946  -41.4288974  -41.35473131 -41.27204701
 -41.26835888 -41.17517134 -41.09778195 -41.05637645 -41.03450743
 -40.88277159 -40.7682006  -40.57815481 -40.52517704 -40.51972295
 -40.4735601  -40.44146654 -40.36907268 -40.27883122 -40.20765046
 -40.10250365 -40.09758852 -39.48434976 -39.39946011 -39.38715059
 -39.27347855 -39.23883604 -39.23607073 -39.21869941 -39.21327141
 -39.20948267 -39.10415592 -39.02946046 -39.01949066 -39.00315068
 -38.70707769 -38.40734868 -38.32682998 -38.09085041 -37.84470936
 -37.52355928 -37.45493938 -37.19212668 -36.58641248 -36.39170138
 -36.34182169 -36.30418157 -36.28270385 -36.11080763 -36.03273585
 -35.76965183 -35.72835706 -35.45254062 -35.34226992 -34.87824674
 -34.56405996 -34.47986837 -34.42748575 -34.13144584 -34.0228191
 -33.83098719 -33.80045069 -33.61206111 -33.59175617 -33.55287238
 -33.51725369 -33.49044026 -33.40995872 -33.3865897  -33.37339807
 -33.34018377 -33.27546848 -33.26935494 -32.98496162 -32.8995352
 -32.81082139 -32.78886158 -32.74101924 -32.7313686  -32.46498395
 -32.03564157 -31.90682883 -31.83556376 -31.79116041 -31.50474657
 -30.99431461 -30.93631085 -30.90992683 -30.86825618 -30.8534927
 -30.80257739 -30.68244687 -30.48486935 -29.90514552 -29.87882928
 -29.5623783  -29.49907263 -29.48688888 -29.47008372 -29.4431262
 -29.16728751 -29.08632404 -28.75963456 -28.44248532 -28.26842147
 -28.13673546 -28.08369991 -28.02777081 -27.84489659 -27.61843292
 -27.55718389 -27.53369262 -27.06419421 -27.0029758  -26.76388365
 -26.60775678 -26.41157825 -26.14307162 -26.10745646 -26.09478846
 -25.91146004 -25.38206609 -25.33112486 -25.30351849 -25.18999305
 -25.13806635 -25.01985189 -24.2272877  -24.05919843 -23.72346546
 -23.61704597 -23.57894664 -23.49865824 -23.3885205  -23.23362913
 -22.8922421  -22.71273286 -22.65599334 -22.64864934 -22.55715908
 -22.47349563 -22.28075152 -22.20139031 -22.12605556 -22.02464667
 -21.98062894 -21.90842064 -21.73813504 -21.51677505 -20.49946861
 -20.14066085 -20.06168433 -19.95070877 -19.94914683 -19.89507051
 -19.78533441 -19.75469854 -19.7198622  -19.71871673 -19.69991772
 -19.51898865 -19.35763998 -19.06566814 -19.02836343 -18.79967717
 -18.79931946 -18.69683752 -18.69338489 -18.23945715 -18.12685193
 -18.0241144  -17.92817501 -17.84544435 -17.74148009 -17.68079099
 -17.26560969 -17.20010184 -17.06094997 -16.98261229 -16.7595022
 -16.72635288 -16.67622467 -16.29122883 -16.26549553 -16.090455
 -16.05119129 -15.66572915 -15.29501063 -15.23723291 -15.00526075
 -14.96920977 -14.77403271 -14.66804697 -14.39781398 -14.11669085
 -14.111545   -13.79702407 -13.60070757 -13.54966089 -13.4867368
 -13.39595816 -13.26567075 -13.24583955 -13.16316874 -12.69460232
 -12.34665554 -12.30014847 -12.04804099 -11.49544096 -11.43360312
 -11.39697889 -11.20504127 -10.93302499 -10.65880318 -10.58831597
 -10.51085023 -10.28383593 -10.20667265  -9.80901492  -9.4678784
  -8.96235737  -8.40276084  -8.36555609  -8.32638751  -8.0269637
  -7.83599816  -7.70107293  -7.68743448  -7.67259169  -7.57539849
  -7.5455064   -7.54460172  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.10832736  -6.95906356  -6.92008425
  -6.77694649  -6.72206384  -6.64795748  -6.51820418  -6.29894775
  -6.05448903  -5.94629658  -5.89467275  -5.85405865  -5.64485143
  -5.39544196  -5.38326081  -5.3472021   -5.25793019  -5.24856721
  -5.07848501  -5.06486011  -5.02795798  -4.90228293  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.329867
  -3.29356852  -3.07904644  -2.88591659  -2.83192847  -2.67390706
  -2.64166233  -2.49009827  -2.24005036  -1.91361965]
sorted_val_rewards: [-52.04485265 -46.40659067 -45.1889403  -42.71498917 -40.98849948
 -38.45325451 -37.4434499  -37.12776422 -36.83830962 -36.66844362
 -34.50306279 -29.8114822  -28.72161062 -28.13429517 -28.10052984
 -21.48630258 -21.28560677 -19.41242087 -18.59323781 -18.14667207
 -17.40776893 -16.04208483 -15.35726758 -14.96583859 -13.72364022
 -13.03199125 -13.02215529 -12.57904671 -12.05112503 -10.25505313
  -6.74096238  -6.71997062  -6.47884361  -5.61579673  -4.82757292
  -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 630
num val_labels 630
ModuleList(
  (0): Linear(in_features=3, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 3
Number of trainable paramters: 3
device: cuda:1
end of epoch 0: val_loss 0.04291367709039438, val_acc 0.9777777777777777
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.03487259389013541, val_acc 0.9809523809523809
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.03602736336896694, val_acc 0.9841269841269841
trigger times: 1
end of epoch 3: val_loss 0.054974451040134494, val_acc 0.9793650793650793
trigger times: 2
end of epoch 4: val_loss 0.043662021918936623, val_acc 0.9777777777777777
trigger times: 3
end of epoch 5: val_loss 0.05717269716086314, val_acc 0.9761904761904762
trigger times: 4
end of epoch 6: val_loss 0.03621637761954213, val_acc 0.9793650793650793
trigger times: 5
end of epoch 7: val_loss 0.039865925270493784, val_acc 0.9777777777777777
trigger times: 6
end of epoch 8: val_loss 0.03927412837890859, val_acc 0.9793650793650793
trigger times: 7
end of epoch 9: val_loss 0.04214480759336493, val_acc 0.9841269841269841
trigger times: 8
end of epoch 10: val_loss 0.075634248138297, val_acc 0.973015873015873
trigger times: 9
end of epoch 11: val_loss 0.038119751938557156, val_acc 0.9793650793650793
trigger times: 10
Early stopping.
0 -120.00959247350693 -52.04485264929628
1 -112.15504306554794 -46.406590674661416
2 -108.6191759109497 -45.18894029889883
3 -101.68981611728668 -42.714989172361854
4 -102.72579085826874 -40.98849948078526
5 -98.88971614837646 -38.45325450777397
6 -94.06010091304779 -37.44344989811742
7 -96.11244916915894 -37.12776421579827
8 -92.25612005591393 -36.83830961736186
9 -93.71827214956284 -36.668443624441274
10 -87.10545134544373 -34.503062789662465
11 -81.03617003560066 -29.811482202647895
12 -78.02125418186188 -28.721610616461756
13 -78.9670979976654 -28.134295168749553
14 -76.8884103000164 -28.100529843216183
15 -60.85232776403427 -21.486302579536073
16 -61.95148351788521 -21.285606773519348
17 -58.84011933207512 -19.412420866484695
18 -59.330363884568214 -18.593237812600588
19 -56.44093731045723 -18.14667206650413
20 -53.07748532295227 -17.40776892653412
21 -49.41938000917435 -16.04208483259092
22 -50.71565291285515 -15.35726757618557
23 -50.75654277205467 -14.96583859023408
24 -44.99379367008805 -13.723640217508256
25 -43.381140276789665 -13.031991254792343
26 -43.64237295091152 -13.022155288614591
27 -45.72992490231991 -12.579046708284254
28 -41.20900295674801 -12.051125026239662
29 -37.37591043114662 -10.255053134746943
30 -31.761594533920288 -6.740962377474064
31 -28.68544927984476 -6.719970621583102
32 -28.01345558464527 -6.47884361057096
33 -27.857590071856976 -5.615796733870542
34 -23.77912689372897 -4.827572916892203
35 -21.347923450171947 -3.3322555012187633
train accuracy: 0.981672591063716
validation accuracy: 0.9793650793650793

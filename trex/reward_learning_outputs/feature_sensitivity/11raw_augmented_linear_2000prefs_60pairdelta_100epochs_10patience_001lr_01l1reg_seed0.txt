demos: (120, 50, 12)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.03505593981545033, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5101e-03,  7.9278e-05,  8.6257e-02, -5.0714e-03, -1.9344e-01,
          5.8704e-04, -3.1303e-03,  2.3544e-02, -9.5110e-06, -8.1038e-05,
         -2.6687e-03, -2.0558e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.08440830047814689, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.02475716893026657, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.3367e-02, -1.3836e-02,  1.2293e-01, -5.5259e-02, -4.4805e-01,
          1.8876e-02,  9.1616e-03,  4.3864e-03,  5.8303e-02, -1.8393e-02,
         -2.6691e-03, -3.0841e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.04276363362114459, val_acc 0.995
trigger times: 1
end of epoch 4: val_loss 0.47528285184021557, val_acc 0.925
trigger times: 2
end of epoch 5: val_loss 0.02516554849011776, val_acc 0.995
trigger times: 3
end of epoch 6: val_loss 0.42088556792593806, val_acc 0.93
trigger times: 4
end of epoch 7: val_loss 0.02333174368695218, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.1696e-02, -2.8050e-02,  2.7821e-03, -3.2698e-02, -1.3557e-01,
         -3.9101e-03,  3.6546e-03,  3.8512e-02,  9.9958e-05, -3.6837e-04,
         -2.6700e-03, -2.5567e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.03257739159823537, val_acc 0.995
trigger times: 1
end of epoch 9: val_loss 0.15590706013612077, val_acc 0.97
trigger times: 2
end of epoch 10: val_loss 0.007212568586354102, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.1023e-05, -4.1971e-04,  2.6398e-03,  1.5569e-03, -2.7700e-04,
          5.3148e-05,  8.9438e-04,  1.7698e-03,  3.4591e-04,  3.1273e-04,
         -2.6706e-03, -2.5758e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.007515842649227018, val_acc 0.995
trigger times: 1
end of epoch 12: val_loss 0.06747263511537742, val_acc 0.99
trigger times: 2
end of epoch 13: val_loss 2.151458528112778, val_acc 0.855
trigger times: 3
end of epoch 14: val_loss 0.026667992553301757, val_acc 0.985
trigger times: 4
end of epoch 15: val_loss 0.04537147756460012, val_acc 0.995
trigger times: 5
end of epoch 16: val_loss 0.07351800062929371, val_acc 0.99
trigger times: 6
end of epoch 17: val_loss 0.8315874916502821, val_acc 0.905
trigger times: 7
end of epoch 18: val_loss 0.048177686244595855, val_acc 0.98
trigger times: 8
end of epoch 19: val_loss 0.010222865674165185, val_acc 0.995
trigger times: 9
end of epoch 20: val_loss 0.7947880042063606, val_acc 0.91
trigger times: 10
Early stopping.
0 -48.63920995593071 -50.00222059884506
1 -25.920660331845284 -48.823739140882175
2 -26.430007353425026 -47.15605336419176
3 -29.694781571626663 -46.19619104961985
4 -42.13310620188713 -45.594227093057754
5 -25.593961536884308 -45.36842966452394
6 -55.22468888759613 -45.19068756322445
7 -29.261772841215134 -44.16490839583478
8 -12.294100433588028 -44.078303125872196
9 -19.56685283780098 -43.993555290419714
10 -25.28909293562174 -43.86306534422809
11 -44.01083081960678 -43.84874807044028
12 -42.020355850458145 -43.84199129025074
13 -27.031812608242035 -43.806385671938365
14 -17.11836513876915 -43.80581985978556
15 -35.72852313518524 -42.7594767358323
16 -6.948899522423744 -42.66316468983175
17 -58.35437536239624 -42.33177224591743
18 -28.8275083899498 -41.774963389485094
19 -35.84370742738247 -41.410068073767725
20 -58.843619242310524 -41.17786296442943
21 -30.578475892543793 -40.723520424948155
22 -33.320429146289825 -40.527189756101116
23 -25.804668620228767 -40.49595848244517
24 -27.597643792629242 -40.429389880911344
25 -41.16533198952675 -40.05653450521898
26 -35.60066995024681 -39.59232357792555
27 -32.98676961660385 -39.54162101198148
28 -22.729131788015366 -39.195237471709476
29 -45.75687837600708 -39.172389579378766
30 -31.944880686700344 -38.51095963496708
31 -19.65717127174139 -38.447265769744824
32 -30.796587496995926 -38.392107037026264
33 -30.956280812621117 -38.00745349944469
34 -19.646949127316475 -37.46482488602393
35 -36.212771475315094 -37.10988160586883
36 -22.490799993276596 -34.27116723637227
37 -30.54931950569153 -34.14139118114101
38 -32.64878894388676 -33.263072731706835
39 -27.821371488273144 -33.13344797200536
40 -12.748373538255692 -33.07825234291984
41 -30.2562994658947 -33.0321314765637
42 -48.47066122293472 -32.44934973065406
43 -23.09119276702404 -32.4007978120153
44 -27.993863746523857 -32.40063925734975
45 -18.41386516392231 -30.734403792103194
46 -10.443116761744022 -30.57151371770873
47 -27.58184276521206 -30.131236504472803
48 -16.684459701180458 -29.99326722619033
49 -27.125744462013245 -29.66908258985071
50 -39.32531334459782 -29.297233511513635
51 -33.68583278730512 -29.288890423975797
52 -12.302485890686512 -29.145878352769948
53 -15.72999830916524 -28.49601894351319
54 -23.006983041763306 -28.492023661124072
55 -16.459438499063253 -28.315961465855167
56 -11.832822144031525 -27.121110566589827
57 -12.406647384166718 -26.064532595535336
58 -31.316375605762005 -25.520524278341334
59 -19.544820934534073 -25.27101421179229
60 -15.866466760635376 -25.066644278800943
61 -23.44729670137167 -24.925849381327673
62 -12.207481972873211 -24.188105669766596
63 -18.595051877200603 -23.48479966198816
64 -13.307789074257016 -23.153943559703283
65 -17.509433940052986 -22.954730295117237
66 -18.86725340038538 -22.74124885266394
67 -22.86678176559508 -22.739273544503753
68 -8.596553832292557 -22.264945050603636
69 -28.761709593236446 -22.15569724300287
70 -12.874131299555302 -21.055920928583344
71 -12.243568874895573 -20.543356562348553
72 -11.720618039369583 -20.33499633836848
73 -20.359612431377172 -20.18157658281111
74 -26.21440015733242 -19.58144410477429
75 -20.29598133265972 -19.377225745334304
76 -14.078093707561493 -19.243135617403095
77 -14.719629310071468 -19.060620225371707
78 -14.29707595333457 -18.964124524696246
79 -28.82413686811924 -18.448962308005108
80 -28.673787362873554 -17.740722019993825
81 -4.543083269149065 -16.85889369985028
82 -9.881458171643317 -16.3381194095591
83 -18.45749282836914 -14.535892564189266
84 -12.479707062244415 -14.443670567499144
85 -12.840702503919601 -14.200413010108107
86 -27.75480043143034 -13.936976181618805
87 -4.421119727194309 -13.862253042167257
88 -20.035031452775 -13.483098530680483
89 -12.098401833325624 -13.455892754889845
90 -7.103922111913562 -13.355868275096913
91 -20.314072297886014 -12.278515244993585
92 -5.747256342321634 -12.227387460046547
93 -7.476873576641083 -12.020717825467683
94 -11.140210419893265 -11.910094799877324
95 -1.7798529732972383 -11.400284019256157
96 -17.146191710606217 -11.134618158086587
97 -2.03527516964823 -10.859166921158222
98 -3.522275546565652 -9.595137958067907
99 -10.798151597380638 -9.289921608799773
100 -11.810483353212476 -8.230877068641124
101 -21.705046694725752 -7.882363241796725
102 -24.2030341476202 -7.6478984168416355
103 -20.46889928355813 -7.459623237418707
104 -23.63818760216236 -7.124357312750265
105 -5.374257057905197 -7.05379065585803
106 -18.20593223720789 -6.853091098326624
107 -20.302048850804567 -6.6211384471641495
108 -6.003185234963894 -6.494555224953677
109 -15.689902856945992 -6.117354180737655
110 -4.62072847224772 -6.087055095509873
111 -16.923994842916727 -5.43500831968483
112 -8.004026034846902 -5.105291741614599
113 -9.794311836361885 -4.628649413275992
114 -7.798352567479014 -4.471031187897325
115 -13.09356920234859 -4.455504779070034
116 -2.4637124594300985 -4.2805498188182405
117 -2.8481757137924433 -3.7944735717969627
118 -4.6513831205666065 -2.9512438456190186
119 -3.538096128962934 -2.541618164765197
train accuracy: 0.9205555555555556
validation accuracy: 0.91

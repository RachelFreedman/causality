demos: (120, 50, 9)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=9, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 9
Number of trainable paramters: 9
device: cuda:1
end of epoch 0: val_loss 0.0003265361631513386, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 1.671892336929659e-06, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 3.0744154252282384e-05, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 5.125979715003837e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 2.6940573661704547e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.425056540469169, val_acc 0.955
trigger times: 1
end of epoch 6: val_loss 0.0005971305604100508, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 2.0801984828011656e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 2.861022117883749e-08, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 3.8736517275950175e-06, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 1.0317497952172516e-06, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 5.245206132542535e-08, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 1.3113019647903456e-08, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 4.076496691851617e-06, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.003083812171862519, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 0.003873333506962808, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 1.8185165102480028e-06, val_acc 1.0
trigger times: 7
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.37832062961345114, val_acc 0.955
trigger times: 1
end of epoch 20: val_loss 6.576561432325434e-05, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 3.7371809348485384e-07, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 3.3378588675248013e-08, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 9.57800580181356e-07, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 2.384185471271394e-09, val_acc 1.0
trigger times: 6
end of epoch 25: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 7
end of epoch 26: val_loss 2.3895344122593086e-05, val_acc 1.0
trigger times: 8
end of epoch 27: val_loss 5.364417567932378e-09, val_acc 1.0
trigger times: 9
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0033632948068828485, val_acc 1.0
trigger times: 1
end of epoch 31: val_loss 3.7073982788626836e-07, val_acc 1.0
trigger times: 2
end of epoch 32: val_loss 3.6006410072886295e-06, val_acc 1.0
trigger times: 3
end of epoch 33: val_loss 0.0001618304951379912, val_acc 1.0
trigger times: 4
end of epoch 34: val_loss 1.1691475241094351, val_acc 0.95
trigger times: 5
end of epoch 35: val_loss 3.3378588675248013e-08, val_acc 1.0
trigger times: 6
end of epoch 36: val_loss 1.668929730413993e-08, val_acc 1.0
trigger times: 7
end of epoch 37: val_loss 6.4404621713620715e-06, val_acc 1.0
trigger times: 8
end of epoch 38: val_loss 3.5762754890811264e-08, val_acc 1.0
trigger times: 9
end of epoch 39: val_loss 3.6954036303882275e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -61.16116026043892 -54.98547503240923
1 -57.075462862849236 -50.492268601198035
2 -51.0411571264267 -50.03933801517046
3 -56.47273102402687 -49.75347184620696
4 -48.65887452661991 -49.72654640753777
5 -58.13650357723236 -46.98011874490918
6 -54.58510911464691 -45.7351542845057
7 -48.84997379779816 -45.670579884154705
8 -53.78033638000488 -44.99030608142343
9 -48.821135729551315 -44.14602409201361
10 -55.189220041036606 -43.81326882122305
11 -45.70305134356022 -43.18878399086166
12 -50.564053043723106 -42.29180714825394
13 -54.49572506546974 -42.00401746161006
14 -52.488975554704666 -41.6910044370425
15 -44.464775525033474 -41.68588229294918
16 -45.9495916813612 -41.281777102712205
17 -42.0889330804348 -40.44278203413966
18 -46.81823391467333 -40.34838365523108
19 -50.58428467810154 -39.599701153458774
20 -47.21494399011135 -39.57586365327889
21 -45.845798671245575 -39.31972693233231
22 -43.85357913374901 -39.024610555047154
23 -50.256248980760574 -38.45534493538269
24 -41.77172567695379 -38.41270390343083
25 -46.06346495449543 -38.35634328077039
26 -45.02344810962677 -37.79713616772368
27 -42.8011099845171 -37.741528994987384
28 -42.374329656362534 -37.66475323879293
29 -45.85283862054348 -37.513139380385574
30 -48.39565587043762 -37.1809993033689
31 -45.716519214212894 -37.100703136010694
32 -45.167848974466324 -37.00630588930485
33 -43.32795564830303 -36.821916772458344
34 -50.5860493183136 -36.48799015296732
35 -43.967218674719334 -36.20965269874363
36 -42.054999493062496 -36.19207561676116
37 -46.87224742770195 -36.114459029559086
38 -45.784317061305046 -35.78149902167743
39 -41.57490982860327 -35.394503873250635
40 -47.27048486471176 -35.26282499693737
41 -46.95277659595013 -35.24303541418371
42 -44.198982425034046 -35.209705244501436
43 -37.97491402924061 -35.0654408505187
44 -41.520153403282166 -34.80241747531743
45 -48.442097529768944 -34.64469044638467
46 -40.02292415499687 -33.84284985953318
47 -35.142970725893974 -32.70706485357069
48 -36.28290192782879 -31.969099402548657
49 -33.1235858052969 -31.7109134007892
50 -33.838840417563915 -31.64414355845032
51 -41.131335735321045 -31.392382758954444
52 -41.71412621438503 -31.223196019713853
53 -37.65794061124325 -31.12953085092458
54 -44.25578923523426 -29.39157139549552
55 -36.5628657490015 -29.340125609942326
56 -40.89151802659035 -29.106189988903285
57 -37.32703695446253 -27.41102349748205
58 -31.126935675740242 -27.343722362182305
59 -30.7924692183733 -27.196681629483837
60 -38.51948665082455 -27.07399028854534
61 -32.3656260073185 -26.7047217556024
62 -28.05602093040943 -26.244794902859052
63 -29.925464443862438 -25.548365085275513
64 -35.01471595466137 -25.45878528601009
65 -31.361229199916124 -24.879106999799365
66 -36.725129038095474 -24.828695359328833
67 -35.160568192601204 -24.592745144504722
68 -39.40716801583767 -23.978745577896312
69 -24.593066975474358 -23.57262108435893
70 -38.53398422896862 -23.44970807952351
71 -38.88508275151253 -22.745309160183492
72 -30.87364063411951 -22.60679894414887
73 -26.43611265718937 -22.19891031871716
74 -36.95937652885914 -20.656863763892378
75 -31.883300483226776 -20.444472560731253
76 -21.412845082581043 -20.19699010077007
77 -32.31159506738186 -20.13839114930498
78 -33.01581960916519 -19.63760343800059
79 -29.20184152573347 -19.515598718228343
80 -20.569384425878525 -18.92838809611677
81 -21.529643639922142 -17.994774057192853
82 -20.0932523496449 -17.55742370467821
83 -18.481402464210987 -16.823073927842348
84 -16.89110378921032 -14.855082803515382
85 -27.8247130215168 -14.531424598833084
86 -21.999363780021667 -14.442420089224363
87 -15.867844067513943 -13.596012850960644
88 -9.826862916350365 -12.68135972540495
89 -23.97320006787777 -12.66418205637357
90 -13.955256789922714 -12.30017947419658
91 -11.840332992374897 -12.151904772081672
92 -17.930348094552755 -11.788852141676486
93 -10.95355199277401 -10.869989101210326
94 -11.427343241870403 -10.327681503524177
95 -23.66821876168251 -9.8572159761571
96 -8.910300426185131 -8.330116995310416
97 -12.197795689105988 -8.133195842510668
98 -10.908762399107218 -8.108197691178031
99 -5.126986064016819 -7.57539849177145
100 -4.4689711183309555 -7.362443126623615
101 -3.417049542069435 -7.108327355338034
102 -6.575070891529322 -6.959063561385431
103 -16.32405000925064 -6.776946485018116
104 -2.7977596446871758 -6.7220638398623045
105 -17.61408281326294 -6.719970621583102
106 -9.298534251749516 -6.535447341844848
107 -7.0144915394485 -6.51820418055673
108 -5.755402788519859 -5.615796733870542
109 -15.283598519861698 -5.34720210027791
110 -3.3513511642813683 -5.078485007852753
111 -3.972934700548649 -5.027957977402961
112 -17.01501475274563 -4.827572916892203
113 -2.6960901394486427 -4.63049541560991
114 -17.42753578722477 -4.230832004686763
115 -12.40614264830947 -4.031048624093466
116 -3.2698801159858704 -3.3844671463622564
117 -10.517837774008512 -3.3322555012187633
118 -3.7962954081594944 -2.6416623314910934
119 -6.740454353392124 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

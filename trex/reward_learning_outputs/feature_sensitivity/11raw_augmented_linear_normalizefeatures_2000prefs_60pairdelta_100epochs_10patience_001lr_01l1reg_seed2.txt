demos: (120, 50, 12)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
Normalizing input features...
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 36
Number of trainable paramters: 36
device: cuda:0
end of epoch 0: val_loss 0.4585377762280405, val_acc 0.8
trigger times: 0
saving model weights...
Weights: OrderedDict([('layer_norm.weight', tensor([ 1.9158e-03,  3.6657e-05,  2.3225e-01,  6.2506e-02, -1.1044e-04,
        -1.7677e-03,  2.6401e-04,  1.3013e-04, -5.0945e-04,  2.9646e-04,
         2.4325e-03,  6.4623e-04], device='cuda:0')), ('layer_norm.bias', tensor([-3.8375e-03,  4.0996e-03, -4.1730e-03, -3.9404e-03, -4.3304e-03,
        -3.8761e-03, -2.0681e-04,  4.5884e-03, -3.8319e-03,  4.6285e-03,
        -7.7706e-05,  1.4162e-04], device='cuda:0')), ('fcs.0.weight', tensor([[-1.2527e-05,  5.6103e-05,  1.4921e-01, -5.7941e-02,  8.6824e-05,
         -1.7102e-04,  1.0441e-05,  8.7475e-06, -2.1739e-04,  1.5711e-05,
         -1.0072e-04,  1.3070e-04]], device='cuda:0'))])
end of epoch 1: val_loss 0.45142134619876745, val_acc 0.765
trigger times: 0
saving model weights...
Weights: OrderedDict([('layer_norm.weight', tensor([-9.9281e-04,  6.5136e-04,  1.9775e-01,  1.1355e-01, -9.7708e-04,
         1.1226e-03,  7.4827e-04, -4.0258e-04,  3.0702e-03,  7.5957e-04,
         4.1619e-04, -8.5436e-05], device='cuda:0')), ('layer_norm.bias', tensor([-0.0038,  0.0041, -0.0041, -0.0040, -0.0043, -0.0039, -0.0002,  0.0046,
        -0.0038,  0.0046, -0.0008,  0.0001], device='cuda:0')), ('fcs.0.weight', tensor([[ 4.0841e-04, -1.6420e-04,  1.7567e-01, -9.4202e-02, -1.7983e-04,
         -2.0010e-04,  4.0450e-05, -9.4172e-05, -2.9102e-05, -4.2449e-05,
         -3.9948e-04,  2.7618e-04]], device='cuda:0'))])
end of epoch 2: val_loss 0.4715557898953557, val_acc 0.805
trigger times: 1
end of epoch 3: val_loss 0.48042451959103344, val_acc 0.805
trigger times: 2
end of epoch 4: val_loss 0.4730557803809643, val_acc 0.805
trigger times: 3
end of epoch 5: val_loss 0.539929442256689, val_acc 0.805
trigger times: 4
end of epoch 6: val_loss 0.4838241808116436, val_acc 0.805
trigger times: 5
end of epoch 7: val_loss 0.48336793404072526, val_acc 0.805
trigger times: 6
end of epoch 8: val_loss 0.4904877333343029, val_acc 0.805
trigger times: 7
end of epoch 9: val_loss 0.49695280473446474, val_acc 0.805
trigger times: 8
end of epoch 10: val_loss 0.4734002214111388, val_acc 0.75
trigger times: 9
end of epoch 11: val_loss 0.47357272531837225, val_acc 0.805
trigger times: 10
Early stopping.
0 -0.245375530430465 -50.00222059884506
1 -0.15256083372514695 -48.823739140882175
2 -0.29239742333447793 -47.15605336419176
3 -0.13551653889589943 -46.19619104961985
4 0.15833931796078105 -45.594227093057754
5 -0.08613557965873042 -45.36842966452394
6 0.03091317741200328 -45.19068756322445
7 0.4202517548110336 -44.16490839583478
8 0.23975829844130203 -44.078303125872196
9 0.3192306252749404 -43.993555290419714
10 0.3246130212646676 -43.86306534422809
11 -0.5764726321795024 -43.84874807044028
12 -0.3162669908051612 -43.84199129025074
13 0.23750564927468076 -43.806385671938365
14 0.15368144353851676 -43.80581985978556
15 -0.06027118908241391 -42.7594767358323
16 0.146193626569584 -42.66316468983175
17 -0.6886781890352722 -42.33177224591743
18 0.3638930888118921 -41.774963389485094
19 -0.37254836538340896 -41.410068073767725
20 -0.4571761118240829 -41.17786296442943
21 -0.6396142038865946 -40.723520424948155
22 0.043112350860610604 -40.527189756101116
23 -0.20269581965112593 -40.49595848244517
24 0.062396605615504086 -40.429389880911344
25 -0.22920278440142283 -40.05653450521898
26 -0.013946158891485538 -39.59232357792555
27 -0.0393826878425898 -39.54162101198148
28 0.11447473887619708 -39.195237471709476
29 0.5070014526427258 -39.172389579378766
30 -0.6279862938681617 -38.51095963496708
31 0.015140680436161347 -38.447265769744824
32 -0.07103726692730561 -38.392107037026264
33 -0.1670940515869006 -38.00745349944469
34 0.20338944206014276 -37.46482488602393
35 0.25619378488045186 -37.10988160586883
36 -0.6541963989875512 -34.27116723637227
37 -0.25746835849713534 -34.14139118114101
38 0.3788093175535323 -33.263072731706835
39 -0.2146094237396028 -33.13344797200536
40 -0.7223003095859895 -33.07825234291984
41 -0.011433633782417019 -33.0321314765637
42 -0.674402696051402 -32.44934973065406
43 -1.017692927038297 -32.4007978120153
44 0.5415997549425811 -32.40063925734975
45 0.15897619011229835 -30.734403792103194
46 -0.03145837044576183 -30.57151371770873
47 0.009947210841346532 -30.131236504472803
48 -0.5013390978697316 -29.99326722619033
49 0.12886307790176943 -29.66908258985071
50 -0.07806378667009994 -29.297233511513635
51 -0.5742846475332044 -29.288890423975797
52 0.7170402441406623 -29.145878352769948
53 0.07914067355159204 -28.49601894351319
54 0.4590482977218926 -28.492023661124072
55 -0.6735539377768873 -28.315961465855167
56 0.1687852693721652 -27.121110566589827
57 0.8294326168252155 -26.064532595535336
58 -0.8511815776582807 -25.520524278341334
59 -0.5392314895507297 -25.27101421179229
60 0.9150848623248748 -25.066644278800943
61 -0.12788649098365568 -24.925849381327673
62 -0.26036367649794556 -24.188105669766596
63 0.3152306970951031 -23.48479966198816
64 0.901542165724095 -23.153943559703283
65 -0.182278143678559 -22.954730295117237
66 0.19930231210310012 -22.74124885266394
67 0.6477153126470512 -22.739273544503753
68 1.0479260246647755 -22.264945050603636
69 0.271885423004278 -22.15569724300287
70 -0.7228623376286123 -21.055920928583344
71 1.1681003379635513 -20.543356562348553
72 1.0750137255854497 -20.33499633836848
73 0.5693526164759533 -20.18157658281111
74 0.37004325637826696 -19.58144410477429
75 0.36365485534770414 -19.377225745334304
76 -1.162558451993391 -19.243135617403095
77 0.7670675448898692 -19.060620225371707
78 0.034546841285191476 -18.964124524696246
79 -0.5246177015942521 -18.448962308005108
80 1.0094944747397676 -17.740722019993825
81 0.35530610535352025 -16.85889369985028
82 1.0478742164559662 -16.3381194095591
83 1.3443716060355655 -14.535892564189266
84 1.1432055155746639 -14.443670567499144
85 0.8768913774983957 -14.200413010108107
86 -0.6938342869398184 -13.936976181618805
87 -1.4158794828836108 -13.862253042167257
88 0.8137572862324305 -13.483098530680483
89 1.3369522420689464 -13.455892754889845
90 0.9967118062777445 -13.355868275096913
91 1.6000689354696078 -12.278515244993585
92 1.1122664196554979 -12.227387460046547
93 1.2127506764954887 -12.020717825467683
94 1.9559402654558653 -11.910094799877324
95 -0.6480063602211885 -11.400284019256157
96 1.511294048599666 -11.134618158086587
97 -0.6521353394491598 -10.859166921158222
98 0.7379014363978058 -9.595137958067907
99 1.7115746144554578 -9.289921608799773
100 0.1598880184465088 -8.230877068641124
101 1.444409198476933 -7.882363241796725
102 1.499961396693834 -7.6478984168416355
103 1.382676864232053 -7.459623237418707
104 1.5111957517656265 -7.124357312750265
105 -1.9958247761969687 -7.05379065585803
106 1.6737022625165991 -6.853091098326624
107 1.8430532056372613 -6.6211384471641495
108 -1.9249310267623514 -6.494555224953677
109 1.9184389798901975 -6.117354180737655
110 -0.9555884975125082 -6.087055095509873
111 0.10670154425315559 -5.43500831968483
112 1.7757227135525682 -5.105291741614599
113 1.5423422976164147 -4.628649413275992
114 1.847580287780147 -4.471031187897325
115 0.8560719040688127 -4.455504779070034
116 -0.0370257013310038 -4.2805498188182405
117 -0.041056543588638306 -3.7944735717969627
118 1.7481237612082623 -2.9512438456190186
119 1.4308303379511926 -2.541618164765197
train accuracy: 0.7794444444444445
validation accuracy: 0.805

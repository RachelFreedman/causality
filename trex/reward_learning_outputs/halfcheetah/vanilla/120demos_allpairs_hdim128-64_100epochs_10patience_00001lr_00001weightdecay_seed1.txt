demos: (480, 1000, 23)
demo_rewards: (480,)
sorted_train_rewards: [-524.69363357 -519.9080064  -428.04712667 -426.07186886 -400.69209088
 -391.95882373 -391.95857823 -388.59399582 -382.64759058 -376.76557862
 -373.51420524 -360.72317391 -359.81019509 -357.01902541 -354.47400187
 -346.3907683  -340.93368667 -338.24427931 -336.02543195 -331.62260389
 -329.80209332 -323.40639163 -319.2332748  -317.83524537 -316.10403797
 -315.72296788 -311.34859504 -308.66558365 -303.90832615 -301.55233277
 -300.05811195 -297.00598958 -290.06408949 -286.79145896 -285.62255496
 -280.86172479 -280.79509685 -278.93507563 -274.17047796 -273.93576074
 -273.83226615 -271.80602473 -268.17088379 -264.55923119 -261.40264183
 -257.70030889 -257.03338564 -256.49969365 -248.79063039 -248.56818267
 -247.59340728 -244.37737256 -239.39040955 -238.0865228  -233.67489674
 -226.4021581  -215.62618485 -211.88658718 -211.36661726 -208.03522829
 -207.41638882 -207.28517741 -205.96911148 -203.9046547  -190.14098957
 -184.9841793  -184.73594166 -180.59042632 -175.50125481 -171.6271578
 -153.5033803  -152.60167504 -144.91600187 -138.2312438  -133.30170121
 -130.30804227 -123.62457903 -120.40905315  -93.1000472   -71.26903173
  -68.60132228  -60.59147508  -57.33491011  -38.29932913  -36.36051483
  -28.46460989  -26.64860697  -25.89971503  -25.24730779  -22.69834944
   18.14094557   18.534251     36.03771571   36.66670793   46.41665444
   52.56239787   57.08738085   61.62771572   64.26879367   72.77987258
   76.22063096   84.71704946   94.00125843   94.19740352   99.27272471
  106.18413986  106.61888653  112.63324094  114.60067786  116.47086787
  116.51098     117.95497782  124.34187604  128.90443776  134.3205758
  144.4467766   147.06057442  154.08827534  156.24618581  156.42911943
  158.51798413  166.92992889  168.12910327  168.32790079  169.99590647
  170.97708371  179.83810426  184.08731548  184.42954181  189.88700793
  190.01224314  191.95774675  195.01398454  196.33453198  201.05317488
  201.78522879  205.26303142  207.61010756  210.43454315  212.15813467
  213.58916802  213.68066085  214.28737943  215.76903125  218.87296056
  219.8938352   221.55427931  221.88051933  223.25804144  229.06331402
  229.89034361  230.83614278  246.00283085  256.83932945  272.70119861
  276.10991136  283.22855048  285.96659066  290.72864991  302.97557119
  314.70262512  320.18594509  323.2057931   344.96494304  366.6351554
  384.68362826  401.19893609  404.85545829  406.27949347  422.31704774
  436.83002729  437.06821619  446.71216718  450.64332733  469.63473588
  475.78724129  482.5534186   497.3271866   502.51838567  508.23621599
  527.7648076   537.05928655  553.32533334  553.48426895  560.46506436
  572.11974327  581.50409261  583.00550482  585.6694137   597.52795318
  600.85493229  605.95370052  606.89618343  624.70681015  630.23736491
  631.5518751   635.90294353  658.72608238  662.37365481  666.1488369
  668.77079518  671.3080771   680.56533067  683.13000451  685.21886664
  688.58993175  710.31589512  716.22536939  722.3609598   726.11333231
  726.16233976  726.47925137  734.56779392  736.63583003  744.30138878
  767.70105489  768.73209782  774.75191394  774.91311809  779.47144797
  788.53688263  793.82716573  799.17615147  799.74418213  803.53481759
  809.76264369  810.97531053  818.76684211  827.03676572  833.27242674
  837.12768435  837.18291032  852.97506692  855.23698313  856.06693798
  867.72120341  868.1633077   870.96953141  871.39424881  877.52097954
  877.78695006  886.97441062  905.39040983  909.9186257   911.49951501
  912.66890385  918.67761934  923.04099951  927.21378749  928.17981275
  937.01547782  952.96707777  958.0699434   962.84124522  967.21586262
  972.47460426  996.85650278 1025.00347782 1039.16402885 1039.78397173
 1069.35602053 1069.96912738 1079.52750826 1081.27763751 1084.26627495
 1122.50176508 1145.64247268 1156.74610959 1165.94831239 1172.65408898
 1191.35557305 1231.31773398 1281.40819681 1291.63482155 1298.98143007
 1313.68336221 1332.76858676 1370.8854603  1373.68641894 1389.96137052
 1408.32927479 1415.37100158 1415.95662921 1418.79397866 1425.64201846
 1436.7703022  1444.8742987  1455.10103315 1460.30079394 1464.81151459
 1480.27876503 1480.73126013 1491.97755394 1501.39430535 1512.23108748
 1525.58827426 1525.82648228 1538.38225196 1539.56531557 1553.18718808
 1553.40060648 1553.67248127 1554.2785549  1573.13710957 1578.57170849
 1583.32001802 1587.18145893 1587.78856335 1600.05548118 1637.33115926
 1648.38966521 1662.66715374 1670.14070122 1670.87137062 1671.74729172
 1679.99597469 1683.90874741 1700.93624107 1702.52424931 1705.57520837
 1729.56297887 1735.16955392 1756.39335374 1757.82710459 1790.32408447
 1806.9001545  1814.74908046 1817.87221105 1874.99064848 1897.44938647
 1903.37414384 1940.04647372 1955.52695868 1989.8060044  1990.4471889
 1999.71073314 2002.79461683 2030.25906501 2059.6721564  2067.38126332
 2073.84052963 2081.41663239 2098.26393327 2163.9281288  2169.4214639
 2188.24289228 2213.59247392 2214.79038033 2236.76410259 2240.25649811
 2259.29310248 2316.13575875 2350.74327548 2528.06277524 2529.57128632
 2558.45425896 2610.04155006 2615.28414462 2618.61361065 2645.18727726
 2646.06899759 2646.2884004  2655.46915359 2713.48495737 2745.16246933
 2746.14021096 2751.45964996 2805.70013928 2823.98410982 2832.18301075
 2865.68703628 2866.65517781 2868.3962322  2873.7900389  2908.17840563
 2911.74741678 2919.89904375 2939.98546055 2989.07503168 3000.82229205
 3026.52472405 3126.80889538 3188.6198344  3218.69401887 3230.75246286
 3266.16351614 3600.19661451 3617.23374779 3812.28142926 3813.16447538
 4015.87846288 4031.94536718 4071.18016174 4079.05565608 4079.96795983
 4089.53158236 4116.6937375  4117.41090968 4127.2547502  4167.1747594
 4184.17992205 4207.38529871 4210.25253259 4212.52885125 4215.11172055
 4222.00212054 4223.81264398 4226.98787167 4227.76468527 4234.10144303
 4243.33767626 4249.4217244  4261.11475591 4265.40573288 4283.31217151
 4290.96762481 4305.60777891 4328.6025185  4330.38249264 4340.43328662
 4343.30329165 4350.83517393 4357.79194471 4391.63916041 4408.45567702
 4418.19675027 4474.0948897  4526.63478974 4531.20723922 4542.61529274
 4584.4027012  4771.14676225]
sorted_val_rewards: [-417.85007471 -395.02788639 -393.33566725 -377.7472339  -356.52299073
 -293.61621306 -264.37152489 -229.04306351 -121.70501653  -84.51907957
   18.69225769   25.17834962   31.89666741   32.4050049    54.25957428
   79.97704618   95.07232442  105.91989234  116.59548472  154.46689919
  182.16470192  242.86029354  293.14863056  473.79372057  481.47003396
  555.5199792   578.16420374  620.51299525  701.81295875  819.72967372
  932.93634177 1172.88001302 1223.34001438 1483.93039685 1521.56198243
 1596.85943303 1776.35494333 1777.61207979 1844.10564114 2081.17454623
 3078.96260724 4108.47914019 4187.95937903 4200.0771443  4302.28859443
 4318.0498578  4362.47487658 4647.53502786]
maximum traj length 1000
maximum traj length 1000
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=23, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11392
Number of trainable paramters: 11392
device: cuda:0
end of epoch 0: val_loss 0.18376478305300756, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.053166239562929415, val_acc 0.9920212765957447
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.15349555746549137, val_acc 0.9831560283687943
trigger times: 1
end of epoch 3: val_loss 0.017642395194436673, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.13103594763370788, val_acc 0.9831560283687943
trigger times: 1
end of epoch 5: val_loss 0.03263626971825334, val_acc 0.9911347517730497
trigger times: 2
end of epoch 6: val_loss 0.02461169712715132, val_acc 0.9920212765957447
trigger times: 3
end of epoch 7: val_loss 0.15197966555424144, val_acc 0.9813829787234043
trigger times: 4
end of epoch 8: val_loss 0.13315231227429292, val_acc 0.9867021276595744
trigger times: 5
end of epoch 9: val_loss 0.012519884556376343, val_acc 0.9955673758865248
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.03412556130380632, val_acc 0.9946808510638298
trigger times: 1
end of epoch 11: val_loss 0.019832853962904753, val_acc 0.9920212765957447
trigger times: 2
end of epoch 12: val_loss 0.018290363155459094, val_acc 0.9920212765957447
trigger times: 3
end of epoch 13: val_loss 0.008978309209820956, val_acc 0.9955673758865248
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.02056197378553626, val_acc 0.9875886524822695
trigger times: 1
end of epoch 15: val_loss 0.021987205835622168, val_acc 0.9920212765957447
trigger times: 2
end of epoch 16: val_loss 0.036757467590814726, val_acc 0.9893617021276596
trigger times: 3
end of epoch 17: val_loss 0.015591878146487659, val_acc 0.9937943262411347
trigger times: 4
end of epoch 18: val_loss 0.01447762800725903, val_acc 0.9937943262411347
trigger times: 5
end of epoch 19: val_loss 0.04142183763285135, val_acc 0.9875886524822695
trigger times: 6
end of epoch 20: val_loss 0.019463932785297598, val_acc 0.9911347517730497
trigger times: 7
end of epoch 21: val_loss 0.012553249772935047, val_acc 0.9937943262411347
trigger times: 8
end of epoch 22: val_loss 0.03526725053480457, val_acc 0.9911347517730497
trigger times: 9
end of epoch 23: val_loss 0.020672652167780695, val_acc 0.9902482269503546
trigger times: 10
Early stopping.
0 -22.277309738332406 -417.8500747112217
1 -21.629478686401853 -395.0278863934491
2 -21.77268805814674 -393.33566724796447
3 -17.796012641570996 -377.74723390148745
4 -10.006255039370444 -356.52299073299355
5 -0.895926543395035 -293.61621306390157
6 -0.7544333592813928 -264.37152488512595
7 6.595927709553507 -229.04306350923414
8 31.51400172466674 -121.70501653144265
9 37.5006475734408 -84.51907956755302
10 55.5589710228669 18.692257691900576
11 56.45828929259733 25.178349617410355
12 52.733883258886635 31.896667411820435
13 54.56405705938232 32.40500490386286
14 56.05853283920442 54.25957427857534
15 62.02999801831902 79.97704618322362
16 68.25814431451727 95.07232442353506
17 68.84056246175896 105.9198923418008
18 68.17762181583385 116.59548471755453
19 77.20689250942087 154.46689919193952
20 84.26921471918467 182.1647019201146
21 89.63384736681473 242.860293539308
22 103.44299356064585 293.1486305602138
23 132.69639687950257 473.7937205693257
24 135.70628896681592 481.47003395927266
25 146.6657451093197 555.5199792011284
26 159.92201537487563 578.1642037415102
27 165.28074774214474 620.5129952457143
28 174.36389897478512 701.8129587451294
29 197.398192175664 819.7296737154431
30 218.30360059224768 932.9363417650754
31 257.66288007443654 1172.8800130151055
32 277.5746497019427 1223.340014376065
33 327.9886613818817 1483.9303968479487
34 326.8457903144008 1521.5619824319874
35 343.110844197392 1596.8594330279877
36 373.7384045945946 1776.3549433333546
37 373.07950948012876 1777.612079786244
38 386.19411202857736 1844.1056411379384
39 428.58051866909955 2081.1745462322774
40 610.8022009395063 3078.9626072426972
41 804.4408736207988 4108.479140186715
42 823.4027469083667 4187.95937902959
43 819.4016286907718 4200.077144304393
44 839.0646396055818 4302.2885944270065
45 844.6359034851193 4318.049857801658
46 847.8378490740433 4362.47487658048
47 892.8926387419924 4647.535027861095
train accuracy: 0.9987394957983193
validation accuracy: 0.9902482269503546

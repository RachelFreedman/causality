demos: (480, 1000, 23)
demo_rewards: (480,)
sorted_train_rewards: [-524.69363357 -519.9080064  -428.04712667 -426.07186886 -417.85007471
 -400.69209088 -395.02788639 -393.33566725 -391.95882373 -391.95857823
 -388.59399582 -382.64759058 -377.7472339  -376.76557862 -373.51420524
 -360.72317391 -359.81019509 -357.01902541 -356.52299073 -354.47400187
 -346.3907683  -338.24427931 -336.02543195 -331.62260389 -329.80209332
 -323.40639163 -319.2332748  -317.83524537 -315.72296788 -311.34859504
 -308.66558365 -303.90832615 -301.55233277 -300.05811195 -297.00598958
 -290.06408949 -286.79145896 -285.62255496 -280.86172479 -280.79509685
 -278.93507563 -274.17047796 -273.93576074 -273.83226615 -268.17088379
 -264.55923119 -264.37152489 -261.40264183 -257.70030889 -257.03338564
 -256.49969365 -248.79063039 -248.56818267 -247.59340728 -244.37737256
 -239.39040955 -238.0865228  -229.04306351 -226.4021581  -215.62618485
 -211.88658718 -211.36661726 -208.03522829 -207.41638882 -207.28517741
 -205.96911148 -203.9046547  -190.14098957 -184.9841793  -184.73594166
 -180.59042632 -175.50125481 -171.6271578  -153.5033803  -152.60167504
 -138.2312438  -133.30170121 -130.30804227 -123.62457903 -120.40905315
  -93.1000472   -84.51907957  -71.26903173  -68.60132228  -60.59147508
  -57.33491011  -38.29932913  -36.36051483  -28.46460989  -26.64860697
  -25.89971503  -25.24730779  -22.69834944   18.14094557   18.534251
   18.69225769   25.17834962   31.89666741   32.4050049    36.03771571
   36.66670793   52.56239787   54.25957428   57.08738085   61.62771572
   72.77987258   79.97704618   84.71704946   94.00125843   94.19740352
   95.07232442   99.27272471  105.91989234  106.18413986  106.61888653
  112.63324094  114.60067786  116.47086787  116.51098     116.59548472
  117.95497782  124.34187604  128.90443776  134.3205758   144.4467766
  147.06057442  154.46689919  156.24618581  156.42911943  158.51798413
  166.92992889  168.12910327  168.32790079  169.99590647  179.83810426
  182.16470192  184.08731548  184.42954181  189.88700793  190.01224314
  191.95774675  196.33453198  201.05317488  201.78522879  207.61010756
  210.43454315  212.15813467  213.58916802  213.68066085  215.76903125
  218.87296056  219.8938352   221.55427931  223.25804144  229.06331402
  229.89034361  230.83614278  242.86029354  256.83932945  272.70119861
  276.10991136  283.22855048  290.72864991  293.14863056  320.18594509
  323.2057931   344.96494304  366.6351554   384.68362826  401.19893609
  404.85545829  436.83002729  437.06821619  446.71216718  450.64332733
  469.63473588  473.79372057  475.78724129  481.47003396  482.5534186
  497.3271866   502.51838567  508.23621599  527.7648076   537.05928655
  553.32533334  553.48426895  555.5199792   560.46506436  572.11974327
  578.16420374  581.50409261  583.00550482  597.52795318  600.85493229
  605.95370052  606.89618343  620.51299525  624.70681015  630.23736491
  631.5518751   658.72608238  662.37365481  668.77079518  671.3080771
  688.58993175  701.81295875  710.31589512  716.22536939  722.3609598
  726.11333231  726.47925137  734.56779392  736.63583003  744.30138878
  767.70105489  768.73209782  774.91311809  779.47144797  788.53688263
  793.82716573  799.74418213  803.53481759  809.76264369  810.97531053
  818.76684211  819.72967372  827.03676572  833.27242674  837.12768435
  837.18291032  852.97506692  855.23698313  856.06693798  867.72120341
  868.1633077   870.96953141  871.39424881  877.52097954  877.78695006
  909.9186257   911.49951501  912.66890385  918.67761934  923.04099951
  927.21378749  928.17981275  932.93634177  937.01547782  952.96707777
  958.0699434   962.84124522  967.21586262  996.85650278 1025.00347782
 1039.16402885 1039.78397173 1069.35602053 1069.96912738 1079.52750826
 1081.27763751 1084.26627495 1122.50176508 1145.64247268 1156.74610959
 1165.94831239 1172.65408898 1191.35557305 1223.34001438 1231.31773398
 1281.40819681 1291.63482155 1298.98143007 1313.68336221 1332.76858676
 1373.68641894 1389.96137052 1408.32927479 1415.37100158 1415.95662921
 1418.79397866 1425.64201846 1436.7703022  1444.8742987  1455.10103315
 1464.81151459 1480.27876503 1480.73126013 1483.93039685 1491.97755394
 1512.23108748 1521.56198243 1525.58827426 1525.82648228 1538.38225196
 1539.56531557 1553.18718808 1553.40060648 1553.67248127 1554.2785549
 1578.57170849 1583.32001802 1587.18145893 1587.78856335 1596.85943303
 1600.05548118 1648.38966521 1662.66715374 1670.14070122 1670.87137062
 1671.74729172 1679.99597469 1683.90874741 1700.93624107 1702.52424931
 1705.57520837 1735.16955392 1756.39335374 1757.82710459 1776.35494333
 1777.61207979 1790.32408447 1806.9001545  1814.74908046 1817.87221105
 1844.10564114 1897.44938647 1903.37414384 1940.04647372 1955.52695868
 1989.8060044  1990.4471889  1999.71073314 2002.79461683 2030.25906501
 2059.6721564  2067.38126332 2073.84052963 2081.17454623 2081.41663239
 2098.26393327 2163.9281288  2169.4214639  2188.24289228 2213.59247392
 2214.79038033 2236.76410259 2240.25649811 2316.13575875 2350.74327548
 2528.06277524 2529.57128632 2558.45425896 2610.04155006 2615.28414462
 2618.61361065 2645.18727726 2646.06899759 2646.2884004  2655.46915359
 2713.48495737 2745.16246933 2746.14021096 2751.45964996 2805.70013928
 2823.98410982 2832.18301075 2865.68703628 2866.65517781 2868.3962322
 2873.7900389  2908.17840563 2911.74741678 2919.89904375 2939.98546055
 2989.07503168 3000.82229205 3026.52472405 3078.96260724 3126.80889538
 3188.6198344  3218.69401887 3230.75246286 3266.16351614 3600.19661451
 3617.23374779 3812.28142926 3813.16447538 4031.94536718 4071.18016174
 4079.05565608 4079.96795983 4089.53158236 4108.47914019 4117.41090968
 4127.2547502  4167.1747594  4184.17992205 4187.95937903 4207.38529871
 4210.25253259 4212.52885125 4215.11172055 4222.00212054 4223.81264398
 4226.98787167 4227.76468527 4234.10144303 4243.33767626 4249.4217244
 4261.11475591 4265.40573288 4283.31217151 4290.96762481 4302.28859443
 4305.60777891 4328.6025185  4330.38249264 4343.30329165 4350.83517393
 4357.79194471 4362.47487658 4391.63916041 4408.45567702 4418.19675027
 4474.0948897  4526.63478974 4531.20723922 4542.61529274 4584.4027012
 4647.53502786 4771.14676225]
sorted_val_rewards: [-340.93368667 -316.10403797 -293.61621306 -271.80602473 -233.67489674
 -144.91600187 -121.70501653   46.41665444   64.26879367   76.22063096
  154.08827534  170.97708371  195.01398454  205.26303142  214.28737943
  221.88051933  246.00283085  285.96659066  302.97557119  314.70262512
  406.27949347  422.31704774  585.6694137   635.90294353  666.1488369
  680.56533067  683.13000451  685.21886664  726.16233976  774.75191394
  799.17615147  886.97441062  905.39040983  972.47460426 1172.88001302
 1370.8854603  1460.30079394 1501.39430535 1573.13710957 1637.33115926
 1729.56297887 1874.99064848 2259.29310248 4015.87846288 4116.6937375
 4200.0771443  4318.0498578  4340.43328662]
maximum traj length 1000
maximum traj length 1000
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=23, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11392
Number of trainable paramters: 11392
device: cuda:0
end of epoch 0: val_loss 0.14589150974721266, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.08881750863359866, val_acc 0.9884751773049646
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.10099206411644196, val_acc 0.9831560283687943
trigger times: 1
end of epoch 3: val_loss 0.44513737981510104, val_acc 0.9716312056737588
trigger times: 2
end of epoch 4: val_loss 0.10048536210021258, val_acc 0.9884751773049646
trigger times: 3
end of epoch 5: val_loss 0.07885876765900793, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.07886992657075333, val_acc 0.9893617021276596
trigger times: 1
end of epoch 7: val_loss 0.054133289553206086, val_acc 0.9920212765957447
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.05734840771090167, val_acc 0.9911347517730497
trigger times: 1
end of epoch 9: val_loss 0.1216423147599537, val_acc 0.9858156028368794
trigger times: 2
end of epoch 10: val_loss 0.12432648686157571, val_acc 0.9858156028368794
trigger times: 3
end of epoch 11: val_loss 0.0686104909681618, val_acc 0.9902482269503546
trigger times: 4
end of epoch 12: val_loss 0.06949228907026157, val_acc 0.9893617021276596
trigger times: 5
end of epoch 13: val_loss 0.15496142453735887, val_acc 0.9849290780141844
trigger times: 6
end of epoch 14: val_loss 0.34155981184795803, val_acc 0.973404255319149
trigger times: 7
end of epoch 15: val_loss 0.10293578976197965, val_acc 0.9884751773049646
trigger times: 8
end of epoch 16: val_loss 0.03507351010484629, val_acc 0.9902482269503546
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.042246937019677674, val_acc 0.9884751773049646
trigger times: 1
end of epoch 18: val_loss 0.038977083807651865, val_acc 0.9920212765957447
trigger times: 2
end of epoch 19: val_loss 0.056826116305256864, val_acc 0.9893617021276596
trigger times: 3
end of epoch 20: val_loss 0.03856527004443794, val_acc 0.9893617021276596
trigger times: 4
end of epoch 21: val_loss 0.045456535279745504, val_acc 0.9911347517730497
trigger times: 5
end of epoch 22: val_loss 0.04235943433372285, val_acc 0.9902482269503546
trigger times: 6
end of epoch 23: val_loss 0.03253596952115571, val_acc 0.9920212765957447
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.05118032136711859, val_acc 0.9920212765957447
trigger times: 1
end of epoch 25: val_loss 0.06601892398586715, val_acc 0.9902482269503546
trigger times: 2
end of epoch 26: val_loss 0.05140387574995731, val_acc 0.9902482269503546
trigger times: 3
end of epoch 27: val_loss 0.03497475513072748, val_acc 0.9929078014184397
trigger times: 4
end of epoch 28: val_loss 0.06547552672337842, val_acc 0.9911347517730497
trigger times: 5
end of epoch 29: val_loss 0.023413812812588967, val_acc 0.9946808510638298
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.17372541907105987, val_acc 0.9858156028368794
trigger times: 1
end of epoch 31: val_loss 0.03199269966721192, val_acc 0.9920212765957447
trigger times: 2
end of epoch 32: val_loss 0.04175142899081133, val_acc 0.9920212765957447
trigger times: 3
end of epoch 33: val_loss 0.035240696506660986, val_acc 0.9929078014184397
trigger times: 4
end of epoch 34: val_loss 0.04525874212230954, val_acc 0.9911347517730497
trigger times: 5
end of epoch 35: val_loss 0.04220448609314848, val_acc 0.9911347517730497
trigger times: 6
end of epoch 36: val_loss 0.02445325583730692, val_acc 0.9946808510638298
trigger times: 7
end of epoch 37: val_loss 0.03340467231896866, val_acc 0.9946808510638298
trigger times: 8
end of epoch 38: val_loss 0.047860895762692215, val_acc 0.9929078014184397
trigger times: 9
end of epoch 39: val_loss 0.031957154678601454, val_acc 0.9937943262411347
trigger times: 10
Early stopping.
0 29.945525641232962 -340.93368667132495
1 39.7880679044174 -316.1040379685105
2 41.249316937988624 -293.61621306390157
3 46.318288225331344 -271.8060247270487
4 49.68242926860694 -233.6748967447529
5 71.80169298185501 -144.91600187495783
6 90.40809672873002 -121.70501653144265
7 129.95437583397143 46.416654441560325
8 132.44884154200554 64.26879367316346
9 142.03835321532097 76.22063095813924
10 162.59489807777572 154.08827533872434
11 158.6250726034632 170.97708370598906
12 173.26266423569177 195.01398453916917
13 172.85341536370106 205.26303141538762
14 180.02757444803137 214.2873794271587
15 184.29175913508516 221.88051933186094
16 182.34146275080275 246.00283085027885
17 195.927849335887 285.96659066445415
18 200.4690204411745 302.9755711907817
19 213.06028679665178 314.70262511518507
20 232.12747708859388 406.2794934682273
21 236.3186540126917 422.31704773603093
22 292.2399829935166 585.6694137038423
23 290.2015141188167 635.9029435282087
24 301.1423820354976 666.1488368982799
25 317.9263975728536 680.5653306731639
26 308.11209767102264 683.1300045105083
27 311.31690612842795 685.2188666363189
28 318.73541968950303 726.1623397609577
29 344.6015171784675 774.7519139406528
30 335.83849411451956 799.1761514716857
31 364.40284332900774 886.9744106175162
32 374.8442381575005 905.3904098331311
33 400.4549801531248 972.4746042648616
34 439.98875003674766 1172.8800130151055
35 510.5372621032002 1370.8854602954927
36 535.2706159874797 1460.300793939679
37 536.0704691614083 1501.3943053524936
38 567.5476395013975 1573.137109571316
39 568.148456512019 1637.3311592555465
40 599.8192108129151 1729.562978866093
41 653.7279518020223 1874.9906484750777
42 752.4313697083853 2259.2931024822797
43 1230.898690706119 4015.8784628803237
44 1257.4707039743662 4116.693737500812
45 1274.3278087768704 4200.077144304393
46 1305.113929361105 4318.049857801658
47 1308.8455523978919 4340.433286622604
train accuracy: 0.9981792717086835
validation accuracy: 0.9937943262411347

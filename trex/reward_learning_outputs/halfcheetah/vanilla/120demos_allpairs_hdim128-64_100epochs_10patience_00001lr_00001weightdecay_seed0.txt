demos: (480, 1000, 23)
demo_rewards: (480,)
sorted_train_rewards: [-524.69363357 -428.04712667 -426.07186886 -417.85007471 -400.69209088
 -395.02788639 -393.33566725 -391.95882373 -388.59399582 -377.7472339
 -376.76557862 -373.51420524 -360.72317391 -359.81019509 -357.01902541
 -356.52299073 -354.47400187 -346.3907683  -340.93368667 -338.24427931
 -336.02543195 -331.62260389 -329.80209332 -319.2332748  -317.83524537
 -315.72296788 -311.34859504 -308.66558365 -303.90832615 -301.55233277
 -300.05811195 -297.00598958 -293.61621306 -290.06408949 -286.79145896
 -285.62255496 -280.86172479 -278.93507563 -274.17047796 -273.93576074
 -273.83226615 -271.80602473 -268.17088379 -264.55923119 -264.37152489
 -261.40264183 -257.70030889 -256.49969365 -248.79063039 -248.56818267
 -247.59340728 -244.37737256 -239.39040955 -238.0865228  -233.67489674
 -229.04306351 -226.4021581  -215.62618485 -211.88658718 -211.36661726
 -208.03522829 -207.41638882 -207.28517741 -205.96911148 -203.9046547
 -190.14098957 -184.9841793  -184.73594166 -180.59042632 -175.50125481
 -171.6271578  -153.5033803  -138.2312438  -133.30170121 -123.62457903
 -120.40905315  -93.1000472   -84.51907957  -71.26903173  -68.60132228
  -60.59147508  -57.33491011  -38.29932913  -36.36051483  -28.46460989
  -26.64860697  -25.89971503  -25.24730779  -22.69834944   18.14094557
   18.534251     18.69225769   25.17834962   31.89666741   32.4050049
   36.03771571   36.66670793   46.41665444   52.56239787   54.25957428
   57.08738085   61.62771572   64.26879367   72.77987258   76.22063096
   79.97704618   84.71704946   94.00125843  105.91989234  106.18413986
  106.61888653  112.63324094  116.51098     116.59548472  117.95497782
  128.90443776  134.3205758   144.4467766   147.06057442  154.08827534
  154.46689919  156.24618581  156.42911943  158.51798413  166.92992889
  168.12910327  170.97708371  179.83810426  182.16470192  184.08731548
  184.42954181  189.88700793  191.95774675  195.01398454  196.33453198
  201.05317488  201.78522879  205.26303142  207.61010756  210.43454315
  212.15813467  213.58916802  213.68066085  214.28737943  215.76903125
  218.87296056  219.8938352   221.55427931  221.88051933  223.25804144
  229.06331402  229.89034361  230.83614278  242.86029354  246.00283085
  256.83932945  272.70119861  276.10991136  283.22855048  285.96659066
  290.72864991  293.14863056  302.97557119  314.70262512  320.18594509
  323.2057931   344.96494304  366.6351554   384.68362826  401.19893609
  404.85545829  406.27949347  422.31704774  436.83002729  437.06821619
  446.71216718  450.64332733  475.78724129  481.47003396  482.5534186
  497.3271866   502.51838567  508.23621599  527.7648076   537.05928655
  553.32533334  555.5199792   560.46506436  572.11974327  578.16420374
  581.50409261  583.00550482  585.6694137   597.52795318  600.85493229
  606.89618343  620.51299525  624.70681015  630.23736491  631.5518751
  635.90294353  658.72608238  662.37365481  666.1488369   668.77079518
  680.56533067  683.13000451  685.21886664  688.58993175  710.31589512
  716.22536939  722.3609598   726.11333231  726.16233976  726.47925137
  734.56779392  736.63583003  767.70105489  768.73209782  774.75191394
  779.47144797  788.53688263  793.82716573  799.17615147  799.74418213
  809.76264369  810.97531053  818.76684211  819.72967372  827.03676572
  833.27242674  837.12768435  837.18291032  852.97506692  855.23698313
  867.72120341  868.1633077   871.39424881  877.52097954  877.78695006
  886.97441062  905.39040983  909.9186257   911.49951501  912.66890385
  918.67761934  923.04099951  927.21378749  928.17981275  932.93634177
  937.01547782  952.96707777  962.84124522  967.21586262  972.47460426
  996.85650278 1025.00347782 1039.16402885 1039.78397173 1069.35602053
 1069.96912738 1079.52750826 1081.27763751 1084.26627495 1122.50176508
 1145.64247268 1156.74610959 1165.94831239 1172.65408898 1172.88001302
 1191.35557305 1223.34001438 1231.31773398 1281.40819681 1298.98143007
 1313.68336221 1332.76858676 1370.8854603  1373.68641894 1389.96137052
 1415.37100158 1415.95662921 1418.79397866 1425.64201846 1436.7703022
 1444.8742987  1455.10103315 1460.30079394 1464.81151459 1480.73126013
 1483.93039685 1491.97755394 1501.39430535 1512.23108748 1521.56198243
 1525.58827426 1538.38225196 1539.56531557 1553.18718808 1553.40060648
 1553.67248127 1554.2785549  1573.13710957 1578.57170849 1583.32001802
 1587.78856335 1596.85943303 1600.05548118 1637.33115926 1648.38966521
 1662.66715374 1670.87137062 1671.74729172 1679.99597469 1683.90874741
 1700.93624107 1702.52424931 1705.57520837 1729.56297887 1735.16955392
 1756.39335374 1757.82710459 1776.35494333 1777.61207979 1790.32408447
 1806.9001545  1817.87221105 1844.10564114 1903.37414384 1940.04647372
 1955.52695868 1989.8060044  1990.4471889  1999.71073314 2002.79461683
 2030.25906501 2059.6721564  2067.38126332 2081.17454623 2081.41663239
 2098.26393327 2163.9281288  2169.4214639  2188.24289228 2213.59247392
 2214.79038033 2236.76410259 2240.25649811 2259.29310248 2316.13575875
 2350.74327548 2528.06277524 2529.57128632 2558.45425896 2610.04155006
 2615.28414462 2618.61361065 2645.18727726 2646.06899759 2646.2884004
 2655.46915359 2745.16246933 2746.14021096 2751.45964996 2805.70013928
 2823.98410982 2832.18301075 2865.68703628 2866.65517781 2868.3962322
 2873.7900389  2908.17840563 2911.74741678 2919.89904375 2939.98546055
 2989.07503168 3026.52472405 3078.96260724 3126.80889538 3188.6198344
 3218.69401887 3230.75246286 3266.16351614 3600.19661451 3617.23374779
 3812.28142926 3813.16447538 4015.87846288 4071.18016174 4079.05565608
 4079.96795983 4108.47914019 4116.6937375  4117.41090968 4167.1747594
 4184.17992205 4187.95937903 4200.0771443  4207.38529871 4210.25253259
 4212.52885125 4215.11172055 4222.00212054 4223.81264398 4226.98787167
 4227.76468527 4234.10144303 4243.33767626 4249.4217244  4261.11475591
 4265.40573288 4283.31217151 4290.96762481 4302.28859443 4305.60777891
 4318.0498578  4328.6025185  4330.38249264 4340.43328662 4343.30329165
 4350.83517393 4357.79194471 4362.47487658 4391.63916041 4408.45567702
 4418.19675027 4474.0948897  4526.63478974 4531.20723922 4542.61529274
 4647.53502786 4771.14676225]
sorted_val_rewards: [-519.9080064  -391.95857823 -382.64759058 -323.40639163 -316.10403797
 -280.79509685 -257.03338564 -152.60167504 -144.91600187 -130.30804227
 -121.70501653   94.19740352   95.07232442   99.27272471  114.60067786
  116.47086787  124.34187604  168.32790079  169.99590647  190.01224314
  469.63473588  473.79372057  553.48426895  605.95370052  671.3080771
  701.81295875  744.30138878  774.91311809  803.53481759  856.06693798
  870.96953141  958.0699434  1291.63482155 1408.32927479 1480.27876503
 1525.82648228 1587.18145893 1670.14070122 1814.74908046 1874.99064848
 1897.44938647 2073.84052963 2713.48495737 3000.82229205 4031.94536718
 4089.53158236 4127.2547502  4584.4027012 ]
maximum traj length 1000
maximum traj length 1000
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=23, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11392
Number of trainable paramters: 11392
device: cuda:0
end of epoch 0: val_loss 0.22317445416429724, val_acc 0.9707446808510638
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2253709826813157, val_acc 0.973404255319149
trigger times: 1
end of epoch 2: val_loss 0.17982630790059792, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.2028263432802633, val_acc 0.9813829787234043
trigger times: 1
end of epoch 4: val_loss 0.2182841266329038, val_acc 0.9796099290780141
trigger times: 2
end of epoch 5: val_loss 0.08384616454879447, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.04480425882457136, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.043338406964404816, val_acc 0.9902482269503546
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.04230059994594848, val_acc 0.9902482269503546
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.05136005954954242, val_acc 0.9884751773049646
trigger times: 1
end of epoch 10: val_loss 0.029482607099425302, val_acc 0.9911347517730497
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.04102314659571443, val_acc 0.9858156028368794
trigger times: 1
end of epoch 12: val_loss 0.03954998167874399, val_acc 0.9911347517730497
trigger times: 2
end of epoch 13: val_loss 0.02074175570224501, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.022905429496000978, val_acc 0.9920212765957447
trigger times: 1
end of epoch 15: val_loss 0.023346826777819307, val_acc 0.9920212765957447
trigger times: 2
end of epoch 16: val_loss 0.023400635366960926, val_acc 0.9911347517730497
trigger times: 3
end of epoch 17: val_loss 0.06209486425650846, val_acc 0.9822695035460993
trigger times: 4
end of epoch 18: val_loss 0.017054415585001995, val_acc 0.9946808510638298
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.012641554258648307, val_acc 0.9955673758865248
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.011165617473203982, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.03914008886498757, val_acc 0.9849290780141844
trigger times: 1
end of epoch 22: val_loss 0.013213988886227811, val_acc 0.9946808510638298
trigger times: 2
end of epoch 23: val_loss 0.03726017812268886, val_acc 0.9902482269503546
trigger times: 3
end of epoch 24: val_loss 0.011901261197101366, val_acc 0.9929078014184397
trigger times: 4
end of epoch 25: val_loss 0.02832012002966908, val_acc 0.9911347517730497
trigger times: 5
end of epoch 26: val_loss 0.015001493028787727, val_acc 0.9955673758865248
trigger times: 6
end of epoch 27: val_loss 0.014905475724866914, val_acc 0.9946808510638298
trigger times: 7
end of epoch 28: val_loss 0.012959468926048028, val_acc 0.9946808510638298
trigger times: 8
end of epoch 29: val_loss 0.014907514733383781, val_acc 0.9946808510638298
trigger times: 9
end of epoch 30: val_loss 0.015848261490963567, val_acc 0.9946808510638298
trigger times: 10
Early stopping.
0 -32.67936100438237 -519.9080063971747
1 -14.371739253401756 -391.9585782336556
2 -8.023239466361701 -382.6475905757797
3 -0.7790380958467722 -323.40639162743133
4 0.18255244847387075 -316.1040379685105
5 4.1455009509809315 -280.7950968544581
6 7.211992342956364 -257.03338563895727
7 21.068991454318166 -152.60167504208923
8 22.729803924448788 -144.91600187495783
9 31.152166090905666 -130.30804226838904
10 29.422601143829525 -121.70501653144265
11 72.53853639867157 94.19740352247452
12 76.33142152242362 95.07232442353506
13 73.60362225398421 99.27272471081422
14 72.82655198220164 114.60067785589865
15 76.51551906764507 116.47086786642583
16 78.84769650734961 124.34187603836524
17 85.33434230089188 168.32790078559515
18 88.35420922655612 169.99590647048413
19 86.48340924736112 190.0122431419386
20 136.89662769250572 469.6347358842159
21 139.48525093961507 473.7937205693257
22 154.87348431907594 553.4842689452406
23 158.0727559933439 605.9537005157925
24 177.29416777845472 671.3080771046588
25 179.89161807484925 701.8129587451294
26 189.82230464555323 744.301388782645
27 197.87837202101946 774.9131180922687
28 204.00629252940416 803.5348175925284
29 217.4992382451892 856.0669379761373
30 213.91171567700803 870.9695314142782
31 227.30542045086622 958.069943397442
32 287.3237816412002 1291.6348215451665
33 308.86891598813236 1408.3292747852752
34 324.51698124967515 1480.2787650285316
35 334.23445372469723 1525.8264822752312
36 347.01183231920004 1587.1814589329433
37 358.3985092006624 1670.1407012219743
38 382.95072772726417 1814.74908045765
39 397.7460779119283 1874.9906484750777
40 402.65208756923676 1897.4493864736492
41 434.4811977837235 2073.8405296250257
42 549.3134584128857 2713.484957372366
43 598.6979042589664 3000.822292046694
44 789.9853709763847 4031.9453671833917
45 798.1918057650328 4089.5315823594406
46 802.0977362506092 4127.254750202243
47 881.9752188324928 4584.402701204601
train accuracy: 0.9988795518207283
validation accuracy: 0.9946808510638298

FINISHED REGISTRATION
demos: (480, 1000, 23)
demo_rewards: (480,)
sorted_train_rewards: [-524.69363357 -519.9080064  -428.04712667 -426.07186886 -417.85007471
 -400.69209088 -395.02788639 -393.33566725 -391.95882373 -391.95857823
 -388.59399582 -382.64759058 -377.7472339  -376.76557862 -373.51420524
 -360.72317391 -359.81019509 -357.01902541 -356.52299073 -354.47400187
 -346.3907683  -338.24427931 -336.02543195 -331.62260389 -329.80209332
 -323.40639163 -319.2332748  -317.83524537 -315.72296788 -311.34859504
 -308.66558365 -303.90832615 -301.55233277 -300.05811195 -297.00598958
 -290.06408949 -286.79145896 -285.62255496 -280.86172479 -280.79509685
 -278.93507563 -274.17047796 -273.93576074 -273.83226615 -268.17088379
 -264.55923119 -264.37152489 -261.40264183 -257.70030889 -257.03338564
 -256.49969365 -248.79063039 -248.56818267 -247.59340728 -244.37737256
 -239.39040955 -238.0865228  -229.04306351 -226.4021581  -215.62618485
 -211.88658718 -211.36661726 -208.03522829 -207.41638882 -207.28517741
 -205.96911148 -203.9046547  -190.14098957 -184.9841793  -184.73594166
 -180.59042632 -175.50125481 -171.6271578  -153.5033803  -152.60167504
 -138.2312438  -133.30170121 -130.30804227 -123.62457903 -120.40905315
  -93.1000472   -84.51907957  -71.26903173  -68.60132228  -60.59147508
  -57.33491011  -38.29932913  -36.36051483  -28.46460989  -26.64860697
  -25.89971503  -25.24730779  -22.69834944   18.14094557   18.534251
   18.69225769   25.17834962   31.89666741   32.4050049    36.03771571
   36.66670793   52.56239787   54.25957428   57.08738085   61.62771572
   72.77987258   79.97704618   84.71704946   94.00125843   94.19740352
   95.07232442   99.27272471  105.91989234  106.18413986  106.61888653
  112.63324094  114.60067786  116.47086787  116.51098     116.59548472
  117.95497782  124.34187604  128.90443776  134.3205758   144.4467766
  147.06057442  154.46689919  156.24618581  156.42911943  158.51798413
  166.92992889  168.12910327  168.32790079  169.99590647  179.83810426
  182.16470192  184.08731548  184.42954181  189.88700793  190.01224314
  191.95774675  196.33453198  201.05317488  201.78522879  207.61010756
  210.43454315  212.15813467  213.58916802  213.68066085  215.76903125
  218.87296056  219.8938352   221.55427931  223.25804144  229.06331402
  229.89034361  230.83614278  242.86029354  256.83932945  272.70119861
  276.10991136  283.22855048  290.72864991  293.14863056  320.18594509
  323.2057931   344.96494304  366.6351554   384.68362826  401.19893609
  404.85545829  436.83002729  437.06821619  446.71216718  450.64332733
  469.63473588  473.79372057  475.78724129  481.47003396  482.5534186
  497.3271866   502.51838567  508.23621599  527.7648076   537.05928655
  553.32533334  553.48426895  555.5199792   560.46506436  572.11974327
  578.16420374  581.50409261  583.00550482  597.52795318  600.85493229
  605.95370052  606.89618343  620.51299525  624.70681015  630.23736491
  631.5518751   658.72608238  662.37365481  668.77079518  671.3080771
  688.58993175  701.81295875  710.31589512  716.22536939  722.3609598
  726.11333231  726.47925137  734.56779392  736.63583003  744.30138878
  767.70105489  768.73209782  774.91311809  779.47144797  788.53688263
  793.82716573  799.74418213  803.53481759  809.76264369  810.97531053
  818.76684211  819.72967372  827.03676572  833.27242674  837.12768435
  837.18291032  852.97506692  855.23698313  856.06693798  867.72120341
  868.1633077   870.96953141  871.39424881  877.52097954  877.78695006
  909.9186257   911.49951501  912.66890385  918.67761934  923.04099951
  927.21378749  928.17981275  932.93634177  937.01547782  952.96707777
  958.0699434   962.84124522  967.21586262  996.85650278 1025.00347782
 1039.16402885 1039.78397173 1069.35602053 1069.96912738 1079.52750826
 1081.27763751 1084.26627495 1122.50176508 1145.64247268 1156.74610959
 1165.94831239 1172.65408898 1191.35557305 1223.34001438 1231.31773398
 1281.40819681 1291.63482155 1298.98143007 1313.68336221 1332.76858676
 1373.68641894 1389.96137052 1408.32927479 1415.37100158 1415.95662921
 1418.79397866 1425.64201846 1436.7703022  1444.8742987  1455.10103315
 1464.81151459 1480.27876503 1480.73126013 1483.93039685 1491.97755394
 1512.23108748 1521.56198243 1525.58827426 1525.82648228 1538.38225196
 1539.56531557 1553.18718808 1553.40060648 1553.67248127 1554.2785549
 1578.57170849 1583.32001802 1587.18145893 1587.78856335 1596.85943303
 1600.05548118 1648.38966521 1662.66715374 1670.14070122 1670.87137062
 1671.74729172 1679.99597469 1683.90874741 1700.93624107 1702.52424931
 1705.57520837 1735.16955392 1756.39335374 1757.82710459 1776.35494333
 1777.61207979 1790.32408447 1806.9001545  1814.74908046 1817.87221105
 1844.10564114 1897.44938647 1903.37414384 1940.04647372 1955.52695868
 1989.8060044  1990.4471889  1999.71073314 2002.79461683 2030.25906501
 2059.6721564  2067.38126332 2073.84052963 2081.17454623 2081.41663239
 2098.26393327 2163.9281288  2169.4214639  2188.24289228 2213.59247392
 2214.79038033 2236.76410259 2240.25649811 2316.13575875 2350.74327548
 2528.06277524 2529.57128632 2558.45425896 2610.04155006 2615.28414462
 2618.61361065 2645.18727726 2646.06899759 2646.2884004  2655.46915359
 2713.48495737 2745.16246933 2746.14021096 2751.45964996 2805.70013928
 2823.98410982 2832.18301075 2865.68703628 2866.65517781 2868.3962322
 2873.7900389  2908.17840563 2911.74741678 2919.89904375 2939.98546055
 2989.07503168 3000.82229205 3026.52472405 3078.96260724 3126.80889538
 3188.6198344  3218.69401887 3230.75246286 3266.16351614 3600.19661451
 3617.23374779 3812.28142926 3813.16447538 4031.94536718 4071.18016174
 4079.05565608 4079.96795983 4089.53158236 4108.47914019 4117.41090968
 4127.2547502  4167.1747594  4184.17992205 4187.95937903 4207.38529871
 4210.25253259 4212.52885125 4215.11172055 4222.00212054 4223.81264398
 4226.98787167 4227.76468527 4234.10144303 4243.33767626 4249.4217244
 4261.11475591 4265.40573288 4283.31217151 4290.96762481 4302.28859443
 4305.60777891 4328.6025185  4330.38249264 4343.30329165 4350.83517393
 4357.79194471 4362.47487658 4391.63916041 4408.45567702 4418.19675027
 4474.0948897  4526.63478974 4531.20723922 4542.61529274 4584.4027012
 4647.53502786 4771.14676225]
sorted_val_rewards: [-340.93368667 -316.10403797 -293.61621306 -271.80602473 -233.67489674
 -144.91600187 -121.70501653   46.41665444   64.26879367   76.22063096
  154.08827534  170.97708371  195.01398454  205.26303142  214.28737943
  221.88051933  246.00283085  285.96659066  302.97557119  314.70262512
  406.27949347  422.31704774  585.6694137   635.90294353  666.1488369
  680.56533067  683.13000451  685.21886664  726.16233976  774.75191394
  799.17615147  886.97441062  905.39040983  972.47460426 1172.88001302
 1370.8854603  1460.30079394 1501.39430535 1573.13710957 1637.33115926
 1729.56297887 1874.99064848 2259.29310248 4015.87846288 4116.6937375
 4200.0771443  4318.0498578  4340.43328662]
maximum traj length 1000
maximum traj length 1000
num train_obs 780
num train_labels 780
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=23, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11392
Number of trainable paramters: 11392
device: cuda:3
end of epoch 0: val_loss 0.6757554150516677, val_acc 0.9388297872340425
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.482211666599332, val_acc 0.9485815602836879
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.7136270210131953, val_acc 0.9485815602836879
trigger times: 1
end of epoch 3: val_loss 0.46518554434367393, val_acc 0.9556737588652482
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.4012863527600462, val_acc 0.9618794326241135
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.3888240876971012, val_acc 0.9636524822695035
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.7126232088315049, val_acc 0.9618794326241135
trigger times: 1
end of epoch 7: val_loss 0.4143809392046731, val_acc 0.9654255319148937
trigger times: 2
end of epoch 8: val_loss 0.4734459523674037, val_acc 0.9609929078014184
trigger times: 3
end of epoch 9: val_loss 1.3315314867917125, val_acc 0.9512411347517731
trigger times: 4
end of epoch 10: val_loss 0.44023421773783555, val_acc 0.9663120567375887
trigger times: 5
end of epoch 11: val_loss 0.3998450563325169, val_acc 0.9680851063829787
trigger times: 6
end of epoch 12: val_loss 0.4183472906786001, val_acc 0.9689716312056738
trigger times: 7
end of epoch 13: val_loss 0.3591994548506469, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.3572155787909579, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.35682838353777857, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.35739984244237455, val_acc 0.9689716312056738
trigger times: 1
end of epoch 17: val_loss 0.35750220915818814, val_acc 0.9689716312056738
trigger times: 2
end of epoch 18: val_loss 0.3572298631452926, val_acc 0.9689716312056738
trigger times: 3
end of epoch 19: val_loss 0.3577717829608409, val_acc 0.9689716312056738
trigger times: 4
end of epoch 20: val_loss 0.3490338700799336, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.36390873672108626, val_acc 0.9680851063829787
trigger times: 1
end of epoch 22: val_loss 0.48867668495097816, val_acc 0.9680851063829787
trigger times: 2
end of epoch 23: val_loss 0.3850252083002022, val_acc 0.9698581560283688
trigger times: 3
end of epoch 24: val_loss 0.3839881175388021, val_acc 0.9698581560283688
trigger times: 4
end of epoch 25: val_loss 0.3823206379958523, val_acc 0.9707446808510638
trigger times: 5
end of epoch 26: val_loss 0.38046295917222, val_acc 0.9707446808510638
trigger times: 6
end of epoch 27: val_loss 0.379142556251575, val_acc 0.9716312056737588
trigger times: 7
end of epoch 28: val_loss 0.37800958313300553, val_acc 0.9725177304964538
trigger times: 8
end of epoch 29: val_loss 0.3768063859666591, val_acc 0.9725177304964538
trigger times: 9
end of epoch 30: val_loss 0.37576414706061767, val_acc 0.9725177304964538
trigger times: 10
Early stopping.
0 397.87383961107116 -340.93368667132495
1 416.7318780981004 -316.1040379685105
2 402.2597380541265 -293.61621306390157
3 394.3702938295901 -271.8060247270487
4 401.6114271003753 -233.6748967447529
5 417.6819998091087 -144.91600187495783
6 484.663382941857 -121.70501653144265
7 510.6356765674427 46.416654441560325
8 502.3752552426886 64.26879367316346
9 505.3183220019564 76.22063095813924
10 523.5877430252731 154.08827533872434
11 511.2589098261669 170.97708370598906
12 548.5466600079089 195.01398453916917
13 544.6111621158198 205.26303141538762
14 539.1202140450478 214.2873794271587
15 548.0302050877362 221.88051933186094
16 550.2073841709644 246.00283085027885
17 535.802167722024 285.96659066445415
18 549.7715478739701 302.9755711907817
19 592.0698971040547 314.70262511518507
20 617.5619428863283 406.2794934682273
21 601.3040215391666 422.31704773603093
22 654.885380320251 585.6694137038423
23 636.1715241782367 635.9029435282087
24 658.9680033549666 666.1488368982799
25 697.8635951429605 680.5653306731639
26 661.7579851103947 683.1300045105083
27 665.21495109424 685.2188666363189
28 687.0671034436673 726.1623397609577
29 688.2798562720418 774.7519139406528
30 675.7754381392151 799.1761514716857
31 738.0300759710371 886.9744106175162
32 738.5190419647843 905.3904098331311
33 749.904628528282 972.4746042648616
34 795.752030916512 1172.8800130151055
35 844.5320527274162 1370.8854602954927
36 892.7792696356773 1460.300793939679
37 873.7859218502417 1501.3943053524936
38 929.6804985869676 1573.137109571316
39 876.3993633296341 1637.3311592555465
40 934.033823940903 1729.562978866093
41 984.2359149334952 1874.9906484750777
42 1039.6441204492003 2259.2931024822797
43 1428.8932780567557 4015.8784628803237
44 1477.0468254182488 4116.693737500812
45 1483.92266459018 4200.077144304393
46 1524.9314008429646 4318.049857801658
47 1537.596798915416 4340.433286622604
train accuracy: 1.0
train loss: 1.4124018066151113e-05
validation accuracy: 0.9725177304964538
validation loss: 0.37576414706061767

demos: (480, 1000, 23)
demo_rewards: (480,)
sorted_train_rewards: [-524.69363357 -428.04712667 -426.07186886 -417.85007471 -400.69209088
 -395.02788639 -393.33566725 -391.95882373 -388.59399582 -377.7472339
 -376.76557862 -373.51420524 -360.72317391 -359.81019509 -357.01902541
 -356.52299073 -354.47400187 -346.3907683  -340.93368667 -338.24427931
 -336.02543195 -331.62260389 -329.80209332 -319.2332748  -317.83524537
 -315.72296788 -311.34859504 -308.66558365 -303.90832615 -301.55233277
 -300.05811195 -297.00598958 -293.61621306 -290.06408949 -286.79145896
 -285.62255496 -280.86172479 -278.93507563 -274.17047796 -273.93576074
 -273.83226615 -271.80602473 -268.17088379 -264.55923119 -264.37152489
 -261.40264183 -257.70030889 -256.49969365 -248.79063039 -248.56818267
 -247.59340728 -244.37737256 -239.39040955 -238.0865228  -233.67489674
 -229.04306351 -226.4021581  -215.62618485 -211.88658718 -211.36661726
 -208.03522829 -207.41638882 -207.28517741 -205.96911148 -203.9046547
 -190.14098957 -184.9841793  -184.73594166 -180.59042632 -175.50125481
 -171.6271578  -153.5033803  -138.2312438  -133.30170121 -123.62457903
 -120.40905315  -93.1000472   -84.51907957  -71.26903173  -68.60132228
  -60.59147508  -57.33491011  -38.29932913  -36.36051483  -28.46460989
  -26.64860697  -25.89971503  -25.24730779  -22.69834944   18.14094557
   18.534251     18.69225769   25.17834962   31.89666741   32.4050049
   36.03771571   36.66670793   46.41665444   52.56239787   54.25957428
   57.08738085   61.62771572   64.26879367   72.77987258   76.22063096
   79.97704618   84.71704946   94.00125843  105.91989234  106.18413986
  106.61888653  112.63324094  116.51098     116.59548472  117.95497782
  128.90443776  134.3205758   144.4467766   147.06057442  154.08827534
  154.46689919  156.24618581  156.42911943  158.51798413  166.92992889
  168.12910327  170.97708371  179.83810426  182.16470192  184.08731548
  184.42954181  189.88700793  191.95774675  195.01398454  196.33453198
  201.05317488  201.78522879  205.26303142  207.61010756  210.43454315
  212.15813467  213.58916802  213.68066085  214.28737943  215.76903125
  218.87296056  219.8938352   221.55427931  221.88051933  223.25804144
  229.06331402  229.89034361  230.83614278  242.86029354  246.00283085
  256.83932945  272.70119861  276.10991136  283.22855048  285.96659066
  290.72864991  293.14863056  302.97557119  314.70262512  320.18594509
  323.2057931   344.96494304  366.6351554   384.68362826  401.19893609
  404.85545829  406.27949347  422.31704774  436.83002729  437.06821619
  446.71216718  450.64332733  475.78724129  481.47003396  482.5534186
  497.3271866   502.51838567  508.23621599  527.7648076   537.05928655
  553.32533334  555.5199792   560.46506436  572.11974327  578.16420374
  581.50409261  583.00550482  585.6694137   597.52795318  600.85493229
  606.89618343  620.51299525  624.70681015  630.23736491  631.5518751
  635.90294353  658.72608238  662.37365481  666.1488369   668.77079518
  680.56533067  683.13000451  685.21886664  688.58993175  710.31589512
  716.22536939  722.3609598   726.11333231  726.16233976  726.47925137
  734.56779392  736.63583003  767.70105489  768.73209782  774.75191394
  779.47144797  788.53688263  793.82716573  799.17615147  799.74418213
  809.76264369  810.97531053  818.76684211  819.72967372  827.03676572
  833.27242674  837.12768435  837.18291032  852.97506692  855.23698313
  867.72120341  868.1633077   871.39424881  877.52097954  877.78695006
  886.97441062  905.39040983  909.9186257   911.49951501  912.66890385
  918.67761934  923.04099951  927.21378749  928.17981275  932.93634177
  937.01547782  952.96707777  962.84124522  967.21586262  972.47460426
  996.85650278 1025.00347782 1039.16402885 1039.78397173 1069.35602053
 1069.96912738 1079.52750826 1081.27763751 1084.26627495 1122.50176508
 1145.64247268 1156.74610959 1165.94831239 1172.65408898 1172.88001302
 1191.35557305 1223.34001438 1231.31773398 1281.40819681 1298.98143007
 1313.68336221 1332.76858676 1370.8854603  1373.68641894 1389.96137052
 1415.37100158 1415.95662921 1418.79397866 1425.64201846 1436.7703022
 1444.8742987  1455.10103315 1460.30079394 1464.81151459 1480.73126013
 1483.93039685 1491.97755394 1501.39430535 1512.23108748 1521.56198243
 1525.58827426 1538.38225196 1539.56531557 1553.18718808 1553.40060648
 1553.67248127 1554.2785549  1573.13710957 1578.57170849 1583.32001802
 1587.78856335 1596.85943303 1600.05548118 1637.33115926 1648.38966521
 1662.66715374 1670.87137062 1671.74729172 1679.99597469 1683.90874741
 1700.93624107 1702.52424931 1705.57520837 1729.56297887 1735.16955392
 1756.39335374 1757.82710459 1776.35494333 1777.61207979 1790.32408447
 1806.9001545  1817.87221105 1844.10564114 1903.37414384 1940.04647372
 1955.52695868 1989.8060044  1990.4471889  1999.71073314 2002.79461683
 2030.25906501 2059.6721564  2067.38126332 2081.17454623 2081.41663239
 2098.26393327 2163.9281288  2169.4214639  2188.24289228 2213.59247392
 2214.79038033 2236.76410259 2240.25649811 2259.29310248 2316.13575875
 2350.74327548 2528.06277524 2529.57128632 2558.45425896 2610.04155006
 2615.28414462 2618.61361065 2645.18727726 2646.06899759 2646.2884004
 2655.46915359 2745.16246933 2746.14021096 2751.45964996 2805.70013928
 2823.98410982 2832.18301075 2865.68703628 2866.65517781 2868.3962322
 2873.7900389  2908.17840563 2911.74741678 2919.89904375 2939.98546055
 2989.07503168 3026.52472405 3078.96260724 3126.80889538 3188.6198344
 3218.69401887 3230.75246286 3266.16351614 3600.19661451 3617.23374779
 3812.28142926 3813.16447538 4015.87846288 4071.18016174 4079.05565608
 4079.96795983 4108.47914019 4116.6937375  4117.41090968 4167.1747594
 4184.17992205 4187.95937903 4200.0771443  4207.38529871 4210.25253259
 4212.52885125 4215.11172055 4222.00212054 4223.81264398 4226.98787167
 4227.76468527 4234.10144303 4243.33767626 4249.4217244  4261.11475591
 4265.40573288 4283.31217151 4290.96762481 4302.28859443 4305.60777891
 4318.0498578  4328.6025185  4330.38249264 4340.43328662 4343.30329165
 4350.83517393 4357.79194471 4362.47487658 4391.63916041 4408.45567702
 4418.19675027 4474.0948897  4526.63478974 4531.20723922 4542.61529274
 4647.53502786 4771.14676225]
sorted_val_rewards: [-519.9080064  -391.95857823 -382.64759058 -323.40639163 -316.10403797
 -280.79509685 -257.03338564 -152.60167504 -144.91600187 -130.30804227
 -121.70501653   94.19740352   95.07232442   99.27272471  114.60067786
  116.47086787  124.34187604  168.32790079  169.99590647  190.01224314
  469.63473588  473.79372057  553.48426895  605.95370052  671.3080771
  701.81295875  744.30138878  774.91311809  803.53481759  856.06693798
  870.96953141  958.0699434  1291.63482155 1408.32927479 1480.27876503
 1525.82648228 1587.18145893 1670.14070122 1814.74908046 1874.99064848
 1897.44938647 2073.84052963 2713.48495737 3000.82229205 4031.94536718
 4089.53158236 4127.2547502  4584.4027012 ]
maximum traj length 1000
maximum traj length 1000
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=23, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11392
Number of trainable paramters: 11392
device: cuda:0
end of epoch 0: val_loss 0.030089106127384305, val_acc 0.9884751773049646
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.012867237297717299, val_acc 0.9937943262411347
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.00994562388679984, val_acc 0.9964539007092199
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.009141339867319744, val_acc 0.9964539007092199
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.011632816725597654, val_acc 0.9946808510638298
trigger times: 1
end of epoch 5: val_loss 0.00809671612985187, val_acc 0.9955673758865248
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0058367649262763876, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.008042504053233012, val_acc 0.9946808510638298
trigger times: 1
end of epoch 8: val_loss 0.009190919931678889, val_acc 0.9955673758865248
trigger times: 2
end of epoch 9: val_loss 0.012989727503289365, val_acc 0.9955673758865248
trigger times: 3
end of epoch 10: val_loss 0.00588243876763931, val_acc 0.9973404255319149
trigger times: 4
end of epoch 11: val_loss 0.004972247493715386, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.011991999598471139, val_acc 0.9964539007092199
trigger times: 1
end of epoch 13: val_loss 0.007742384186913369, val_acc 0.9964539007092199
trigger times: 2
end of epoch 14: val_loss 0.004826480986924792, val_acc 0.9964539007092199
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0075460896036624436, val_acc 0.9973404255319149
trigger times: 1
end of epoch 16: val_loss 0.005613050073152153, val_acc 0.9973404255319149
trigger times: 2
end of epoch 17: val_loss 0.009241179042057995, val_acc 0.9973404255319149
trigger times: 3
end of epoch 18: val_loss 0.004547739039630117, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.007202715340880264, val_acc 0.9964539007092199
trigger times: 1
end of epoch 20: val_loss 0.007752924325702648, val_acc 0.9964539007092199
trigger times: 2
end of epoch 21: val_loss 0.0054437646475522, val_acc 0.9964539007092199
trigger times: 3
end of epoch 22: val_loss 0.005673455923160545, val_acc 0.9973404255319149
trigger times: 4
end of epoch 23: val_loss 0.004669230223923299, val_acc 0.99822695035461
trigger times: 5
end of epoch 24: val_loss 0.004540291115342383, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.004867805488025328, val_acc 0.9964539007092199
trigger times: 1
end of epoch 26: val_loss 0.006151911439536646, val_acc 0.9973404255319149
trigger times: 2
end of epoch 27: val_loss 0.004337199939882952, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.004370458651155469, val_acc 0.9973404255319149
trigger times: 1
end of epoch 29: val_loss 0.005697600369174652, val_acc 0.9973404255319149
trigger times: 2
end of epoch 30: val_loss 0.007444931595558775, val_acc 0.9964539007092199
trigger times: 3
end of epoch 31: val_loss 0.0032206005942975596, val_acc 0.99822695035461
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.006146786660888303, val_acc 0.9955673758865248
trigger times: 1
end of epoch 33: val_loss 0.008373350369235241, val_acc 0.9973404255319149
trigger times: 2
end of epoch 34: val_loss 0.006115051978513735, val_acc 0.9964539007092199
trigger times: 3
end of epoch 35: val_loss 0.005441728816940018, val_acc 0.9973404255319149
trigger times: 4
end of epoch 36: val_loss 0.005757811621098474, val_acc 0.9973404255319149
trigger times: 5
end of epoch 37: val_loss 0.004806208287506009, val_acc 0.9973404255319149
trigger times: 6
end of epoch 38: val_loss 0.005363810348971516, val_acc 0.9973404255319149
trigger times: 7
end of epoch 39: val_loss 0.010793976721203358, val_acc 0.9955673758865248
trigger times: 8
end of epoch 40: val_loss 0.010200137434030336, val_acc 0.9955673758865248
trigger times: 9
end of epoch 41: val_loss 0.004919620293280543, val_acc 0.99822695035461
trigger times: 10
Early stopping.
0 -4.633679714053869 -519.9080063971747
1 40.54018548876047 -391.9585782336556
2 42.969001315534115 -382.6475905757797
3 65.37862277030945 -323.40639162743133
4 69.723253371194 -316.1040379685105
5 82.13577939942479 -280.7950968544581
6 86.22841843031347 -257.03338563895727
7 115.77125749737024 -152.60167504208923
8 119.42747927829623 -144.91600187495783
9 131.0586551167071 -130.30804226838904
10 139.1318116672337 -121.70501653144265
11 213.55439971387386 94.19740352247452
12 212.99050984159112 95.07232442353506
13 217.80542055517435 99.27272471081422
14 220.62667934782803 114.60067785589865
15 223.59824647009373 116.47086786642583
16 224.97121779248118 124.34187603836524
17 239.2366836592555 168.32790078559515
18 242.72325614839792 169.99590647048413
19 247.88587974570692 190.0122431419386
20 352.00487211346626 469.6347358842159
21 348.0391299389303 473.7937205693257
22 376.82294055074453 553.4842689452406
23 391.68567264452577 605.9537005157925
24 417.65545941889286 671.3080771046588
25 433.8429377824068 701.8129587451294
26 450.1410866305232 744.301388782645
27 458.17505207285285 774.9131180922687
28 471.04714876785874 803.5348175925284
29 483.0605389177799 856.0669379761373
30 490.9586411193013 870.9695314142782
31 518.163937676698 958.069943397442
32 638.1009285114706 1291.6348215451665
33 683.996224924922 1408.3292747852752
34 707.9261901602149 1480.2787650285316
35 719.422393810004 1525.8264822752312
36 749.6140889823437 1587.1814589329433
37 775.1062011346221 1670.1407012219743
38 825.6682471334934 1814.74908045765
39 843.6988150980324 1874.9906484750777
40 854.8259642124176 1897.4493864736492
41 918.0114830136299 2073.8405296250257
42 1145.2021527811885 2713.484957372366
43 1245.96282415092 3000.822292046694
44 1606.863210823387 4031.9453671833917
45 1625.1557816714048 4089.5315823594406
46 1638.0533917099237 4127.254750202243
47 1800.3314244449139 4584.402701204601
train accuracy: 0.998929786339487
validation accuracy: 0.99822695035461

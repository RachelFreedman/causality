FINISHED REGISTRATION
demos: (480, 1000, 9)
demo_rewards: (480,)
sorted_train_rewards: [-5.17529558e+02 -4.83550523e+02 -3.81546354e+02 -3.77409101e+02
 -3.69432856e+02 -3.62275129e+02 -3.50478715e+02 -3.45382744e+02
 -3.44868534e+02 -3.31447324e+02 -3.13466169e+02 -3.11344158e+02
 -3.10797466e+02 -3.09110683e+02 -3.07595090e+02 -3.00601780e+02
 -2.97213086e+02 -2.95457016e+02 -2.92971968e+02 -2.87555036e+02
 -2.73994380e+02 -2.60526163e+02 -2.58376495e+02 -2.52390992e+02
 -2.51617666e+02 -2.49972622e+02 -2.48418949e+02 -2.37032333e+02
 -2.34077317e+02 -2.31837606e+02 -2.25478889e+02 -2.21593472e+02
 -2.14716696e+02 -2.13290418e+02 -2.08849904e+02 -1.97273488e+02
 -1.96088322e+02 -1.90559675e+02 -1.90541660e+02 -1.88302976e+02
 -1.87941759e+02 -1.87145800e+02 -1.84567148e+02 -1.79763680e+02
 -1.72024877e+02 -1.60392530e+02 -1.59610388e+02 -1.56892497e+02
 -1.56690710e+02 -1.50110533e+02 -1.48240974e+02 -1.44533808e+02
 -1.44105590e+02 -1.41012729e+02 -1.36249901e+02 -1.31698175e+02
 -1.29831521e+02 -1.29587816e+02 -1.29078727e+02 -1.29071077e+02
 -1.28810282e+02 -1.26720839e+02 -1.26675663e+02 -1.26015076e+02
 -1.25245108e+02 -1.24791739e+02 -1.14492967e+02 -1.12895331e+02
 -1.12574673e+02 -1.10516185e+02 -1.09810901e+02 -1.09290940e+02
 -1.09000289e+02 -1.08756732e+02 -1.08459758e+02 -1.08156827e+02
 -1.06325016e+02 -1.04952188e+02 -1.04376385e+02 -1.04082473e+02
 -1.03270724e+02 -1.03034780e+02 -1.01788939e+02 -1.00740682e+02
 -1.00626724e+02 -9.93079384e+01 -9.82430953e+01 -9.75922404e+01
 -9.72157047e+01 -9.69369722e+01 -9.66151117e+01 -9.62144964e+01
 -9.42904165e+01 -9.42143274e+01 -9.39116749e+01 -9.37223299e+01
 -9.31393582e+01 -9.23679858e+01 -8.91046975e+01 -8.73076297e+01
 -8.69565508e+01 -8.60955211e+01 -8.60814293e+01 -8.56528919e+01
 -8.52954333e+01 -8.51947671e+01 -8.51522733e+01 -8.49991965e+01
 -8.48768991e+01 -8.32947405e+01 -8.29888611e+01 -8.28197895e+01
 -8.26763624e+01 -8.25486820e+01 -8.15039902e+01 -8.01953050e+01
 -8.01671058e+01 -7.99036760e+01 -7.92717361e+01 -7.92696977e+01
 -7.86226825e+01 -7.81079480e+01 -7.72932816e+01 -7.71273488e+01
 -7.66155611e+01 -7.64630234e+01 -7.59188145e+01 -7.38744569e+01
 -7.32504471e+01 -7.31040190e+01 -7.30870760e+01 -7.19993983e+01
 -7.03775657e+01 -7.02937514e+01 -7.01868696e+01 -6.84572675e+01
 -6.82348917e+01 -6.78410655e+01 -6.73828071e+01 -6.56927252e+01
 -6.54681151e+01 -6.41823353e+01 -6.34099146e+01 -6.32284667e+01
 -6.16229119e+01 -6.13310733e+01 -6.11947751e+01 -6.08893846e+01
 -6.00533033e+01 -5.97428740e+01 -5.96789324e+01 -5.95646968e+01
 -5.92670890e+01 -5.87023048e+01 -5.80072614e+01 -5.77416323e+01
 -5.63710853e+01 -5.63134086e+01 -5.59145048e+01 -5.56095711e+01
 -5.46999231e+01 -5.43244013e+01 -5.38923554e+01 -5.35343571e+01
 -5.33223015e+01 -5.32930671e+01 -5.20320219e+01 -4.91645734e+01
 -4.87687115e+01 -4.87449080e+01 -4.73209297e+01 -4.42348165e+01
 -4.41535396e+01 -4.38346642e+01 -4.33808937e+01 -4.32102257e+01
 -4.19808710e+01 -4.11573211e+01 -4.02983962e+01 -3.92977585e+01
 -3.76910430e+01 -3.68954521e+01 -3.60836294e+01 -3.51457285e+01
 -3.27360994e+01 -3.21637513e+01 -3.14342485e+01 -3.08767304e+01
 -3.05834223e+01 -3.03923769e+01 -3.00549334e+01 -2.69214892e+01
 -2.65781736e+01 -2.61892431e+01 -2.40186709e+01 -2.34105292e+01
 -2.32859985e+01 -2.27812936e+01 -2.27259541e+01 -2.23283679e+01
 -2.18964820e+01 -1.79372277e+01 -1.75102017e+01 -1.68441326e+01
 -1.58811428e+01 -1.51577086e+01 -1.32860875e+01 -1.10026071e+01
 -9.57975571e+00 -8.99483774e+00 -7.43485049e+00 -6.54991997e+00
 -6.33018949e+00 -5.65528936e+00 -4.03226309e+00 -3.76983251e+00
 -2.62467353e+00 -1.89580632e+00  8.37306245e-02  1.88336337e+00
  1.95130509e+00  2.06757239e+00  2.44986350e+00  2.82333720e+00
  3.24518158e+00  3.41681858e+00  4.33648635e+00  4.45142428e+00
  4.51224760e+00  8.01184608e+00  9.95238118e+00  1.10201451e+01
  1.28665786e+01  1.30824157e+01  1.35991331e+01  1.58459073e+01
  1.92372514e+01  2.19881242e+01  2.22071175e+01  2.24464725e+01
  2.57684334e+01  2.65632939e+01  2.68371047e+01  2.79327696e+01
  2.99614289e+01  3.08087071e+01  3.17619258e+01  3.27789273e+01
  3.28416472e+01  3.40865969e+01  3.42500191e+01  3.43686399e+01
  3.45401081e+01  3.54378449e+01  3.66699936e+01  3.85892744e+01
  3.92793569e+01  4.03882111e+01  4.07523095e+01  4.29226721e+01
  4.29311024e+01  4.31669843e+01  4.44130163e+01  4.55238513e+01
  4.59297113e+01  5.08875967e+01  5.36861330e+01  5.41092849e+01
  5.51347390e+01  5.68482604e+01  5.92505205e+01  6.00882243e+01
  8.36807129e+01  8.71017367e+01  8.91837476e+01  9.26210279e+01
  9.69823757e+01  9.92340234e+01  9.98594136e+01  9.99106092e+01
  1.01554085e+02  1.05849931e+02  1.07680804e+02  1.07949210e+02
  1.08725150e+02  1.09668060e+02  1.10494197e+02  1.10760795e+02
  1.16053271e+02  1.16291458e+02  1.17682751e+02  1.20192926e+02
  1.20317099e+02  1.21362944e+02  1.23466742e+02  1.23751118e+02
  1.24730650e+02  1.25267832e+02  1.28172794e+02  1.29066327e+02
  1.32633242e+02  1.33660944e+02  1.33807958e+02  1.34858974e+02
  1.38625800e+02  1.39160438e+02  1.39181873e+02  1.40018716e+02
  1.40434657e+02  1.40883673e+02  1.41762771e+02  1.42359227e+02
  1.42805535e+02  1.43271041e+02  1.43793082e+02  1.46345459e+02
  1.47084064e+02  1.48077563e+02  1.48370003e+02  1.51510111e+02
  1.55376317e+02  1.59678797e+02  1.62253475e+02  1.63261511e+02
  1.64484374e+02  1.64700310e+02  1.65791665e+02  1.73946639e+02
  2.09226064e+02  2.12974636e+02  2.20231471e+02  2.21617533e+02
  2.21754411e+02  2.29106711e+02  2.30342775e+02  2.32918138e+02
  2.33967008e+02  2.34320877e+02  2.36981370e+02  2.37820201e+02
  2.38363681e+02  2.38793862e+02  2.39831240e+02  2.40202654e+02
  2.41206601e+02  2.42533883e+02  2.44107223e+02  2.44360127e+02
  2.44787774e+02  2.45130705e+02  2.45302486e+02  2.45409371e+02
  2.45524570e+02  2.45758793e+02  2.46055618e+02  2.46443651e+02
  2.46582273e+02  2.47119146e+02  2.48248632e+02  2.48522583e+02
  2.48894511e+02  2.49922999e+02  2.50280341e+02  2.50360947e+02
  2.50667900e+02  2.51331648e+02  2.54744324e+02  2.56162735e+02
  2.56185461e+02  2.56276249e+02  2.57225817e+02  2.57560493e+02
  2.57902408e+02  2.57904280e+02  2.58848137e+02  2.58910101e+02
  2.59104464e+02  2.59202923e+02  2.60247726e+02  2.60535357e+02
  2.61212177e+02  2.61274076e+02  2.61450010e+02  2.61810674e+02
  2.62100542e+02  2.62410009e+02  2.62926235e+02  2.62935174e+02
  2.63041138e+02  2.63045233e+02  2.63245142e+02  2.63332726e+02
  2.64251640e+02  2.64876300e+02  2.67379590e+02  2.67565648e+02
  2.68127579e+02  2.69324966e+02  2.69811118e+02  2.70476276e+02
  2.70796421e+02  2.70942244e+02  2.71844593e+02  2.72676170e+02
  2.73208121e+02  2.73312286e+02  2.74276745e+02  2.75376573e+02
  2.76006697e+02  2.76016250e+02  2.77220423e+02  2.77338302e+02
  2.78959823e+02  2.82128969e+02  2.83669133e+02  2.85268187e+02
  2.88361437e+02  2.91178450e+02  2.91262559e+02  2.91530071e+02
  2.93038456e+02  2.94814905e+02  2.95553480e+02  2.99027510e+02
  2.99167679e+02  2.99903898e+02  3.00714605e+02  3.01722647e+02
  3.02446852e+02  3.04395724e+02  3.10778723e+02  3.11978625e+02]
sorted_val_rewards: [-409.59613635 -363.38662637 -279.62050043 -278.59374339 -161.46247912
 -154.12683049 -153.76944773 -149.33306441 -136.39474401 -122.15321969
 -113.87315285 -112.00237672 -110.39489689  -93.8497135   -87.90411206
  -82.4995557   -82.23366584  -80.45639056  -77.30782459  -69.10717211
  -60.66137225  -59.49376063  -58.70021419  -58.18379145  -37.11665644
  -34.39031943  -25.21100034  -25.02106109  -24.20942461  -19.3705173
  -11.16715173   12.97148738   20.31507174   21.83953328   24.37345404
   99.90970035  135.23738658  156.58414447  236.06318731  245.14311093
  246.45706897  247.1166101   248.86802148  258.06939305  267.56584374
  267.76468779  279.70380253  289.63471461]
maximum traj length 1000
maximum traj length 1000
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=9, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 9600
Number of trainable paramters: 9600
device: cuda:2
end of epoch 0: val_loss 0.4618779815950793, val_acc 0.8023049645390071
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2634751368881221, val_acc 0.9113475177304965
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1518548244924437, val_acc 0.9414893617021277
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.3976452192464394, val_acc 0.924645390070922
trigger times: 1
end of epoch 4: val_loss 1.0401060434497482, val_acc 0.8475177304964538
trigger times: 2
end of epoch 5: val_loss 0.17318341140644575, val_acc 0.9432624113475178
trigger times: 3
end of epoch 6: val_loss 0.22649230394781503, val_acc 0.9281914893617021
trigger times: 4
end of epoch 7: val_loss 0.17570024391650158, val_acc 0.9441489361702128
trigger times: 5
end of epoch 8: val_loss 0.23235882659733093, val_acc 0.9343971631205674
trigger times: 6
end of epoch 9: val_loss 0.16588015805771694, val_acc 0.9406028368794326
trigger times: 7
end of epoch 10: val_loss 0.15610059706986143, val_acc 0.9397163120567376
trigger times: 8
end of epoch 11: val_loss 0.12549655724454745, val_acc 0.949468085106383
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.10377466277448841, val_acc 0.9574468085106383
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.2140212600227751, val_acc 0.9370567375886525
trigger times: 1
end of epoch 14: val_loss 0.14559408512863664, val_acc 0.9441489361702128
trigger times: 2
end of epoch 15: val_loss 0.20661132360537665, val_acc 0.9326241134751773
trigger times: 3
end of epoch 16: val_loss 0.15079181377408402, val_acc 0.9432624113475178
trigger times: 4
end of epoch 17: val_loss 0.1253325977137614, val_acc 0.949468085106383
trigger times: 5
end of epoch 18: val_loss 0.18960589520951476, val_acc 0.9388297872340425
trigger times: 6
end of epoch 19: val_loss 0.20047636427318194, val_acc 0.9423758865248227
trigger times: 7
end of epoch 20: val_loss 0.16636645669702654, val_acc 0.9406028368794326
trigger times: 8
end of epoch 21: val_loss 0.21108090117798048, val_acc 0.9361702127659575
trigger times: 9
end of epoch 22: val_loss 0.15321727726652, val_acc 0.9459219858156028
trigger times: 10
Early stopping.
0 -49.484903746750206 -409.59613635079813
1 -46.594577794894576 -363.38662636798153
2 -41.55076073948294 -279.620500434066
3 -42.23209644854069 -278.593743393895
4 -25.817480702884495 -161.4624791151494
5 -26.09695205464959 -154.12683049221144
6 -21.785932715050876 -153.76944773192582
7 -28.10245819343254 -149.33306440934575
8 -27.578136280179024 -136.39474401297213
9 -22.786126762628555 -122.15321969260529
10 -21.32626982452348 -113.87315284733153
11 -26.316378977149725 -112.00237672316835
12 -25.83026842586696 -110.39489689297827
13 -20.653399628587067 -93.84971350273861
14 -20.913583169691265 -87.90411206233028
15 -18.844201606698334 -82.49955570373007
16 -20.11137034278363 -82.23366584233335
17 -18.65077326912433 -80.4563905642366
18 -17.58959645871073 -77.3078245916513
19 -16.851962537504733 -69.10717210787473
20 -17.497369925491512 -60.6613722487345
21 -16.891129274852574 -59.49376063024023
22 -15.947884275577962 -58.70021418689325
23 -15.619101407937706 -58.18379145107471
24 -15.638990103267133 -37.11665643773067
25 -14.236511290073395 -34.39031943487255
26 -15.364997819066048 -25.21100033842761
27 -19.228542962111533 -25.02106109394221
28 -13.077343597076833 -24.20942461004499
29 -11.049174452200532 -19.37051729561709
30 -13.796682983636856 -11.167151727676497
31 -11.089800429530442 12.971487381678784
32 -12.861000574193895 20.31507173710493
33 -11.464456184767187 21.839533281391496
34 -13.923937577754259 24.373454036608678
35 -6.967085098382086 99.90970034673629
36 -6.844710137462243 135.23738658312894
37 -6.680268250405788 156.5841444745833
38 -5.487214260036126 236.06318731085068
39 -6.396630550501868 245.14311092671568
40 -6.039935007924214 246.45706896645925
41 -5.42508565238677 247.116610103496
42 -5.905264116823673 248.86802148461348
43 -5.944386385381222 258.06939305101866
44 -5.355411754222587 267.56584373582893
45 -5.652316017774865 267.7646877901251
46 -5.556735083227977 279.7038025273106
47 -5.078052597120404 289.63471460518747
train accuracy: 0.9397759103641457
train loss: 0.16025661373900513
validation accuracy: 0.9459219858156028
validation loss: 0.15321727726652

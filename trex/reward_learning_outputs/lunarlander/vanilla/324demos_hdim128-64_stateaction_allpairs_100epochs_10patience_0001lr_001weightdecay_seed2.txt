FINISHED REGISTRATION
demos: (480, 1000, 9)
demo_rewards: (480,)
sorted_train_rewards: [-4.83550523e+02 -4.09596136e+02 -3.81546354e+02 -3.77409101e+02
 -3.69432856e+02 -3.63386626e+02 -3.62275129e+02 -3.50478715e+02
 -3.45382744e+02 -3.44868534e+02 -3.31447324e+02 -3.13466169e+02
 -3.11344158e+02 -3.10797466e+02 -3.09110683e+02 -3.07595090e+02
 -3.00601780e+02 -2.97213086e+02 -2.95457016e+02 -2.92971968e+02
 -2.87555036e+02 -2.79620500e+02 -2.78593743e+02 -2.73994380e+02
 -2.60526163e+02 -2.52390992e+02 -2.51617666e+02 -2.48418949e+02
 -2.37032333e+02 -2.34077317e+02 -2.31837606e+02 -2.25478889e+02
 -2.21593472e+02 -2.14716696e+02 -2.13290418e+02 -2.08849904e+02
 -1.97273488e+02 -1.96088322e+02 -1.90559675e+02 -1.90541660e+02
 -1.88302976e+02 -1.87941759e+02 -1.87145800e+02 -1.84567148e+02
 -1.79763680e+02 -1.72024877e+02 -1.61462479e+02 -1.60392530e+02
 -1.59610388e+02 -1.56892497e+02 -1.56690710e+02 -1.54126830e+02
 -1.53769448e+02 -1.50110533e+02 -1.49333064e+02 -1.48240974e+02
 -1.44105590e+02 -1.41012729e+02 -1.36394744e+02 -1.31698175e+02
 -1.29831521e+02 -1.29587816e+02 -1.29078727e+02 -1.29071077e+02
 -1.28810282e+02 -1.26720839e+02 -1.26675663e+02 -1.26015076e+02
 -1.25245108e+02 -1.24791739e+02 -1.14492967e+02 -1.12574673e+02
 -1.12002377e+02 -1.10394897e+02 -1.09810901e+02 -1.09290940e+02
 -1.09000289e+02 -1.08459758e+02 -1.08156827e+02 -1.06325016e+02
 -1.04952188e+02 -1.04376385e+02 -1.04082473e+02 -1.03270724e+02
 -1.03034780e+02 -1.00740682e+02 -1.00626724e+02 -9.93079384e+01
 -9.82430953e+01 -9.75922404e+01 -9.72157047e+01 -9.69369722e+01
 -9.66151117e+01 -9.62144964e+01 -9.42904165e+01 -9.39116749e+01
 -9.38497135e+01 -9.37223299e+01 -9.31393582e+01 -9.23679858e+01
 -8.91046975e+01 -8.79041121e+01 -8.69565508e+01 -8.60955211e+01
 -8.56528919e+01 -8.52954333e+01 -8.51947671e+01 -8.51522733e+01
 -8.49991965e+01 -8.48768991e+01 -8.29888611e+01 -8.28197895e+01
 -8.26763624e+01 -8.25486820e+01 -8.24995557e+01 -8.22336658e+01
 -8.15039902e+01 -8.04563906e+01 -8.01953050e+01 -7.99036760e+01
 -7.92717361e+01 -7.92696977e+01 -7.86226825e+01 -7.81079480e+01
 -7.73078246e+01 -7.72932816e+01 -7.71273488e+01 -7.66155611e+01
 -7.64630234e+01 -7.59188145e+01 -7.38744569e+01 -7.32504471e+01
 -7.31040190e+01 -7.19993983e+01 -7.03775657e+01 -7.02937514e+01
 -7.01868696e+01 -6.91071721e+01 -6.84572675e+01 -6.82348917e+01
 -6.78410655e+01 -6.54681151e+01 -6.41823353e+01 -6.34099146e+01
 -6.32284667e+01 -6.16229119e+01 -6.13310733e+01 -6.11947751e+01
 -6.08893846e+01 -6.06613722e+01 -6.00533033e+01 -5.97428740e+01
 -5.96789324e+01 -5.95646968e+01 -5.92670890e+01 -5.87023048e+01
 -5.87002142e+01 -5.81837915e+01 -5.80072614e+01 -5.77416323e+01
 -5.59145048e+01 -5.56095711e+01 -5.46999231e+01 -5.43244013e+01
 -5.35343571e+01 -5.33223015e+01 -5.32930671e+01 -5.20320219e+01
 -4.91645734e+01 -4.87687115e+01 -4.87449080e+01 -4.73209297e+01
 -4.41535396e+01 -4.38346642e+01 -4.33808937e+01 -4.32102257e+01
 -4.19808710e+01 -4.11573211e+01 -4.02983962e+01 -3.76910430e+01
 -3.71166564e+01 -3.68954521e+01 -3.60836294e+01 -3.51457285e+01
 -3.43903194e+01 -3.27360994e+01 -3.21637513e+01 -3.14342485e+01
 -3.08767304e+01 -3.05834223e+01 -3.03923769e+01 -3.00549334e+01
 -2.69214892e+01 -2.52110003e+01 -2.50210611e+01 -2.42094246e+01
 -2.40186709e+01 -2.32859985e+01 -2.27812936e+01 -2.27259541e+01
 -2.23283679e+01 -2.18964820e+01 -1.93705173e+01 -1.79372277e+01
 -1.75102017e+01 -1.58811428e+01 -1.51577086e+01 -1.32860875e+01
 -1.11671517e+01 -1.10026071e+01 -9.57975571e+00 -8.99483774e+00
 -7.43485049e+00 -6.54991997e+00 -5.65528936e+00 -4.03226309e+00
 -3.76983251e+00 -2.62467353e+00 -1.89580632e+00  8.37306245e-02
  1.88336337e+00  1.95130509e+00  2.06757239e+00  2.44986350e+00
  2.82333720e+00  3.24518158e+00  4.45142428e+00  4.51224760e+00
  8.01184608e+00  9.95238118e+00  1.10201451e+01  1.29714874e+01
  1.30824157e+01  1.35991331e+01  1.92372514e+01  2.03150717e+01
  2.18395333e+01  2.19881242e+01  2.22071175e+01  2.24464725e+01
  2.43734540e+01  2.57684334e+01  2.65632939e+01  2.68371047e+01
  2.79327696e+01  2.99614289e+01  3.17619258e+01  3.27789273e+01
  3.28416472e+01  3.40865969e+01  3.42500191e+01  3.43686399e+01
  3.45401081e+01  3.54378449e+01  3.66699936e+01  3.85892744e+01
  3.92793569e+01  4.03882111e+01  4.07523095e+01  4.29226721e+01
  4.29311024e+01  4.31669843e+01  4.44130163e+01  4.55238513e+01
  4.59297113e+01  5.08875967e+01  5.36861330e+01  5.41092849e+01
  5.51347390e+01  5.68482604e+01  5.92505205e+01  6.00882243e+01
  8.36807129e+01  8.71017367e+01  8.91837476e+01  9.69823757e+01
  9.92340234e+01  9.98594136e+01  9.99097003e+01  9.99106092e+01
  1.01554085e+02  1.05849931e+02  1.07680804e+02  1.07949210e+02
  1.08725150e+02  1.09668060e+02  1.10494197e+02  1.10760795e+02
  1.16053271e+02  1.16291458e+02  1.17682751e+02  1.20192926e+02
  1.20317099e+02  1.21362944e+02  1.23466742e+02  1.24730650e+02
  1.25267832e+02  1.28172794e+02  1.29066327e+02  1.32633242e+02
  1.33660944e+02  1.33807958e+02  1.34858974e+02  1.35237387e+02
  1.38625800e+02  1.39160438e+02  1.39181873e+02  1.40018716e+02
  1.40434657e+02  1.40883673e+02  1.41762771e+02  1.42359227e+02
  1.42805535e+02  1.43271041e+02  1.43793082e+02  1.48077563e+02
  1.51510111e+02  1.55376317e+02  1.56584144e+02  1.59678797e+02
  1.63261511e+02  1.64484374e+02  1.64700310e+02  1.65791665e+02
  2.09226064e+02  2.12974636e+02  2.20231471e+02  2.21617533e+02
  2.21754411e+02  2.29106711e+02  2.30342775e+02  2.32918138e+02
  2.33967008e+02  2.34320877e+02  2.36063187e+02  2.36981370e+02
  2.37820201e+02  2.38363681e+02  2.38793862e+02  2.39831240e+02
  2.40202654e+02  2.41206601e+02  2.42533883e+02  2.44107223e+02
  2.44360127e+02  2.44787774e+02  2.45130705e+02  2.45143111e+02
  2.45302486e+02  2.45409371e+02  2.45524570e+02  2.45758793e+02
  2.46055618e+02  2.46443651e+02  2.46457069e+02  2.46582273e+02
  2.47116610e+02  2.47119146e+02  2.48248632e+02  2.48522583e+02
  2.48868021e+02  2.48894511e+02  2.49922999e+02  2.50280341e+02
  2.50360947e+02  2.50667900e+02  2.51331648e+02  2.54744324e+02
  2.56162735e+02  2.56185461e+02  2.56276249e+02  2.57225817e+02
  2.57560493e+02  2.57902408e+02  2.57904280e+02  2.58069393e+02
  2.58848137e+02  2.58910101e+02  2.59104464e+02  2.59202923e+02
  2.60247726e+02  2.60535357e+02  2.61212177e+02  2.61450010e+02
  2.61810674e+02  2.62100542e+02  2.62410009e+02  2.62926235e+02
  2.62935174e+02  2.63041138e+02  2.63045233e+02  2.63245142e+02
  2.63332726e+02  2.64251640e+02  2.64876300e+02  2.67379590e+02
  2.67565648e+02  2.67565844e+02  2.68127579e+02  2.69324966e+02
  2.69811118e+02  2.70476276e+02  2.70796421e+02  2.71844593e+02
  2.72676170e+02  2.74276745e+02  2.75376573e+02  2.76006697e+02
  2.76016250e+02  2.77220423e+02  2.77338302e+02  2.78959823e+02
  2.82128969e+02  2.83669133e+02  2.85268187e+02  2.88361437e+02
  2.89634715e+02  2.91178450e+02  2.91262559e+02  2.91530071e+02
  2.93038456e+02  2.94814905e+02  2.95553480e+02  2.99027510e+02
  2.99167679e+02  2.99903898e+02  3.00714605e+02  3.01722647e+02
  3.02446852e+02  3.04395724e+02  3.10778723e+02  3.11978625e+02]
sorted_val_rewards: [-517.5295579  -258.37649525 -249.97262175 -144.53380828 -136.2499014
 -122.15321969 -113.87315285 -112.89533115 -110.5161847  -108.7567322
 -101.78893869  -94.2143274   -87.30762967  -86.0814293   -83.29474052
  -80.16710577  -73.08707603  -67.3828071   -65.69272516  -59.49376063
  -56.37108533  -56.31340857  -53.89235539  -44.23481652  -39.29775847
  -26.57817359  -26.18924314  -23.41052919  -16.84413264   -6.33018949
    3.41681858    4.33648635   12.86657864   15.8459073    30.80870712
   92.62102788  123.75111824  146.34545877  147.08406413  148.37000284
  162.25347515  173.94663868  261.27407581  267.76468779  270.94224394
  273.20812084  273.31228621  279.70380253]
maximum traj length 1000
maximum traj length 1000
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=9, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 9600
Number of trainable paramters: 9600
device: cuda:0
end of epoch 0: val_loss 0.18116804399796652, val_acc 0.9237588652482269
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.15829166736662165, val_acc 0.9485815602836879
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.12034312268052567, val_acc 0.9521276595744681
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.22404518240544483, val_acc 0.9317375886524822
trigger times: 1
end of epoch 4: val_loss 0.16415869829813878, val_acc 0.9379432624113475
trigger times: 2
end of epoch 5: val_loss 0.30811526702891395, val_acc 0.9113475177304965
trigger times: 3
end of epoch 6: val_loss 0.2210439955181407, val_acc 0.9370567375886525
trigger times: 4
end of epoch 7: val_loss 0.13046977062767995, val_acc 0.949468085106383
trigger times: 5
end of epoch 8: val_loss 0.173394280194968, val_acc 0.9406028368794326
trigger times: 6
end of epoch 9: val_loss 0.14950385431851101, val_acc 0.9397163120567376
trigger times: 7
end of epoch 10: val_loss 0.16088707570925204, val_acc 0.9388297872340425
trigger times: 8
end of epoch 11: val_loss 0.1347298779548304, val_acc 0.9414893617021277
trigger times: 9
end of epoch 12: val_loss 0.13436726569223156, val_acc 0.9432624113475178
trigger times: 10
Early stopping.
0 -75.43102092086338 -517.529557895617
1 -46.19417480286211 -258.3764952457298
2 -28.215551869943738 -249.9726217492005
3 -29.8032592269592 -144.53380828000297
4 -28.218238454312086 -136.24990139861143
5 -29.061564398929477 -122.15321969260529
6 -25.03875294420868 -113.87315284733153
7 -27.679844146128744 -112.89533115123422
8 -25.54289128817618 -110.51618470347677
9 -30.331794936209917 -108.75673220380507
10 -26.29807737097144 -101.788938686272
11 -26.41015585511923 -94.21432739914145
12 -27.44769641570747 -87.30762967460453
13 -23.186938975006342 -86.08142930417554
14 -24.103429871611297 -83.29474051523388
15 -23.461457043420523 -80.16710576887495
16 -22.9913021158427 -73.08707602649669
17 -23.117933409288526 -67.38280710387886
18 -21.783168591558933 -65.69272515513943
19 -20.895234397612512 -59.49376063024023
20 -20.888333478942513 -56.371085332992536
21 -19.78981066774577 -56.31340856850025
22 -23.061522845178843 -53.89235538707237
23 -22.17096214275807 -44.234816517576114
24 -21.339238403365016 -39.297758471595614
25 -17.910677080973983 -26.578173593643584
26 -16.300814293324947 -26.189243144328344
27 -19.646377570927143 -23.41052918843768
28 -18.618479528697208 -16.84413263580221
29 -16.835102035664022 -6.330189489211961
30 -12.70213770121336 3.4168185754241165
31 -14.88467897195369 4.336486347244374
32 -15.63751401938498 12.8665786383729
33 -17.21585993282497 15.845907300977615
34 -12.776899301446974 30.80870712248469
35 -10.073798310244456 92.62102788229886
36 -7.8960126037709415 123.75111824098029
37 -8.279379747691564 146.34545876890164
38 -7.3813900098903105 147.0840641287317
39 -7.472463999642059 148.37000284123897
40 -6.400283638620749 162.25347514685302
41 -6.689915343536995 173.94663867871992
42 -5.96049692411907 261.2740758125304
43 -5.764280940289609 267.7646877901251
44 -5.381034221732989 270.94224394197516
45 -6.054627441451885 273.2081208413378
46 -7.207524275640026 273.31228620523893
47 -5.688059643027373 279.7038025273106
train accuracy: 0.9438520047395177
train loss: 0.1258802270751291
validation accuracy: 0.9432624113475178
validation loss: 0.13436726569223156

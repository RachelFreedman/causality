demos: (480, 1000, 9)
demo_rewards: (480,)
sorted_train_rewards: [-4.83550523e+02 -4.09596136e+02 -3.81546354e+02 -3.77409101e+02
 -3.69432856e+02 -3.63386626e+02 -3.50478715e+02 -3.45382744e+02
 -3.44868534e+02 -3.31447324e+02 -3.11344158e+02 -3.10797466e+02
 -3.09110683e+02 -3.07595090e+02 -3.00601780e+02 -2.95457016e+02
 -2.92971968e+02 -2.87555036e+02 -2.79620500e+02 -2.78593743e+02
 -2.60526163e+02 -2.52390992e+02 -2.51617666e+02 -2.49972622e+02
 -2.48418949e+02 -2.34077317e+02 -2.31837606e+02 -2.21593472e+02
 -2.14716696e+02 -2.13290418e+02 -2.08849904e+02 -1.97273488e+02
 -1.96088322e+02 -1.90559675e+02 -1.90541660e+02 -1.87941759e+02
 -1.87145800e+02 -1.84567148e+02 -1.79763680e+02 -1.72024877e+02
 -1.61462479e+02 -1.60392530e+02 -1.59610388e+02 -1.56892497e+02
 -1.56690710e+02 -1.54126830e+02 -1.50110533e+02 -1.49333064e+02
 -1.44533808e+02 -1.44105590e+02 -1.41012729e+02 -1.36394744e+02
 -1.36249901e+02 -1.31698175e+02 -1.29831521e+02 -1.29587816e+02
 -1.29078727e+02 -1.29071077e+02 -1.28810282e+02 -1.26720839e+02
 -1.26675663e+02 -1.26015076e+02 -1.24791739e+02 -1.22153220e+02
 -1.14492967e+02 -1.12895331e+02 -1.12574673e+02 -1.12002377e+02
 -1.10516185e+02 -1.10394897e+02 -1.09810901e+02 -1.09290940e+02
 -1.09000289e+02 -1.08756732e+02 -1.08459758e+02 -1.08156827e+02
 -1.06325016e+02 -1.04952188e+02 -1.04376385e+02 -1.04082473e+02
 -1.03270724e+02 -1.03034780e+02 -1.01788939e+02 -1.00626724e+02
 -9.93079384e+01 -9.82430953e+01 -9.75922404e+01 -9.69369722e+01
 -9.66151117e+01 -9.42904165e+01 -9.42143274e+01 -9.39116749e+01
 -9.38497135e+01 -9.37223299e+01 -9.31393582e+01 -9.23679858e+01
 -8.91046975e+01 -8.79041121e+01 -8.73076297e+01 -8.69565508e+01
 -8.60814293e+01 -8.56528919e+01 -8.52954333e+01 -8.51947671e+01
 -8.51522733e+01 -8.49991965e+01 -8.48768991e+01 -8.32947405e+01
 -8.28197895e+01 -8.26763624e+01 -8.25486820e+01 -8.24995557e+01
 -8.15039902e+01 -8.04563906e+01 -8.01953050e+01 -8.01671058e+01
 -7.99036760e+01 -7.92717361e+01 -7.92696977e+01 -7.86226825e+01
 -7.81079480e+01 -7.73078246e+01 -7.72932816e+01 -7.71273488e+01
 -7.66155611e+01 -7.64630234e+01 -7.59188145e+01 -7.38744569e+01
 -7.32504471e+01 -7.31040190e+01 -7.30870760e+01 -7.19993983e+01
 -7.03775657e+01 -7.01868696e+01 -6.91071721e+01 -6.84572675e+01
 -6.82348917e+01 -6.78410655e+01 -6.73828071e+01 -6.56927252e+01
 -6.54681151e+01 -6.41823353e+01 -6.34099146e+01 -6.32284667e+01
 -6.16229119e+01 -6.13310733e+01 -6.11947751e+01 -6.08893846e+01
 -6.06613722e+01 -6.00533033e+01 -5.97428740e+01 -5.96789324e+01
 -5.95646968e+01 -5.94937606e+01 -5.92670890e+01 -5.87023048e+01
 -5.87002142e+01 -5.81837915e+01 -5.80072614e+01 -5.63710853e+01
 -5.63134086e+01 -5.59145048e+01 -5.56095711e+01 -5.43244013e+01
 -5.38923554e+01 -5.35343571e+01 -5.33223015e+01 -5.32930671e+01
 -5.20320219e+01 -4.87687115e+01 -4.87449080e+01 -4.73209297e+01
 -4.42348165e+01 -4.41535396e+01 -4.33808937e+01 -4.32102257e+01
 -4.19808710e+01 -4.11573211e+01 -4.02983962e+01 -3.92977585e+01
 -3.76910430e+01 -3.71166564e+01 -3.68954521e+01 -3.60836294e+01
 -3.51457285e+01 -3.43903194e+01 -3.27360994e+01 -3.21637513e+01
 -3.08767304e+01 -3.03923769e+01 -3.00549334e+01 -2.69214892e+01
 -2.65781736e+01 -2.61892431e+01 -2.52110003e+01 -2.42094246e+01
 -2.40186709e+01 -2.34105292e+01 -2.32859985e+01 -2.27812936e+01
 -2.27259541e+01 -2.18964820e+01 -1.93705173e+01 -1.79372277e+01
 -1.75102017e+01 -1.68441326e+01 -1.58811428e+01 -1.51577086e+01
 -1.32860875e+01 -1.11671517e+01 -1.10026071e+01 -9.57975571e+00
 -8.99483774e+00 -7.43485049e+00 -6.54991997e+00 -6.33018949e+00
 -5.65528936e+00 -4.03226309e+00 -3.76983251e+00 -2.62467353e+00
 -1.89580632e+00  8.37306245e-02  1.88336337e+00  1.95130509e+00
  2.06757239e+00  2.44986350e+00  2.82333720e+00  3.24518158e+00
  3.41681858e+00  4.33648635e+00  4.45142428e+00  4.51224760e+00
  9.95238118e+00  1.10201451e+01  1.28665786e+01  1.29714874e+01
  1.30824157e+01  1.35991331e+01  1.58459073e+01  1.92372514e+01
  2.03150717e+01  2.18395333e+01  2.19881242e+01  2.22071175e+01
  2.24464725e+01  2.43734540e+01  2.65632939e+01  2.68371047e+01
  2.79327696e+01  2.99614289e+01  3.08087071e+01  3.17619258e+01
  3.27789273e+01  3.28416472e+01  3.40865969e+01  3.42500191e+01
  3.43686399e+01  3.45401081e+01  3.54378449e+01  3.66699936e+01
  3.85892744e+01  3.92793569e+01  4.03882111e+01  4.07523095e+01
  4.29311024e+01  4.31669843e+01  4.44130163e+01  4.55238513e+01
  4.59297113e+01  5.08875967e+01  5.36861330e+01  5.41092849e+01
  5.51347390e+01  5.68482604e+01  6.00882243e+01  8.36807129e+01
  8.71017367e+01  8.91837476e+01  9.26210279e+01  9.69823757e+01
  9.92340234e+01  9.98594136e+01  9.99097003e+01  9.99106092e+01
  1.01554085e+02  1.05849931e+02  1.07680804e+02  1.07949210e+02
  1.08725150e+02  1.09668060e+02  1.10494197e+02  1.10760795e+02
  1.16053271e+02  1.16291458e+02  1.17682751e+02  1.20192926e+02
  1.20317099e+02  1.21362944e+02  1.23466742e+02  1.23751118e+02
  1.24730650e+02  1.25267832e+02  1.28172794e+02  1.29066327e+02
  1.32633242e+02  1.33660944e+02  1.33807958e+02  1.35237387e+02
  1.38625800e+02  1.39160438e+02  1.39181873e+02  1.40018716e+02
  1.40434657e+02  1.40883673e+02  1.41762771e+02  1.42359227e+02
  1.42805535e+02  1.43271041e+02  1.46345459e+02  1.47084064e+02
  1.48077563e+02  1.48370003e+02  1.51510111e+02  1.55376317e+02
  1.56584144e+02  1.59678797e+02  1.62253475e+02  1.64700310e+02
  1.65791665e+02  1.73946639e+02  2.09226064e+02  2.12974636e+02
  2.20231471e+02  2.21617533e+02  2.21754411e+02  2.29106711e+02
  2.30342775e+02  2.32918138e+02  2.34320877e+02  2.36063187e+02
  2.36981370e+02  2.37820201e+02  2.38363681e+02  2.38793862e+02
  2.39831240e+02  2.40202654e+02  2.41206601e+02  2.44107223e+02
  2.44360127e+02  2.44787774e+02  2.45130705e+02  2.45143111e+02
  2.45302486e+02  2.45409371e+02  2.45524570e+02  2.45758793e+02
  2.46055618e+02  2.46443651e+02  2.46457069e+02  2.46582273e+02
  2.47116610e+02  2.47119146e+02  2.48248632e+02  2.48522583e+02
  2.48868021e+02  2.48894511e+02  2.49922999e+02  2.50280341e+02
  2.50360947e+02  2.50667900e+02  2.51331648e+02  2.54744324e+02
  2.56162735e+02  2.56185461e+02  2.56276249e+02  2.57225817e+02
  2.57560493e+02  2.57902408e+02  2.57904280e+02  2.58069393e+02
  2.58848137e+02  2.58910101e+02  2.59202923e+02  2.60247726e+02
  2.60535357e+02  2.61212177e+02  2.61274076e+02  2.61450010e+02
  2.61810674e+02  2.62100542e+02  2.62926235e+02  2.62935174e+02
  2.63041138e+02  2.63045233e+02  2.63245142e+02  2.63332726e+02
  2.64251640e+02  2.64876300e+02  2.67379590e+02  2.67565648e+02
  2.67565844e+02  2.67764688e+02  2.68127579e+02  2.70476276e+02
  2.70796421e+02  2.70942244e+02  2.71844593e+02  2.72676170e+02
  2.73312286e+02  2.74276745e+02  2.75376573e+02  2.76006697e+02
  2.76016250e+02  2.77220423e+02  2.77338302e+02  2.78959823e+02
  2.79703803e+02  2.83669133e+02  2.85268187e+02  2.88361437e+02
  2.89634715e+02  2.91262559e+02  2.91530071e+02  2.93038456e+02
  2.94814905e+02  2.95553480e+02  2.99903898e+02  3.00714605e+02
  3.01722647e+02  3.04395724e+02  3.10778723e+02  3.11978625e+02]
sorted_val_rewards: [-517.5295579  -362.27512886 -313.46616917 -297.21308551 -273.99437991
 -258.37649525 -237.0323333  -225.47888891 -188.30297553 -153.76944773
 -148.24097448 -125.24510822 -113.87315285 -100.74068178  -97.21570471
  -96.21449639  -86.09552113  -82.98886107  -82.23366584  -70.29375144
  -57.74163232  -54.69992314  -49.16457336  -43.8346642   -31.43424849
  -30.58342234  -25.02106109  -22.3283679     8.01184608   25.76843344
   42.92267207   59.25052049  134.85897422  143.79308198  163.26151097
  164.48437385  233.96700816  242.5338829   259.10446425  262.41000933
  269.32496631  269.81111825  273.20812084  282.12896871  291.17844959
  299.02751037  299.16767939  302.44685198]
maximum traj length 1000
maximum traj length 1000
num train_obs 780
num train_labels 780
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=9, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 9600
Number of trainable paramters: 9600
device: cuda:0
end of epoch 0: val_loss 3.510615137010073, val_acc 0.9184397163120568
trigger times: 0
saving model weights...
end of epoch 1: val_loss 1.836771023093181, val_acc 0.9264184397163121
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.6222164414808073, val_acc 0.9317375886524822
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.6816142266585817, val_acc 0.9219858156028369
trigger times: 1
end of epoch 4: val_loss 0.7435055718499588, val_acc 0.9113475177304965
trigger times: 2
end of epoch 5: val_loss 1.0246240857229516, val_acc 0.9024822695035462
trigger times: 3
end of epoch 6: val_loss 1.6672204750981752, val_acc 0.9042553191489362
trigger times: 4
end of epoch 7: val_loss 0.5273734008751637, val_acc 0.9193262411347518
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.8316602074532983, val_acc 0.9166666666666666
trigger times: 1
end of epoch 9: val_loss 0.6226570644432674, val_acc 0.9104609929078015
trigger times: 2
end of epoch 10: val_loss 0.7527010008285506, val_acc 0.9033687943262412
trigger times: 3
end of epoch 11: val_loss 1.3466829537227012, val_acc 0.8820921985815603
trigger times: 4
end of epoch 12: val_loss 0.5775929706359872, val_acc 0.9148936170212766
trigger times: 5
end of epoch 13: val_loss 0.8430990506055601, val_acc 0.9131205673758865
trigger times: 6
end of epoch 14: val_loss 0.696517564832756, val_acc 0.9033687943262412
trigger times: 7
end of epoch 15: val_loss 0.6061058908163648, val_acc 0.9148936170212766
trigger times: 8
end of epoch 16: val_loss 0.7732779466962091, val_acc 0.9166666666666666
trigger times: 9
end of epoch 17: val_loss 0.5191178208541408, val_acc 0.9210992907801419
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.568355168056792, val_acc 0.9060283687943262
trigger times: 1
end of epoch 19: val_loss 0.7583097104247323, val_acc 0.9042553191489362
trigger times: 2
end of epoch 20: val_loss 2.129244560543494, val_acc 0.8962765957446809
trigger times: 3
end of epoch 21: val_loss 0.6335621645425077, val_acc 0.9157801418439716
trigger times: 4
end of epoch 22: val_loss 0.5124527340196566, val_acc 0.9202127659574468
trigger times: 0
saving model weights...
end of epoch 23: val_loss 1.8001664023373298, val_acc 0.899822695035461
trigger times: 1
end of epoch 24: val_loss 0.9313176859660401, val_acc 0.9157801418439716
trigger times: 2
end of epoch 25: val_loss 0.9781037839681915, val_acc 0.9202127659574468
trigger times: 3
end of epoch 26: val_loss 0.8623358121322435, val_acc 0.9193262411347518
trigger times: 4
end of epoch 27: val_loss 0.5709908411447387, val_acc 0.9237588652482269
trigger times: 5
end of epoch 28: val_loss 1.2493707968728083, val_acc 0.9042553191489362
trigger times: 6
end of epoch 29: val_loss 0.5414023145686382, val_acc 0.9237588652482269
trigger times: 7
end of epoch 30: val_loss 0.5825542112117861, val_acc 0.9228723404255319
trigger times: 8
end of epoch 31: val_loss 0.27961010090786875, val_acc 0.925531914893617
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.43488197451654664, val_acc 0.9184397163120568
trigger times: 1
end of epoch 33: val_loss 0.596209947218253, val_acc 0.9140070921985816
trigger times: 2
end of epoch 34: val_loss 0.5714026580642877, val_acc 0.9140070921985816
trigger times: 3
end of epoch 35: val_loss 0.6305010636988774, val_acc 0.9228723404255319
trigger times: 4
end of epoch 36: val_loss 0.5554836585991358, val_acc 0.9290780141843972
trigger times: 5
end of epoch 37: val_loss 0.6929221018513947, val_acc 0.9095744680851063
trigger times: 6
end of epoch 38: val_loss 0.5839616218252661, val_acc 0.9228723404255319
trigger times: 7
end of epoch 39: val_loss 0.7129719025542344, val_acc 0.9166666666666666
trigger times: 8
end of epoch 40: val_loss 1.0228458888421028, val_acc 0.9148936170212766
trigger times: 9
end of epoch 41: val_loss 1.1906141849674252, val_acc 0.8971631205673759
trigger times: 10
Early stopping.
0 -92.01766436937032 -517.529557895617
1 -27.779156599601265 -362.2751288556848
2 -17.972394804033684 -313.4661691736576
3 -59.479103455960285 -297.2130855123045
4 -36.2410717185121 -273.99437990512837
5 -41.93921004474396 -258.3764952457298
6 -26.758532284438843 -237.03233330417288
7 -47.538607877912 -225.47888891205258
8 9.492784330417635 -188.30297553101315
9 -7.2036115897935815 -153.76944773192582
10 4.479447682213504 -148.2409744805755
11 -6.289161163207609 -125.24510821501842
12 3.746922771446407 -113.87315284733153
13 3.5131452775676735 -100.74068178293723
14 -1.7277791858068667 -97.21570471018686
15 3.0080961520725396 -96.21449638516864
16 -3.7490700851776637 -86.09552113455516
17 46.335903512343066 -82.98886107059623
18 -2.1552265324862674 -82.23366584233335
19 7.263212718244176 -70.29375143551131
20 32.94538765546167 -57.74163231940187
21 21.77914310269989 -54.699923138060974
22 -5.659130984859075 -49.16457335620919
23 12.718996566109126 -43.83466419540005
24 1.856346900720382 -31.434248493772998
25 59.44277533242712 -30.58342234159491
26 19.250400923774578 -25.02106109394221
27 23.529659906140296 -22.328367901800874
28 35.175320842070505 8.011846077196338
29 21.61135034216568 25.768433443913807
30 36.77735354943434 42.922672066570016
31 46.283932375896256 59.250520487354464
32 65.03845801742864 134.85897422210454
33 59.94631348634721 143.79308198385849
34 71.38961959106382 163.26151097291478
35 64.91736522258725 164.4843738486571
36 77.65708209737204 233.9670081640142
37 77.40461093903286 242.5338829010272
38 74.22632432426326 259.10446424765166
39 78.04645818611607 262.4100093344912
40 73.98905542612192 269.3249663133127
41 77.8303002412431 269.8111182543307
42 73.62356791639468 273.2081208413378
43 79.13724569024635 282.12896870686944
44 78.23933681688504 291.1784495930924
45 82.22393299874966 299.02751037036626
46 81.15327493223595 299.1676793912864
47 77.8927745904657 302.4468519773699
train accuracy: 0.9564102564102565
train loss: 0.14051099160535338
validation accuracy: 0.8971631205673759
validation loss: 1.1906141849674252

demos: (480, 1000, 9)
demo_rewards: (480,)
sorted_train_rewards: [-4.83550523e+02 -4.09596136e+02 -3.81546354e+02 -3.77409101e+02
 -3.69432856e+02 -3.63386626e+02 -3.50478715e+02 -3.45382744e+02
 -3.44868534e+02 -3.31447324e+02 -3.11344158e+02 -3.10797466e+02
 -3.09110683e+02 -3.07595090e+02 -3.00601780e+02 -2.95457016e+02
 -2.92971968e+02 -2.87555036e+02 -2.79620500e+02 -2.78593743e+02
 -2.60526163e+02 -2.52390992e+02 -2.51617666e+02 -2.49972622e+02
 -2.48418949e+02 -2.34077317e+02 -2.31837606e+02 -2.21593472e+02
 -2.14716696e+02 -2.13290418e+02 -2.08849904e+02 -1.97273488e+02
 -1.96088322e+02 -1.90559675e+02 -1.90541660e+02 -1.87941759e+02
 -1.87145800e+02 -1.84567148e+02 -1.79763680e+02 -1.72024877e+02
 -1.61462479e+02 -1.60392530e+02 -1.59610388e+02 -1.56892497e+02
 -1.56690710e+02 -1.54126830e+02 -1.50110533e+02 -1.49333064e+02
 -1.44533808e+02 -1.44105590e+02 -1.41012729e+02 -1.36394744e+02
 -1.36249901e+02 -1.31698175e+02 -1.29831521e+02 -1.29587816e+02
 -1.29078727e+02 -1.29071077e+02 -1.28810282e+02 -1.26720839e+02
 -1.26675663e+02 -1.26015076e+02 -1.24791739e+02 -1.22153220e+02
 -1.14492967e+02 -1.12895331e+02 -1.12574673e+02 -1.12002377e+02
 -1.10516185e+02 -1.10394897e+02 -1.09810901e+02 -1.09290940e+02
 -1.09000289e+02 -1.08756732e+02 -1.08459758e+02 -1.08156827e+02
 -1.06325016e+02 -1.04952188e+02 -1.04376385e+02 -1.04082473e+02
 -1.03270724e+02 -1.03034780e+02 -1.01788939e+02 -1.00626724e+02
 -9.93079384e+01 -9.82430953e+01 -9.75922404e+01 -9.69369722e+01
 -9.66151117e+01 -9.42904165e+01 -9.42143274e+01 -9.39116749e+01
 -9.38497135e+01 -9.37223299e+01 -9.31393582e+01 -9.23679858e+01
 -8.91046975e+01 -8.79041121e+01 -8.73076297e+01 -8.69565508e+01
 -8.60814293e+01 -8.56528919e+01 -8.52954333e+01 -8.51947671e+01
 -8.51522733e+01 -8.49991965e+01 -8.48768991e+01 -8.32947405e+01
 -8.28197895e+01 -8.26763624e+01 -8.25486820e+01 -8.24995557e+01
 -8.15039902e+01 -8.04563906e+01 -8.01953050e+01 -8.01671058e+01
 -7.99036760e+01 -7.92717361e+01 -7.92696977e+01 -7.86226825e+01
 -7.81079480e+01 -7.73078246e+01 -7.72932816e+01 -7.71273488e+01
 -7.66155611e+01 -7.64630234e+01 -7.59188145e+01 -7.38744569e+01
 -7.32504471e+01 -7.31040190e+01 -7.30870760e+01 -7.19993983e+01
 -7.03775657e+01 -7.01868696e+01 -6.91071721e+01 -6.84572675e+01
 -6.82348917e+01 -6.78410655e+01 -6.73828071e+01 -6.56927252e+01
 -6.54681151e+01 -6.41823353e+01 -6.34099146e+01 -6.32284667e+01
 -6.16229119e+01 -6.13310733e+01 -6.11947751e+01 -6.08893846e+01
 -6.06613722e+01 -6.00533033e+01 -5.97428740e+01 -5.96789324e+01
 -5.95646968e+01 -5.94937606e+01 -5.92670890e+01 -5.87023048e+01
 -5.87002142e+01 -5.81837915e+01 -5.80072614e+01 -5.63710853e+01
 -5.63134086e+01 -5.59145048e+01 -5.56095711e+01 -5.43244013e+01
 -5.38923554e+01 -5.35343571e+01 -5.33223015e+01 -5.32930671e+01
 -5.20320219e+01 -4.87687115e+01 -4.87449080e+01 -4.73209297e+01
 -4.42348165e+01 -4.41535396e+01 -4.33808937e+01 -4.32102257e+01
 -4.19808710e+01 -4.11573211e+01 -4.02983962e+01 -3.92977585e+01
 -3.76910430e+01 -3.71166564e+01 -3.68954521e+01 -3.60836294e+01
 -3.51457285e+01 -3.43903194e+01 -3.27360994e+01 -3.21637513e+01
 -3.08767304e+01 -3.03923769e+01 -3.00549334e+01 -2.69214892e+01
 -2.65781736e+01 -2.61892431e+01 -2.52110003e+01 -2.42094246e+01
 -2.40186709e+01 -2.34105292e+01 -2.32859985e+01 -2.27812936e+01
 -2.27259541e+01 -2.18964820e+01 -1.93705173e+01 -1.79372277e+01
 -1.75102017e+01 -1.68441326e+01 -1.58811428e+01 -1.51577086e+01
 -1.32860875e+01 -1.11671517e+01 -1.10026071e+01 -9.57975571e+00
 -8.99483774e+00 -7.43485049e+00 -6.54991997e+00 -6.33018949e+00
 -5.65528936e+00 -4.03226309e+00 -3.76983251e+00 -2.62467353e+00
 -1.89580632e+00  8.37306245e-02  1.88336337e+00  1.95130509e+00
  2.06757239e+00  2.44986350e+00  2.82333720e+00  3.24518158e+00
  3.41681858e+00  4.33648635e+00  4.45142428e+00  4.51224760e+00
  9.95238118e+00  1.10201451e+01  1.28665786e+01  1.29714874e+01
  1.30824157e+01  1.35991331e+01  1.58459073e+01  1.92372514e+01
  2.03150717e+01  2.18395333e+01  2.19881242e+01  2.22071175e+01
  2.24464725e+01  2.43734540e+01  2.65632939e+01  2.68371047e+01
  2.79327696e+01  2.99614289e+01  3.08087071e+01  3.17619258e+01
  3.27789273e+01  3.28416472e+01  3.40865969e+01  3.42500191e+01
  3.43686399e+01  3.45401081e+01  3.54378449e+01  3.66699936e+01
  3.85892744e+01  3.92793569e+01  4.03882111e+01  4.07523095e+01
  4.29311024e+01  4.31669843e+01  4.44130163e+01  4.55238513e+01
  4.59297113e+01  5.08875967e+01  5.36861330e+01  5.41092849e+01
  5.51347390e+01  5.68482604e+01  6.00882243e+01  8.36807129e+01
  8.71017367e+01  8.91837476e+01  9.26210279e+01  9.69823757e+01
  9.92340234e+01  9.98594136e+01  9.99097003e+01  9.99106092e+01
  1.01554085e+02  1.05849931e+02  1.07680804e+02  1.07949210e+02
  1.08725150e+02  1.09668060e+02  1.10494197e+02  1.10760795e+02
  1.16053271e+02  1.16291458e+02  1.17682751e+02  1.20192926e+02
  1.20317099e+02  1.21362944e+02  1.23466742e+02  1.23751118e+02
  1.24730650e+02  1.25267832e+02  1.28172794e+02  1.29066327e+02
  1.32633242e+02  1.33660944e+02  1.33807958e+02  1.35237387e+02
  1.38625800e+02  1.39160438e+02  1.39181873e+02  1.40018716e+02
  1.40434657e+02  1.40883673e+02  1.41762771e+02  1.42359227e+02
  1.42805535e+02  1.43271041e+02  1.46345459e+02  1.47084064e+02
  1.48077563e+02  1.48370003e+02  1.51510111e+02  1.55376317e+02
  1.56584144e+02  1.59678797e+02  1.62253475e+02  1.64700310e+02
  1.65791665e+02  1.73946639e+02  2.09226064e+02  2.12974636e+02
  2.20231471e+02  2.21617533e+02  2.21754411e+02  2.29106711e+02
  2.30342775e+02  2.32918138e+02  2.34320877e+02  2.36063187e+02
  2.36981370e+02  2.37820201e+02  2.38363681e+02  2.38793862e+02
  2.39831240e+02  2.40202654e+02  2.41206601e+02  2.44107223e+02
  2.44360127e+02  2.44787774e+02  2.45130705e+02  2.45143111e+02
  2.45302486e+02  2.45409371e+02  2.45524570e+02  2.45758793e+02
  2.46055618e+02  2.46443651e+02  2.46457069e+02  2.46582273e+02
  2.47116610e+02  2.47119146e+02  2.48248632e+02  2.48522583e+02
  2.48868021e+02  2.48894511e+02  2.49922999e+02  2.50280341e+02
  2.50360947e+02  2.50667900e+02  2.51331648e+02  2.54744324e+02
  2.56162735e+02  2.56185461e+02  2.56276249e+02  2.57225817e+02
  2.57560493e+02  2.57902408e+02  2.57904280e+02  2.58069393e+02
  2.58848137e+02  2.58910101e+02  2.59202923e+02  2.60247726e+02
  2.60535357e+02  2.61212177e+02  2.61274076e+02  2.61450010e+02
  2.61810674e+02  2.62100542e+02  2.62926235e+02  2.62935174e+02
  2.63041138e+02  2.63045233e+02  2.63245142e+02  2.63332726e+02
  2.64251640e+02  2.64876300e+02  2.67379590e+02  2.67565648e+02
  2.67565844e+02  2.67764688e+02  2.68127579e+02  2.70476276e+02
  2.70796421e+02  2.70942244e+02  2.71844593e+02  2.72676170e+02
  2.73312286e+02  2.74276745e+02  2.75376573e+02  2.76006697e+02
  2.76016250e+02  2.77220423e+02  2.77338302e+02  2.78959823e+02
  2.79703803e+02  2.83669133e+02  2.85268187e+02  2.88361437e+02
  2.89634715e+02  2.91262559e+02  2.91530071e+02  2.93038456e+02
  2.94814905e+02  2.95553480e+02  2.99903898e+02  3.00714605e+02
  3.01722647e+02  3.04395724e+02  3.10778723e+02  3.11978625e+02]
sorted_val_rewards: [-517.5295579  -362.27512886 -313.46616917 -297.21308551 -273.99437991
 -258.37649525 -237.0323333  -225.47888891 -188.30297553 -153.76944773
 -148.24097448 -125.24510822 -113.87315285 -100.74068178  -97.21570471
  -96.21449639  -86.09552113  -82.98886107  -82.23366584  -70.29375144
  -57.74163232  -54.69992314  -49.16457336  -43.8346642   -31.43424849
  -30.58342234  -25.02106109  -22.3283679     8.01184608   25.76843344
   42.92267207   59.25052049  134.85897422  143.79308198  163.26151097
  164.48437385  233.96700816  242.5338829   259.10446425  262.41000933
  269.32496631  269.81111825  273.20812084  282.12896871  291.17844959
  299.02751037  299.16767939  302.44685198]
maximum traj length 1000
maximum traj length 1000
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=9, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 9600
Number of trainable paramters: 9600
device: cuda:1
end of epoch 0: val_loss 0.3155374538661097, val_acc 0.9361702127659575
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.10708450707182603, val_acc 0.949468085106383
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.09168656575472127, val_acc 0.9592198581560284
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.12227484849902602, val_acc 0.9459219858156028
trigger times: 1
end of epoch 4: val_loss 0.11709772557102532, val_acc 0.9521276595744681
trigger times: 2
end of epoch 5: val_loss 0.10263763350886655, val_acc 0.9574468085106383
trigger times: 3
end of epoch 6: val_loss 0.11617251705003889, val_acc 0.9530141843971631
trigger times: 4
end of epoch 7: val_loss 0.1510747553079703, val_acc 0.9459219858156028
trigger times: 5
end of epoch 8: val_loss 0.14533310679485154, val_acc 0.9556737588652482
trigger times: 6
end of epoch 9: val_loss 0.13864392308805026, val_acc 0.9379432624113475
trigger times: 7
end of epoch 10: val_loss 0.26161376824506777, val_acc 0.901595744680851
trigger times: 8
end of epoch 11: val_loss 0.1592073216475535, val_acc 0.9441489361702128
trigger times: 9
end of epoch 12: val_loss 0.11670756557451799, val_acc 0.9583333333333334
trigger times: 10
Early stopping.
0 -98.99719306017505 -517.529557895617
1 -87.80950441543246 -362.2751288556848
2 -75.46102735958993 -313.4661691736576
3 -58.78711183433188 -297.2130855123045
4 -52.01767215831205 -273.99437990512837
5 -57.17216549254954 -258.3764952457298
6 -50.472067969851196 -237.03233330417288
7 -50.23008249234408 -225.47888891205258
8 -50.86311735585332 -188.30297553101315
9 -35.74540703371167 -153.76944773192582
10 -44.655959085561335 -148.2409744805755
11 -31.443822112865746 -125.24510821501842
12 -29.650874777740682 -113.87315284733153
13 -30.907535347156227 -100.74068178293723
14 -31.77060120832175 -97.21570471018686
15 -28.21182407066226 -96.21449638516864
16 -27.946318788453937 -86.09552113455516
17 -34.72864059731364 -82.98886107059623
18 -27.91357376612723 -82.23366584233335
19 -28.883978886529803 -70.29375143551131
20 -25.145236677024513 -57.74163231940187
21 -25.340419908985496 -54.699923138060974
22 -26.707641635090113 -49.16457335620919
23 -23.773072032257915 -43.83466419540005
24 -24.545560608734377 -31.434248493772998
25 -23.17955345660448 -30.58342234159491
26 -26.935106337070465 -25.02106109394221
27 -21.435350797139108 -22.328367901800874
28 -19.565758630633354 8.011846077196338
29 -18.02730112010613 25.768433443913807
30 -16.554165152134374 42.922672066570016
31 -12.330638487823308 59.250520487354464
32 -10.201139716315083 134.85897422210454
33 -9.126302191763898 143.79308198385849
34 -7.853323744609952 163.26151097291478
35 -8.850211152537668 164.4843738486571
36 -6.68568265682552 233.9670081640142
37 -6.150054799101781 242.5338829010272
38 -7.381593279205845 259.10446424765166
39 -7.07947215414606 262.4100093344912
40 -6.820259084692225 269.3249663133127
41 -6.558561887941323 269.8111182543307
42 -7.061719553777948 273.2081208413378
43 -5.802005831967108 282.12896870686944
44 -6.296457501171972 291.1784495930924
45 -5.583409454586217 299.02751037036626
46 -5.680324601649772 299.1676793912864
47 -6.463244645652594 302.4468519773699
train accuracy: 0.9523809523809523
train loss: 0.1125882242014336
validation accuracy: 0.9583333333333334
validation loss: 0.11670756557451799

demos: (360, 50, 13)
demo_rewards: (360,)
sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.58850771 -48.19509289
 -47.96991522 -47.53054283 -47.33487662 -47.14538324 -47.10889581
 -46.92993068 -46.92815321 -46.7141986  -46.51840523 -46.50762306
 -46.46343149 -46.08033935 -45.96910073 -45.94004954 -45.47189047
 -45.34946654 -45.23627882 -45.23541431 -44.99494244 -44.99184926
 -44.7644634  -44.62310756 -44.53780078 -44.32699505 -43.84464734
 -43.7920154  -43.22211069 -43.2041764  -43.14444939 -42.63170505
 -42.49903983 -42.18990615 -42.17868583 -42.08941791 -41.96459824
 -41.91819405 -41.89252759 -41.86250044 -41.76630373 -41.75643755
 -41.50687738 -41.35020296 -41.32515547 -41.22179983 -41.18200046
 -41.03992135 -41.03294083 -40.99991982 -40.98448758 -40.86627436
 -40.84937019 -40.63970267 -40.33775477 -40.11576908 -39.92712396
 -39.89750906 -39.72931271 -39.35357961 -39.17708227 -39.14272956
 -39.07321282 -38.91826709 -38.77785603 -38.52213956 -38.29413029
 -38.28199013 -38.28138827 -38.04661848 -37.88709355 -37.79694607
 -37.70658771 -37.51412288 -37.37133772 -37.3251261  -37.15321081
 -37.11841822 -37.10344577 -36.99477768 -36.69656462 -36.65639269
 -36.58958472 -36.38667894 -36.33815177 -36.03378032 -36.02351695
 -35.97994531 -35.89835563 -35.77878287 -35.74239985 -35.67327882
 -35.43726783 -35.41556943 -35.28979253 -35.12234482 -35.07963015
 -35.06137062 -35.02402835 -35.01091702 -34.95588623 -34.91427059
 -34.88495647 -34.86472602 -34.34266183 -33.86129846 -33.77802335
 -33.680383   -33.4779641  -33.26579709 -33.17157461 -33.04852912
 -32.41611773 -32.41053817 -32.02508724 -31.71028623 -31.64335581
 -31.48876757 -31.44406811 -31.37649579 -31.35230126 -31.24419956
 -31.21642916 -31.15850305 -31.06744474 -31.00231541 -30.95117262
 -30.76937686 -30.64081715 -30.63466958 -30.59673213 -30.52049104
 -30.42034344 -30.36982897 -30.36743577 -30.27374553 -30.10812976
 -30.06299271 -29.98083048 -29.97767593 -29.94526105 -29.66568464
 -29.22737048 -29.20595883 -28.99559546 -28.95484843 -28.59191373
 -28.5494411  -28.31342715 -28.1712167  -28.13409378 -28.11939797
 -27.79654204 -27.77638947 -27.70726756 -27.60103341 -27.50815322
 -27.27876998 -26.73963502 -26.7058308  -26.50715502 -26.36197542
 -26.19987885 -25.99907479 -25.47507675 -25.29134047 -25.11681575
 -24.96039421 -24.91350015 -24.71538539 -24.58444108 -24.25434383
 -24.12244562 -23.96980649 -23.76917367 -23.71252839 -23.48246476
 -23.05835519 -22.79553869 -22.61211306 -22.55283027 -22.53572133
 -21.86139195 -21.84262733 -21.7167854  -21.64185366 -21.62657756
 -21.62597106 -21.44290047 -20.68104247 -20.66287704 -20.66020817
 -20.5100558  -20.35567458 -20.10309328 -19.94369578 -19.67432429
 -19.50391949 -19.30989567 -19.03153444 -18.9341265  -18.87189699
 -18.77655165 -18.71737606 -18.65414818 -18.4634721  -18.36358054
 -18.2465562  -18.08445937 -17.85124778 -17.82515236 -17.77364067
 -17.67989719 -17.64064603 -17.48133145 -17.27921081 -17.10217865
 -17.06769934 -16.68041488 -16.44066556 -16.35270212 -16.325741
 -16.24688067 -16.20186147 -16.01542464 -15.4984665  -15.44345121
 -15.29527985 -15.26574292 -15.017961   -14.79738881 -14.47834852
 -14.39794675 -14.24892226 -14.15625316 -13.88057881 -13.73405689
 -13.69675016 -13.43873671 -13.16397094 -12.97987294 -12.6606731
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.9523875  -11.75626016 -11.47155848
 -11.04421156 -10.859881   -10.76409188 -10.64954177 -10.49423816
  -9.92399886  -9.67992171  -9.56750101  -9.4003034   -8.4761537
  -8.40276084  -8.32638751  -8.0269637   -7.68743448  -7.57539849
  -7.5455064   -7.54460172  -7.52029309  -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.10832736  -7.08431127  -6.95906356
  -6.92008425  -6.72206384  -6.71997062  -6.64795748  -6.51820418
  -6.47884361  -6.05448903  -5.85405865  -5.64485143  -5.61579673
  -5.39544196  -5.38326081  -5.3472021   -5.25793019  -5.24856721
  -5.07848501  -5.06486011  -4.90228293  -4.82757292  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.3322555
  -3.329867    -3.29356852  -3.07904644  -2.88591659  -2.83192847
  -2.67390706  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-44.29565561 -41.97868948 -39.97085477 -37.94300503 -37.47672391
 -36.28177521 -35.19353216 -34.63851695 -33.7629591  -31.90686017
 -30.772676   -28.42026397 -26.39763335 -25.31217996 -22.14406511
 -21.72370465 -20.76562386 -19.02121155 -17.94319772 -16.25152473
 -16.23288157 -13.8953595  -12.5410027  -11.97407887 -11.96626278
 -11.4307611  -10.78272714 -10.01645863  -7.70107293  -7.67259169
  -7.3740849   -6.77694649  -6.29894775  -5.89467275  -5.02795798
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 780
num train_labels 780
num val_obs 630
num val_labels 630
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.28197990965415975, val_acc 0.8968253968253969
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1764048737615632, val_acc 0.926984126984127
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.20314584178154688, val_acc 0.9015873015873016
trigger times: 1
end of epoch 3: val_loss 0.1388497238218402, val_acc 0.946031746031746
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.14278369203920477, val_acc 0.9317460317460318
trigger times: 1
end of epoch 5: val_loss 0.1004263845273816, val_acc 0.9523809523809523
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.18378387692620327, val_acc 0.9349206349206349
trigger times: 1
end of epoch 7: val_loss 0.10297515694189341, val_acc 0.9492063492063492
trigger times: 2
end of epoch 8: val_loss 0.6370650186729476, val_acc 0.8587301587301587
trigger times: 3
end of epoch 9: val_loss 0.18806856634622676, val_acc 0.9428571428571428
trigger times: 4
end of epoch 10: val_loss 0.11915608955440507, val_acc 0.9634920634920635
trigger times: 5
end of epoch 11: val_loss 6.704723860608601, val_acc 0.6619047619047619
trigger times: 6
end of epoch 12: val_loss 0.28297269707331135, val_acc 0.9492063492063492
trigger times: 7
end of epoch 13: val_loss 0.10752095164262183, val_acc 0.9619047619047619
trigger times: 8
end of epoch 14: val_loss 0.08675501502737355, val_acc 0.9555555555555556
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.08862268038590228, val_acc 0.9650793650793651
trigger times: 1
end of epoch 16: val_loss 0.18400386388537637, val_acc 0.9428571428571428
trigger times: 2
end of epoch 17: val_loss 0.10744040094365528, val_acc 0.9555555555555556
trigger times: 3
end of epoch 18: val_loss 0.6521612000059361, val_acc 0.9349206349206349
trigger times: 4
end of epoch 19: val_loss 0.07413516790421817, val_acc 0.9761904761904762
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.11639068384152607, val_acc 0.9603174603174603
trigger times: 1
end of epoch 21: val_loss 0.055786136505652256, val_acc 0.9793650793650793
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.11757201042815657, val_acc 0.9571428571428572
trigger times: 1
end of epoch 23: val_loss 0.09699357083767701, val_acc 0.9603174603174603
trigger times: 2
end of epoch 24: val_loss 0.2449017444619295, val_acc 0.9349206349206349
trigger times: 3
end of epoch 25: val_loss 0.08972616415175722, val_acc 0.9698412698412698
trigger times: 4
end of epoch 26: val_loss 0.09776833018071628, val_acc 0.9587301587301588
trigger times: 5
end of epoch 27: val_loss 0.13361265100354183, val_acc 0.9476190476190476
trigger times: 6
end of epoch 28: val_loss 0.05377305441457137, val_acc 0.9809523809523809
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.06798085978587357, val_acc 0.973015873015873
trigger times: 1
end of epoch 30: val_loss 0.0695920474027954, val_acc 0.9714285714285714
trigger times: 2
end of epoch 31: val_loss 0.11848828627907451, val_acc 0.9634920634920635
trigger times: 3
end of epoch 32: val_loss 0.05438536763036108, val_acc 0.9809523809523809
trigger times: 4
end of epoch 33: val_loss 0.0690664549773666, val_acc 0.9698412698412698
trigger times: 5
end of epoch 34: val_loss 0.0629766224674062, val_acc 0.9682539682539683
trigger times: 6
end of epoch 35: val_loss 0.08101716731224, val_acc 0.9650793650793651
trigger times: 7
end of epoch 36: val_loss 2.4804636856589415, val_acc 0.8714285714285714
trigger times: 8
end of epoch 37: val_loss 0.35070012329390077, val_acc 0.9555555555555556
trigger times: 9
end of epoch 38: val_loss 0.5241090559628807, val_acc 0.919047619047619
trigger times: 10
Early stopping.
0 -97.27071487903595 -44.29565561256425
1 -112.11918887495995 -41.978689481633225
2 -92.99874170124531 -39.97085476609623
3 -89.17829039692879 -37.94300503359082
4 -85.19441336393356 -37.47672390906202
5 -100.42323613166809 -36.28177521453294
6 -91.69086661934853 -35.19353215618221
7 -89.13330966234207 -34.63851695272603
8 -92.06438472867012 -33.762959097993274
9 -94.91379123926163 -31.906860172355092
10 -80.81708088517189 -30.772676001251654
11 -83.94922694563866 -28.42026396583319
12 -61.335791409015656 -26.397633350086274
13 -83.89522883296013 -25.312179958453797
14 -58.996543407440186 -22.144065107815585
15 -63.852178797125816 -21.723704649211356
16 -58.469411224126816 -20.7656238590111
17 -56.37721371650696 -19.02121154786651
18 -57.1485770046711 -17.943197718572
19 -58.27600400149822 -16.251524732424823
20 -58.032382756471634 -16.23288157067845
21 -45.69618561863899 -13.895359503086498
22 -50.11501584947109 -12.541002696898001
23 -38.957669764757156 -11.974078865914656
24 -38.06263244152069 -11.966262779359818
25 -47.95178779959679 -11.430761099930217
26 -38.730200722813606 -10.782727135522462
27 -33.03382487595081 -10.016458630355537
28 -23.05960577726364 -7.70107292515296
29 -35.50941091775894 -7.67259168928286
30 -43.28710016608238 -7.374084902530061
31 -42.52287518978119 -6.776946485018116
32 -31.08038178086281 -6.298947752982548
33 -35.18501949310303 -5.894672754152052
34 -22.109417662024498 -5.027957977402961
35 -20.680519372224808 -2.49009826539426
train accuracy: 0.9179487179487179
validation accuracy: 0.919047619047619

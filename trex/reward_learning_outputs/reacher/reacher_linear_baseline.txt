demos: (120, 50, 11)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=11, out_features=1, bias=False)
)
Total number of parameters: 11
Number of trainable paramters: 11
device: cuda:0
end of epoch 0: val_loss 0.5893627855250481, val_acc 0.81
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0541,  0.0383,  0.1517, -0.1112, -0.3873, -0.1410, -0.0016,  0.0227,
         -0.3549, -0.2250,  0.0024]], device='cuda:0'))])
end of epoch 1: val_loss 0.5532221655307217, val_acc 0.81
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.5796e-02,  2.6606e-04,  1.3740e-01, -1.1812e-01, -4.9953e-01,
         -1.5657e-01, -4.3950e-03,  1.1932e-02, -2.4402e-01, -2.6830e-01,
          2.4190e-03]], device='cuda:0'))])
end of epoch 2: val_loss 0.643380686730688, val_acc 0.81
trigger times: 1
end of epoch 3: val_loss 0.9221755352797664, val_acc 0.785
trigger times: 2
end of epoch 4: val_loss 1.2337266007840864, val_acc 0.755
trigger times: 3
end of epoch 5: val_loss 3.3553678892357333, val_acc 0.665
trigger times: 4
end of epoch 6: val_loss 1.9031236188122296, val_acc 0.69
trigger times: 5
end of epoch 7: val_loss 1.0891029256107139, val_acc 0.725
trigger times: 6
end of epoch 8: val_loss 2.2172606823258394, val_acc 0.66
trigger times: 7
end of epoch 9: val_loss 0.715697898329745, val_acc 0.795
trigger times: 8
end of epoch 10: val_loss 1.611483739931257, val_acc 0.665
trigger times: 9
end of epoch 11: val_loss 1.5172456497867648, val_acc 0.765
trigger times: 10
Early stopping.
0 -2.9119825530797243 -50.00222059884506
1 0.8655401710420847 -48.823739140882175
2 7.16879628226161 -47.15605336419176
3 8.200465714558959 -46.19619104961985
4 0.8757577128708363 -45.594227093057754
5 -6.567275028675795 -45.36842966452394
6 13.070839567109942 -45.19068756322445
7 0.7543831896036863 -44.16490839583478
8 3.6531547103077173 -44.078303125872196
9 0.3681234270334244 -43.993555290419714
10 4.743897333741188 -43.86306534422809
11 7.527040529996157 -43.84874807044028
12 0.10950612556189299 -43.84199129025074
13 7.190461676567793 -43.806385671938365
14 1.4224662482738495 -43.80581985978556
15 -3.719216540455818 -42.7594767358323
16 2.1641962826251984 -42.66316468983175
17 0.1847848054021597 -42.33177224591743
18 6.418148823082447 -41.774963389485094
19 -9.118706548586488 -41.410068073767725
20 0.8169663324952126 -41.17786296442943
21 -13.310610678046942 -40.723520424948155
22 4.539639446884394 -40.527189756101116
23 -6.866045315749943 -40.49595848244517
24 -4.020792990922928 -40.429389880911344
25 -4.205301249399781 -40.05653450521898
26 1.6053469935432076 -39.59232357792555
27 -8.493380684405565 -39.54162101198148
28 -3.4892729353159666 -39.195237471709476
29 10.808619966730475 -39.172389579378766
30 3.3814265090040863 -38.51095963496708
31 -5.880547066684812 -38.447265769744824
32 -0.5586819183081388 -38.392107037026264
33 1.9891717238351703 -38.00745349944469
34 5.322914967313409 -37.46482488602393
35 0.19236262328922749 -37.10988160586883
36 -7.11849301122129 -34.27116723637227
37 0.42452048882842064 -34.14139118114101
38 9.866273472085595 -33.263072731706835
39 -6.459450786933303 -33.13344797200536
40 -10.65531906252727 -33.07825234291984
41 3.1043060924857855 -33.0321314765637
42 -6.006856836378574 -32.44934973065406
43 -0.04735361225903034 -32.4007978120153
44 2.7600273452699184 -32.40063925734975
45 11.752219411544502 -30.734403792103194
46 -3.8333320850506425 -30.57151371770873
47 -3.7536680810153484 -30.131236504472803
48 -12.209661062806845 -29.99326722619033
49 -1.0441772770136595 -29.66908258985071
50 10.74620783701539 -29.297233511513635
51 3.2097262386232615 -29.288890423975797
52 0.6259503664914519 -29.145878352769948
53 -0.1647845976985991 -28.49601894351319
54 0.2906909519806504 -28.492023661124072
55 -13.235960783436894 -28.315961465855167
56 3.2991109620779753 -27.121110566589827
57 8.793252982199192 -26.064532595535336
58 -2.5006845965981483 -25.520524278341334
59 -6.072773622348905 -25.27101421179229
60 3.006366178393364 -25.066644278800943
61 7.64179421775043 -24.925849381327673
62 -5.155816400423646 -24.188105669766596
63 1.3860362311825156 -23.48479966198816
64 8.400090798735619 -23.153943559703283
65 -4.015906593762338 -22.954730295117237
66 9.820100269280374 -22.74124885266394
67 15.03139178827405 -22.739273544503753
68 8.167286990210414 -22.264945050603636
69 4.827363211661577 -22.15569724300287
70 6.466366265900433 -21.055920928583344
71 8.084673893637955 -20.543356562348553
72 2.621206824667752 -20.33499633836848
73 11.659735896624625 -20.18157658281111
74 3.6476453342474997 -19.58144410477429
75 7.32393548451364 -19.377225745334304
76 -5.524095757398754 -19.243135617403095
77 12.731770113110542 -19.060620225371707
78 -1.5623591700568795 -18.964124524696246
79 2.7983984779566526 -18.448962308005108
80 14.281807743012905 -17.740722019993825
81 7.289308975916356 -16.85889369985028
82 0.7498721638694406 -16.3381194095591
83 11.80956569686532 -14.535892564189266
84 7.212010529823601 -14.443670567499144
85 6.1133417810779065 -14.200413010108107
86 1.1677591321058571 -13.936976181618805
87 -3.3081957665272057 -13.862253042167257
88 14.35822369903326 -13.483098530680483
89 3.653712445870042 -13.455892754889845
90 8.088392146863043 -13.355868275096913
91 14.532310105860233 -12.278515244993585
92 11.423627764452249 -12.227387460046547
93 11.28430855832994 -12.020717825467683
94 4.002330265007913 -11.910094799877324
95 -0.6076944535598159 -11.400284019256157
96 13.744906514883041 -11.134618158086587
97 0.8622609814628959 -10.859166921158222
98 8.795105254277587 -9.595137958067907
99 5.1076859398745 -9.289921608799773
100 -1.4988696178188547 -8.230877068641124
101 9.578228242695332 -7.882363241796725
102 12.79835457354784 -7.6478984168416355
103 9.46705162152648 -7.459623237418707
104 12.920666314661503 -7.124357312750265
105 -7.829376120120287 -7.05379065585803
106 12.34727157652378 -6.853091098326624
107 14.998114414513111 -6.6211384471641495
108 -7.918368319049478 -6.494555224953677
109 12.103372722864151 -6.117354180737655
110 -3.2508852314203978 -6.087055095509873
111 12.456294633448124 -5.43500831968483
112 2.044802881544456 -5.105291741614599
113 7.756921291351318 -4.628649413275992
114 7.728607111610472 -4.471031187897325
115 12.187679842114449 -4.455504779070034
116 3.8307032007724047 -4.2805498188182405
117 4.1153842699714005 -3.7944735717969627
118 11.846580842509866 -2.9512438456190186
119 9.687352202832699 -2.541618164765197
train accuracy: 0.7672222222222222
validation accuracy: 0.765

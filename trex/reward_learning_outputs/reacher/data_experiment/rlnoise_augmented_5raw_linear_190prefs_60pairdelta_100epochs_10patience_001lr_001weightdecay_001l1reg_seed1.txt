demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 171
num training_labels 171
num val_obs 19
num val_labels 19
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.08204797634664972, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0801,  0.0110,  0.0632, -0.0253, -0.2340, -0.4719]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.053987085065608356, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0146,  0.0161,  0.0395, -0.0136, -0.1937, -0.6608]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.027473493111564926, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0196, -0.0121,  0.0368, -0.0099, -0.1693, -0.7678]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.025410384737800707, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0032, -0.0019,  0.0227, -0.0009, -0.1382, -0.8588]],
       device='cuda:0'))])
end of epoch 4: val_loss 0.023632113624970724, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0066,  0.0054,  0.0094, -0.0043, -0.1366, -0.9305]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.09269410961324435, val_acc 0.9473684210526315
trigger times: 1
end of epoch 6: val_loss 0.015322313155976238, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0050,  0.0061,  0.0318,  0.0035, -0.1815, -1.0896]],
       device='cuda:0'))])
end of epoch 7: val_loss 0.013006764296158891, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.0851e-03, -4.5621e-03,  5.3186e-03, -3.6702e-04, -1.7489e-01,
         -1.1389e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.011650700290087307, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.6587e-03,  3.6255e-04,  1.8841e-02,  1.5272e-02, -1.4973e-01,
         -1.1545e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.010314069942164918, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0037,  0.0060,  0.0120,  0.0063, -0.1658, -1.2167]],
       device='cuda:0'))])
end of epoch 10: val_loss 0.01009023785079415, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0054,  0.0037,  0.0084,  0.0064, -0.1441, -1.2171]],
       device='cuda:0'))])
end of epoch 11: val_loss 0.014388946525933102, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.021146709292344513, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.007065337391180181, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0122, -0.0359,  0.0266,  0.0110, -0.0980, -1.2439]],
       device='cuda:0'))])
end of epoch 14: val_loss 0.7181072588580683, val_acc 0.7894736842105263
trigger times: 1
end of epoch 15: val_loss 0.004660467265213846, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4531e-03,  3.0625e-03,  1.0145e-02,  4.7652e-03, -2.1313e-01,
         -1.5396e+00]], device='cuda:0'))])
end of epoch 16: val_loss 0.0045660494816957, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0016, -0.0116,  0.0123,  0.0053, -0.1813, -1.4705]],
       device='cuda:0'))])
end of epoch 17: val_loss 0.008262044081525824, val_acc 1.0
trigger times: 1
end of epoch 18: val_loss 0.005176212320380178, val_acc 1.0
trigger times: 2
end of epoch 19: val_loss 0.004873151956416758, val_acc 1.0
trigger times: 3
end of epoch 20: val_loss 0.015592912987902557, val_acc 1.0
trigger times: 4
end of epoch 21: val_loss 0.004648495423882231, val_acc 1.0
trigger times: 5
end of epoch 22: val_loss 0.00634320827143591, val_acc 1.0
trigger times: 6
end of epoch 23: val_loss 0.28772925034845054, val_acc 0.8947368421052632
trigger times: 7
end of epoch 24: val_loss 0.005615338445756224, val_acc 1.0
trigger times: 8
end of epoch 25: val_loss 0.009406821548551355, val_acc 1.0
trigger times: 9
end of epoch 26: val_loss 0.019249441152115458, val_acc 1.0
trigger times: 10
Early stopping.
0 -17.103955017402768 -50.00222059884506
1 -13.601903796195984 -48.823739140882175
2 -13.106583826243877 -47.15605336419176
3 -14.307258874177933 -46.19619104961985
4 -18.184022024273872 -45.594227093057754
5 -16.922963961958885 -45.36842966452394
6 -19.37084200978279 -45.19068756322445
7 -13.86840058863163 -44.16490839583478
8 -11.157156679779291 -44.078303125872196
9 -16.884085413068533 -43.993555290419714
10 -10.249742286279798 -43.86306534422809
11 -15.753323812037706 -43.84874807044028
12 -14.226219471544027 -43.84199129025074
13 -9.503027342259884 -43.806385671938365
14 -9.451070603914559 -43.80581985978556
15 -10.527326628565788 -42.7594767358323
16 -10.878996819257736 -42.66316468983175
17 -13.572842217981815 -42.33177224591743
18 -13.977723136544228 -41.774963389485094
19 -13.441935680806637 -41.410068073767725
20 -13.66363862156868 -41.17786296442943
21 -11.685790300369263 -40.723520424948155
22 -14.813690260052681 -40.527189756101116
23 -15.56763780117035 -40.49595848244517
24 -15.83981004357338 -40.429389880911344
25 -13.94642025232315 -40.05653450521898
26 -14.183907069265842 -39.59232357792555
27 -18.958860859274864 -39.54162101198148
28 -14.614245366305113 -39.195237471709476
29 -14.983400762081146 -39.172389579378766
30 -12.425785526633263 -38.51095963496708
31 -10.08477885182947 -38.447265769744824
32 -13.818173374980688 -38.392107037026264
33 -10.034516707062721 -38.00745349944469
34 -9.837304443120956 -37.46482488602393
35 -13.431389302015305 -37.10988160586883
36 -13.496179163455963 -34.27116723637227
37 -8.498232958838344 -34.14139118114101
38 -7.658797923475504 -33.263072731706835
39 -12.22062199562788 -33.13344797200536
40 -9.525678887963295 -33.07825234291984
41 -12.031994864344597 -33.0321314765637
42 -14.832373600453138 -32.44934973065406
43 -8.841542169451714 -32.4007978120153
44 -11.805489301681519 -32.40063925734975
45 -6.6686097383499146 -30.734403792103194
46 -8.018280729651451 -30.57151371770873
47 -10.73939185589552 -30.131236504472803
48 -8.16594310477376 -29.99326722619033
49 -9.760008305311203 -29.66908258985071
50 -13.400872845202684 -29.297233511513635
51 -8.363316860049963 -29.288890423975797
52 -7.689396262168884 -29.145878352769948
53 -6.2059012008830905 -28.49601894351319
54 -9.795724734663963 -28.492023661124072
55 -11.591786280274391 -28.315961465855167
56 -8.372067227959633 -27.121110566589827
57 -6.45103931799531 -26.064532595535336
58 -7.529706489294767 -25.520524278341334
59 -6.790644886903465 -25.27101421179229
60 -8.826994098722935 -25.066644278800943
61 -8.973653454333544 -24.925849381327673
62 -9.682735100388527 -24.188105669766596
63 -6.511038318276405 -23.48479966198816
64 -3.1058691143989563 -23.153943559703283
65 -6.107271453831345 -22.954730295117237
66 -6.469251807779074 -22.74124885266394
67 -6.180825782706961 -22.739273544503753
68 -5.918392486870289 -22.264945050603636
69 -7.050139654427767 -22.15569724300287
70 -7.1143177300691605 -21.055920928583344
71 -6.242236863821745 -20.543356562348553
72 -7.210406519472599 -20.33499633836848
73 -4.9397066263481975 -20.18157658281111
74 -5.012464877218008 -19.58144410477429
75 -5.27343843691051 -19.377225745334304
76 -5.982614383101463 -19.243135617403095
77 -5.870053298771381 -19.060620225371707
78 -8.45012230426073 -18.964124524696246
79 -6.889813780784607 -18.448962308005108
80 -7.070352980867028 -17.740722019993825
81 -5.430495135486126 -16.85889369985028
82 -5.83318511210382 -16.3381194095591
83 -3.6608743397518992 -14.535892564189266
84 -6.437990039587021 -14.443670567499144
85 -4.837974024936557 -14.200413010108107
86 -6.04543530382216 -13.936976181618805
87 -5.885977163910866 -13.862253042167257
88 -5.151175064384006 -13.483098530680483
89 -5.946799263358116 -13.455892754889845
90 -5.91895454749465 -13.355868275096913
91 -3.944964675232768 -12.278515244993585
92 -4.384072311222553 -12.227387460046547
93 -5.2524494752287865 -12.020717825467683
94 -6.983631797134876 -11.910094799877324
95 -5.901728771626949 -11.400284019256157
96 -3.168133600614965 -11.134618158086587
97 -5.267189085483551 -10.859166921158222
98 -5.055075000971556 -9.595137958067907
99 -5.729583337903023 -9.289921608799773
100 -3.9859942849725485 -8.230877068641124
101 -3.829597344622016 -7.882363241796725
102 -4.792298417072743 -7.6478984168416355
103 -3.5885677523911 -7.459623237418707
104 -4.532008928246796 -7.124357312750265
105 -6.461032617837191 -7.05379065585803
106 -3.2301382580772042 -6.853091098326624
107 -3.8060612068511546 -6.6211384471641495
108 -7.078255079686642 -6.494555224953677
109 -2.734850070439279 -6.117354180737655
110 -6.837131775915623 -6.087055095509873
111 -4.7347097881138325 -5.43500831968483
112 -4.554448680952191 -5.105291741614599
113 -5.127123694866896 -4.628649413275992
114 -3.914109142497182 -4.471031187897325
115 -2.8984178783139214 -4.455504779070034
116 -5.570470057427883 -4.2805498188182405
117 -5.514654718339443 -3.7944735717969627
118 -3.24695897474885 -2.9512438456190186
119 -3.479112159460783 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

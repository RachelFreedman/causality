Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 171
num training_labels 171
num val_obs 19
num val_labels 19
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.07907761395754757, val_acc 0.9473684210526315
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0819,  0.0172, -0.0069, -0.0240,  0.2563, -0.3952]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.03190085025093331, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0559, -0.0142, -0.0069, -0.0027,  0.2576, -0.6062]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.01856396188285496, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0053, -0.0159,  0.0354, -0.0318,  0.2015, -0.7231]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.01530122138310138, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0477, -0.0137,  0.0111, -0.0079,  0.1951, -0.8199]],
       device='cuda:0'))])
end of epoch 4: val_loss 0.00880901596298444, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0389,  0.0265, -0.0070, -0.0223,  0.1904, -0.8383]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.012632167507945269, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.013733472025610115, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 0.006954599945438052, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0286,  0.0471, -0.0138, -0.0214,  0.1418, -0.9453]],
       device='cuda:0'))])
end of epoch 8: val_loss 0.011006474118163934, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.006639464673370591, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.7982e-02,  4.0828e-04,  1.2169e-02, -2.1084e-02,  1.2009e-01,
         -9.5299e-01]], device='cuda:0'))])
end of epoch 10: val_loss 0.011816274081483195, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 0.008093366488777209, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 0.004226389589539873, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0339,  0.0184,  0.0106, -0.0346,  0.1425, -0.9639]],
       device='cuda:0'))])
end of epoch 13: val_loss 0.0022471028358995084, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0338,  0.0206,  0.0113, -0.0474,  0.1846, -1.0316]],
       device='cuda:0'))])
end of epoch 14: val_loss 0.007702179434354069, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.005929622590022183, val_acc 1.0
trigger times: 2
end of epoch 16: val_loss 0.015492829763149385, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 0.033522836623259354, val_acc 1.0
trigger times: 4
end of epoch 18: val_loss 0.03564756700873212, val_acc 1.0
trigger times: 5
end of epoch 19: val_loss 0.005349229174121424, val_acc 1.0
trigger times: 6
end of epoch 20: val_loss 0.005924982435470689, val_acc 1.0
trigger times: 7
end of epoch 21: val_loss 0.00391640231135314, val_acc 1.0
trigger times: 8
end of epoch 22: val_loss 0.005429002571938662, val_acc 1.0
trigger times: 9
end of epoch 23: val_loss 0.013817067463378407, val_acc 1.0
trigger times: 10
Early stopping.
0 -17.018836736679077 -21.698217245906225
1 -15.73112866282463 -19.949715584796184
2 -11.857869729399681 -18.768323469697297
3 -14.154462203383446 -18.758804345874903
4 -11.394386127591133 -17.538967424246174
5 -11.521892428398132 -16.915411032748093
6 -10.78210518695414 -16.05718210808024
7 -8.49769667442888 -15.915815703668287
8 -9.32193062081933 -15.33402056686152
9 -8.657580027356744 -15.271252072853205
10 -7.890096306800842 -15.208145749687734
11 -8.809398986399174 -14.050045254361477
12 -10.969679173082113 -13.75955413810145
13 -5.821813981980085 -13.396239081718472
14 -6.007620804011822 -12.467903050162427
15 -5.162226941436529 -12.22796644484045
16 -6.8380246087908745 -11.94379352386303
17 -5.366055738180876 -11.871155187939209
18 -5.111918408423662 -10.822297454194436
19 -1.9421481564640999 -9.812197786666262
20 -8.871366441249847 -8.908673775575991
21 -7.38925626128912 -8.515160348153305
22 -6.996273443102837 -8.432900628058379
23 -6.39613252133131 -8.1574703320884
24 -5.86506381072104 -8.114623899357289
25 -7.049532994627953 -8.046310874369832
26 -6.073859576135874 -8.033434648498933
27 -6.17176928371191 -7.987663518560797
28 -5.280237175524235 -7.941312820022507
29 -5.5732550993561745 -7.911792849843923
30 -5.376732245087624 -7.8403085472035166
31 -4.739972822368145 -7.6695346082072655
32 -6.04232807084918 -7.623631472187833
33 -5.941449426114559 -7.517092847713959
34 -6.294613905251026 -7.375773401152629
35 -4.55356103554368 -7.337722462601442
36 -6.574639119207859 -7.212595973698941
37 -6.822035565972328 -7.209372197564873
38 -5.535351794213057 -7.13670271909991
39 -5.494414336979389 -6.752736311974305
40 -5.4747065752744675 -6.739358173065801
41 -5.423979349434376 -6.707093660958046
42 -5.064439732581377 -6.6943312910442465
43 -5.216305732727051 -6.689321861831577
44 -5.559276130050421 -6.686652102155316
45 -4.391599012538791 -6.6154467075651695
46 -3.9260992016643286 -6.524460542127663
47 -4.230573855340481 -6.5047738194148765
48 -4.40005506016314 -6.437540250165604
49 -5.469164356589317 -6.430684532606243
50 -4.04201839491725 -6.428909365437914
51 -5.180865556001663 -6.347921267036356
52 -4.149378105998039 -6.268422498242901
53 -3.4976654294878244 -6.103656113764034
54 -2.8624205589294434 -6.102549838089866
55 -3.1024724896997213 -6.0542305827232905
56 -4.40196517854929 -6.021838489818482
57 -3.9396666064858437 -5.960071728074796
58 -4.18437734246254 -5.889090204198977
59 -1.8206523545086384 -5.85687031698118
60 -3.4592477194964886 -5.832268177877353
61 -4.299818158149719 -5.758964645019952
62 -3.49283704534173 -5.725271574488492
63 -4.532787747681141 -5.521965023222128
64 -3.293407764285803 -5.482446099710484
65 -2.526089958846569 -5.402674139281961
66 -3.572154000401497 -5.37320826895953
67 -2.453520999290049 -5.327996155721218
68 -3.9912345074117184 -5.319251163344653
69 -2.2959978096187115 -5.246628989103434
70 -2.9810739010572433 -5.218349993849201
71 -2.328259279951453 -5.167763591366873
72 -2.044944340363145 -5.158536740652007
73 -1.0810647085309029 -5.15118490151008
74 -2.174937288276851 -5.087424824193218
75 -0.5035843923687935 -4.8945284164838885
76 -2.052557598799467 -4.8012787804619315
77 -1.3624125570058823 -4.780550794817313
78 -1.5769697353243828 -4.779959729642383
79 -2.2332128155976534 -4.684858983685201
80 -1.0961079010739923 -4.63376638301532
81 -1.5706420307978988 -4.544765918076566
82 -1.613234881311655 -4.541101762358445
83 -2.0226118955761194 -4.499705122960396
84 -0.7065069545060396 -4.46305722691507
85 -0.39010873436927795 -4.360082787577702
86 -0.2756343223154545 -4.347557790582064
87 0.017892926931381226 -4.32717610371528
88 -0.4549021627753973 -4.293149032506209
89 -1.1998396944254637 -4.28328411059534
90 -0.5925790518522263 -4.261857985421295
91 -0.8901041792705655 -4.2556554048544175
92 -0.9171475321054459 -4.21892374827055
93 0.14377526380121708 -4.212760778810066
94 -0.8874794358853251 -4.189471756120161
95 -0.33939220383763313 -4.077055383091453
96 0.14746040850877762 -4.039226517389183
97 -0.4449899075552821 -4.022574133686165
98 -1.0971680921502411 -4.000807316013546
99 0.49851537309587 -3.958259499152596
100 0.15178375132381916 -3.9026108386832026
101 0.8231156580150127 -3.898062481547635
102 1.2683848142623901 -3.8736058235511592
103 0.4496428146958351 -3.815576608227037
104 0.2676515197381377 -3.7794012502260146
105 0.3357445625588298 -3.7456394227913563
106 1.531061939895153 -3.58313093174521
107 0.15825562179088593 -3.548328387478394
108 0.14553449302911758 -3.4694240992184446
109 0.10204779729247093 -3.467874494555462
110 0.7844833228737116 -3.355536092072393
111 0.9635732434689999 -3.2582378101026293
112 2.0716579891741276 -3.2489250657416147
113 0.7235696632415056 -3.2288126285346226
114 1.0744483042508364 -3.1565816117673453
115 1.81892853602767 -3.098861944472417
116 1.8440112471580505 -2.6651274851973605
117 2.5766678787767887 -2.161582813943857
118 3.532495029270649 -1.9522217156632942
119 3.399117808789015 -1.8580213384140898
train accuracy: 1.0
validation accuracy: 1.0

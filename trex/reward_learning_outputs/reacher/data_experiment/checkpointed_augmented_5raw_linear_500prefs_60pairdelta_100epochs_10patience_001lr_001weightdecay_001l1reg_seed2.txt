Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 450
num training_labels 450
num val_obs 50
num val_labels 50
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.05232102973884821, val_acc 0.96
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0488,  0.0038, -0.0211, -0.0198,  0.4151, -0.6443]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.01204670119355626, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0151,  0.0058,  0.0082, -0.0218,  0.3572, -0.7646]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.022714099029214906, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.019531646811985298, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.004866064221604773, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0636,  0.0240,  0.0060, -0.0032,  0.3113, -0.8993]],
       device='cuda:0'))])
end of epoch 5: val_loss 0.005430903080845013, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.011932322357398419, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 0.00824100751493674, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 0.11895497476869409, val_acc 0.96
trigger times: 4
end of epoch 9: val_loss 0.007548292421594169, val_acc 1.0
trigger times: 5
end of epoch 10: val_loss 0.07708360491807582, val_acc 0.98
trigger times: 6
end of epoch 11: val_loss 0.02246255781652067, val_acc 0.98
trigger times: 7
end of epoch 12: val_loss 0.0042548730064226, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.6829e-02,  9.8796e-04,  1.3307e-02, -9.5179e-03,  3.3658e-01,
         -1.1211e+00]], device='cuda:0'))])
end of epoch 13: val_loss 0.005775234942577328, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 0.010196008367341278, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 0.004830943284694911, val_acc 1.0
trigger times: 3
end of epoch 16: val_loss 0.0059378693742968384, val_acc 1.0
trigger times: 4
end of epoch 17: val_loss 0.0038920043163940933, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1387, -0.0367,  0.0210, -0.0294,  0.2821, -1.2283]],
       device='cuda:0'))])
end of epoch 18: val_loss 0.06418709453030245, val_acc 0.98
trigger times: 1
end of epoch 19: val_loss 0.01844870406662011, val_acc 0.98
trigger times: 2
end of epoch 20: val_loss 0.006592979581819094, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 0.004444196982415747, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 0.0016808246813086213, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1731,  0.0259,  0.0318, -0.0234,  0.4183, -1.4176]],
       device='cuda:0'))])
end of epoch 23: val_loss 0.004074761625817587, val_acc 1.0
trigger times: 1
end of epoch 24: val_loss 0.19815064390010748, val_acc 0.94
trigger times: 2
end of epoch 25: val_loss 0.0013391611671769966, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1498,  0.0894, -0.0212,  0.0091,  0.4268, -1.4618]],
       device='cuda:0'))])
end of epoch 26: val_loss 0.0020410333585105887, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 0.2621288832168824, val_acc 0.9
trigger times: 2
end of epoch 28: val_loss 0.004072150659603011, val_acc 1.0
trigger times: 3
end of epoch 29: val_loss 0.03511105448345901, val_acc 0.98
trigger times: 4
end of epoch 30: val_loss 0.0008174600455285486, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1000,  0.0727,  0.0316, -0.0137,  0.6510, -1.6433]],
       device='cuda:0'))])
end of epoch 31: val_loss 0.009369381668866339, val_acc 1.0
trigger times: 1
end of epoch 32: val_loss 0.0070307484785541875, val_acc 1.0
trigger times: 2
end of epoch 33: val_loss 0.08356323037830421, val_acc 0.98
trigger times: 3
end of epoch 34: val_loss 0.030409130387927946, val_acc 0.98
trigger times: 4
end of epoch 35: val_loss 0.05793585634164302, val_acc 0.98
trigger times: 5
end of epoch 36: val_loss 0.005962239252531845, val_acc 1.0
trigger times: 6
end of epoch 37: val_loss 0.012531063304700042, val_acc 1.0
trigger times: 7
end of epoch 38: val_loss 0.0021290313855372744, val_acc 1.0
trigger times: 8
end of epoch 39: val_loss 0.0054269982036302625, val_acc 1.0
trigger times: 9
end of epoch 40: val_loss 0.005548572812750621, val_acc 1.0
trigger times: 10
Early stopping.
0 -20.992359220981598 -21.698217245906225
1 -18.849548369646072 -19.949715584796184
2 -12.294629007577896 -18.768323469697297
3 -17.790848091244698 -18.758804345874903
4 -11.632161095738411 -17.538967424246174
5 -12.999482035636902 -16.915411032748093
6 -15.75624107196927 -16.05718210808024
7 -12.200030483305454 -15.915815703668287
8 -8.270399257540703 -15.33402056686152
9 -12.263729196041822 -15.271252072853205
10 -6.9833133816719055 -15.208145749687734
11 -11.2250205129385 -14.050045254361477
12 -15.509755872189999 -13.75955413810145
13 -4.677639722824097 -13.396239081718472
14 -4.631665356457233 -12.467903050162427
15 -4.483668759465218 -12.22796644484045
16 -2.8043989203870296 -11.94379352386303
17 -6.634599655866623 -11.871155187939209
18 -1.964773252606392 -10.822297454194436
19 0.7434056922793388 -9.812197786666262
20 -12.08692479133606 -8.908673775575991
21 -10.32259152829647 -8.515160348153305
22 -15.76844222843647 -8.432900628058379
23 -8.897500522434711 -8.1574703320884
24 -12.089854538440704 -8.114623899357289
25 -10.748060405254364 -8.046310874369832
26 -13.217173874378204 -8.033434648498933
27 -9.771899610757828 -7.987663518560797
28 -14.099418759346008 -7.941312820022507
29 -11.190241314470768 -7.911792849843923
30 -0.8920599892735481 -7.8403085472035166
31 -8.755601465702057 -7.6695346082072655
32 -15.359838083386421 -7.623631472187833
33 -13.535336405038834 -7.517092847713959
34 -9.375172890722752 -7.375773401152629
35 -8.898273542523384 -7.337722462601442
36 -7.8358248472213745 -7.212595973698941
37 -9.913472712039948 -7.209372197564873
38 -6.628576362505555 -7.13670271909991
39 -2.5442855153232813 -6.752736311974305
40 -5.477145705372095 -6.739358173065801
41 -5.7586536929011345 -6.707093660958046
42 -1.9539568461477757 -6.6943312910442465
43 -5.06831968575716 -6.689321861831577
44 -5.231942765414715 -6.686652102155316
45 -6.5765834003686905 -6.6154467075651695
46 -5.481305234134197 -6.524460542127663
47 -1.2965997532010078 -6.5047738194148765
48 -3.871327970176935 -6.437540250165604
49 -4.5036246152594686 -6.430684532606243
50 -5.169232293963432 -6.428909365437914
51 -2.580383863300085 -6.347921267036356
52 -0.00459308922290802 -6.268422498242901
53 -1.6217295713722706 -6.103656113764034
54 0.7829193323850632 -6.102549838089866
55 -4.234475649893284 -6.0542305827232905
56 -4.164082668721676 -6.021838489818482
57 -4.521276902407408 -5.960071728074796
58 -0.09159419685602188 -5.889090204198977
59 1.4014476388692856 -5.85687031698118
60 -3.8183054104447365 -5.832268177877353
61 -0.2736049070954323 -5.758964645019952
62 0.43909144774079323 -5.725271574488492
63 -1.6425145799294114 -5.521965023222128
64 -0.40994757413864136 -5.482446099710484
65 2.468929246068001 -5.402674139281961
66 -1.6400627763941884 -5.37320826895953
67 0.41552888602018356 -5.327996155721218
68 -2.723183582536876 -5.319251163344653
69 0.0909147821366787 -5.246628989103434
70 2.563482344150543 -5.218349993849201
71 -0.14037003740668297 -5.167763591366873
72 -0.442788265645504 -5.158536740652007
73 3.051447667181492 -5.15118490151008
74 0.06985827535390854 -5.087424824193218
75 3.5417802557349205 -4.8945284164838885
76 -1.702851738780737 -4.8012787804619315
77 3.723141297698021 -4.780550794817313
78 1.9340224862098694 -4.779959729642383
79 1.7312297895550728 -4.684858983685201
80 0.5437818688806146 -4.63376638301532
81 3.6090157330036163 -4.544765918076566
82 -0.8113914811983705 -4.541101762358445
83 0.5771669009700418 -4.499705122960396
84 5.794364377856255 -4.46305722691507
85 4.2946943789720535 -4.360082787577702
86 5.073738269507885 -4.347557790582064
87 3.066505752503872 -4.32717610371528
88 2.4710559099912643 -4.293149032506209
89 5.66320476680994 -4.28328411059534
90 1.7551164701581001 -4.261857985421295
91 6.490437626838684 -4.2556554048544175
92 0.4073190279304981 -4.21892374827055
93 5.211120776832104 -4.212760778810066
94 6.547009028494358 -4.189471756120161
95 2.4707442559301853 -4.077055383091453
96 3.868679754436016 -4.039226517389183
97 5.941833704710007 -4.022574133686165
98 3.9258791506290436 -4.000807316013546
99 3.681905895471573 -3.958259499152596
100 6.852904707193375 -3.9026108386832026
101 6.998745679855347 -3.898062481547635
102 6.377486824989319 -3.8736058235511592
103 7.316094763576984 -3.815576608227037
104 7.347107067704201 -3.7794012502260146
105 5.242093313485384 -3.7456394227913563
106 6.355672292411327 -3.58313093174521
107 6.9104048833251 -3.548328387478394
108 6.200711876153946 -3.4694240992184446
109 7.69142858684063 -3.467874494555462
110 8.307840451598167 -3.355536092072393
111 1.8176952507346869 -3.2582378101026293
112 7.528032153844833 -3.2489250657416147
113 8.513140432536602 -3.2288126285346226
114 6.316306069493294 -3.1565816117673453
115 9.165994703769684 -3.098861944472417
116 5.093730568885803 -2.6651274851973605
117 5.81932807713747 -2.161582813943857
118 10.642906323075294 -1.9522217156632942
119 7.449691794812679 -1.8580213384140898
train accuracy: 1.0
validation accuracy: 1.0

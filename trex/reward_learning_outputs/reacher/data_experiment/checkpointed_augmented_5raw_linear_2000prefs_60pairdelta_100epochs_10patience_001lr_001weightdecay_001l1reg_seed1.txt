Using trajectories from checkpointed policy...
demos: (120, 50, 6)
demo_rewards: (120,)
[-21.69821725 -19.94971558 -18.76832347 -18.75880435 -17.53896742
 -16.91541103 -16.05718211 -15.9158157  -15.33402057 -15.27125207
 -15.20814575 -14.05004525 -13.75955414 -13.39623908 -12.46790305
 -12.22796644 -11.94379352 -11.87115519 -10.82229745  -9.81219779
  -8.90867378  -8.51516035  -8.43290063  -8.15747033  -8.1146239
  -8.04631087  -8.03343465  -7.98766352  -7.94131282  -7.91179285
  -7.84030855  -7.66953461  -7.62363147  -7.51709285  -7.3757734
  -7.33772246  -7.21259597  -7.2093722   -7.13670272  -6.75273631
  -6.73935817  -6.70709366  -6.69433129  -6.68932186  -6.6866521
  -6.61544671  -6.52446054  -6.50477382  -6.43754025  -6.43068453
  -6.42890937  -6.34792127  -6.2684225   -6.10365611  -6.10254984
  -6.05423058  -6.02183849  -5.96007173  -5.8890902   -5.85687032
  -5.83226818  -5.75896465  -5.72527157  -5.52196502  -5.4824461
  -5.40267414  -5.37320827  -5.32799616  -5.31925116  -5.24662899
  -5.21834999  -5.16776359  -5.15853674  -5.1511849   -5.08742482
  -4.89452842  -4.80127878  -4.78055079  -4.77995973  -4.68485898
  -4.63376638  -4.54476592  -4.54110176  -4.49970512  -4.46305723
  -4.36008279  -4.34755779  -4.3271761   -4.29314903  -4.28328411
  -4.26185799  -4.2556554   -4.21892375  -4.21276078  -4.18947176
  -4.07705538  -4.03922652  -4.02257413  -4.00080732  -3.9582595
  -3.90261084  -3.89806248  -3.87360582  -3.81557661  -3.77940125
  -3.74563942  -3.58313093  -3.54832839  -3.4694241   -3.46787449
  -3.35553609  -3.25823781  -3.24892507  -3.22881263  -3.15658161
  -3.09886194  -2.66512749  -2.16158281  -1.95222172  -1.85802134]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.03339334433558317, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0181,  0.0027, -0.0052, -0.0484,  0.1975, -0.9028]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.0013901029473874615, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1549,  0.0482,  0.0600, -0.0449,  0.3288, -1.4519]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.004170381416156808, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.010701433579632394, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.009571404941830685, val_acc 0.995
trigger times: 3
end of epoch 5: val_loss 0.0034039005089026375, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 0.0014892250360804837, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.001888032433259781, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 0.002609507663210664, val_acc 1.0
trigger times: 7
end of epoch 9: val_loss 0.0019905186273875587, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 0.0010856016640965293, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0466,  0.0019,  0.0411, -0.0493,  0.4733, -1.6356]],
       device='cuda:0'))])
end of epoch 11: val_loss 0.0019919811829752733, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.002211880735480172, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.00224358011856431, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.009371030130095975, val_acc 0.995
trigger times: 4
end of epoch 15: val_loss 0.0031997959112301188, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 0.017285868562392538, val_acc 0.995
trigger times: 6
end of epoch 17: val_loss 0.0010519511058737052, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0695,  0.0097,  0.0174, -0.0179,  0.1439, -1.3458]],
       device='cuda:0'))])
end of epoch 18: val_loss 0.004524644600669135, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.007759692285129453, val_acc 0.995
trigger times: 2
end of epoch 20: val_loss 0.0025750366922610724, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 0.00627043434404829, val_acc 0.995
trigger times: 4
end of epoch 22: val_loss 0.0027769345118149147, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 0.0026202541459813132, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.04306321750479796, val_acc 0.975
trigger times: 7
end of epoch 25: val_loss 0.05514256373100597, val_acc 0.99
trigger times: 8
end of epoch 26: val_loss 0.009738849821719348, val_acc 0.995
trigger times: 9
end of epoch 27: val_loss 0.022846099580208765, val_acc 0.995
trigger times: 10
Early stopping.
0 -30.884195029735565 -21.698217245906225
1 -28.317047655582428 -19.949715584796184
2 -23.175171554088593 -18.768323469697297
3 -29.03568783402443 -18.758804345874903
4 -17.431529760360718 -17.538967424246174
5 -22.600508153438568 -16.915411032748093
6 -22.142251195386052 -16.05718210808024
7 -19.43746455013752 -15.915815703668287
8 -13.20266442373395 -15.33402056686152
9 -19.848041102290154 -15.271252072853205
10 -16.0411134660244 -15.208145749687734
11 -15.857628643512726 -14.050045254361477
12 -22.44054116308689 -13.75955413810145
13 -12.049012925475836 -13.396239081718472
14 -11.406064510345459 -12.467903050162427
15 -11.631672203540802 -12.22796644484045
16 -5.577118657529354 -11.94379352386303
17 -14.02944016456604 -11.871155187939209
18 -3.420256108045578 -10.822297454194436
19 -6.417322620749474 -9.812197786666262
20 -15.909544661641121 -8.908673775575991
21 -15.134076461195946 -8.515160348153305
22 -21.510467559099197 -8.432900628058379
23 -11.936661571264267 -8.1574703320884
24 -18.10359837114811 -8.114623899357289
25 -15.414535745978355 -8.046310874369832
26 -19.672493785619736 -8.033434648498933
27 -12.585721231997013 -7.987663518560797
28 -21.69245272874832 -7.941312820022507
29 -17.030595123767853 -7.911792849843923
30 -4.873978681862354 -7.8403085472035166
31 -15.253473341464996 -7.6695346082072655
32 -22.437807321548462 -7.623631472187833
33 -17.23626273870468 -7.517092847713959
34 -14.831148192286491 -7.375773401152629
35 -15.274308755993843 -7.337722462601442
36 -11.287869796156883 -7.212595973698941
37 -13.99009083211422 -7.209372197564873
38 -4.775511600077152 -7.13670271909991
39 -1.0716375932097435 -6.752736311974305
40 -6.303871847689152 -6.739358173065801
41 -8.57621905207634 -6.707093660958046
42 -5.32702761143446 -6.6943312910442465
43 -7.035213036462665 -6.689321861831577
44 -7.47126330062747 -6.686652102155316
45 -10.834928072988987 -6.6154467075651695
46 -10.855544663965702 -6.524460542127663
47 -4.33966001495719 -6.5047738194148765
48 -5.44451530277729 -6.437540250165604
49 -7.560996316373348 -6.430684532606243
50 -9.477462835609913 -6.428909365437914
51 -0.9208986237645149 -6.347921267036356
52 -1.9199730828404427 -6.268422498242901
53 -6.437010362744331 -6.103656113764034
54 -3.320194847881794 -6.102549838089866
55 -8.96811955049634 -6.0542305827232905
56 -7.61192125082016 -6.021838489818482
57 -6.900635439902544 -5.960071728074796
58 -1.6752051040530205 -5.889090204198977
59 -4.1946007907390594 -5.85687031698118
60 -6.5054687559604645 -5.832268177877353
61 0.8134841918945312 -5.758964645019952
62 -0.2313394397497177 -5.725271574488492
63 -0.302823007106781 -5.521965023222128
64 -3.1973458006978035 -5.482446099710484
65 1.6022715345025063 -5.402674139281961
66 1.2718358300626278 -5.37320826895953
67 -1.818486511707306 -5.327996155721218
68 -5.133037468418479 -5.319251163344653
69 -1.878283642232418 -5.246628989103434
70 1.822938583791256 -5.218349993849201
71 -2.4090698547661304 -5.167763591366873
72 -5.721517323516309 -5.158536740652007
73 -1.5949595421552658 -5.15118490151008
74 -2.617530297487974 -5.087424824193218
75 -1.6251380145549774 -4.8945284164838885
76 -3.88742932677269 -4.8012787804619315
77 2.573990061879158 -4.780550794817313
78 -0.1622651070356369 -4.779959729642383
79 2.6934799253940582 -4.684858983685201
80 -2.162567318882793 -4.63376638301532
81 0.9415542595088482 -4.544765918076566
82 -6.603901976719499 -4.541101762358445
83 1.2740440294146538 -4.499705122960396
84 3.02766901999712 -4.46305722691507
85 2.3290265426039696 -4.360082787577702
86 0.8593708090484142 -4.347557790582064
87 -2.0924368808045983 -4.32717610371528
88 -2.7056904847268015 -4.293149032506209
89 4.688940659165382 -4.28328411059534
90 -3.8185864887200296 -4.261857985421295
91 5.508657723665237 -4.2556554048544175
92 -5.509854610078037 -4.21892374827055
93 2.6862941347062588 -4.212760778810066
94 6.284406587481499 -4.189471756120161
95 0.20023419335484505 -4.077055383091453
96 -0.9721408672630787 -4.039226517389183
97 3.964064233005047 -4.022574133686165
98 2.9887849017977715 -4.000807316013546
99 -0.49868158996105194 -3.958259499152596
100 4.25895220041275 -3.9026108386832026
101 2.8791533187031746 -3.898062481547635
102 0.547547236084938 -3.8736058235511592
103 4.899377506226301 -3.815576608227037
104 5.456753026694059 -3.7794012502260146
105 3.703673031181097 -3.7456394227913563
106 1.484761904925108 -3.58313093174521
107 7.050454564392567 -3.548328387478394
108 6.158272922039032 -3.4694240992184446
109 6.8816596530377865 -3.467874494555462
110 6.499966982752085 -3.355536092072393
111 -3.9293636977672577 -3.2582378101026293
112 2.6122976988554 -3.2489250657416147
113 7.548172563314438 -3.2288126285346226
114 1.9851255845278502 -3.1565816117673453
115 6.500551238656044 -3.098861944472417
116 1.0679319836199284 -2.6651274851973605
117 1.9154063649475574 -2.161582813943857
118 6.566094748675823 -1.9522217156632942
119 3.3639554046094418 -1.8580213384140898
train accuracy: 0.9966666666666667
validation accuracy: 0.995

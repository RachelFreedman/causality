demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.010162489386792508, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0048,  0.0034,  0.0502,  0.0022, -0.1697, -1.2653]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.0046308620335312155, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0052,  0.0079,  0.0177, -0.0138, -0.1976, -1.3885]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.0030979023379308756, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0076,  0.0024,  0.0203, -0.0035, -0.2345, -1.4775]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.005842045177502975, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.004550217920427784, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.1253948135959801, val_acc 0.955
trigger times: 3
end of epoch 6: val_loss 0.001720135226205528, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.8972e-05,  5.6525e-02,  8.5636e-03, -3.4791e-02, -3.4479e-01,
         -2.1275e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.0025617824468833917, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.007274578852762375, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 0.005740462606103378, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.0013535079399053628, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0170,  0.0029,  0.0284, -0.0150, -0.2171, -1.9295]],
       device='cuda:0'))])
end of epoch 11: val_loss 0.012672184323105568, val_acc 0.995
trigger times: 1
end of epoch 12: val_loss 0.0017802789060468527, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.0009779171431164712, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0154,  0.0073,  0.0314, -0.0185, -0.3685, -1.9596]],
       device='cuda:0'))])
end of epoch 14: val_loss 0.004507642611850251, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.02184729955236783, val_acc 0.985
trigger times: 2
end of epoch 16: val_loss 0.004764429324785482, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 0.0008391603963664096, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.0472e-03,  8.4703e-03,  2.0492e-03,  9.1236e-03, -2.9304e-01,
         -2.0897e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.00168851093107115, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.0033683092897831913, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 0.025446306164504193, val_acc 0.985
trigger times: 3
end of epoch 21: val_loss 0.005281977515003788, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 0.004504847499698954, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 0.002810810589585131, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.0021502953878676577, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.0028365441155838054, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 0.0035904175841388053, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 0.005660079298440657, val_acc 1.0
trigger times: 10
Early stopping.
0 -18.674163945019245 -50.00222059884506
1 -13.7082679271698 -48.823739140882175
2 -14.78454901278019 -47.15605336419176
3 -15.063266038894653 -46.19619104961985
4 -18.615187227725983 -45.594227093057754
5 -17.254978507757187 -45.36842966452394
6 -20.99445077776909 -45.19068756322445
7 -15.878973744809628 -44.16490839583478
8 -10.895924334414303 -44.078303125872196
9 -18.84721629321575 -43.993555290419714
10 -12.095613289624453 -43.86306534422809
11 -16.941736925393343 -43.84874807044028
12 -15.856624364852905 -43.84199129025074
13 -12.032424814999104 -43.806385671938365
14 -10.579319290816784 -43.80581985978556
15 -12.084251843392849 -42.7594767358323
16 -10.52563345991075 -42.66316468983175
17 -15.458358347415924 -42.33177224591743
18 -15.09597222507 -41.774963389485094
19 -14.298355054110289 -41.410068073767725
20 -14.99539639800787 -41.17786296442943
21 -13.278449822217226 -40.723520424948155
22 -16.59377472102642 -40.527189756101116
23 -15.405869200825691 -40.49595848244517
24 -15.911723464727402 -40.429389880911344
25 -14.18740239739418 -40.05653450521898
26 -15.343560066074133 -39.59232357792555
27 -20.482414424419403 -39.54162101198148
28 -15.685370273888111 -39.195237471709476
29 -18.12834271788597 -39.172389579378766
30 -12.838523713871837 -38.51095963496708
31 -12.621454620733857 -38.447265769744824
32 -16.42516927793622 -38.392107037026264
33 -9.627484500408173 -38.00745349944469
34 -10.498913869261742 -37.46482488602393
35 -15.915917731821537 -37.10988160586883
36 -12.946537464857101 -34.27116723637227
37 -9.152075134217739 -34.14139118114101
38 -10.287093649618328 -33.263072731706835
39 -12.24168703891337 -33.13344797200536
40 -8.859362587332726 -33.07825234291984
41 -13.856313347816467 -33.0321314765637
42 -17.533793542534113 -32.44934973065406
43 -8.364655271172523 -32.4007978120153
44 -12.768749251961708 -32.40063925734975
45 -7.570695102214813 -30.734403792103194
46 -6.585832276381552 -30.57151371770873
47 -11.650459036231041 -30.131236504472803
48 -9.861199021339417 -29.99326722619033
49 -9.373448185622692 -29.66908258985071
50 -15.322801481932402 -29.297233511513635
51 -9.300801411271095 -29.288890423975797
52 -8.095358520746231 -29.145878352769948
53 -8.209188090637326 -28.49601894351319
54 -10.41180757433176 -28.492023661124072
55 -11.407884450920392 -28.315961465855167
56 -7.744571581482887 -27.121110566589827
57 -7.021440897136927 -26.064532595535336
58 -8.688829757273197 -25.520524278341334
59 -8.84714936837554 -25.27101421179229
60 -9.14828659594059 -25.066644278800943
61 -9.55994027107954 -24.925849381327673
62 -8.429092444479465 -24.188105669766596
63 -9.005720008164644 -23.48479966198816
64 -5.182698803022504 -23.153943559703283
65 -7.934623904526234 -22.954730295117237
66 -6.946561107411981 -22.74124885266394
67 -7.819443903164938 -22.739273544503753
68 -5.64345283433795 -22.264945050603636
69 -9.747420336818323 -22.15569724300287
70 -6.414909400045872 -21.055920928583344
71 -6.357918780297041 -20.543356562348553
72 -7.120262689888477 -20.33499633836848
73 -7.3008260485948995 -20.18157658281111
74 -7.879047719761729 -19.58144410477429
75 -7.427725440822542 -19.377225745334304
76 -6.503199033439159 -19.243135617403095
77 -6.597796905785799 -19.060620225371707
78 -8.248079977929592 -18.964124524696246
79 -7.650341177824885 -18.448962308005108
80 -8.987571768462658 -17.740722019993825
81 -4.184517078101635 -16.85889369985028
82 -6.000482582487166 -16.3381194095591
83 -6.101396096055396 -14.535892564189266
84 -6.885725885629654 -14.443670567499144
85 -5.765919838100672 -14.200413010108107
86 -6.706857480108738 -13.936976181618805
87 -4.5033189952373505 -13.862253042167257
88 -6.530015003401786 -13.483098530680483
89 -6.178436506539583 -13.455892754889845
90 -5.021545548923314 -13.355868275096913
91 -6.028094219131162 -12.278515244993585
92 -4.101443273946643 -12.227387460046547
93 -5.019747402518988 -12.020717825467683
94 -6.755908586084843 -11.910094799877324
95 -3.649502876214683 -11.400284019256157
96 -5.406930126948282 -11.134618158086587
97 -3.486346445977688 -10.859166921158222
98 -3.674199001863599 -9.595137958067907
99 -5.895581122487783 -9.289921608799773
100 -6.421752915717661 -8.230877068641124
101 -6.610371698625386 -7.882363241796725
102 -7.093004610389471 -7.6478984168416355
103 -6.329137891763821 -7.459623237418707
104 -6.61603182181716 -7.124357312750265
105 -5.170993796084076 -7.05379065585803
106 -5.561789022060111 -6.853091098326624
107 -5.680638792924583 -6.6211384471641495
108 -5.679487683810294 -6.494555224953677
109 -5.154554621316493 -6.117354180737655
110 -4.703470048494637 -6.087055095509873
111 -4.905950063839555 -5.43500831968483
112 -4.569482268765569 -5.105291741614599
113 -5.477872092276812 -4.628649413275992
114 -4.284808890894055 -4.471031187897325
115 -4.022008730098605 -4.455504779070034
116 -3.559909244067967 -4.2805498188182405
117 -3.7001191433519125 -3.7944735717969627
118 -3.399451531469822 -2.9512438456190186
119 -3.2722427621483803 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

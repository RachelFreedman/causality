[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 7.34484521010259e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.6209e-06,  4.5725e-05,  1.7736e-02, -2.4443e-02, -3.3815e-05,
         -6.1433e-06, -1.1172e-03,  4.0032e-03,  4.4889e-05,  4.5889e-05,
         -1.4320e-03, -3.1382e-01, -4.2325e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.000818679228917425, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.6879e-02,  8.3502e-02,  1.9249e-01, -1.3437e-01,  1.1336e-01,
         -3.3780e-01,  1.6842e-08, -1.1649e-02, -7.7195e-05,  1.4286e-01,
         -1.4320e-03, -1.4758e+00, -2.1030e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.6655e-05, -8.1847e-06,  6.9481e-02, -3.1815e-02, -7.0539e-06,
          8.5837e-05,  5.4839e-07, -1.4053e-05,  1.1104e-05,  2.5842e-05,
         -1.4320e-03, -6.9155e-01, -1.9047e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.3326e-05,  1.8030e-05,  4.6075e-05, -3.7660e-05, -2.3729e-05,
          2.1206e-04, -3.7177e-07, -3.3048e-05,  5.9541e-06, -4.7445e-05,
         -1.4320e-03, -2.6104e-05, -1.4165e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.03992829093286215, val_acc 0.995
trigger times: 1
end of epoch 6: val_loss 5.364415187614213e-09, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 6.91409104547347e-08, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.6979e-05, -3.1784e-05, -2.9319e-05,  2.1150e-05,  1.8615e-04,
         -2.2752e-04, -1.2821e-02,  2.1753e-02,  2.2794e-04, -1.6721e-04,
         -1.4320e-03,  5.1363e-05, -1.8686e+00]], device='cuda:0'))])
end of epoch 9: val_loss 6.705486391567206e-07, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 7.795696146786212e-05, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 5.781616891908925e-08, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.1226e-05, -1.2499e-05, -6.8197e-05, -2.4996e-05,  3.0659e-04,
         -1.8206e-04,  1.8691e-08,  2.0251e-05,  3.7475e-05,  1.0438e-04,
         -1.4321e-03,  6.7312e-05, -1.5912e+00]], device='cuda:0'))])
end of epoch 13: val_loss 0.00015073191791000083, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.3084e-05,  2.0023e-06,  4.7923e-06, -6.9113e-03, -3.0124e-05,
          5.7198e-05, -1.4014e-02, -1.1720e-07,  1.5790e-04, -3.8621e-05,
         -1.4322e-03, -4.5855e-06, -1.5997e+00]], device='cuda:0'))])
end of epoch 16: val_loss 1.8358198971668572e-07, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 1.9786098164331634e-06, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 1.304386596640228e-05, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 6.258469497311125e-08, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.02487783122807741, val_acc 0.995
trigger times: 5
end of epoch 21: val_loss 0.0005068247020241401, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.5745e-05,  7.8301e-02, -7.4785e-06, -3.7119e-02,  1.7123e-05,
         -3.5736e-05, -6.8336e-03,  8.9061e-03,  9.8596e-06, -4.6934e-07,
         -1.4325e-03, -9.5985e-01, -2.4816e+00]], device='cuda:0'))])
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.4520e-05,  7.7396e-05,  5.8325e-06,  4.3031e-05, -1.3578e-04,
          2.0544e-04, -1.1654e-06,  2.3485e-06,  3.1454e-05, -3.2519e-04,
         -1.4325e-03, -8.3176e-05, -1.8632e+00]], device='cuda:0'))])
end of epoch 24: val_loss 0.00040632727680332435, val_acc 1.0
trigger times: 1
end of epoch 25: val_loss 5.9604610669339305e-09, val_acc 1.0
trigger times: 2
end of epoch 26: val_loss 4.172324743478839e-09, val_acc 1.0
trigger times: 3
end of epoch 27: val_loss 0.13237857097235062, val_acc 0.98
trigger times: 4
end of epoch 28: val_loss 1.8477410179684738e-08, val_acc 1.0
trigger times: 5
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6653e-05, -1.1013e-05,  1.5915e-01, -3.1603e-05, -1.4074e-05,
          2.4871e-05, -9.0224e-03, -3.4593e-06, -6.3548e-05, -1.5799e-04,
         -1.4327e-03, -3.2087e-05, -1.8014e+00]], device='cuda:0'))])
end of epoch 30: val_loss 2.384185137316308e-08, val_acc 1.0
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.0582e-02,  2.7211e-01,  6.1946e-02, -9.0814e-02, -2.5236e-05,
          8.2165e-03,  2.9462e-07,  1.3748e-06,  1.4830e-05, -1.8656e-01,
         -1.4328e-03, -1.1944e+00, -2.1178e+00]], device='cuda:0'))])
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6314e-05,  1.3044e-01, -2.5717e-05, -3.4617e-03, -6.3332e-05,
         -1.3340e-04, -8.9418e-08,  2.8287e-06, -7.7499e-05, -8.6088e-05,
         -1.4328e-03, -4.1832e-01, -1.9265e+00]], device='cuda:0'))])
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.6414e-05, -3.1944e-05, -6.2172e-05, -4.4689e-05, -1.5617e-04,
         -3.4135e-04, -3.6234e-08,  4.2772e-06, -2.0097e-04, -2.0742e-04,
         -1.4328e-03, -1.9336e-05, -1.4561e+00]], device='cuda:0'))])
end of epoch 34: val_loss 0.00022950458745814471, val_acc 1.0
trigger times: 1
end of epoch 35: val_loss 1.9287768518552186e-05, val_acc 1.0
trigger times: 2
end of epoch 36: val_loss 1.412610163242789e-07, val_acc 1.0
trigger times: 3
end of epoch 37: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 4
end of epoch 38: val_loss 3.174131561536342e-06, val_acc 1.0
trigger times: 5
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.5201e-07,  1.5271e-01,  7.9931e-02, -9.0312e-02,  1.2126e-05,
          4.2929e-05,  4.4475e-07, -4.0496e-03,  1.1004e-04, -8.9328e-05,
         -1.4331e-03, -7.2080e-01, -1.9621e+00]], device='cuda:0'))])
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5202e-06, -3.2546e-05,  1.1865e-05,  8.4955e-06,  2.9369e-05,
          8.3621e-05,  1.1163e-06,  4.5224e-06,  2.6176e-04, -2.1727e-04,
         -1.4331e-03, -1.6053e-04, -1.6015e+00]], device='cuda:0'))])
end of epoch 41: val_loss 2.7578558576024646e-06, val_acc 1.0
trigger times: 1
end of epoch 42: val_loss 0.00027914983725391807, val_acc 1.0
trigger times: 2
end of epoch 43: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 3
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.3312e-05,  8.7072e-06, -3.0288e-05, -2.1991e-05,  2.4014e-04,
          1.9332e-04,  1.0457e-02, -3.7491e-06, -8.1401e-05, -3.0663e-04,
         -1.4332e-03, -3.0080e-04, -1.8691e+00]], device='cuda:0'))])
end of epoch 45: val_loss 3.3438107955419127e-07, val_acc 1.0
trigger times: 1
end of epoch 46: val_loss 3.8025222602300345e-06, val_acc 1.0
trigger times: 2
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2801e-05,  1.9940e-02,  4.2616e-06, -1.9114e-01, -4.4605e-05,
          1.2165e-04,  1.2776e-06, -4.2463e-08, -7.4589e-05, -6.5002e-05,
         -1.4333e-03, -6.7977e-01, -1.7983e+00]], device='cuda:0'))])
end of epoch 48: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 49: val_loss 4.627377697943302e-06, val_acc 1.0
trigger times: 2
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.6026e-02,  3.8410e-02,  6.5356e-02,  1.0448e-05, -5.9686e-05,
          5.1297e-05,  4.6303e-07, -2.8001e-06, -6.7077e-05, -4.4624e-05,
         -1.4334e-03, -1.4415e-01, -1.6961e+00]], device='cuda:0'))])
end of epoch 51: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 52: val_loss 0.04918108368912755, val_acc 0.99
trigger times: 2
end of epoch 53: val_loss 9.440509796831975e-07, val_acc 1.0
trigger times: 3
end of epoch 54: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 4
end of epoch 55: val_loss 3.635871968299398e-07, val_acc 1.0
trigger times: 5
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.7780e-02,  2.8651e-02,  3.0053e-01, -1.9722e-01,  1.8204e-01,
         -3.3747e-01, -1.0650e-03,  3.1831e-07, -1.1633e-01, -3.9150e-06,
         -1.4336e-03, -1.6155e+00, -2.0398e+00]], device='cuda:0'))])
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4667e-05,  4.9682e-06,  1.6352e-01, -8.7170e-02, -1.6145e-04,
         -2.5036e-05, -2.4184e-07, -2.7228e-06, -3.7517e-05, -3.0081e-05,
         -1.4337e-03, -8.5698e-01, -1.8533e+00]], device='cuda:0'))])
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.4307e-05, -5.9584e-05, -1.3982e-05,  1.9176e-05, -3.8977e-04,
         -4.6279e-05, -2.4869e-06, -6.9919e-06,  4.3660e-04, -9.2661e-05,
         -1.4337e-03,  3.2171e-04, -1.3948e+00]], device='cuda:0'))])
end of epoch 59: val_loss 0.0007570761182343944, val_acc 1.0
trigger times: 1
end of epoch 60: val_loss 1.5905949112493546e-06, val_acc 1.0
trigger times: 2
end of epoch 61: val_loss 2.258964923385065e-07, val_acc 1.0
trigger times: 3
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3289e-05, -1.3818e-05,  7.5255e-02,  2.9361e-05, -1.2251e-05,
          2.3826e-04, -2.9888e-06, -6.4286e-05, -3.8459e-05, -2.9474e-04,
         -1.4339e-03,  8.6126e-05, -2.1529e+00]], device='cuda:0'))])
end of epoch 63: val_loss 2.1934465493700373e-07, val_acc 1.0
trigger times: 1
end of epoch 64: val_loss 5.185583773936742e-08, val_acc 1.0
trigger times: 2
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1479e-06, -3.2516e-08, -3.3206e-06,  5.0154e-06, -1.7961e-04,
          4.7904e-06,  3.8326e-03,  1.5364e-06, -1.6181e-04,  1.4738e-04,
         -1.4340e-03, -8.1789e-05, -1.4133e+00]], device='cuda:0'))])
end of epoch 66: val_loss 2.1850732452577406e-06, val_acc 1.0
trigger times: 1
end of epoch 67: val_loss 0.02034730295615901, val_acc 0.995
trigger times: 2
end of epoch 68: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 3
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.1185e-05, -5.3902e-05,  1.4991e-05, -2.5653e-05,  2.6823e-04,
         -1.7634e-04, -2.7776e-07,  1.3382e-05,  8.1365e-05,  2.4466e-05,
         -1.4341e-03, -2.5387e-05, -1.9276e+00]], device='cuda:0'))])
end of epoch 70: val_loss 2.008672744224782e-07, val_acc 1.0
trigger times: 1
end of epoch 71: val_loss 0.04709154605865479, val_acc 0.995
trigger times: 2
end of epoch 72: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 3
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1160e-05,  2.0923e-01, -5.2458e-06, -1.6779e-01, -4.2076e-05,
          2.1477e-04, -9.5040e-03, -1.2662e-06, -1.8300e-05, -1.8151e-04,
         -1.4342e-03, -2.5282e-01, -2.3683e+00]], device='cuda:0'))])
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2941e-05, -1.9601e-05, -9.7471e-06, -1.2944e-05, -1.3476e-04,
          5.2052e-04, -5.2535e-06, -4.2632e-06,  1.4355e-04, -3.6535e-04,
         -1.4343e-03,  1.8361e-04, -1.6805e+00]], device='cuda:0'))])
end of epoch 75: val_loss 2.0116237730007926e-05, val_acc 1.0
trigger times: 1
end of epoch 76: val_loss 8.332054005677492e-07, val_acc 1.0
trigger times: 2
end of epoch 77: val_loss 3.270906616929636e-05, val_acc 1.0
trigger times: 3
end of epoch 78: val_loss 1.1324880659913106e-08, val_acc 1.0
trigger times: 4
end of epoch 79: val_loss 1.7881392366803084e-09, val_acc 1.0
trigger times: 5
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.9320e-05,  8.3999e-02,  1.0173e-02, -7.0050e-06, -1.6698e-05,
          1.6117e-05, -3.6946e-06,  4.6661e-07,  1.9375e-05,  5.2543e-05,
         -1.4345e-03,  1.9719e-04, -1.4881e+00]], device='cuda:0'))])
end of epoch 81: val_loss 1.7344923818995995e-07, val_acc 1.0
trigger times: 1
end of epoch 82: val_loss 7.467911916236858e-07, val_acc 1.0
trigger times: 2
end of epoch 83: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 3
end of epoch 84: val_loss 8.94069543733167e-09, val_acc 1.0
trigger times: 4
end of epoch 85: val_loss 0.01588373515740358, val_acc 0.99
trigger times: 5
end of epoch 86: val_loss 2.0086361473659054e-07, val_acc 1.0
trigger times: 6
end of epoch 87: val_loss 1.9669494122354082e-08, val_acc 1.0
trigger times: 7
end of epoch 88: val_loss 3.695486402222059e-08, val_acc 1.0
trigger times: 8
end of epoch 89: val_loss 0.00023576252371082518, val_acc 1.0
trigger times: 9
end of epoch 90: val_loss 1.5258559862729726e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -159.48314344882965 -54.98547503240923
1 -166.58849173784256 -50.492268601198035
2 -172.52498829364777 -50.03933801517046
3 -149.8556072115898 -49.75347184620696
4 -160.8983432650566 -49.72654640753777
5 -119.1852096915245 -46.98011874490918
6 -132.02048313617706 -45.7351542845057
7 -118.89217126369476 -45.670579884154705
8 -123.68045341968536 -44.99030608142343
9 -125.91223174333572 -44.14602409201361
10 -120.57609006762505 -43.81326882122305
11 -136.1738502383232 -43.18878399086166
12 -163.46838438510895 -42.29180714825394
13 -107.58398935198784 -42.00401746161006
14 -154.4625461101532 -41.6910044370425
15 -126.9019027352333 -41.68588229294918
16 -126.15391007065773 -41.281777102712205
17 -103.45446038246155 -40.44278203413966
18 -92.89269588887691 -40.34838365523108
19 -115.1020635664463 -39.599701153458774
20 -115.26082491874695 -39.57586365327889
21 -158.0199865102768 -39.31972693233231
22 -140.98684811592102 -39.024610555047154
23 -137.05640351772308 -38.45534493538269
24 -129.461016446352 -38.41270390343083
25 -107.13015860319138 -38.35634328077039
26 -145.82267585396767 -37.79713616772368
27 -126.12673079967499 -37.741528994987384
28 -120.99192330241203 -37.66475323879293
29 -77.80682396888733 -37.513139380385574
30 -108.1931574344635 -37.1809993033689
31 -111.22294436395168 -37.100703136010694
32 -150.23205864429474 -37.00630588930485
33 -100.0066370666027 -36.821916772458344
34 -120.79713386297226 -36.48799015296732
35 -142.24635854363441 -36.20965269874363
36 -142.85106706619263 -36.19207561676116
37 -129.16723841428757 -36.114459029559086
38 -143.24744194746017 -35.78149902167743
39 -89.60853964090347 -35.394503873250635
40 -103.53920125961304 -35.26282499693737
41 -80.13203996419907 -35.24303541418371
42 -133.5574283003807 -35.209705244501436
43 -101.98820376396179 -35.0654408505187
44 -132.18507209420204 -34.80241747531743
45 -110.52676439285278 -34.64469044638467
46 -134.59613940119743 -33.84284985953318
47 -104.3528136909008 -32.70706485357069
48 -98.74911975860596 -31.969099402548657
49 -111.17348301410675 -31.7109134007892
50 -77.07198348641396 -31.64414355845032
51 -126.80857926607132 -31.392382758954444
52 -121.2000378370285 -31.223196019713853
53 -102.50415650010109 -31.12953085092458
54 -103.99248249828815 -29.39157139549552
55 -95.3950966000557 -29.340125609942326
56 -81.1334157884121 -29.106189988903285
57 -101.65229123085737 -27.41102349748205
58 -105.0888007581234 -27.343722362182305
59 -93.49766428768635 -27.196681629483837
60 -113.52762313187122 -27.07399028854534
61 -93.65539288520813 -26.7047217556024
62 -106.24215734004974 -26.244794902859052
63 -101.56041035056114 -25.548365085275513
64 -68.61268736422062 -25.45878528601009
65 -75.92002213001251 -24.879106999799365
66 -106.4350273013115 -24.828695359328833
67 -111.46548017859459 -24.592745144504722
68 -102.37301740050316 -23.978745577896312
69 -72.35133096575737 -23.57262108435893
70 -78.40943779051304 -23.44970807952351
71 -71.30599543452263 -22.745309160183492
72 -73.16801285743713 -22.60679894414887
73 -97.18048757314682 -22.19891031871716
74 -43.953810937702656 -20.656863763892378
75 -66.81993117928505 -20.444472560731253
76 -69.93524052202702 -20.19699010077007
77 -102.73158085346222 -20.13839114930498
78 -82.9133994281292 -19.63760343800059
79 -101.07228049635887 -19.515598718228343
80 -71.41594457626343 -18.92838809611677
81 -67.12301188707352 -17.994774057192853
82 -72.6636192202568 -17.55742370467821
83 -54.22768975049257 -16.823073927842348
84 -66.89497074484825 -14.855082803515382
85 -83.41711497306824 -14.531424598833084
86 -32.306993551552296 -14.442420089224363
87 -65.00965732336044 -13.596012850960644
88 -26.84363293647766 -12.68135972540495
89 -75.45371223986149 -12.66418205637357
90 -63.87202160060406 -12.30017947419658
91 -44.87919779121876 -12.151904772081672
92 -69.9466753154993 -11.788852141676486
93 -37.328013613820076 -10.869989101210326
94 -56.067547082901 -10.327681503524177
95 -59.964555472135544 -9.8572159761571
96 -49.41277113556862 -8.330116995310416
97 -59.505895644426346 -8.133195842510668
98 -56.47233336418867 -8.108197691178031
99 -29.48661044239998 -7.57539849177145
100 -30.64543730765581 -7.362443126623615
101 -10.442851483821869 -7.108327355338034
102 -37.452223025262356 -6.959063561385431
103 -19.80843610316515 -6.776946485018116
104 -24.28404302801937 -6.7220638398623045
105 -20.47813507169485 -6.719970621583102
106 -54.54623708128929 -6.535447341844848
107 -44.79628299176693 -6.51820418055673
108 -43.5717489272356 -5.615796733870542
109 -56.4118437319994 -5.34720210027791
110 -36.547698467969894 -5.078485007852753
111 -37.107385501265526 -5.027957977402961
112 -50.22299346327782 -4.827572916892203
113 -38.08448424935341 -4.63049541560991
114 -48.306897044181824 -4.230832004686763
115 -53.05129572749138 -4.031048624093466
116 -43.53267348557711 -3.3844671463622564
117 -50.058127254247665 -3.3322555012187633
118 -40.433081798255444 -2.6416623314910934
119 -42.69626221060753 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -49.75347185 -49.72654641 -46.98011874
 -45.67057988 -44.14602409 -43.81326882 -41.68588229 -40.34838366
 -39.31972693 -39.02461056 -38.4127039  -38.35634328 -37.79713617
 -37.51313938 -37.00630589 -36.82191677 -35.78149902 -35.262825
 -34.80241748 -34.64469045 -33.84284986 -31.9690994  -31.7109134
 -31.39238276 -31.12953085 -29.34012561 -27.4110235  -26.70472176
 -24.879107   -24.59274514 -23.57262108 -20.65686376 -20.44447256
 -20.1969901  -20.14639341 -20.13839115 -19.63760344 -18.80921454
 -18.044517   -18.03144852 -17.75722423 -17.5574237  -17.23019795
 -16.5242622  -16.26837749 -16.14121158 -15.89694064 -15.83003844
 -15.78415695 -15.69280389 -15.37109581 -15.34268497 -14.8550828
 -14.63875307 -14.5314246  -14.44242009 -14.41737652 -14.39275635
 -14.20006363 -13.91502532 -13.78018322 -13.66988719 -13.59601285
 -13.44629011 -13.22298681 -12.87372449 -12.81742153 -12.76355045
 -12.73050093 -12.68135973 -12.66418206 -12.62509983 -12.38510174
 -12.32902923 -12.15190477 -12.13173992 -12.12567407 -12.02598574
 -11.96307096 -11.86499382 -11.44438299 -11.35854929 -11.01128148
 -10.95748581 -10.77945084 -10.59291678 -10.58251952 -10.28539303
  -9.81285059  -9.54333913  -9.49690189  -9.49613418  -9.48435448
  -9.40693038  -9.196973    -9.16500624  -9.12207241  -9.01295803
  -8.95849189  -8.93078698  -8.74764016  -8.69461336  -8.13319584
  -7.7218093   -7.57539849  -7.36244313  -7.10832736  -6.95906356
  -6.77694649  -6.72206384  -6.53544734  -5.61579673  -5.07848501
  -5.02795798  -4.67957515  -4.230832    -3.3322555   -2.64166233]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 9.753900999633913e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.8443e-02, -1.7936e-01, -9.4361e-03, -2.2844e-02,  4.4140e-02,
          1.1893e-01, -1.5545e-03,  2.2740e-02, -3.5527e-02, -3.6724e-01,
         -4.8571e-04, -4.8128e-01, -1.4561e+00]], device='cuda:1'))])
end of epoch 1: val_loss 3.576272366245803e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.2159e-07, -8.7304e-02,  1.3742e-02, -2.1401e-01,  1.2234e-01,
          6.2861e-01, -2.2640e-02,  3.1355e-02,  3.7899e-06, -9.6554e-01,
          8.0731e-04, -1.1982e+00, -1.6629e+00]], device='cuda:1'))])
end of epoch 2: val_loss 4.037496950033415e-06, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 3.084470027943098e-06, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 1.8394412365054792e-05, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.0003284348427040484, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 6.556509006827582e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4183e-02,  1.2510e-05, -1.1905e-05,  7.2776e-06,  2.9988e-02,
         -9.8782e-05, -2.1504e-02,  1.8119e-02,  1.7371e-06, -3.7923e-01,
         -5.9202e-04, -1.9097e+00, -2.3178e+00]], device='cuda:1'))])
end of epoch 7: val_loss 0.000315789900282617, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 8.75517355041211e-07, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 1.547753657671791e-05, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.05798974436528279, val_acc 0.975
trigger times: 4
end of epoch 11: val_loss 2.384185471271394e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.1926e-05,  3.2953e-02, -5.5200e-07,  1.9424e-06,  4.1382e-05,
          2.8586e-01,  2.1488e-02,  2.3283e-02,  5.5419e-05, -7.3556e-01,
         -1.9255e-03, -1.8213e+00, -1.9136e+00]], device='cuda:1'))])
end of epoch 12: val_loss 0.00019266901842112104, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 5.697879532817751e-07, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 1.7881392366803084e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.9216e-06, -1.3156e-02,  4.2771e-06,  1.9115e-05,  2.1601e-05,
          2.5913e-05,  2.5404e-02, -3.5634e-06,  2.3203e-05, -5.4816e-01,
         -5.5053e-04, -1.8945e+00, -1.9847e+00]], device='cuda:1'))])
end of epoch 15: val_loss 0.0005013437333808924, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 4.515161312156124e-05, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 9.667427900694747e-07, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 7.34559155652903e-05, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 0.0013530466699106824, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 2.595393541469093e-06, val_acc 1.0
trigger times: 6
end of epoch 21: val_loss 0.00029482575895684705, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 0.004372094980974275, val_acc 0.995
trigger times: 8
end of epoch 23: val_loss 5.7059848649032575e-05, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.00023637059028143171, val_acc 1.0
trigger times: 10
Early stopping.
0 -75.15089732408524 -50.492268601198035
1 -75.18485870957375 -50.03933801517046
2 -79.39636296033859 -49.75347184620696
3 -76.82178258895874 -49.72654640753777
4 -79.47446262836456 -46.98011874490918
5 -70.55816876888275 -45.670579884154705
6 -65.67621845006943 -44.14602409201361
7 -69.78131705522537 -43.81326882122305
8 -69.11150294542313 -41.68588229294918
9 -62.86167153716087 -40.34838365523108
10 -64.33750933408737 -39.31972693233231
11 -63.75539940595627 -39.024610555047154
12 -65.13074535131454 -38.41270390343083
13 -67.64694917201996 -38.35634328077039
14 -62.53039927780628 -37.79713616772368
15 -62.63561090826988 -37.513139380385574
16 -61.54863089323044 -37.00630588930485
17 -61.38537019491196 -36.821916772458344
18 -60.32017835974693 -35.78149902167743
19 -59.50696212053299 -35.26282499693737
20 -58.079108864068985 -34.80241747531743
21 -58.227568835020065 -34.64469044638467
22 -56.23187670111656 -33.84284985953318
23 -59.38424676656723 -31.969099402548657
24 -56.35972735285759 -31.7109134007892
25 -54.673167780041695 -31.392382758954444
26 -53.57078745961189 -31.12953085092458
27 -51.998881965875626 -29.340125609942326
28 -47.40759942680597 -27.41102349748205
29 -49.28057020902634 -26.7047217556024
30 -45.79953524470329 -24.879106999799365
31 -46.633075937628746 -24.592745144504722
32 -42.994533121585846 -23.57262108435893
33 -39.39164064079523 -20.656863763892378
34 -42.28405824303627 -20.444472560731253
35 -39.15781228244305 -20.19699010077007
36 -29.385724663734436 -20.14639340520683
37 -41.675719916820526 -20.13839114930498
38 -38.352532617747784 -19.63760343800059
39 -28.320404678583145 -18.80921454113777
40 -28.029507637023926 -18.0445170022403
41 -26.889491856098175 -18.031448518446563
42 -27.58246323466301 -17.757224229985034
43 -36.04394522309303 -17.55742370467821
44 -25.921081632375717 -17.23019795467766
45 -26.319518208503723 -16.524262196349287
46 -25.679097026586533 -16.268377485564507
47 -24.88762304186821 -16.14121158081339
48 -25.31186944246292 -15.896940640588715
49 -25.077975809574127 -15.830038444290448
50 -24.76950791478157 -15.784156946214557
51 -25.592357963323593 -15.692803891868339
52 -24.19556638598442 -15.371095806381128
53 -24.833465039730072 -15.342684972390225
54 -32.342722207307816 -14.855082803515382
55 -23.279490798711777 -14.638753074704448
56 -31.688965372741222 -14.531424598833084
57 -32.402292400598526 -14.442420089224363
58 -23.46475227177143 -14.417376523530542
59 -23.50950327515602 -14.392756349554643
60 -23.275386571884155 -14.200063625764946
61 -21.681113049387932 -13.915025323241748
62 -22.46602353453636 -13.780183220758824
63 -22.90141010284424 -13.669887192823591
64 -30.62278037145734 -13.596012850960644
65 -22.427688390016556 -13.44629010629923
66 -22.5167179107666 -13.222986806340158
67 -21.657843068242073 -12.87372448547943
68 -22.21310380101204 -12.817421527018682
69 -21.802918314933777 -12.76355044993674
70 -21.780705705285072 -12.73050092716382
71 -25.858386397361755 -12.68135972540495
72 -28.513768304139376 -12.66418205637357
73 -21.617700949311256 -12.625099829956115
74 -21.615792870521545 -12.385101738522765
75 -21.122694239020348 -12.329029228139593
76 -28.281550109386444 -12.151904772081672
77 -20.473727717995644 -12.131739920967043
78 -20.597837179899216 -12.125674070661839
79 -20.650094479322433 -12.02598574203919
80 -23.152469739317894 -11.963070958179225
81 -20.59272436797619 -11.86499381851086
82 -20.174688786268234 -11.444382994356506
83 -20.467264026403427 -11.3585492926771
84 -19.169050082564354 -11.011281484362295
85 -18.856335401535034 -10.957485806355631
86 -19.803392812609673 -10.779450838897304
87 -18.21161361038685 -10.592916775181704
88 -18.842283338308334 -10.582519521058313
89 -18.902906388044357 -10.285393026183336
90 -18.257648020982742 -9.812850592131694
91 -17.3546894043684 -9.543339127196136
92 -17.855486527085304 -9.49690188847023
93 -18.57834267616272 -9.49613417557871
94 -17.693626075983047 -9.48435448123352
95 -17.84446633607149 -9.406930380769252
96 -16.621137216687202 -9.196972997960966
97 -17.243108063936234 -9.165006239467832
98 -17.983929250389338 -9.122072413092232
99 -17.949762113392353 -9.012958025974026
100 -16.61757341772318 -8.958491894158888
101 -17.277581989765167 -8.930786980911165
102 -17.565968491137028 -8.747640159538284
103 -16.85380031168461 -8.694613361672676
104 -21.217432141304016 -8.133195842510668
105 -15.376625023782253 -7.721809301539436
106 -19.559058286249638 -7.57539849177145
107 -18.908619232475758 -7.362443126623615
108 -16.90199475735426 -7.108327355338034
109 -18.283392772078514 -6.959063561385431
110 -19.64467979967594 -6.776946485018116
111 -17.54935336858034 -6.7220638398623045
112 -20.97772492095828 -6.535447341844848
113 -19.41351867839694 -5.615796733870542
114 -16.180153265595436 -5.078485007852753
115 -16.25017316453159 -5.027957977402961
116 -11.567622542381287 -4.679575149964716
117 -14.562793746590614 -4.230832004686763
118 -13.418668363243341 -3.3322555012187633
119 -12.878554865717888 -2.6416623314910934
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -46.98011874 -45.67057988 -44.14602409
 -38.4127039  -38.35634328 -36.82191677 -35.262825   -34.64469045
 -33.84284986 -31.9690994  -27.4110235  -26.70472176 -24.879107
 -24.59274514 -20.65686376 -20.14639341 -20.13839115 -18.03144852
 -17.5574237  -16.5242622  -16.26837749 -16.14121158 -15.89694064
 -15.83003844 -15.78415695 -15.69280389 -15.34268497 -14.8550828
 -14.5314246  -14.44242009 -14.41737652 -14.39275635 -13.91502532
 -13.22298681 -12.81742153 -12.76355045 -12.68135973 -12.66418206
 -12.38510174 -12.15190477 -12.13173992 -12.02598574 -11.86499382
 -11.40435426 -11.35854929 -10.77945084 -10.58251952 -10.11136898
 -10.02621336  -9.91790646  -9.90605453  -9.83641254  -9.76800596
  -9.74347774  -9.54333913  -9.49613418  -9.48435448  -9.47599188
  -9.40693038  -9.31291291  -9.23351133  -9.15509222  -9.06048191
  -8.92445169  -8.74764016  -8.62111938  -8.42639177  -8.42622057
  -8.30887462  -8.24787338  -8.16366408  -8.13319584  -7.98301429
  -7.7218093   -7.68693585  -7.57539849  -7.56490588  -7.3637022
  -7.04262595  -7.03168235  -6.77694649  -6.72781588  -6.72206384
  -6.57890646  -6.51748522  -6.50685764  -6.2707216   -6.19084038
  -6.09899733  -6.05001387  -6.04497503  -6.03994711  -5.96353949
  -5.85907691  -5.84606044  -5.78109829  -5.67425613  -5.4021304
  -5.28384363  -5.2790235   -5.20206376  -5.14524209  -5.02795798
  -4.96049816  -4.83869756  -4.80925481  -4.77655567  -4.67957515
  -4.65389367  -4.25334254  -4.12525283  -4.09420712  -3.93971056
  -3.39975506  -3.20615197  -2.73595565  -2.54391033  -1.73966328]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.0004977978443568177, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2426e-04, -7.0277e-03, -7.3333e-04, -2.3414e-02,  4.7856e-04,
          9.2566e-05, -1.0119e-02, -1.2361e-02,  1.8748e-04, -2.1749e-03,
          7.3323e-04, -6.5070e-01, -9.2159e-01]], device='cuda:2'))])
end of epoch 1: val_loss 8.940692985959231e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4831e-01,  1.8197e-01, -4.7313e-02, -2.5862e-02,  2.7701e-02,
          2.2387e-01,  6.7409e-02, -1.1158e-02,  4.8701e-01, -1.0993e+00,
         -1.9759e-03, -1.9203e+00, -1.9933e+00]], device='cuda:2'))])
end of epoch 2: val_loss 6.437289883365338e-08, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.00025570891600043663, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 9.452466938597581e-07, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 6.24746197921766e-05, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 1.7926267071288748e-06, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 1.7906569102805748e-05, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 0.00035842332936979914, val_acc 1.0
trigger times: 7
end of epoch 9: val_loss 3.9339013220285324e-08, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 3.973462795947569e-06, val_acc 1.0
trigger times: 9
end of epoch 11: val_loss 0.03495836891664283, val_acc 0.99
trigger times: 10
Early stopping.
0 -61.54564443230629 -50.492268601198035
1 -67.47952139377594 -50.03933801517046
2 -23.444114565849304 -46.98011874490918
3 -42.213416270911694 -45.670579884154705
4 -57.011429488658905 -44.14602409201361
5 -49.251761958003044 -38.41270390343083
6 -34.66056761145592 -38.35634328077039
7 -35.158038064837456 -36.821916772458344
8 -42.87222570925951 -35.26282499693737
9 -35.949902936816216 -34.64469044638467
10 -51.29751282930374 -33.84284985953318
11 -33.18002896010876 -31.969099402548657
12 -40.250695675611496 -27.41102349748205
13 -39.94451093673706 -26.7047217556024
14 -28.11725753545761 -24.879106999799365
15 -38.717512503266335 -24.592745144504722
16 -14.05124133080244 -20.656863763892378
17 -31.95349085330963 -20.14639340520683
18 -30.367651894688606 -20.13839114930498
19 -31.22856956720352 -18.031448518446563
20 -29.572917215526104 -17.55742370467821
21 -29.46907603740692 -16.524262196349287
22 -29.399562299251556 -16.268377485564507
23 -29.662380069494247 -16.14121158081339
24 -29.03156480193138 -15.896940640588715
25 -29.544710606336594 -15.830038444290448
26 -29.234382897615433 -15.784156946214557
27 -28.391531229019165 -15.692803891868339
28 -25.73548999428749 -15.342684972390225
29 -26.84810010716319 -14.855082803515382
30 -20.63961547240615 -14.531424598833084
31 -14.386200234293938 -14.442420089224363
32 -27.512193620204926 -14.417376523530542
33 -28.13935950398445 -14.392756349554643
34 -26.847877860069275 -13.915025323241748
35 -26.751806497573853 -13.222986806340158
36 -26.175189524888992 -12.817421527018682
37 -26.501448392868042 -12.76355044993674
38 -16.88727279007435 -12.68135972540495
39 -20.7292197085917 -12.66418205637357
40 -26.47831556200981 -12.385101738522765
41 -21.72285360097885 -12.151904772081672
42 -25.142759054899216 -12.131739920967043
43 -25.13923379778862 -12.02598574203919
44 -24.588045239448547 -11.86499381851086
45 -24.529420033097267 -11.404354261624825
46 -23.818090736865997 -11.3585492926771
47 -24.58823822438717 -10.779450838897304
48 -23.71359322965145 -10.582519521058313
49 -23.581550419330597 -10.11136897706654
50 -19.047612011432648 -10.026213357004814
51 -23.899839878082275 -9.917906456508634
52 -20.172369450330734 -9.9060545262617
53 -19.796635892242193 -9.836412538439424
54 -20.233078457415104 -9.768005957417687
55 -24.2427988499403 -9.74347774029709
56 -22.16355660557747 -9.543339127196136
57 -22.51291786134243 -9.49613417557871
58 -20.097147300839424 -9.48435448123352
59 -23.250280484557152 -9.475991884543017
60 -21.305171698331833 -9.406930380769252
61 -23.328410282731056 -9.312912913709471
62 -17.329709572717547 -9.233511325902679
63 -22.31438549607992 -9.155092222787195
64 -17.05000306852162 -9.060481913722967
65 -22.93607945740223 -8.92445168917736
66 -21.6337501257658 -8.747640159538284
67 -22.38844347000122 -8.62111937592296
68 -18.86196829378605 -8.426391769750548
69 -22.84639163315296 -8.426220566128212
70 -17.554605163633823 -8.308874624013281
71 -10.679520271718502 -8.247873383953989
72 -21.66692104190588 -8.163664081664084
73 -18.72444394789636 -8.133195842510668
74 -17.232104554772377 -7.983014293913402
75 -17.660559445619583 -7.721809301539436
76 -17.5828090980649 -7.686935851179042
77 -19.87204346060753 -7.57539849177145
78 -21.223182752728462 -7.564905881747173
79 -17.646221790462732 -7.363702196278379
80 -20.411160558462143 -7.042625948189377
81 -10.647151753306389 -7.031682348811821
82 -8.430620066821575 -6.776946485018116
83 -19.955517947673798 -6.727815877609332
84 -17.440120317041874 -6.7220638398623045
85 -16.47003337740898 -6.578906457558751
86 -15.648052740842104 -6.517485217038298
87 -16.557129360735416 -6.506857639283532
88 -15.182229340076447 -6.270721602451359
89 -9.856199339032173 -6.190840383324697
90 -16.55881312303245 -6.098997331083035
91 -18.345711305737495 -6.050013868666715
92 -9.955531852319837 -6.044975028976545
93 -18.675664719194174 -6.039947108537933
94 -10.172060444951057 -5.963539489167897
95 -8.570506304502487 -5.859076913422368
96 -9.532682299613953 -5.846060443208046
97 -16.135644666850567 -5.781098288441135
98 -18.1785222068429 -5.674256129061747
99 -13.15085407346487 -5.402130403881674
100 -13.048706207424402 -5.283843628144702
101 -9.926840726286173 -5.27902350143228
102 -16.33436667174101 -5.202063760547981
103 -14.492342956364155 -5.145242087356255
104 -18.164021659642458 -5.027957977402961
105 -12.719624657183886 -4.960498155357883
106 -14.172376863658428 -4.838697556049422
107 -15.554679967463017 -4.809254810564498
108 -13.87269501388073 -4.776555671880162
109 -14.69648066163063 -4.679575149964716
110 -14.296921407803893 -4.653893670484118
111 -14.384736016392708 -4.253342537192318
112 -13.835013408213854 -4.125252829103858
113 -13.408702477812767 -4.0942071248829865
114 -12.480180338025093 -3.9397105602482654
115 -12.165633324533701 -3.399755063580103
116 -13.773417321033776 -3.206151970713792
117 -11.585929945111275 -2.7359556504549967
118 -12.783237680792809 -2.5439103284338525
119 -11.780210889875889 -1.739663281790734
train accuracy: 0.98
validation accuracy: 0.99
[-67.12713113 -67.12578006 -66.93579214 -66.90186314 -66.68267514
 -66.22024701 -66.18532127 -66.17029296 -66.08150577 -66.05968839
 -65.37420221 -65.26234369 -65.17525915 -65.10965363 -65.00918361
 -64.97710099 -64.94544339 -64.92965081 -64.5834554  -64.55279941
 -64.50420712 -64.49602013 -64.4850343  -64.41300137 -64.22725556
 -64.21408848 -64.17216566 -64.13584041 -64.12989242 -64.12862712
 -64.09735506 -64.06482464 -64.04370518 -64.01269837 -63.88427119
 -63.82788907 -63.82611957 -63.75733048 -63.66038351 -63.61634412
 -63.61369321 -63.486755   -63.32198218 -63.32121653 -63.29918661
 -63.29078961 -63.09965478 -63.07640778 -63.05017846 -62.90033563
 -62.74161417 -62.68188134 -62.67650639 -62.63090968 -62.51468237
 -62.51311624 -62.13756481 -61.84122886 -61.69391299 -61.52299261
 -50.4922686  -50.03933802 -45.67057988 -44.14602409 -38.4127039
 -31.9690994  -26.70472176 -24.879107   -20.14639341 -20.13839115
 -17.5574237  -16.5242622  -16.14121158 -15.89694064 -15.83003844
 -15.34268497 -14.5314246  -14.44242009 -14.41737652 -12.76355045
 -12.68135973 -12.38510174 -12.15190477 -12.13173992 -12.02598574
 -11.35854929 -10.11136898 -10.02621336  -9.83641254  -9.76800596
  -9.48435448  -9.23351133  -8.74764016  -8.62111938  -8.42622057
  -8.30887462  -8.24787338  -8.13319584  -7.56490588  -7.3637022
  -6.72781588  -6.57890646  -6.2707216   -6.09899733  -6.04497503
  -5.85907691  -5.84606044  -5.78109829  -5.67425613  -5.4021304
  -5.28384363  -5.20206376  -5.02795798  -4.96049816  -4.80925481
  -4.12525283  -4.09420712  -3.93971056  -3.20615197  -1.73966328]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0739e-01, -1.3267e-05,  1.2033e-05,  4.3708e-05, -6.2760e-06,
         -7.7538e-05, -5.1395e-02, -2.3454e-02,  1.9248e-01, -4.8382e-01,
          1.3362e-03, -1.3551e+00, -2.0383e+00]], device='cuda:3'))])
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.2645e-05, -1.0583e-04,  4.5554e-05,  1.0627e-04, -4.2012e-05,
         -2.0829e-04, -4.8159e-02, -5.8557e-05, -2.0255e-08,  3.8730e-04,
          1.3361e-03,  3.8069e-04, -1.8011e+00]], device='cuda:3'))])
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.3363e-04, -2.5210e-04,  9.2101e-05,  2.5442e-04,  2.7845e-04,
         -5.0454e-04, -3.9959e-02, -1.4786e-04,  7.6374e-04,  8.9438e-04,
          1.3359e-03,  8.2521e-04, -1.2002e+00]], device='cuda:3'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.6343e-04,  5.7306e-05,  9.5890e-05,  5.9934e-04,  1.0257e-04,
         -1.0571e-03, -1.9686e-02, -3.6464e-04,  1.5344e-03,  1.7133e-03,
          1.3357e-03,  1.4774e-03,  1.1918e-04]], device='cuda:3'))])
end of epoch 4: val_loss 2.848501563903483e-06, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 4.046550568546081e-06, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 2.4509396970984197e-06, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 2.3299426143807978e-06, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 6.012897042069199e-06, val_acc 1.0
trigger times: 5
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7355e-03,  6.5721e-05, -4.3871e-04, -9.9438e-04,  2.6710e-03,
          1.8382e-03, -1.7337e-01, -2.3337e-04,  1.7700e-03,  5.7653e-05,
          1.3347e-03,  2.2493e-03, -3.7721e-03]], device='cuda:3'))])
end of epoch 10: val_loss 2.8240636413556784e-06, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 2.223846619529013e-06, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0022, -0.0016, -0.0015, -0.0017, -0.0011,  0.0006, -0.1201, -0.0018,
         -0.0015, -0.0004,  0.0013, -0.0017,  0.0002]], device='cuda:3'))])
end of epoch 13: val_loss 1.1289112666190704e-06, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4359e-03,  2.1592e-03,  8.6042e-04,  3.2290e-03, -8.2692e-04,
         -3.7955e-04, -4.0665e-02,  1.1260e-03,  1.3851e-04,  8.2791e-05,
          1.3340e-03,  2.3166e-05,  2.5792e-03]], device='cuda:3'))])
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0714e-03, -4.1216e-04,  6.8201e-04,  4.1225e-03, -1.6879e-03,
         -1.3237e-03, -7.7095e-03, -6.3630e-04,  4.4796e-05,  3.6129e-04,
          1.3338e-03, -4.7404e-04,  2.7733e-04]], device='cuda:3'))])
end of epoch 16: val_loss 1.9413211671803767e-06, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 3.4511082972699116e-07, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.9448e-04, -1.5182e-03,  1.5119e-04, -3.5905e-04, -1.3254e-03,
         -3.1214e-05, -5.0767e-02,  7.2032e-05, -3.7929e-04,  4.6115e-04,
          1.3333e-03,  2.4963e-04,  9.5044e-05]], device='cuda:3'))])
end of epoch 19: val_loss 3.955355555831375e-06, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 8.112188540465582e-07, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 2.5540554696590335e-06, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.1343e-03, -8.1668e-05, -4.0242e-04, -1.0804e-04,  7.8212e-04,
          1.1247e-03, -6.4527e-02, -6.6402e-03, -3.8839e-04, -8.5442e-04,
          1.3327e-03,  9.3313e-04, -3.1354e-03]], device='cuda:3'))])
end of epoch 23: val_loss 4.100194397551604e-06, val_acc 1.0
trigger times: 1
end of epoch 24: val_loss 1.1390440425884663e-06, val_acc 1.0
trigger times: 2
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.8294e-06, -6.6922e-04, -3.2556e-04, -8.5065e-04,  2.5845e-04,
         -3.7692e-04, -5.3837e-03,  4.5668e-05, -3.0219e-04, -1.2256e-03,
          1.3323e-03, -1.0739e-03, -1.6954e-04]], device='cuda:3'))])
end of epoch 26: val_loss 2.8157191715649788e-06, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 3.1036086062385947e-06, val_acc 1.0
trigger times: 2
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0021,  0.0003, -0.0007, -0.0010, -0.0036,  0.0015, -0.0131,  0.0009,
          0.0004, -0.0012,  0.0013, -0.0001, -0.0022]], device='cuda:3'))])
end of epoch 29: val_loss 3.240102819859203e-06, val_acc 1.0
trigger times: 1
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3748e-03, -8.3057e-05, -4.0253e-04,  4.2670e-04,  8.9131e-04,
          8.3108e-04, -5.9589e-03,  7.0309e-05,  5.3014e-05, -9.6381e-04,
          1.3315e-03,  1.2257e-03,  1.4551e-03]], device='cuda:3'))])
end of epoch 31: val_loss 3.7479324515743428e-06, val_acc 1.0
trigger times: 1
end of epoch 32: val_loss 2.69770223951582e-06, val_acc 1.0
trigger times: 2
end of epoch 33: val_loss 1.2278555914235766e-07, val_acc 1.0
trigger times: 3
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2364e-03, -1.7620e-03,  2.9731e-03,  6.3010e-06,  4.9660e-04,
          1.0079e-03, -1.1553e-01, -1.6292e-03,  3.7703e-04, -3.4319e-04,
          1.3309e-03,  1.4219e-03,  6.7421e-04]], device='cuda:3'))])
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.2439e-04,  9.9827e-04, -1.4071e-03,  3.7045e-04, -8.7906e-04,
          1.3434e-03, -2.7478e-01,  8.0221e-05,  1.9924e-03, -7.4984e-04,
          1.3307e-03,  2.6206e-03, -7.1958e-04]], device='cuda:3'))])
end of epoch 36: val_loss 2.8330044074209582e-06, val_acc 1.0
trigger times: 1
end of epoch 37: val_loss 4.277219030086599e-06, val_acc 1.0
trigger times: 2
end of epoch 38: val_loss 2.3609369083033015e-06, val_acc 1.0
trigger times: 3
end of epoch 39: val_loss 4.086485502057258e-06, val_acc 1.0
trigger times: 4
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0012,  0.0004, -0.0001,  0.0028, -0.0019, -0.0001, -0.0060, -0.0019,
         -0.0007,  0.0001,  0.0013, -0.0023,  0.0007]], device='cuda:3'))])
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0031,  0.0009, -0.0016, -0.0004, -0.0009, -0.0006, -0.0523, -0.0007,
         -0.0007,  0.0016,  0.0013,  0.0001, -0.0041]], device='cuda:3'))])
end of epoch 42: val_loss 1.0395043973687735e-06, val_acc 1.0
trigger times: 1
end of epoch 43: val_loss 9.906286560124044e-07, val_acc 1.0
trigger times: 2
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6717e-05,  2.9906e-03, -2.1586e-03,  9.2312e-04,  5.3204e-04,
          8.9840e-05, -1.2687e-01,  3.0592e-03, -1.8491e-03,  3.6954e-04,
          1.3293e-03,  2.1170e-03,  2.2099e-03]], device='cuda:3'))])
end of epoch 45: val_loss 4.0411940531726033e-07, val_acc 1.0
trigger times: 1
end of epoch 46: val_loss 5.922298095129009e-06, val_acc 1.0
trigger times: 2
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0010,  0.0010, -0.0003, -0.0017,  0.0012,  0.0009, -0.1742, -0.0047,
         -0.0013, -0.0012,  0.0013, -0.0033, -0.0016]], device='cuda:3'))])
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7894e-03,  2.2728e-03, -2.9704e-03,  1.3441e-03, -2.4237e-03,
         -3.7457e-03, -8.7727e-03, -7.6893e-05, -1.3384e-03,  1.7476e-03,
          1.3287e-03,  9.9621e-04,  3.5537e-05]], device='cuda:3'))])
end of epoch 49: val_loss 1.0299676840475058e-06, val_acc 1.0
trigger times: 1
end of epoch 50: val_loss 1.671312686823967e-06, val_acc 1.0
trigger times: 2
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0008,  0.0010, -0.0002,  0.0015,  0.0004,  0.0010, -0.0237,  0.0006,
         -0.0012,  0.0034,  0.0013, -0.0004, -0.0005]], device='cuda:3'))])
end of epoch 52: val_loss 2.0366883305200644e-06, val_acc 1.0
trigger times: 1
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.2468e-03, -6.1424e-03, -2.9109e-04, -1.2621e-03, -4.2957e-04,
          7.8040e-04, -2.5178e-01,  8.2462e-03, -1.9334e-03,  1.8852e-03,
          1.3279e-03,  7.0029e-05,  2.8720e-03]], device='cuda:3'))])
end of epoch 54: val_loss 1.059173919912837e-06, val_acc 1.0
trigger times: 1
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1376e-03, -4.9637e-04, -1.6605e-03,  8.2173e-04, -6.3016e-04,
         -6.9071e-05, -1.8073e-01,  1.5043e-03,  3.8193e-04,  8.3566e-05,
          1.3275e-03, -1.1859e-03, -1.3879e-04]], device='cuda:3'))])
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0004,  0.0021, -0.0019,  0.0014,  0.0012, -0.0005, -0.0058, -0.0004,
          0.0008, -0.0017,  0.0013, -0.0004, -0.0011]], device='cuda:3'))])
end of epoch 57: val_loss 4.49417926802198e-06, val_acc 1.0
trigger times: 1
end of epoch 58: val_loss 5.625469131587124e-06, val_acc 1.0
trigger times: 2
end of epoch 59: val_loss 1.947281666616618e-06, val_acc 1.0
trigger times: 3
end of epoch 60: val_loss 1.8304568277471844e-06, val_acc 1.0
trigger times: 4
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0003,  0.0014,  0.0024,  0.0006, -0.0004, -0.0032, -0.0619, -0.0009,
         -0.0008,  0.0022,  0.0013, -0.0024,  0.0018]], device='cuda:3'))])
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.8594e-05, -1.7175e-03, -9.4506e-04, -6.6581e-05,  2.0744e-03,
         -2.9742e-05, -8.5027e-02,  1.3308e-03,  4.3339e-04, -3.0825e-03,
          1.3264e-03, -1.4252e-03,  2.6139e-04]], device='cuda:3'))])
end of epoch 63: val_loss 3.882042432223898e-06, val_acc 1.0
trigger times: 1
end of epoch 64: val_loss 3.142351515066366e-06, val_acc 1.0
trigger times: 2
end of epoch 65: val_loss 2.8949930657518052e-06, val_acc 1.0
trigger times: 3
end of epoch 66: val_loss 1.1324881768359774e-07, val_acc 1.0
trigger times: 4
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.5057e-03,  1.6721e-03,  2.6933e-03, -1.7177e-03, -9.7106e-05,
          5.8942e-04, -1.6999e-02,  1.9395e-04, -1.9249e-03,  6.5373e-06,
          1.3255e-03,  5.2057e-04,  4.5616e-04]], device='cuda:3'))])
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.4022e-04,  1.4254e-04,  1.4465e-03,  5.2848e-04,  6.7237e-04,
         -3.2453e-03, -3.0580e-01,  2.1251e-03,  1.8622e-04, -4.2146e-03,
          1.3254e-03, -7.1931e-04,  1.0517e-03]], device='cuda:3'))])
end of epoch 69: val_loss 2.3627250476465633e-06, val_acc 1.0
trigger times: 1
end of epoch 70: val_loss 1.650451084742599e-06, val_acc 1.0
trigger times: 2
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9097e-03, -2.2420e-03, -1.8130e-03, -2.6318e-05,  2.8724e-04,
         -6.2131e-04, -2.0901e-01, -6.4839e-04,  3.3452e-04,  2.0510e-04,
          1.3249e-03, -1.9561e-03,  1.1539e-03]], device='cuda:3'))])
end of epoch 72: val_loss 4.54722701533683e-06, val_acc 1.0
trigger times: 1
end of epoch 73: val_loss 1.9943692439028383e-06, val_acc 1.0
trigger times: 2
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.3500e-03,  4.0383e-04, -6.6556e-04,  2.6720e-04,  1.9701e-03,
         -1.2443e-03, -8.3630e-02, -3.1538e-03, -1.0773e-03, -4.9883e-05,
          1.3244e-03,  5.6456e-04,  1.7594e-04]], device='cuda:3'))])
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.7601e-05, -1.0172e-04,  1.4611e-03, -2.9437e-03,  1.0038e-04,
          8.7502e-04, -2.9213e-02, -8.0110e-05,  8.8721e-04,  1.9220e-03,
          1.3242e-03, -2.1994e-03,  1.3745e-03]], device='cuda:3'))])
end of epoch 76: val_loss 2.005694110209788e-06, val_acc 1.0
trigger times: 1
end of epoch 77: val_loss 1.8995980775571297e-06, val_acc 1.0
trigger times: 2
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.9121e-05,  1.2045e-03,  6.7961e-05,  3.5773e-03, -4.4333e-04,
          2.1997e-04, -7.4304e-03,  4.9353e-04,  1.3684e-03, -1.1303e-03,
          1.3237e-03, -2.6701e-03, -1.8560e-03]], device='cuda:3'))])
end of epoch 79: val_loss 1.5252815646249473e-06, val_acc 1.0
trigger times: 1
end of epoch 80: val_loss 4.153838270894994e-06, val_acc 1.0
trigger times: 2
end of epoch 81: val_loss 1.1575214726633476e-06, val_acc 1.0
trigger times: 3
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0023,  0.0006, -0.0001, -0.0008, -0.0001,  0.0009, -0.0951,  0.0010,
          0.0020, -0.0001,  0.0013, -0.0026,  0.0013]], device='cuda:3'))])
end of epoch 83: val_loss 2.9557896635878934e-06, val_acc 1.0
trigger times: 1
end of epoch 84: val_loss 1.885037438796644e-05, val_acc 1.0
trigger times: 2
end of epoch 85: val_loss 6.055829956252978e-07, val_acc 1.0
trigger times: 3
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.1630e-04, -1.4495e-03,  1.6014e-03,  2.6961e-04,  4.9689e-05,
          5.6305e-05, -1.9881e-01, -2.2416e-04,  3.7763e-03, -2.9510e-04,
          1.3224e-03, -4.8470e-04, -5.9500e-04]], device='cuda:3'))])
end of epoch 87: val_loss 3.095264106036666e-06, val_acc 1.0
trigger times: 1
end of epoch 88: val_loss 1.857278833199416e-06, val_acc 1.0
trigger times: 2
end of epoch 89: val_loss 5.329831396920781e-06, val_acc 1.0
trigger times: 3
end of epoch 90: val_loss 3.2246107362965403e-07, val_acc 1.0
trigger times: 4
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.2978e-05,  4.0625e-03, -1.2699e-03, -2.7710e-03, -1.7686e-04,
          2.8835e-04, -1.5296e-02,  4.6124e-04, -2.5865e-03,  2.0044e-04,
          1.3215e-03, -1.3319e-03, -7.1591e-04]], device='cuda:3'))])
end of epoch 92: val_loss 2.3841856489070778e-09, val_acc 1.0
trigger times: 1
end of epoch 93: val_loss 2.2739143236094605e-06, val_acc 1.0
trigger times: 2
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0009, -0.0013, -0.0001, -0.0005,  0.0014,  0.0008, -0.0555,  0.0017,
         -0.0010,  0.0009,  0.0013,  0.0006, -0.0004]], device='cuda:3'))])
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0014,  0.0045, -0.0038, -0.0012, -0.0006,  0.0005, -0.2068,  0.0022,
          0.0003, -0.0004,  0.0013, -0.0023,  0.0013]], device='cuda:3'))])
end of epoch 96: val_loss 1.1759988812798384e-06, val_acc 1.0
trigger times: 1
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.6131e-06,  2.8092e-04,  2.0806e-03, -2.1295e-03,  1.9414e-03,
         -1.3589e-03, -3.0291e-02,  1.8055e-03,  1.7090e-03,  4.1666e-03,
          1.3205e-03, -4.3947e-04, -3.4085e-04]], device='cuda:3'))])
end of epoch 98: val_loss 3.244275164888677e-06, val_acc 1.0
trigger times: 1
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7003e-03, -1.2001e-04, -5.5411e-04,  4.5153e-04, -2.0269e-03,
         -3.4251e-04, -7.0390e-02, -1.2525e-03, -1.5380e-03, -8.5440e-04,
          1.3202e-03, -7.5824e-05,  2.8527e-03]], device='cuda:3'))])
Finished training.
0 -244.22134067479055 -67.12713112953104
1 -244.86375430366024 -67.12578005509693
2 -245.62299434049055 -66.93579213843823
3 -247.1120502264239 -66.90186314028179
4 -243.41866340627894 -66.68267514182082
5 -245.72103242669255 -66.22024700759806
6 -243.43148709041998 -66.1853212708828
7 -240.983896299731 -66.17029295970487
8 -246.64077725505922 -66.08150577388366
9 -243.29220512020402 -66.05968839191016
10 -243.18564323848113 -65.37420220884395
11 -237.9362661736086 -65.26234368947888
12 -244.77121888450347 -65.17525914707994
13 -240.9853657002095 -65.10965363160832
14 -245.40769989602268 -65.00918360597713
15 -241.59669961826876 -64.97710098500251
16 -238.906270254869 -64.94544339394874
17 -244.84767389600165 -64.92965080830456
18 -243.80899564130232 -64.58345540248003
19 -244.87441196176223 -64.55279941415036
20 -243.5629490383435 -64.50420711541688
21 -239.40903596626595 -64.49602012826797
22 -241.94067658437416 -64.48503429739685
23 -244.46092885988764 -64.41300136656521
24 -244.08550161658786 -64.22725555720297
25 -241.58757821656764 -64.21408847518106
26 -245.65380629547872 -64.17216566021338
27 -243.21358988131396 -64.13584040850124
28 -244.85096423095092 -64.12989242457422
29 -246.0196249927394 -64.12862711951551
30 -246.1429340384202 -64.09735505661433
31 -245.4090018060524 -64.0648246391729
32 -244.60626760683954 -64.04370518153469
33 -244.62208692287095 -64.01269836844845
34 -239.94045246299356 -63.88427119223866
35 -245.08926309854724 -63.82788906557428
36 -244.43125882861204 -63.82611956975095
37 -241.8210743588861 -63.757330483917684
38 -247.26133991079405 -63.66038350878433
39 -244.74001647648402 -63.61634412442828
40 -242.021275783889 -63.61369320946677
41 -242.2697608313756 -63.48675499590127
42 -245.6776297155302 -63.32198218320319
43 -239.29641167679802 -63.32121652631737
44 -243.53518933919258 -63.299186611486306
45 -245.8534297195729 -63.290789610610936
46 -246.2435606205836 -63.09965477843316
47 -242.5782935593743 -63.07640778269787
48 -243.99212163093034 -63.05017846269812
49 -242.12175128259696 -62.90033562835867
50 -241.78032162971795 -62.741614170355874
51 -242.46287373173982 -62.68188134258881
52 -243.7023219622206 -62.67650638755237
53 -244.12427636282519 -62.63090968175874
54 -244.6868241627235 -62.51468237107452
55 -242.76142805838026 -62.513116242886774
56 -241.3960670104716 -62.13756480575132
57 -242.4336187816225 -61.84122885690816
58 -243.91043724259362 -61.69391298799091
59 -243.993282396812 -61.522992611489414
60 -28.68347804772202 -50.492268601198035
61 -28.153592014452443 -50.03933801517046
62 2.103660559747368 -45.670579884154705
63 -39.88590781344101 -44.14602409201361
64 -9.237120961188339 -38.41270390343083
65 18.383086416637525 -31.969099402548657
66 -11.196201522252522 -26.7047217556024
67 -3.3260413601528853 -24.879106999799365
68 -5.2048465717816725 -20.14639340520683
69 1.4509501524735242 -20.13839114930498
70 -9.212235442595556 -17.55742370467821
71 -7.993019442539662 -16.524262196349287
72 -8.871529614785686 -16.14121158081339
73 -6.752529806108214 -15.896940640588715
74 -8.83057987561915 -15.830038444290448
75 -5.489201759453863 -15.342684972390225
76 1.997751161805354 -14.531424598833084
77 12.734890572959557 -14.442420089224363
78 -8.183652894338593 -14.417376523530542
79 -8.931427633506246 -12.76355044993674
80 -2.744817593600601 -12.68135972540495
81 -9.523507889127359 -12.385101738522765
82 -5.796579118235968 -12.151904772081672
83 -9.307704464066774 -12.131739920967043
84 -8.798286932054907 -12.02598574203919
85 -9.740106463432312 -11.3585492926771
86 -11.571944501949474 -10.11136897706654
87 -7.1970544416690245 -10.026213357004814
88 -7.060128020704724 -9.836412538439424
89 -6.169002634705976 -9.768005957417687
90 -5.241653617704287 -9.48435448123352
91 -6.711144919274375 -9.233511325902679
92 -9.125979226606432 -8.747640159538284
93 -11.071588726714253 -8.62111937592296
94 -11.684816315537319 -8.426220566128212
95 -6.065098854945973 -8.308874624013281
96 8.480133977835067 -8.247873383953989
97 -1.3060201091575436 -8.133195842510668
98 -10.259683095151559 -7.564905881747173
99 -8.416723521309905 -7.363702196278379
100 -9.044154526898637 -6.727815877609332
101 -4.854848402086645 -6.578906457558751
102 -2.7687237341306172 -6.270721602451359
103 -5.509528335183859 -6.098997331083035
104 8.023467409657314 -6.044975028976545
105 5.594542191945948 -5.859076913422368
106 5.769887791073415 -5.846060443208046
107 -5.261638478958048 -5.781098288441135
108 -7.835410680039786 -5.674256129061747
109 -0.72931582655292 -5.402130403881674
110 -0.7329361027805135 -5.283843628144702
111 -7.2005576618539635 -5.202063760547981
112 -7.3835047335596755 -5.027957977402961
113 0.2494662636599969 -4.960498155357883
114 -5.940561139665078 -4.809254810564498
115 -4.449158558971249 -4.125252829103858
116 -2.772196183213964 -4.0942071248829865
117 -2.993177885597106 -3.9397105602482654
118 -4.833826600108296 -3.206151970713792
119 -0.9741174660157412 -1.739663281790734
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -110.7546746  -110.36543266 -110.35299605 -110.33801696
 -110.29927539 -110.26549485 -110.02721186 -109.8988298  -109.79936793
 -109.79148147 -109.74172827 -109.70897659 -109.19206019 -109.08967489
 -108.89087532 -108.82374781 -108.81083757 -108.71124898 -108.5458264
 -108.45700366 -108.31581469 -108.12053473 -108.01805119 -108.0161428
 -107.70817146 -107.67837949 -107.6761845  -107.52220794 -107.26673763
 -107.21870368 -107.20060036 -106.96224593 -106.77350902 -106.35469037
 -106.26741762 -106.17722508 -106.12986975 -106.11224682 -105.98763553
 -105.89116676 -105.72064989 -105.67858096 -105.6600533  -105.64353104
 -105.55355661 -105.42468713 -105.41153983 -105.20537932 -105.17486273
 -104.98414916 -104.75574551 -104.34422138 -104.2527155  -104.03978747
 -103.72990561 -103.62228556 -103.53627782 -103.0461665  -102.33593583
  -67.12578006  -66.93579214  -66.90186314  -66.18532127  -66.08150577
  -65.37420221  -65.10965363  -65.00918361  -64.55279941  -64.50420712
  -64.49602013  -64.17216566  -64.13584041  -64.12989242  -64.09735506
  -64.06482464  -64.04370518  -63.88427119  -63.82788907  -63.75733048
  -63.32121653  -63.29918661  -63.29078961  -63.07640778  -63.05017846
  -62.90033563  -62.67650639  -62.63090968  -62.51468237  -62.13756481
  -50.4922686   -38.4127039   -31.9690994   -24.879107    -20.14639341
  -20.13839115  -17.5574237   -16.5242622   -15.89694064  -14.44242009
  -14.41737652  -12.68135973  -12.13173992  -12.02598574   -9.76800596
   -9.48435448   -8.62111938   -8.30887462   -8.13319584   -7.56490588
   -6.72781588   -6.2707216    -6.09899733   -6.04497503   -5.85907691
   -5.4021304    -5.28384363   -4.12525283   -4.09420712   -3.20615197]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 8.451835652522278e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.4979e-04,  1.7025e-04,  5.7403e-04,  1.1065e-04,  6.2272e-05,
          1.6448e-03, -2.9708e-02,  2.3929e-02,  2.7199e-04,  1.1420e-04,
         -2.3640e-03,  5.7986e-04, -1.2424e-04]], device='cuda:1'))])
end of epoch 1: val_loss 5.637786509396392e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.9387e-04,  1.7634e-04, -7.1171e-04,  8.5501e-04, -8.8416e-05,
          8.3121e-04, -1.6009e-02,  3.9339e-02,  5.3669e-04,  1.0377e-04,
          2.0542e-03, -6.1667e-06, -2.9984e-04]], device='cuda:1'))])
end of epoch 2: val_loss 2.5748967200911467e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0010,  0.0002,  0.0015,  0.0011,  0.0002,  0.0007, -0.0355,  0.0420,
         -0.0008,  0.0004, -0.0011, -0.0016,  0.0002]], device='cuda:1'))])
end of epoch 3: val_loss 7.039293225830079e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0012,  0.0170, -0.0002,  0.0040, -0.0027, -0.0004, -0.0212,  0.0527,
          0.0007, -0.0002, -0.0007,  0.0005, -0.0110]], device='cuda:1'))])
end of epoch 4: val_loss 2.4244882230277653e-05, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 3.574518887035083e-05, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 9.048093583260198e-05, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 4.7377492192026696e-05, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 2.0955835871063756e-05, val_acc 1.0
trigger times: 5
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.0035e-04, -1.9548e-05, -7.8836e-04, -1.0433e-03,  8.2481e-04,
          1.4585e-03, -1.0000e-01,  8.7171e-02, -2.6837e-03, -2.0540e-04,
          5.2057e-04,  1.7192e-03,  4.6490e-04]], device='cuda:1'))])
end of epoch 10: val_loss 2.789588226733031e-05, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 7.681142924411687e-05, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 0.0002448816979449475, val_acc 1.0
trigger times: 3
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6398e-03, -1.4746e-04, -3.0621e-03, -1.0564e-04, -2.1561e-04,
         -1.1396e-03, -8.8626e-02,  7.5539e-02,  4.1938e-05,  8.4413e-04,
         -1.1136e-03, -9.8578e-04,  9.1351e-04]], device='cuda:1'))])
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.8793e-04, -1.5032e-04, -1.5474e-04, -1.0773e-03, -2.5123e-03,
         -3.7269e-04, -2.1765e-01,  4.9899e-04, -1.5988e-03, -1.0091e-04,
         -7.3268e-04, -6.2195e-05, -2.4116e-04]], device='cuda:1'))])
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0019,  0.0937, -0.0015, -0.0027,  0.0008,  0.0016, -0.0440,  0.1125,
         -0.0006, -0.0019, -0.0002,  0.0028, -0.0214]], device='cuda:1'))])
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0008,  0.1237,  0.0029,  0.0264,  0.0180,  0.0427, -0.0744,  0.1463,
         -0.0354, -0.0498,  0.0007, -0.0018, -0.0950]], device='cuda:1'))])
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.2663e-05, -2.0577e-05,  3.0586e-03, -7.5883e-05, -8.9028e-04,
          6.3072e-04, -2.8386e-01, -2.3184e-04, -2.0277e-03, -1.0878e-03,
         -1.4886e-03,  1.9156e-03, -3.0292e-04]], device='cuda:1'))])
end of epoch 18: val_loss 1.4254396849082695e-05, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 3.397464301002628e-08, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 9.536742595628311e-09, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 4.6318835757119814e-05, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 1.2350023206764149e-06, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 5.159290816663997e-05, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 3.5196623730371354e-05, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 2.6657999496819686e-05, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 0.0003764001408126205, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 3.883199550500649e-06, val_acc 1.0
trigger times: 10
Early stopping.
0 -67.05855619907379 -111.1509139312174
1 -67.41146943811327 -110.7546746021886
2 -67.46673773229122 -110.36543265943791
3 -67.03124422114342 -110.35299605293886
4 -67.14984011463821 -110.33801695908483
5 -67.01390881836414 -110.2992753922497
6 -67.48455157969147 -110.26549484761443
7 -67.47204199712723 -110.0272118598093
8 -67.10942041128874 -109.8988298040389
9 -66.98207682836801 -109.79936792792112
10 -67.471480329521 -109.79148147499572
11 -67.04996481537819 -109.74172827034793
12 -67.22669099085033 -109.70897659338503
13 -67.00333932135254 -109.192060192308
14 -67.46426140982658 -109.08967488557313
15 -67.06601993180811 -108.8908753244873
16 -67.06216787733138 -108.82374780685433
17 -67.23488716501743 -108.81083756860734
18 -67.39122072327882 -108.71124898270375
19 -67.10180407203734 -108.54582640198474
20 -67.23001562524587 -108.45700365695856
21 -66.9958950933069 -108.31581468771003
22 -67.46669790241867 -108.12053472856175
23 -67.0368400644511 -108.0180511928408
24 -66.98185504879802 -108.0161428018986
25 -67.48109586071223 -107.70817145882172
26 -67.48058870434761 -107.67837948595889
27 -67.0441124336794 -107.67618449636431
28 -67.46429134812206 -107.52220794033053
29 -67.04264118988067 -107.26673763034924
30 -67.1698411507532 -107.2187036786349
31 -67.1578579749912 -107.20060036202784
32 -67.40619205217808 -106.96224593171276
33 -67.46395639982074 -106.77350901919232
34 -67.14486277289689 -106.35469036504782
35 -67.46603598911315 -106.26741762098152
36 -67.14698032476008 -106.17722508081916
37 -67.07923329714686 -106.12986974696594
38 -67.47090535517782 -106.11224681921385
39 -67.09518466517329 -105.98763553383849
40 -67.25854421500117 -105.89116676488838
41 -67.48220350686461 -105.72064989124367
42 -67.08050528541207 -105.67858096296958
43 -67.21048575174063 -105.66005329834891
44 -67.08830109704286 -105.64353103691755
45 -67.06005456857383 -105.55355660604569
46 -67.40302457660437 -105.4246871281226
47 -67.47356831748039 -105.41153983260888
48 -67.16298472043127 -105.205379324443
49 -67.18243713583797 -105.1748627281623
50 -67.26057383231819 -104.98414916296157
51 -67.43979973625392 -104.75574550925394
52 -67.03680024202913 -104.34422137718992
53 -67.08963097445667 -104.25271549639196
54 -67.25044697429985 -104.03978746733507
55 -67.17966014053673 -103.7299056083423
56 -67.21640228386968 -103.62228556353936
57 -67.20880216546357 -103.53627782194874
58 -67.26697308197618 -103.0461664951788
59 -67.24653496034443 -102.3359358284459
60 -57.036257511004806 -67.12578005509693
61 -56.41288392338902 -66.93579213843823
62 -57.201004004105926 -66.90186314028179
63 -55.79629528988153 -66.1853212708828
64 -56.337175361812115 -66.08150577388366
65 -56.23746880795807 -65.37420220884395
66 -55.80322656966746 -65.10965363160832
67 -56.58620916400105 -65.00918360597713
68 -56.63761404808611 -64.55279941415036
69 -56.469206298701465 -64.50420711541688
70 -53.96614241041243 -64.49602012826797
71 -56.7870886111632 -64.17216566021338
72 -55.700094850733876 -64.13584040850124
73 -56.50555674172938 -64.12989242457422
74 -56.64542439207435 -64.09735505661433
75 -57.2002493198961 -64.0648246391729
76 -55.51417364925146 -64.04370518153469
77 -55.05881801433861 -63.88427119223866
78 -56.44123982451856 -63.82788906557428
79 -55.93348253238946 -63.757330483917684
80 -54.59606904350221 -63.32121652631737
81 -55.889872489497066 -63.299186611486306
82 -56.26644376292825 -63.290789610610936
83 -55.616175659932196 -63.07640778269787
84 -56.48640984820668 -63.05017846269812
85 -55.39600722771138 -62.90033562835867
86 -56.01815338153392 -62.67650638755237
87 -56.13965705689043 -62.63090968175874
88 -56.45610609091818 -62.51468237107452
89 -55.66164272278547 -62.13756480575132
90 -13.660028799436986 -50.492268601198035
91 -9.337340225465596 -38.41270390343083
92 3.0833006286993623 -31.969099402548657
93 6.4209331347374246 -24.879106999799365
94 -1.00349663857196 -20.14639340520683
95 -3.6017676647752523 -20.13839114930498
96 -8.109133522491902 -17.55742370467821
97 -0.5836710558505729 -16.524262196349287
98 -1.5671248543076217 -15.896940640588715
99 7.184392087161541 -14.442420089224363
100 -2.7506871668156236 -14.417376523530542
101 4.702339103678241 -12.68135972540495
102 -1.7493823091499507 -12.131739920967043
103 -2.571833073423477 -12.02598574203919
104 -9.460274109151214 -9.768005957417687
105 -1.7122575998073444 -9.48435448123352
106 -2.7743408183159772 -8.62111937592296
107 -8.989704752340913 -8.308874624013281
108 -3.346477316925302 -8.133195842510668
109 -3.6787672302452847 -7.564905881747173
110 -4.000780841335654 -6.727815877609332
111 -5.501833695801906 -6.270721602451359
112 -8.898172291577794 -6.098997331083035
113 3.3639074091333896 -6.044975028976545
114 3.3544498779810965 -5.859076913422368
115 -3.411277112085372 -5.402130403881674
116 -3.3382359833922237 -5.283843628144702
117 -5.973728477401892 -4.125252829103858
118 -4.940960564184934 -4.0942071248829865
119 -4.932765000092331 -3.206151970713792
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -110.7546746  -110.7546746  -110.36543266
 -110.35299605 -110.33801696 -110.33801696 -110.29927539 -110.26549485
 -110.02721186 -109.8988298  -109.79936793 -109.79936793 -109.79148147
 -109.79148147 -109.74172827 -109.70897659 -109.19206019 -109.19206019
 -109.08967489 -109.08967489 -108.89087532 -108.89087532 -108.82374781
 -108.81083757 -108.81083757 -108.71124898 -108.71124898 -108.5458264
 -108.45700366 -108.31581469 -108.31581469 -108.12053473 -108.01805119
 -108.01805119 -108.0161428  -107.70817146 -107.70817146 -107.67837949
 -107.6761845  -107.52220794 -107.26673763 -107.26673763 -107.21870368
 -107.21870368 -107.20060036 -107.20060036 -106.96224593 -106.77350902
 -106.77350902 -106.35469037 -106.26741762 -106.26741762 -106.17722508
 -106.17722508 -106.12986975 -106.11224682 -106.11224682 -105.98763553
 -105.89116676 -105.72064989 -105.72064989 -105.67858096 -105.67858096
 -105.6600533  -105.64353104 -105.64353104 -105.55355661 -105.55355661
 -105.42468713 -105.42468713 -105.41153983 -105.20537932 -105.20537932
 -105.17486273 -104.98414916 -104.75574551 -104.75574551 -104.34422138
 -104.2527155  -104.2527155  -104.03978747 -103.72990561 -103.72990561
 -103.62228556 -103.53627782 -103.0461665  -103.0461665  -102.33593583
  -66.90186314  -65.00918361  -64.55279941  -64.50420712  -64.49602013
  -64.17216566  -64.13584041  -64.06482464  -64.04370518  -63.32121653
  -63.29918661  -63.05017846  -62.90033563  -62.67650639  -38.4127039
  -20.14639341  -20.13839115  -17.5574237   -16.5242622   -14.41737652
  -12.68135973   -9.48435448   -8.30887462   -8.13319584   -6.72781588
   -6.04497503   -5.85907691   -5.28384363   -4.12525283   -4.09420712]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.001923022252004074, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0768e-03,  1.9414e-02, -1.2905e-03, -2.2785e-03, -3.8661e-04,
          2.6662e-03, -8.7706e-02,  2.2126e-02,  1.1407e-04, -3.1098e-03,
         -1.0853e-03, -8.8291e-01,  2.0102e-03]], device='cuda:1'))])
end of epoch 1: val_loss 0.0017193762279379143, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0840e-03,  8.0734e-03, -1.8684e-03,  1.0510e-03,  1.8009e-02,
         -4.6577e-03, -7.2265e-02,  6.0007e-03, -2.0730e-02,  5.2376e-03,
         -5.8882e-04, -8.7905e-01, -2.3249e-03]], device='cuda:1'))])
end of epoch 2: val_loss 0.0016980414484024564, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0012,  0.0073, -0.0027, -0.0023,  0.0273, -0.0015, -0.0846,  0.0288,
         -0.0296,  0.0041,  0.0025, -0.8753, -0.0014]], device='cuda:1'))])
end of epoch 3: val_loss 0.003269066227367148, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.002254489385156262, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.0017452946970936622, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 0.0010045727684280337, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7808e-03,  7.7713e-03,  7.0659e-05, -1.1687e-03,  2.8536e-02,
          2.4444e-02, -5.0248e-02,  2.7449e-02, -2.9159e-02, -2.3261e-02,
          1.1664e-03, -9.8710e-01, -3.5811e-04]], device='cuda:1'))])
end of epoch 7: val_loss 0.0016703675476288638, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.001366550739403465, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 0.0027673477947246284, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.0017922816754708038, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.0016472850131356153, val_acc 1.0
trigger times: 5
end of epoch 12: val_loss 0.0021230379391881018, val_acc 1.0
trigger times: 6
end of epoch 13: val_loss 0.001494907870219322, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 0.0021625998651635214, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 0.0018352060229311462, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 0.001958875163072662, val_acc 1.0
trigger times: 10
Early stopping.
0 -232.10533744096756 -111.1509139312174
1 -232.10533744096756 -111.1509139312174
2 -232.14202788472176 -110.7546746021886
3 -232.14202788472176 -110.7546746021886
4 -231.2296569943428 -110.36543265943791
5 -231.2918508052826 -110.35299605293886
6 -231.2676559984684 -110.33801695908483
7 -231.2676559984684 -110.33801695908483
8 -231.4693808555603 -110.2992753922497
9 -231.35939079523087 -110.26549484761443
10 -231.49180588126183 -110.0272118598093
11 -230.93326979875565 -109.8988298040389
12 -231.1472468972206 -109.79936792792112
13 -231.1472468972206 -109.79936792792112
14 -231.18820130825043 -109.79148147499572
15 -231.18820130825043 -109.79148147499572
16 -230.66550368070602 -109.74172827034793
17 -230.7455890774727 -109.70897659338503
18 -230.17442275583744 -109.192060192308
19 -230.17442275583744 -109.192060192308
20 -229.96032790839672 -109.08967488557313
21 -229.96032790839672 -109.08967488557313
22 -230.27883970737457 -108.8908753244873
23 -230.27883970737457 -108.8908753244873
24 -230.2953515946865 -108.82374780685433
25 -230.14334172010422 -108.81083756860734
26 -230.14334172010422 -108.81083756860734
27 -229.2173338793218 -108.71124898270375
28 -229.2173338793218 -108.71124898270375
29 -230.06098699569702 -108.54582640198474
30 -230.00166469812393 -108.45700365695856
31 -229.36410230398178 -108.31581468771003
32 -229.36410230398178 -108.31581468771003
33 -229.759794652462 -108.12053472856175
34 -228.89509619772434 -108.0180511928408
35 -228.89509619772434 -108.0180511928408
36 -228.56801843643188 -108.0161428018986
37 -228.7222982198 -107.70817145882172
38 -228.7222982198 -107.70817145882172
39 -229.36063015460968 -107.67837948595889
40 -229.16557630896568 -107.67618449636431
41 -228.53707921504974 -107.52220794033053
42 -228.26480741798878 -107.26673763034924
43 -228.26480741798878 -107.26673763034924
44 -228.85899382829666 -107.2187036786349
45 -228.85899382829666 -107.2187036786349
46 -228.37154115736485 -107.20060036202784
47 -228.37154115736485 -107.20060036202784
48 -227.85842269286513 -106.96224593171276
49 -228.00369456410408 -106.77350901919232
50 -228.00369456410408 -106.77350901919232
51 -228.05592396855354 -106.35469036504782
52 -227.3952475041151 -106.26741762098152
53 -227.3952475041151 -106.26741762098152
54 -227.28572491556406 -106.17722508081916
55 -227.28572491556406 -106.17722508081916
56 -227.15782774984837 -106.12986974696594
57 -227.31826588511467 -106.11224681921385
58 -227.31826588511467 -106.11224681921385
59 -227.38791178166866 -105.98763553383849
60 -227.53445303440094 -105.89116676488838
61 -227.56928846240044 -105.72064989124367
62 -227.56928846240044 -105.72064989124367
63 -227.15419647097588 -105.67858096296958
64 -227.15419647097588 -105.67858096296958
65 -227.44421237707138 -105.66005329834891
66 -227.0933908969164 -105.64353103691755
67 -227.0933908969164 -105.64353103691755
68 -227.31694278120995 -105.55355660604569
69 -227.31694278120995 -105.55355660604569
70 -227.29069820046425 -105.4246871281226
71 -227.29069820046425 -105.4246871281226
72 -226.78119694441557 -105.41153983260888
73 -226.90653237700462 -105.205379324443
74 -226.90653237700462 -105.205379324443
75 -226.4564202427864 -105.1748627281623
76 -226.72149427235126 -104.98414916296157
77 -226.20853972434998 -104.75574550925394
78 -226.20853972434998 -104.75574550925394
79 -225.80397754907608 -104.34422137718992
80 -226.1434894502163 -104.25271549639196
81 -226.1434894502163 -104.25271549639196
82 -225.65022939443588 -104.03978746733507
83 -225.48692728579044 -103.7299056083423
84 -225.48692728579044 -103.7299056083423
85 -225.58448024094105 -103.62228556353936
86 -225.5221881866455 -103.53627782194874
87 -224.94461680948734 -103.0461664951788
88 -224.94461680948734 -103.0461664951788
89 -224.40926730632782 -102.3359358284459
90 -222.97779928147793 -66.90186314028179
91 -221.75358387827873 -65.00918360597713
92 -221.806530803442 -64.55279941415036
93 -221.60368892550468 -64.50420711541688
94 -218.24768301844597 -64.49602012826797
95 -223.98662254214287 -64.17216566021338
96 -220.64259208738804 -64.13584040850124
97 -222.634873598814 -64.0648246391729
98 -221.93902705609798 -64.04370518153469
99 -219.07532215118408 -63.32121652631737
100 -219.65226905047894 -63.299186611486306
101 -220.4141838401556 -63.05017846269812
102 -219.31939059495926 -62.90033562835867
103 -220.48946076631546 -62.67650638755237
104 -18.063255610177293 -38.41270390343083
105 -21.45764496922493 -20.14639340520683
106 -3.5617532332544215 -20.13839114930498
107 -13.978599952184595 -17.55742370467821
108 -19.731954395771027 -16.524262196349287
109 -18.455662600696087 -14.417376523530542
110 -5.785797538235784 -12.68135972540495
111 -12.596396531909704 -9.48435448123352
112 -13.316964909434319 -8.308874624013281
113 -3.818382272031158 -8.133195842510668
114 -12.834036632673815 -6.727815877609332
115 3.0358848070027307 -6.044975028976545
116 1.2146888691931963 -5.859076913422368
117 -5.1233073842013255 -5.283843628144702
118 -7.833431892096996 -4.125252829103858
119 -6.2804003693163395 -4.0942071248829865
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -111.15091393 -110.7546746  -110.7546746
 -110.36543266 -110.35299605 -110.33801696 -110.33801696 -110.29927539
 -110.26549485 -110.02721186 -109.8988298  -109.79936793 -109.79936793
 -109.79936793 -109.79148147 -109.79148147 -109.79148147 -109.74172827
 -109.70897659 -109.19206019 -109.19206019 -109.08967489 -109.08967489
 -108.89087532 -108.89087532 -108.82374781 -108.81083757 -108.81083757
 -108.81083757 -108.71124898 -108.71124898 -108.5458264  -108.45700366
 -108.31581469 -108.31581469 -108.31581469 -108.12053473 -108.01805119
 -108.01805119 -108.01805119 -108.0161428  -107.70817146 -107.70817146
 -107.67837949 -107.6761845  -107.52220794 -107.26673763 -107.26673763
 -107.26673763 -107.21870368 -107.21870368 -107.20060036 -107.20060036
 -106.96224593 -106.77350902 -106.77350902 -106.77350902 -106.35469037
 -106.26741762 -106.26741762 -106.26741762 -106.17722508 -106.17722508
 -106.12986975 -106.11224682 -106.11224682 -106.11224682 -105.98763553
 -105.89116676 -105.72064989 -105.72064989 -105.72064989 -105.67858096
 -105.67858096 -105.6600533  -105.64353104 -105.64353104 -105.55355661
 -105.55355661 -105.42468713 -105.42468713 -105.41153983 -105.20537932
 -105.20537932 -105.17486273 -104.98414916 -104.75574551 -104.75574551
 -104.75574551 -104.34422138 -104.2527155  -104.2527155  -104.2527155
 -104.03978747 -103.72990561 -103.72990561 -103.62228556 -103.53627782
 -103.0461665  -103.0461665  -103.0461665  -102.33593583  -65.00918361
  -64.50420712  -64.04370518  -63.32121653  -63.29918661  -62.90033563
  -62.67650639  -38.4127039   -20.14639341  -20.13839115  -12.68135973
   -9.48435448   -6.72781588   -6.04497503   -5.85907691   -4.09420712]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.002131392669014076, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.8116e-05,  1.0108e-02, -9.4750e-07, -9.2390e-06, -2.2594e-04,
          1.6030e-02, -6.1774e-02,  8.2992e-03,  1.6710e-04, -1.3340e-02,
          9.8291e-06, -1.1226e+00, -6.5712e-05]], device='cuda:1'))])
end of epoch 1: val_loss 0.0019510841684132175, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6241e-04,  1.9427e-06,  1.0508e-04, -1.3527e-04, -3.4612e-04,
         -3.2220e-03, -6.1994e-02,  3.0102e-03, -2.0733e-04,  6.2084e-03,
         -1.5958e-03, -1.1530e+00,  1.5296e-05]], device='cuda:1'))])
end of epoch 2: val_loss 0.002012883917223007, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.0021174076941136376, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.002401914728154679, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.0020970521982387426, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 0.0019214913512769272, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.5123e-03,  5.6701e-02, -4.9660e-03,  1.3422e-04,  3.5549e-02,
         -1.6130e-02, -1.0701e-01,  5.0254e-02, -4.0779e-02,  1.3043e-02,
         -8.7433e-04, -1.0998e+00,  8.7865e-05]], device='cuda:1'))])
end of epoch 7: val_loss 0.0019565952241418928, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.0019840626315259157, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 0.002076289993528917, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.0017582577030888303, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.8406e-03,  9.9399e-03,  1.1398e-03,  3.2199e-04,  1.9530e-02,
         -1.0130e-02, -7.6728e-02,  1.5745e-02, -2.7863e-02,  8.2365e-03,
          5.4036e-04, -1.1216e+00, -1.1316e-03]], device='cuda:1'))])
end of epoch 11: val_loss 0.001783888732163632, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.003302159732120344, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.0024273353693820355, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.0021608519698020244, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.0013946689001113554, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.1449e-04, -1.2202e-03,  8.6338e-04,  2.6620e-04,  3.5509e-02,
         -1.3862e-02, -6.1612e-02,  5.8489e-03, -3.9366e-02,  1.4691e-02,
         -1.5950e-03, -1.1843e+00,  1.7720e-05]], device='cuda:1'))])
end of epoch 16: val_loss 0.0029413676143303746, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.002351951624094362, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.002039652463749455, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0015973422556271543, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.0020306356735341068, val_acc 1.0
trigger times: 5
end of epoch 21: val_loss 0.002215930074744392, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.0017329812514435261, val_acc 1.0
trigger times: 7
end of epoch 23: val_loss 0.0022701347540077777, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 0.0021473844938009277, val_acc 1.0
trigger times: 9
end of epoch 25: val_loss 0.0017424607820430182, val_acc 1.0
trigger times: 10
Early stopping.
0 -348.5052601993084 -111.1509139312174
1 -348.5052601993084 -111.1509139312174
2 -348.5052601993084 -111.1509139312174
3 -348.76977476477623 -110.7546746021886
4 -348.76977476477623 -110.7546746021886
5 -347.3596212863922 -110.36543265943791
6 -347.3775732219219 -110.35299605293886
7 -347.2822978794575 -110.33801695908483
8 -347.2822978794575 -110.33801695908483
9 -347.64768823981285 -110.2992753922497
10 -347.6288534104824 -110.26549484761443
11 -347.92872846126556 -110.0272118598093
12 -346.9407591223717 -109.8988298040389
13 -347.29532822966576 -109.79936792792112
14 -347.29532822966576 -109.79936792792112
15 -347.29532822966576 -109.79936792792112
16 -347.45960983633995 -109.79148147499572
17 -347.45960983633995 -109.79148147499572
18 -347.45960983633995 -109.79148147499572
19 -346.43438133597374 -109.74172827034793
20 -346.62958627939224 -109.70897659338503
21 -345.8566506803036 -109.192060192308
22 -345.8566506803036 -109.192060192308
23 -345.540081307292 -109.08967488557313
24 -345.540081307292 -109.08967488557313
25 -346.16351902484894 -108.8908753244873
26 -346.16351902484894 -108.8908753244873
27 -346.1973733305931 -108.82374780685433
28 -345.9398006796837 -108.81083756860734
29 -345.9398006796837 -108.81083756860734
30 -345.9398006796837 -108.81083756860734
31 -344.4042043609079 -108.71124898270375
32 -344.4042043609079 -108.71124898270375
33 -345.9066434800625 -108.54582640198474
34 -345.8654894530773 -108.45700365695856
35 -344.76986257731915 -108.31581468771003
36 -344.76986257731915 -108.31581468771003
37 -344.76986257731915 -108.31581468771003
38 -345.6267056465149 -108.12053472856175
39 -344.0613584816456 -108.0180511928408
40 -344.0613584816456 -108.0180511928408
41 -344.0613584816456 -108.0180511928408
42 -343.42516576498747 -108.0161428018986
43 -343.97294630110264 -107.70817145882172
44 -343.97294630110264 -107.70817145882172
45 -345.1013525724411 -107.67837948595889
46 -344.6358135342598 -107.67618449636431
47 -343.65745571255684 -107.52220794033053
48 -343.2458080947399 -107.26673763034924
49 -343.2458080947399 -107.26673763034924
50 -343.2458080947399 -107.26673763034924
51 -344.32937654852867 -107.2187036786349
52 -344.32937654852867 -107.2187036786349
53 -343.42881543934345 -107.20060036202784
54 -343.42881543934345 -107.20060036202784
55 -342.70222103223205 -106.96224593171276
56 -343.0693219602108 -106.77350901919232
57 -343.0693219602108 -106.77350901919232
58 -343.0693219602108 -106.77350901919232
59 -343.25375404953957 -106.35469036504782
60 -342.15773540735245 -106.26741762098152
61 -342.15773540735245 -106.26741762098152
62 -342.15773540735245 -106.26741762098152
63 -341.9618670344353 -106.17722508081916
64 -341.9618670344353 -106.17722508081916
65 -341.71282528713346 -106.12986974696594
66 -342.11213329434395 -106.11224681921385
67 -342.11213329434395 -106.11224681921385
68 -342.11213329434395 -106.11224681921385
69 -342.17187498509884 -105.98763553383849
70 -342.54677334427834 -105.89116676488838
71 -342.71916514635086 -105.72064989124367
72 -342.71916514635086 -105.72064989124367
73 -342.71916514635086 -105.72064989124367
74 -341.8845707178116 -105.67858096296958
75 -341.8845707178116 -105.67858096296958
76 -342.4633481800556 -105.66005329834891
77 -341.8346563130617 -105.64353103691755
78 -341.8346563130617 -105.64353103691755
79 -342.23309302330017 -105.55355660604569
80 -342.23309302330017 -105.55355660604569
81 -342.3218791782856 -105.4246871281226
82 -342.3218791782856 -105.4246871281226
83 -341.41635812819004 -105.41153983260888
84 -341.66033160686493 -105.205379324443
85 -341.66033160686493 -105.205379324443
86 -340.89491622895 -105.1748627281623
87 -341.44304236769676 -104.98414916296157
88 -340.67646215856075 -104.75574550925394
89 -340.67646215856075 -104.75574550925394
90 -340.67646215856075 -104.75574550925394
91 -340.0463933944702 -104.34422137718992
92 -340.6966583430767 -104.25271549639196
93 -340.6966583430767 -104.25271549639196
94 -340.6966583430767 -104.25271549639196
95 -339.93796722590923 -104.03978746733507
96 -339.7657287120819 -103.7299056083423
97 -339.7657287120819 -103.7299056083423
98 -339.97419859468937 -103.62228556353936
99 -339.90898156166077 -103.53627782194874
100 -339.1001886278391 -103.0461664951788
101 -339.1001886278391 -103.0461664951788
102 -339.1001886278391 -103.0461664951788
103 -338.4246705621481 -102.3359358284459
104 -327.6993710845709 -65.00918360597713
105 -327.5625764429569 -64.50420711541688
106 -327.7137612849474 -64.04370518153469
107 -323.3178525865078 -63.32121652631737
108 -324.57802603393793 -63.299186611486306
109 -323.89304235577583 -62.90033562835867
110 -325.9643570482731 -62.67650638755237
111 -26.9353756015189 -38.41270390343083
112 -27.294613003730774 -20.14639340520683
113 -4.691813106648624 -20.13839114930498
114 -6.200231937691569 -12.68135972540495
115 -16.015003563836217 -9.48435448123352
116 -17.845950907561928 -6.727815877609332
117 6.952688872581348 -6.044975028976545
118 4.419491415144876 -5.859076913422368
119 -9.534989604726434 -4.0942071248829865
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -111.15091393 -111.15091393 -110.7546746
 -110.7546746  -110.36543266 -110.35299605 -110.33801696 -110.33801696
 -110.29927539 -110.26549485 -110.02721186 -109.8988298  -109.79936793
 -109.79936793 -109.79936793 -109.79148147 -109.79148147 -109.79148147
 -109.74172827 -109.70897659 -109.19206019 -109.19206019 -109.08967489
 -109.08967489 -108.89087532 -108.89087532 -108.82374781 -108.81083757
 -108.81083757 -108.81083757 -108.81083757 -108.71124898 -108.71124898
 -108.5458264  -108.45700366 -108.31581469 -108.31581469 -108.31581469
 -108.31581469 -108.12053473 -108.01805119 -108.01805119 -108.01805119
 -108.0161428  -107.70817146 -107.70817146 -107.67837949 -107.6761845
 -107.52220794 -107.26673763 -107.26673763 -107.26673763 -107.26673763
 -107.21870368 -107.21870368 -107.20060036 -107.20060036 -106.96224593
 -106.77350902 -106.77350902 -106.77350902 -106.35469037 -106.26741762
 -106.26741762 -106.26741762 -106.17722508 -106.17722508 -106.12986975
 -106.11224682 -106.11224682 -106.11224682 -106.11224682 -105.98763553
 -105.89116676 -105.72064989 -105.72064989 -105.72064989 -105.67858096
 -105.67858096 -105.6600533  -105.64353104 -105.64353104 -105.55355661
 -105.55355661 -105.42468713 -105.42468713 -105.41153983 -105.20537932
 -105.20537932 -105.17486273 -104.98414916 -104.75574551 -104.75574551
 -104.75574551 -104.75574551 -104.34422138 -104.2527155  -104.2527155
 -104.2527155  -104.2527155  -104.03978747 -103.72990561 -103.72990561
 -103.62228556 -103.53627782 -103.0461665  -103.0461665  -103.0461665
 -102.33593583  -65.00918361  -64.50420712  -64.04370518  -63.32121653
  -38.4127039   -20.14639341   -9.48435448   -6.72781588   -6.04497503]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.002201115040492141, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.8304e-07,  2.7345e-02,  7.1354e-06,  9.3464e-05,  7.6969e-03,
          3.6151e-03, -5.4641e-02,  3.5367e-02, -5.7856e-03, -4.4865e-07,
         -4.5252e-04, -1.2219e+00,  1.7870e-04]], device='cuda:1'))])
end of epoch 1: val_loss 0.002249509991306695, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.002275204707693774, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.002056019843330432, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.5350e-04,  2.5029e-04, -7.4601e-04,  4.2849e-04,  3.0129e-02,
         -8.2826e-03, -6.3591e-02,  5.4704e-03, -3.6929e-02,  8.3763e-03,
         -1.1782e-03, -1.2143e+00,  1.7996e-04]], device='cuda:1'))])
end of epoch 4: val_loss 0.0022561315763232415, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.0019803210131805215, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.9939e-05,  3.5569e-02,  2.0826e-04,  9.6545e-04,  2.9914e-02,
         -4.0260e-04, -7.2996e-02,  2.3820e-02, -3.4828e-02, -3.7485e-03,
          1.1068e-03, -1.2035e+00,  1.1316e-03]], device='cuda:1'))])
end of epoch 6: val_loss 0.0022784011421754257, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0018996921913202415, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2659e-03,  7.8634e-05, -1.7420e-04,  2.9091e-04,  1.8765e-02,
          1.3732e-03, -8.4408e-02, -7.5156e-04, -1.4829e-02, -2.3624e-03,
         -4.5290e-04, -1.2257e+00,  1.0853e-03]], device='cuda:1'))])
end of epoch 8: val_loss 0.0036947081181642714, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.001966878433599959, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.0019540020532679138, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.0019575562078148322, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 0.0028693163531806933, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.002432591383185354, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.0018877290252203238, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.7156e-04,  2.4577e-02,  5.9255e-04,  3.0660e-05,  1.4266e-02,
          4.0204e-03, -1.6117e-01,  5.1305e-03, -1.2806e-02, -4.3194e-03,
         -4.5331e-04, -1.2295e+00, -8.6614e-06]], device='cuda:1'))])
end of epoch 15: val_loss 0.0015131274814029893, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6587e-03,  7.5944e-02,  1.5377e-03, -7.5829e-05,  4.5618e-02,
         -5.8759e-03, -1.9776e-01,  4.5975e-02, -4.7937e-02,  6.5635e-03,
          6.7691e-04, -1.2657e+00, -2.4681e-04]], device='cuda:1'))])
end of epoch 16: val_loss 0.0029988267737786245, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.0018538441916280135, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.0025267358301130118, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0021812418193388793, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.0022550725355962344, val_acc 1.0
trigger times: 5
end of epoch 21: val_loss 0.0022678802903510586, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.0024056690488214374, val_acc 1.0
trigger times: 7
end of epoch 23: val_loss 0.0025780398991748755, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 0.0016219460432510856, val_acc 1.0
trigger times: 9
end of epoch 25: val_loss 0.0019278762746715473, val_acc 1.0
trigger times: 10
Early stopping.
0 -398.50031876564026 -111.1509139312174
1 -398.50031876564026 -111.1509139312174
2 -398.50031876564026 -111.1509139312174
3 -398.50031876564026 -111.1509139312174
4 -398.3128066062927 -110.7546746021886
5 -398.3128066062927 -110.7546746021886
6 -397.21953320503235 -110.36543265943791
7 -397.41039645671844 -110.35299605293886
8 -397.3395402133465 -110.33801695908483
9 -397.3395402133465 -110.33801695908483
10 -397.57212659716606 -110.2992753922497
11 -397.3258303999901 -110.26549484761443
12 -397.38150757551193 -110.0272118598093
13 -396.8647993505001 -109.8988298040389
14 -397.0863343179226 -109.79936792792112
15 -397.0863343179226 -109.79936792792112
16 -397.0863343179226 -109.79936792792112
17 -396.9891180098057 -109.79148147499572
18 -396.9891180098057 -109.79148147499572
19 -396.9891180098057 -109.79148147499572
20 -396.5541490614414 -109.74172827034793
21 -396.572730332613 -109.70897659338503
22 -395.88113391399384 -109.192060192308
23 -395.88113391399384 -109.192060192308
24 -395.4850628376007 -109.08967488557313
25 -395.4850628376007 -109.08967488557313
26 -395.8742037117481 -108.8908753244873
27 -395.8742037117481 -108.8908753244873
28 -395.86827993392944 -108.82374780685433
29 -395.6575216650963 -108.81083756860734
30 -395.6575216650963 -108.81083756860734
31 -395.6575216650963 -108.81083756860734
32 -395.6575216650963 -108.81083756860734
33 -394.6133220233023 -108.71124898270375
34 -394.6133220233023 -108.71124898270375
35 -395.5292031466961 -108.54582640198474
36 -395.3911357820034 -108.45700365695856
37 -394.75598952174187 -108.31581468771003
38 -394.75598952174187 -108.31581468771003
39 -394.75598952174187 -108.31581468771003
40 -394.75598952174187 -108.31581468771003
41 -394.9694250822067 -108.12053472856175
42 -394.1576407253742 -108.0180511928408
43 -394.1576407253742 -108.0180511928408
44 -394.1576407253742 -108.0180511928408
45 -393.82409961521626 -108.0161428018986
46 -393.75248298048973 -107.70817145882172
47 -393.75248298048973 -107.70817145882172
48 -394.4079689383507 -107.67837948595889
49 -394.3328632116318 -107.67618449636431
50 -393.5085261464119 -107.52220794033053
51 -393.25739163160324 -107.26673763034924
52 -393.25739163160324 -107.26673763034924
53 -393.25739163160324 -107.26673763034924
54 -393.25739163160324 -107.26673763034924
55 -393.8197746872902 -107.2187036786349
56 -393.8197746872902 -107.2187036786349
57 -393.3159138262272 -107.20060036202784
58 -393.3159138262272 -107.20060036202784
59 -392.6316245868802 -106.96224593171276
60 -392.7155811339617 -106.77350901919232
61 -392.7155811339617 -106.77350901919232
62 -392.7155811339617 -106.77350901919232
63 -392.71926057338715 -106.35469036504782
64 -391.90991201996803 -106.26741762098152
65 -391.90991201996803 -106.26741762098152
66 -391.90991201996803 -106.26741762098152
67 -391.8541535884142 -106.17722508081916
68 -391.8541535884142 -106.17722508081916
69 -391.72582583129406 -106.12986974696594
70 -391.78454864025116 -106.11224681921385
71 -391.78454864025116 -106.11224681921385
72 -391.78454864025116 -106.11224681921385
73 -391.78454864025116 -106.11224681921385
74 -391.91724693775177 -105.98763553383849
75 -392.0011753439903 -105.89116676488838
76 -391.920180529356 -105.72064989124367
77 -391.920180529356 -105.72064989124367
78 -391.920180529356 -105.72064989124367
79 -391.5731571614742 -105.67858096296958
80 -391.5731571614742 -105.67858096296958
81 -391.83510103821754 -105.66005329834891
82 -391.50021037459373 -105.64353103691755
83 -391.50021037459373 -105.64353103691755
84 -391.72262823581696 -105.55355660604569
85 -391.72262823581696 -105.55355660604569
86 -391.5494374334812 -105.4246871281226
87 -391.5494374334812 -105.4246871281226
88 -390.9983032345772 -105.41153983260888
89 -391.1435131430626 -105.205379324443
90 -391.1435131430626 -105.205379324443
91 -390.65904749929905 -105.1748627281623
92 -390.8495810329914 -104.98414916296157
93 -390.20330791175365 -104.75574550925394
94 -390.20330791175365 -104.75574550925394
95 -390.20330791175365 -104.75574550925394
96 -390.20330791175365 -104.75574550925394
97 -389.7570110410452 -104.34422137718992
98 -390.0740100443363 -104.25271549639196
99 -390.0740100443363 -104.25271549639196
100 -390.0740100443363 -104.25271549639196
101 -390.0740100443363 -104.25271549639196
102 -389.44609919190407 -104.03978746733507
103 -389.1957293599844 -103.7299056083423
104 -389.1957293599844 -103.7299056083423
105 -389.2539073228836 -103.62228556353936
106 -389.15664222836494 -103.53627782194874
107 -388.37636336684227 -103.0461664951788
108 -388.37636336684227 -103.0461664951788
109 -388.37636336684227 -103.0461664951788
110 -387.60136127471924 -102.3359358284459
111 -384.31039771437645 -65.00918360597713
112 -383.2222330570221 -64.50420711541688
113 -384.6200273633003 -64.04370518153469
114 -378.7774314582348 -63.32121652631737
115 -25.64179416000843 -38.41270390343083
116 -31.57216528058052 -20.14639340520683
117 -18.731637825723737 -9.48435448123352
118 -19.71508971179719 -6.727815877609332
119 6.259013011585921 -6.044975028976545
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -111.15091393 -111.15091393 -111.15091393
 -110.7546746  -110.7546746  -110.36543266 -110.35299605 -110.33801696
 -110.33801696 -110.29927539 -110.26549485 -110.02721186 -109.8988298
 -109.79936793 -109.79936793 -109.79936793 -109.79148147 -109.79148147
 -109.79148147 -109.74172827 -109.70897659 -109.19206019 -109.19206019
 -109.08967489 -109.08967489 -108.89087532 -108.89087532 -108.82374781
 -108.81083757 -108.81083757 -108.81083757 -108.81083757 -108.81083757
 -108.71124898 -108.71124898 -108.5458264  -108.45700366 -108.31581469
 -108.31581469 -108.31581469 -108.31581469 -108.31581469 -108.12053473
 -108.01805119 -108.01805119 -108.01805119 -108.0161428  -107.70817146
 -107.70817146 -107.67837949 -107.6761845  -107.52220794 -107.26673763
 -107.26673763 -107.26673763 -107.26673763 -107.21870368 -107.21870368
 -107.20060036 -107.20060036 -106.96224593 -106.77350902 -106.77350902
 -106.77350902 -106.35469037 -106.26741762 -106.26741762 -106.26741762
 -106.17722508 -106.17722508 -106.12986975 -106.11224682 -106.11224682
 -106.11224682 -106.11224682 -106.11224682 -105.98763553 -105.89116676
 -105.72064989 -105.72064989 -105.72064989 -105.67858096 -105.67858096
 -105.6600533  -105.64353104 -105.64353104 -105.55355661 -105.55355661
 -105.42468713 -105.42468713 -105.41153983 -105.20537932 -105.20537932
 -105.17486273 -104.98414916 -104.75574551 -104.75574551 -104.75574551
 -104.75574551 -104.34422138 -104.2527155  -104.2527155  -104.2527155
 -104.2527155  -104.03978747 -103.72990561 -103.72990561 -103.62228556
 -103.53627782 -103.0461665  -103.0461665  -103.0461665  -102.33593583
  -64.50420712  -63.32121653   -9.48435448   -6.72781588   -6.04497503]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.002362792521580559, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.8837e-05,  6.9141e-02,  8.5445e-06,  3.2043e-05,  4.3399e-03,
          4.8751e-04, -7.4326e-02,  3.8777e-02, -3.9542e-03, -5.2483e-04,
         -2.8540e-04, -1.2837e+00, -3.4756e-02]], device='cuda:1'))])
end of epoch 1: val_loss 0.002341803004752592, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.2496e-05,  3.3511e-03, -4.6336e-05, -2.9853e-04,  1.6145e-02,
         -7.7998e-03, -7.1867e-02,  3.0688e-03, -1.4491e-02,  6.8846e-03,
          5.4146e-04, -1.2441e+00, -1.8164e-04]], device='cuda:1'))])
end of epoch 2: val_loss 0.0023789227056795425, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.002451651832498101, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.001947567712950331, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.9730e-04,  1.3647e-01,  1.2543e-03, -6.0926e-04,  1.0366e-02,
         -3.2034e-02, -8.5510e-02,  7.4832e-02, -1.0963e-02,  3.8239e-02,
         -8.7282e-04, -1.3486e+00,  2.2025e-04]], device='cuda:1'))])
end of epoch 5: val_loss 0.0022625480463830173, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.0019141827229429964, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.8068e-04,  9.3680e-05,  2.6579e-04,  4.0260e-04,  2.7109e-02,
         -3.3636e-03, -1.1224e-01, -2.4862e-03, -3.0613e-02,  3.8608e-03,
         -1.5939e-03, -1.2703e+00,  2.7250e-04]], device='cuda:1'))])
end of epoch 7: val_loss 0.002419435371666623, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.0018027168417847862, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.9121e-04,  8.6324e-03, -1.5429e-03, -4.4120e-05,  3.0813e-02,
         -2.2841e-02, -1.5767e-01, -1.5390e-04, -2.4980e-02,  2.2915e-02,
          5.4185e-04, -1.2827e+00,  8.0279e-04]], device='cuda:1'))])
end of epoch 9: val_loss 0.001739244747971469, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5923e-03,  2.9019e-01, -1.2211e-02,  2.7124e-04,  1.6246e-01,
          3.2535e-02, -3.2617e-01,  2.0344e-01, -1.6963e-01, -3.2557e-02,
          1.6642e-03, -1.5205e+00, -2.2020e-01]], device='cuda:1'))])
end of epoch 10: val_loss 0.0015479680328940048, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0666e-03, -9.5663e-05, -3.8336e-03, -1.7742e-04,  2.4577e-02,
         -3.5212e-03, -7.3646e-02, -5.1100e-04, -2.5244e-02,  3.8600e-03,
          6.1080e-04, -1.3364e+00, -1.7765e-04]], device='cuda:1'))])
end of epoch 11: val_loss 0.00328489618446838, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.002552216945678083, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.0026018565739013865, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.002061191765278636, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.0016568438759600212, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 0.002307272847137938, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 0.0023920654463288928, val_acc 1.0
trigger times: 7
end of epoch 18: val_loss 0.002274089605671179, val_acc 1.0
trigger times: 8
end of epoch 19: val_loss 0.002498295639356911, val_acc 1.0
trigger times: 9
end of epoch 20: val_loss 0.002720129847843964, val_acc 1.0
trigger times: 10
Early stopping.
0 -1248.803623855114 -111.1509139312174
1 -1248.803623855114 -111.1509139312174
2 -1248.803623855114 -111.1509139312174
3 -1248.803623855114 -111.1509139312174
4 -1248.803623855114 -111.1509139312174
5 -1249.70185983181 -110.7546746021886
6 -1249.70185983181 -110.7546746021886
7 -1249.0814334899187 -110.36543265943791
8 -1247.554658010602 -110.35299605293886
9 -1247.789363220334 -110.33801695908483
10 -1247.789363220334 -110.33801695908483
11 -1247.388136357069 -110.2992753922497
12 -1249.1654780060053 -110.26549484761443
13 -1248.8814966082573 -110.0272118598093
14 -1247.195204809308 -109.8988298040389
15 -1246.67577958107 -109.79936792792112
16 -1246.67577958107 -109.79936792792112
17 -1246.67577958107 -109.79936792792112
18 -1248.4536875486374 -109.79148147499572
19 -1248.4536875486374 -109.79148147499572
20 -1248.4536875486374 -109.79148147499572
21 -1246.578569933772 -109.74172827034793
22 -1247.2062324434519 -109.70897659338503
23 -1245.7622077018023 -109.192060192308
24 -1245.7622077018023 -109.192060192308
25 -1247.0563562065363 -109.08967488557313
26 -1247.0563562065363 -109.08967488557313
27 -1245.743427336216 -108.8908753244873
28 -1245.743427336216 -108.8908753244873
29 -1245.6275568306446 -108.82374780685433
30 -1246.1138692051172 -108.81083756860734
31 -1246.1138692051172 -108.81083756860734
32 -1246.1138692051172 -108.81083756860734
33 -1246.1138692051172 -108.81083756860734
34 -1246.1138692051172 -108.81083756860734
35 -1246.1171395927668 -108.71124898270375
36 -1246.1171395927668 -108.71124898270375
37 -1245.4218938052654 -108.54582640198474
38 -1245.7058775424957 -108.45700365695856
39 -1244.4693454951048 -108.31581468771003
40 -1244.4693454951048 -108.31581468771003
41 -1244.4693454951048 -108.31581468771003
42 -1244.4693454951048 -108.31581468771003
43 -1244.4693454951048 -108.31581468771003
44 -1246.168173611164 -108.12053472856175
45 -1244.0576008446515 -108.0180511928408
46 -1244.0576008446515 -108.0180511928408
47 -1244.0576008446515 -108.0180511928408
48 -1243.5666035860777 -108.0161428018986
49 -1245.2904388792813 -107.70817145882172
50 -1245.2904388792813 -107.70817145882172
51 -1245.5936530083418 -107.67837948595889
52 -1243.8864362984896 -107.67618449636431
53 -1244.8926910944283 -107.52220794033053
54 -1243.0318548418581 -107.26673763034924
55 -1243.0318548418581 -107.26673763034924
56 -1243.0318548418581 -107.26673763034924
57 -1243.0318548418581 -107.26673763034924
58 -1243.7208393067122 -107.2187036786349
59 -1243.7208393067122 -107.2187036786349
60 -1243.3668978959322 -107.20060036202784
61 -1243.3668978959322 -107.20060036202784
62 -1243.8082028329372 -106.96224593171276
63 -1244.0069098062813 -106.77350901919232
64 -1244.0069098062813 -106.77350901919232
65 -1244.0069098062813 -106.77350901919232
66 -1242.43610291183 -106.35469036504782
67 -1243.1080151200294 -106.26741762098152
68 -1243.1080151200294 -106.26741762098152
69 -1243.1080151200294 -106.26741762098152
70 -1241.8274260051548 -106.17722508081916
71 -1241.8274260051548 -106.17722508081916
72 -1241.4321590960026 -106.12986974696594
73 -1243.0149932838976 -106.11224681921385
74 -1243.0149932838976 -106.11224681921385
75 -1243.0149932838976 -106.11224681921385
76 -1243.0149932838976 -106.11224681921385
77 -1243.0149932838976 -106.11224681921385
78 -1241.4844644218683 -105.98763553383849
79 -1242.1736179739237 -105.89116676488838
80 -1242.8542721569538 -105.72064989124367
81 -1242.8542721569538 -105.72064989124367
82 -1242.8542721569538 -105.72064989124367
83 -1240.9951009601355 -105.67858096296958
84 -1240.9951009601355 -105.67858096296958
85 -1241.663571342826 -105.66005329834891
86 -1241.0399521142244 -105.64353103691755
87 -1241.0399521142244 -105.64353103691755
88 -1240.9994652420282 -105.55355660604569
89 -1240.9994652420282 -105.55355660604569
90 -1242.0894590169191 -105.4246871281226
91 -1242.0894590169191 -105.4246871281226
92 -1242.0380167551339 -105.41153983260888
93 -1240.7496564537287 -105.205379324443
94 -1240.7496564537287 -105.205379324443
95 -1240.5658389776945 -105.1748627281623
96 -1240.7815767079592 -104.98414916296157
97 -1241.0369219221175 -104.75574550925394
98 -1241.0369219221175 -104.75574550925394
99 -1241.0369219221175 -104.75574550925394
100 -1241.0369219221175 -104.75574550925394
101 -1238.9380234368145 -104.34422137718992
102 -1239.2759098559618 -104.25271549639196
103 -1239.2759098559618 -104.25271549639196
104 -1239.2759098559618 -104.25271549639196
105 -1239.2759098559618 -104.25271549639196
106 -1239.3801234550774 -104.03978746733507
107 -1238.7557324655354 -103.7299056083423
108 -1238.7557324655354 -103.7299056083423
109 -1238.8288904577494 -103.62228556353936
110 -1238.665406987071 -103.53627782194874
111 -1238.0859984345734 -103.0461664951788
112 -1238.0859984345734 -103.0461664951788
113 -1238.0859984345734 -103.0461664951788
114 -1237.079307153821 -102.3359358284459
115 -1149.337624847889 -64.50420711541688
116 -1127.8935403227806 -63.32121652631737
117 -28.982927657663822 -9.48435448123352
118 -50.01165946031688 -6.727815877609332
119 46.066368519328535 -6.044975028976545
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -111.15091393 -111.15091393 -111.15091393
 -111.15091393 -110.7546746  -110.7546746  -110.36543266 -110.35299605
 -110.33801696 -110.33801696 -110.29927539 -110.26549485 -110.02721186
 -109.8988298  -109.79936793 -109.79936793 -109.79936793 -109.79148147
 -109.79148147 -109.79148147 -109.74172827 -109.70897659 -109.19206019
 -109.19206019 -109.08967489 -109.08967489 -108.89087532 -108.89087532
 -108.82374781 -108.81083757 -108.81083757 -108.81083757 -108.81083757
 -108.81083757 -108.81083757 -108.71124898 -108.71124898 -108.5458264
 -108.45700366 -108.31581469 -108.31581469 -108.31581469 -108.31581469
 -108.31581469 -108.12053473 -108.01805119 -108.01805119 -108.01805119
 -108.0161428  -107.70817146 -107.70817146 -107.67837949 -107.6761845
 -107.52220794 -107.26673763 -107.26673763 -107.26673763 -107.26673763
 -107.21870368 -107.21870368 -107.20060036 -107.20060036 -106.96224593
 -106.77350902 -106.77350902 -106.77350902 -106.35469037 -106.26741762
 -106.26741762 -106.26741762 -106.17722508 -106.17722508 -106.12986975
 -106.11224682 -106.11224682 -106.11224682 -106.11224682 -106.11224682
 -105.98763553 -105.89116676 -105.72064989 -105.72064989 -105.72064989
 -105.67858096 -105.67858096 -105.6600533  -105.64353104 -105.64353104
 -105.55355661 -105.55355661 -105.42468713 -105.42468713 -105.41153983
 -105.20537932 -105.20537932 -105.17486273 -104.98414916 -104.75574551
 -104.75574551 -104.75574551 -104.75574551 -104.34422138 -104.2527155
 -104.2527155  -104.2527155  -104.2527155  -104.03978747 -103.72990561
 -103.72990561 -103.62228556 -103.53627782 -103.0461665  -103.0461665
 -103.0461665  -102.33593583  -64.50420712  -63.32121653   -6.72781588]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.0024910883064148946, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0087e-05,  4.0775e-02,  1.3410e-07,  2.9771e-05,  2.8436e-03,
         -4.2583e-03, -3.9949e-02,  1.0844e-02, -2.3831e-03,  4.7526e-03,
          1.9433e-03, -1.2854e+00, -3.1799e-05]], device='cuda:1'))])
end of epoch 1: val_loss 0.002920951100440199, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0027083619190784703, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.0025166798528437084, val_acc 1.0
trigger times: 3
end of epoch 4: val_loss 0.002382959586020661, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.5772e-03, -7.3629e-04,  5.6920e-04, -6.1194e-04,  1.1678e-02,
          3.2030e-03, -6.9025e-02, -7.9347e-04, -1.6591e-02, -4.0935e-03,
         -7.2582e-04, -1.2888e+00,  2.8730e-04]], device='cuda:1'))])
end of epoch 5: val_loss 0.0022000487111773735, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3147e-03,  7.9667e-05, -2.3562e-03,  7.2093e-05,  1.5927e-02,
          6.6366e-03, -8.1104e-02, -2.6700e-03, -2.3223e-02, -3.3412e-03,
          5.7785e-04, -1.2884e+00,  1.7106e-04]], device='cuda:1'))])
end of epoch 6: val_loss 0.0023505346377214664, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0016564410315197619, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.7941e-04,  2.6952e-01, -2.2810e-03, -1.4748e-03,  6.2563e-02,
          3.9063e-03, -3.3040e-01,  2.3863e-01, -7.0925e-02, -4.0623e-03,
         -7.9145e-04, -1.4661e+00, -1.5028e-01]], device='cuda:1'))])
end of epoch 8: val_loss 0.0021786227476877683, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.002935998719694908, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.0018344541737178588, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.002163437124135541, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 0.002648486102661991, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.0019404945352789582, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.0022617678252936455, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.0017620643236614343, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.0027492192560384866, val_acc 1.0
trigger times: 9
end of epoch 17: val_loss 0.002786332470150228, val_acc 1.0
trigger times: 10
Early stopping.
0 -251.627567589283 -111.1509139312174
1 -251.627567589283 -111.1509139312174
2 -251.627567589283 -111.1509139312174
3 -251.627567589283 -111.1509139312174
4 -251.627567589283 -111.1509139312174
5 -251.627567589283 -111.1509139312174
6 -251.244859457016 -110.7546746021886
7 -251.244859457016 -110.7546746021886
8 -250.63126742839813 -110.36543265943791
9 -250.62338730692863 -110.35299605293886
10 -250.90346905589104 -110.33801695908483
11 -250.90346905589104 -110.33801695908483
12 -250.77467194199562 -110.2992753922497
13 -250.51203247904778 -110.26549484761443
14 -250.29963821172714 -110.0272118598093
15 -250.044725805521 -109.8988298040389
16 -250.05095317959785 -109.79936792792112
17 -250.05095317959785 -109.79936792792112
18 -250.05095317959785 -109.79936792792112
19 -250.09702959656715 -109.79148147499572
20 -250.09702959656715 -109.79148147499572
21 -250.09702959656715 -109.79148147499572
22 -250.15012601017952 -109.74172827034793
23 -250.0900174677372 -109.70897659338503
24 -249.1600850224495 -109.192060192308
25 -249.1600850224495 -109.192060192308
26 -249.31646728515625 -109.08967488557313
27 -249.31646728515625 -109.08967488557313
28 -248.80719274282455 -108.8908753244873
29 -248.80719274282455 -108.8908753244873
30 -248.79951259493828 -108.82374780685433
31 -248.89011979103088 -108.81083756860734
32 -248.89011979103088 -108.81083756860734
33 -248.89011979103088 -108.81083756860734
34 -248.89011979103088 -108.81083756860734
35 -248.89011979103088 -108.81083756860734
36 -248.89011979103088 -108.81083756860734
37 -248.69192807748914 -108.71124898270375
38 -248.69192807748914 -108.71124898270375
39 -248.42790859937668 -108.54582640198474
40 -248.30712720751762 -108.45700365695856
41 -248.05517953634262 -108.31581468771003
42 -248.05517953634262 -108.31581468771003
43 -248.05517953634262 -108.31581468771003
44 -248.05517953634262 -108.31581468771003
45 -248.05517953634262 -108.31581468771003
46 -247.8744491636753 -108.12053472856175
47 -247.7056179344654 -108.0180511928408
48 -247.7056179344654 -108.0180511928408
49 -247.7056179344654 -108.0180511928408
50 -247.88319347798824 -108.0161428018986
51 -247.31796266138554 -107.70817145882172
52 -247.31796266138554 -107.70817145882172
53 -247.30866652727127 -107.67837948595889
54 -247.40756058692932 -107.67618449636431
55 -247.29433074593544 -107.52220794033053
56 -246.7568524479866 -107.26673763034924
57 -246.7568524479866 -107.26673763034924
58 -246.7568524479866 -107.26673763034924
59 -246.7568524479866 -107.26673763034924
60 -246.72575145959854 -107.2187036786349
61 -246.72575145959854 -107.2187036786349
62 -246.86959752440453 -107.20060036202784
63 -246.86959752440453 -107.20060036202784
64 -246.46671406179667 -106.96224593171276
65 -246.13586539030075 -106.77350901919232
66 -246.13586539030075 -106.77350901919232
67 -246.13586539030075 -106.77350901919232
68 -245.61077028512955 -106.35469036504782
69 -245.63555951416492 -106.26741762098152
70 -245.63555951416492 -106.26741762098152
71 -245.63555951416492 -106.26741762098152
72 -245.40404176712036 -106.17722508081916
73 -245.40404176712036 -106.17722508081916
74 -245.44834715127945 -106.12986974696594
75 -245.33599051833153 -106.11224681921385
76 -245.33599051833153 -106.11224681921385
77 -245.33599051833153 -106.11224681921385
78 -245.33599051833153 -106.11224681921385
79 -245.33599051833153 -106.11224681921385
80 -245.28705409169197 -105.98763553383849
81 -244.99885612726212 -105.89116676488838
82 -244.80845093727112 -105.72064989124367
83 -244.80845093727112 -105.72064989124367
84 -244.80845093727112 -105.72064989124367
85 -244.8737052977085 -105.67858096296958
86 -244.8737052977085 -105.67858096296958
87 -244.72703978419304 -105.66005329834891
88 -244.67731109261513 -105.64353103691755
89 -244.67731109261513 -105.64353103691755
90 -244.6525351703167 -105.55355660604569
91 -244.6525351703167 -105.55355660604569
92 -244.45653665065765 -105.4246871281226
93 -244.45653665065765 -105.4246871281226
94 -244.5390569269657 -105.41153983260888
95 -244.23883563280106 -105.205379324443
96 -244.23883563280106 -105.205379324443
97 -244.15687881410122 -105.1748627281623
98 -243.9520117342472 -104.98414916296157
99 -243.6404207199812 -104.75574550925394
100 -243.6404207199812 -104.75574550925394
101 -243.6404207199812 -104.75574550925394
102 -243.6404207199812 -104.75574550925394
103 -243.0893655270338 -104.34422137718992
104 -242.95281380414963 -104.25271549639196
105 -242.95281380414963 -104.25271549639196
106 -242.95281380414963 -104.25271549639196
107 -242.95281380414963 -104.25271549639196
108 -242.70316131412983 -104.03978746733507
109 -242.28068494796753 -103.7299056083423
110 -242.28068494796753 -103.7299056083423
111 -242.19198924303055 -103.62228556353936
112 -242.0392942428589 -103.53627782194874
113 -241.40178681910038 -103.0461664951788
114 -241.40178681910038 -103.0461664951788
115 -241.40178681910038 -103.0461664951788
116 -240.52642184495926 -102.3359358284459
117 -242.97422009706497 -64.50420711541688
118 -240.861657589674 -63.32121652631737
119 -14.619618398486637 -6.727815877609332
train accuracy: 1.0
validation accuracy: 1.0
[-111.15091393 -111.15091393 -111.15091393 -111.15091393 -111.15091393
 -111.15091393 -111.15091393 -110.7546746  -110.7546746  -110.36543266
 -110.35299605 -110.33801696 -110.33801696 -110.29927539 -110.26549485
 -110.02721186 -109.8988298  -109.79936793 -109.79936793 -109.79936793
 -109.79148147 -109.79148147 -109.79148147 -109.74172827 -109.70897659
 -109.19206019 -109.19206019 -109.08967489 -109.08967489 -108.89087532
 -108.89087532 -108.82374781 -108.81083757 -108.81083757 -108.81083757
 -108.81083757 -108.81083757 -108.81083757 -108.81083757 -108.71124898
 -108.71124898 -108.5458264  -108.45700366 -108.31581469 -108.31581469
 -108.31581469 -108.31581469 -108.31581469 -108.12053473 -108.01805119
 -108.01805119 -108.01805119 -108.0161428  -107.70817146 -107.70817146
 -107.67837949 -107.6761845  -107.52220794 -107.26673763 -107.26673763
 -107.26673763 -107.26673763 -107.21870368 -107.21870368 -107.20060036
 -107.20060036 -106.96224593 -106.77350902 -106.77350902 -106.77350902
 -106.35469037 -106.26741762 -106.26741762 -106.26741762 -106.17722508
 -106.17722508 -106.12986975 -106.11224682 -106.11224682 -106.11224682
 -106.11224682 -106.11224682 -105.98763553 -105.89116676 -105.72064989
 -105.72064989 -105.72064989 -105.67858096 -105.67858096 -105.6600533
 -105.64353104 -105.64353104 -105.55355661 -105.55355661 -105.42468713
 -105.42468713 -105.41153983 -105.20537932 -105.20537932 -105.17486273
 -104.98414916 -104.75574551 -104.75574551 -104.75574551 -104.75574551
 -104.34422138 -104.2527155  -104.2527155  -104.2527155  -104.2527155
 -104.03978747 -103.72990561 -103.72990561 -103.62228556 -103.53627782
 -103.0461665  -103.0461665  -103.0461665  -102.33593583  -64.50420712]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.002670545527325885, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0278e-03,  1.5010e-04,  6.5409e-04, -8.2777e-05,  4.7131e-03,
         -5.9118e-03, -5.3731e-02,  1.0110e-02, -7.2720e-03,  5.1091e-03,
          1.5684e-05, -1.3288e+00,  1.7492e-04]], device='cuda:1'))])
end of epoch 1: val_loss 0.0022489500312940437, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.1778e-04,  3.9049e-03, -1.3720e-03, -1.0701e-03,  3.8543e-02,
         -2.6118e-03, -7.1927e-02,  6.4465e-03, -3.3052e-02,  1.0153e-03,
          1.0992e-03, -1.2755e+00, -2.2664e-04]], device='cuda:1'))])
end of epoch 2: val_loss 0.0026926094645301647, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.0021841493502734012, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.3796e-04,  9.3387e-02, -1.3193e-03, -9.7604e-04,  6.8730e-03,
         -1.7243e-02, -2.0996e-01,  7.6342e-04, -7.6232e-03,  1.6078e-02,
         -4.6046e-04, -1.3263e+00,  1.7576e-04]], device='cuda:1'))])
end of epoch 4: val_loss 0.0026253142007722375, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.002071154921047764, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2848e-04,  2.2332e-01,  3.2646e-04,  1.4456e-03,  9.5763e-03,
         -1.9074e-03, -2.9444e-01,  1.1585e-01, -2.2945e-02,  2.4398e-03,
         -2.0394e-03, -1.4024e+00,  1.8423e-04]], device='cuda:1'))])
end of epoch 6: val_loss 0.0021895536885426737, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0034494886491182796, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 0.0032867460742193088, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.0019402877810216523, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.5378e-05,  1.9004e-01, -9.4081e-05,  1.2255e-04,  1.7975e-02,
         -6.0714e-03, -2.5052e-01,  8.8549e-02, -1.8433e-02,  6.5810e-03,
          2.3994e-03, -1.3893e+00, -4.8375e-05]], device='cuda:1'))])
end of epoch 10: val_loss 0.00224635019124662, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 0.002047150956846053, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 0.002271793232280288, val_acc 1.0
trigger times: 3
end of epoch 13: val_loss 0.0021299858946645144, val_acc 1.0
trigger times: 4
end of epoch 14: val_loss 0.0020573410594442974, val_acc 1.0
trigger times: 5
end of epoch 15: val_loss 0.0017843469140257184, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1174e-03,  1.8216e-05, -4.9741e-06, -1.0170e-03,  1.4243e-02,
         -2.2515e-03, -9.0679e-02, -7.5132e-04, -1.3736e-02,  5.3002e-03,
          1.0984e-03, -1.3595e+00, -3.2461e-05]], device='cuda:1'))])
end of epoch 16: val_loss 0.0033571263039038966, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.002199887182823446, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.001955363333660607, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0023962790191490056, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 0.0018973943062474063, val_acc 1.0
trigger times: 5
end of epoch 21: val_loss 0.0022535739876320803, val_acc 1.0
trigger times: 6
end of epoch 22: val_loss 0.002154245129577248, val_acc 1.0
trigger times: 7
end of epoch 23: val_loss 0.002363623276346516, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 0.004802713099716698, val_acc 1.0
trigger times: 9
end of epoch 25: val_loss 0.0016937827184809607, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.4652e-04,  3.1854e-01,  2.7919e-04,  2.7011e-01,  2.0474e-01,
          1.5420e-03, -3.1281e-01,  2.4434e-01, -2.3633e-01,  1.0119e-02,
          6.6852e-04, -1.6780e+00, -2.8372e-01]], device='cuda:1'))])
end of epoch 26: val_loss 0.0019700954113636726, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 0.0022675761445134415, val_acc 1.0
trigger times: 2
end of epoch 28: val_loss 0.007310093137302829, val_acc 1.0
trigger times: 3
end of epoch 29: val_loss 0.0018591387822493744, val_acc 1.0
trigger times: 4
end of epoch 30: val_loss 0.0022216090529946085, val_acc 1.0
trigger times: 5
end of epoch 31: val_loss 0.0019843337612837787, val_acc 1.0
trigger times: 6
end of epoch 32: val_loss 0.0022975885942923924, val_acc 1.0
trigger times: 7
end of epoch 33: val_loss 0.0029321603215771573, val_acc 1.0
trigger times: 8
end of epoch 34: val_loss 0.0025421987098491173, val_acc 1.0
trigger times: 9
end of epoch 35: val_loss 0.002095561258829548, val_acc 1.0
trigger times: 10
Early stopping.
0 -691.8255148530006 -111.1509139312174
1 -691.8255148530006 -111.1509139312174
2 -691.8255148530006 -111.1509139312174
3 -691.8255148530006 -111.1509139312174
4 -691.8255148530006 -111.1509139312174
5 -691.8255148530006 -111.1509139312174
6 -691.8255148530006 -111.1509139312174
7 -691.6293118596077 -110.7546746021886
8 -691.6293118596077 -110.7546746021886
9 -690.4666605293751 -110.36543265943791
10 -690.6754563450813 -110.35299605293886
11 -690.6306165456772 -110.33801695908483
12 -690.6306165456772 -110.33801695908483
13 -690.8554261028767 -110.2992753922497
14 -690.5812172293663 -110.26549484761443
15 -690.6238523721695 -110.0272118598093
16 -690.0800466835499 -109.8988298040389
17 -690.3196586966515 -109.79936792792112
18 -690.3196586966515 -109.79936792792112
19 -690.3196586966515 -109.79936792792112
20 -690.2223252952099 -109.79148147499572
21 -690.2223252952099 -109.79148147499572
22 -690.2223252952099 -109.79148147499572
23 -689.805021494627 -109.74172827034793
24 -689.8047869503498 -109.70897659338503
25 -689.0456351488829 -109.192060192308
26 -689.0456351488829 -109.192060192308
27 -688.6592053622007 -109.08967488557313
28 -688.6592053622007 -109.08967488557313
29 -689.0186310708523 -108.8908753244873
30 -689.0186310708523 -108.8908753244873
31 -689.017726868391 -108.82374780685433
32 -688.8101195394993 -108.81083756860734
33 -688.8101195394993 -108.81083756860734
34 -688.8101195394993 -108.81083756860734
35 -688.8101195394993 -108.81083756860734
36 -688.8101195394993 -108.81083756860734
37 -688.8101195394993 -108.81083756860734
38 -688.8101195394993 -108.81083756860734
39 -687.731056753546 -108.71124898270375
40 -687.731056753546 -108.71124898270375
41 -688.6573865115643 -108.54582640198474
42 -688.4870662689209 -108.45700365695856
43 -687.8421500325203 -108.31581468771003
44 -687.8421500325203 -108.31581468771003
45 -687.8421500325203 -108.31581468771003
46 -687.8421500325203 -108.31581468771003
47 -687.8421500325203 -108.31581468771003
48 -688.0475352704525 -108.12053472856175
49 -687.2124587744474 -108.0180511928408
50 -687.2124587744474 -108.0180511928408
51 -687.2124587744474 -108.0180511928408
52 -686.8908954132348 -108.0161428018986
53 -686.7763981670141 -107.70817145882172
54 -686.7763981670141 -107.70817145882172
55 -687.446612983942 -107.67837948595889
56 -687.3966026008129 -107.67618449636431
57 -686.5543230846524 -107.52220794033053
58 -686.2461741417646 -107.26673763034924
59 -686.2461741417646 -107.26673763034924
60 -686.2461741417646 -107.26673763034924
61 -686.2461741417646 -107.26673763034924
62 -686.8076494932175 -107.2187036786349
63 -686.8076494932175 -107.2187036786349
64 -686.3273277878761 -107.20060036202784
65 -686.3273277878761 -107.20060036202784
66 -685.5993527509272 -106.96224593171276
67 -685.6714800298214 -106.77350901919232
68 -685.6714800298214 -106.77350901919232
69 -685.6714800298214 -106.77350901919232
70 -685.6384280025959 -106.35469036504782
71 -684.8273123521358 -106.26741762098152
72 -684.8273123521358 -106.26741762098152
73 -684.8273123521358 -106.26741762098152
74 -684.7403082586825 -106.17722508081916
75 -684.7403082586825 -106.17722508081916
76 -684.6274370271713 -106.12986974696594
77 -684.6820258535445 -106.11224681921385
78 -684.6820258535445 -106.11224681921385
79 -684.6820258535445 -106.11224681921385
80 -684.6820258535445 -106.11224681921385
81 -684.6820258535445 -106.11224681921385
82 -684.8184982091188 -105.98763553383849
83 -684.8775842636824 -105.89116676488838
84 -684.795490115881 -105.72064989124367
85 -684.795490115881 -105.72064989124367
86 -684.795490115881 -105.72064989124367
87 -684.4377548247576 -105.67858096296958
88 -684.4377548247576 -105.67858096296958
89 -684.683065444231 -105.66005329834891
90 -684.3433259576559 -105.64353103691755
91 -684.3433259576559 -105.64353103691755
92 -684.5941325128078 -105.55355660604569
93 -684.5941325128078 -105.55355660604569
94 -684.3934803307056 -105.4246871281226
95 -684.3934803307056 -105.4246871281226
96 -683.8485909327865 -105.41153983260888
97 -683.9660851508379 -105.205379324443
98 -683.9660851508379 -105.205379324443
99 -683.4614003635943 -105.1748627281623
100 -683.6440191715956 -104.98414916296157
101 -682.9919082261622 -104.75574550925394
102 -682.9919082261622 -104.75574550925394
103 -682.9919082261622 -104.75574550925394
104 -682.9919082261622 -104.75574550925394
105 -682.5002716630697 -104.34422137718992
106 -682.8231525570154 -104.25271549639196
107 -682.8231525570154 -104.25271549639196
108 -682.8231525570154 -104.25271549639196
109 -682.8231525570154 -104.25271549639196
110 -682.1638666838408 -104.03978746733507
111 -681.8871077746153 -103.7299056083423
112 -681.8871077746153 -103.7299056083423
113 -681.9474826902151 -103.62228556353936
114 -681.8270065635443 -103.53627782194874
115 -680.9884981364012 -103.0461664951788
116 -680.9884981364012 -103.0461664951788
117 -680.9884981364012 -103.0461664951788
118 -680.1656235307455 -102.3359358284459
119 -654.0287175774574 -64.50420711541688
train accuracy: 1.0
validation accuracy: 1.0

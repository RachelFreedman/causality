sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.58850771 -48.19509289
 -47.96991522 -47.53054283 -47.33487662 -47.14538324 -47.10889581
 -46.92993068 -46.92815321 -46.7141986  -46.50762306 -46.08033935
 -45.96910073 -45.94004954 -45.47189047 -45.23627882 -45.23541431
 -44.99494244 -44.99184926 -44.7644634  -44.62310756 -44.53780078
 -44.32699505 -44.29565561 -43.84464734 -43.7920154  -43.22211069
 -43.2041764  -43.14444939 -42.63170505 -42.49903983 -42.18990615
 -42.17868583 -42.08941791 -41.97868948 -41.96459824 -41.91819405
 -41.89252759 -41.86250044 -41.75643755 -41.50687738 -41.35020296
 -41.32515547 -41.22179983 -41.03992135 -40.99991982 -40.98448758
 -40.86627436 -40.84937019 -40.63970267 -40.11576908 -39.97085477
 -39.89750906 -39.72931271 -39.35357961 -39.17708227 -39.14272956
 -39.07321282 -38.91826709 -38.77785603 -38.52213956 -38.29413029
 -38.28199013 -38.28138827 -38.04661848 -37.94300503 -37.88709355
 -37.79694607 -37.70658771 -37.51412288 -37.47672391 -37.37133772
 -37.3251261  -37.11841822 -37.10344577 -36.99477768 -36.69656462
 -36.65639269 -36.58958472 -36.38667894 -36.28177521 -36.03378032
 -36.02351695 -35.97994531 -35.89835563 -35.77878287 -35.67327882
 -35.43726783 -35.41556943 -35.28979253 -35.19353216 -35.12234482
 -35.07963015 -35.06137062 -35.02402835 -35.01091702 -34.95588623
 -34.91427059 -34.86472602 -34.63851695 -34.34266183 -33.86129846
 -33.77802335 -33.7629591  -33.680383   -33.4779641  -33.26579709
 -33.17157461 -33.04852912 -32.41611773 -32.41053817 -32.02508724
 -31.90686017 -31.71028623 -31.64335581 -31.48876757 -31.44406811
 -31.37649579 -31.35230126 -31.24419956 -31.21642916 -31.15850305
 -31.06744474 -31.00231541 -30.95117262 -30.772676   -30.64081715
 -30.63466958 -30.59673213 -30.52049104 -30.42034344 -30.36982897
 -30.36743577 -30.27374553 -30.10812976 -30.06299271 -29.98083048
 -29.97767593 -29.94526105 -29.66568464 -29.20595883 -28.99559546
 -28.95484843 -28.59191373 -28.5494411  -28.42026397 -28.31342715
 -28.1712167  -28.13409378 -28.11939797 -27.79654204 -27.77638947
 -27.70726756 -27.60103341 -26.73963502 -26.7058308  -26.50715502
 -26.39763335 -26.36197542 -26.19987885 -25.99907479 -25.47507675
 -25.31217996 -25.29134047 -25.11681575 -24.96039421 -24.91350015
 -24.71538539 -24.58444108 -24.12244562 -23.96980649 -23.76917367
 -23.71252839 -23.48246476 -23.05835519 -22.79553869 -22.61211306
 -22.55283027 -22.53572133 -21.84262733 -21.72370465 -21.7167854
 -21.64185366 -21.62657756 -21.62597106 -21.44290047 -20.76562386
 -20.68104247 -20.66287704 -20.66020817 -20.5100558  -20.35567458
 -20.10309328 -19.94369578 -19.67432429 -19.50391949 -19.30989567
 -19.03153444 -19.02121155 -18.87189699 -18.77655165 -18.71737606
 -18.65414818 -18.4634721  -18.36358054 -18.2465562  -18.08445937
 -17.94319772 -17.85124778 -17.82515236 -17.67989719 -17.64064603
 -17.48133145 -17.27921081 -17.10217865 -17.06769934 -16.68041488
 -16.44066556 -16.35270212 -16.325741   -16.24688067 -16.23288157
 -16.20186147 -16.01542464 -15.4984665  -15.44345121 -15.26574292
 -15.017961   -14.79738881 -14.47834852 -14.39794675 -14.15625316
 -13.8953595  -13.69675016 -13.43873671 -13.16397094 -12.97987294
 -12.6606731  -12.5410027  -12.53347897 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.97407887 -11.96626278 -11.9523875
 -11.75626016 -11.04421156 -10.859881   -10.78272714 -10.76409188
 -10.64954177 -10.49423816  -9.92399886  -9.67992171  -9.56750101
  -9.4003034   -8.4761537   -8.40276084  -8.32638751  -8.0269637
  -7.70107293  -7.68743448  -7.67259169  -7.57539849  -7.5455064
  -7.54460172  -7.52029309  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.10832736  -7.08431127  -6.95906356
  -6.92008425  -6.77694649  -6.72206384  -6.64795748  -6.51820418
  -6.29894775  -6.05448903  -5.89467275  -5.85405865  -5.64485143
  -5.39544196  -5.38326081  -5.3472021   -5.25793019  -5.24856721
  -5.07848501  -5.06486011  -5.02795798  -4.90228293  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.329867
  -3.29356852  -3.07904644  -2.88591659  -2.83192847  -2.67390706
  -2.64166233  -2.49009827  -2.24005036  -1.91361965]
sorted_val_rewards: [-46.51840523 -46.46343149 -45.34946654 -41.76630373 -41.18200046
 -41.03294083 -40.33775477 -39.92712396 -37.15321081 -36.33815177
 -35.74239985 -34.88495647 -30.76937686 -29.22737048 -27.50815322
 -27.27876998 -24.25434383 -22.14406511 -21.86139195 -18.9341265
 -17.77364067 -16.25152473 -15.29527985 -14.24892226 -13.88057881
 -13.73405689 -12.51335454 -12.37040308 -11.47155848 -11.4307611
 -10.01645863  -6.71997062  -6.47884361  -5.61579673  -4.82757292
  -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 630
num val_labels 630
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.11287981782630357, val_acc 0.953968253968254
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.20438373068264765, val_acc 0.9349206349206349
trigger times: 1
end of epoch 2: val_loss 0.09361620353041182, val_acc 0.9523809523809523
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.10340169108453577, val_acc 0.9619047619047619
trigger times: 1
end of epoch 4: val_loss 0.09533321408207546, val_acc 0.9634920634920635
trigger times: 2
end of epoch 5: val_loss 0.11040295464972937, val_acc 0.9555555555555556
trigger times: 3
end of epoch 6: val_loss 0.09775726425322671, val_acc 0.9571428571428572
trigger times: 4
end of epoch 7: val_loss 0.11260490736819673, val_acc 0.9476190476190476
trigger times: 5
end of epoch 8: val_loss 0.23430646685935655, val_acc 0.9380952380952381
trigger times: 6
end of epoch 9: val_loss 0.141557222657357, val_acc 0.9444444444444444
trigger times: 7
end of epoch 10: val_loss 0.09170373696014907, val_acc 0.9587301587301588
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.49562259513044166, val_acc 0.919047619047619
trigger times: 1
end of epoch 12: val_loss 0.5533672038181109, val_acc 0.9396825396825397
trigger times: 2
end of epoch 13: val_loss 0.51472988088929, val_acc 0.8936507936507937
trigger times: 3
end of epoch 14: val_loss 0.18642145893840867, val_acc 0.946031746031746
trigger times: 4
end of epoch 15: val_loss 0.2785447098876305, val_acc 0.9333333333333333
trigger times: 5
end of epoch 16: val_loss 0.07375262216749337, val_acc 0.9682539682539683
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.15782210659759618, val_acc 0.9428571428571428
trigger times: 1
end of epoch 18: val_loss 0.22618677375441382, val_acc 0.9285714285714286
trigger times: 2
end of epoch 19: val_loss 0.11291605466057687, val_acc 0.9492063492063492
trigger times: 3
end of epoch 20: val_loss 0.0926400444345049, val_acc 0.9603174603174603
trigger times: 4
end of epoch 21: val_loss 0.09322733815009598, val_acc 0.9571428571428572
trigger times: 5
end of epoch 22: val_loss 0.10182004975686218, val_acc 0.9587301587301588
trigger times: 6
end of epoch 23: val_loss 0.13177885039364678, val_acc 0.9428571428571428
trigger times: 7
end of epoch 24: val_loss 0.0901094383926753, val_acc 0.9571428571428572
trigger times: 8
end of epoch 25: val_loss 0.12650564585375826, val_acc 0.9476190476190476
trigger times: 9
end of epoch 26: val_loss 0.09064122715727006, val_acc 0.9634920634920635
trigger times: 10
Early stopping.
0 -16.786024541594088 -46.51840522660448
1 -14.426037896657363 -46.463431492780636
2 -15.389377035200596 -45.3494665431493
3 -13.79491836950183 -41.76630372811994
4 -15.146276181098074 -41.182000462242996
5 -14.465003659948707 -41.03294083119757
6 -12.726459432626143 -40.337754773394856
7 -14.049580834805965 -39.927123963254076
8 -10.544713851064444 -37.15321080506123
9 -11.97052522841841 -36.33815176682742
10 -12.965900384820998 -35.74239985466346
11 -11.009754471480846 -34.884956467059645
12 -8.5136537484359 -30.76937686083762
13 -7.029603581875563 -29.227370478407224
14 -7.141887215897441 -27.50815321826314
15 -9.288181889336556 -27.278769979372363
16 -5.409540798558737 -24.254343826223852
17 -2.717131427838467 -22.144065107815585
18 -3.6956526222638786 -21.86139194767502
19 -0.8453469574451447 -18.93412649661846
20 0.32279054168611765 -17.773640667671067
21 1.4447066215798259 -16.251524732424823
22 0.29129952332004905 -15.295279851466828
23 1.7061865603900515 -14.248922255531706
24 2.0329819407197647 -13.880578810015049
25 2.642651218920946 -13.734056892508448
26 2.0774120036512613 -12.513354538352278
27 3.332038126682164 -12.370403083970322
28 3.0807076437049545 -11.47155847649776
29 2.657475396990776 -11.430761099930217
30 4.410861393902451 -10.016458630355537
31 6.328460216522217 -6.719970621583102
32 7.175300486385822 -6.47884361057096
33 7.2756486851722 -5.615796733870542
34 7.75179441832006 -4.827572916892203
35 8.804526222869754 -3.3322555012187633
train accuracy: 0.9641096204563697
validation accuracy: 0.9634920634920635
sorted_train_rewards: [-110.23395378 -110.0113642  -108.38004168 -107.21337534 -107.08029309
 -106.45562936 -105.45837507 -105.34419894 -104.79090088 -104.0151959
 -103.94856718 -103.58343946 -101.45123452 -100.96305521 -100.90629508
 -100.90444629 -100.52427394  -99.51009357  -55.21445752  -54.46505855
  -51.17814647  -49.6099766   -49.13001653  -49.03866708  -49.0331838
  -48.84818037  -48.58850771  -48.19509289  -47.53054283  -47.33487662
  -47.14538324  -47.10889581  -46.92993068  -46.92815321  -46.7141986
  -46.50762306  -46.46343149  -46.08033935  -45.96910073  -45.94004954
  -45.47189047  -45.34946654  -45.23627882  -45.23541431  -44.99494244
  -44.99184926  -44.7644634   -44.62310756  -44.53780078  -44.32699505
  -44.29565561  -43.84464734  -43.7920154   -43.22211069  -43.14444939
  -42.63170505  -42.49903983  -42.18990615  -42.17868583  -42.08941791
  -41.97868948  -41.96459824  -41.91819405  -41.89252759  -41.76630373
  -41.75643755  -41.50687738  -41.35020296  -41.32515547  -41.22179983
  -41.03992135  -41.03294083  -40.99991982  -40.86627436  -40.84937019
  -40.63970267  -40.33775477  -40.11576908  -39.97085477  -39.92712396
  -39.89750906  -39.72931271  -39.35357961  -39.14272956  -39.07321282
  -38.91826709  -38.52213956  -38.29413029  -38.28199013  -38.28138827
  -38.04661848  -37.94300503  -37.88709355  -37.79694607  -37.70658771
  -37.51412288  -37.47672391  -37.37133772  -37.11841822  -37.10344577
  -36.69656462  -36.65639269  -36.58958472  -36.38667894  -36.33815177
  -36.28177521  -36.03378032  -36.02351695  -35.97994531  -35.89835563
  -35.77878287  -35.74239985  -35.43726783  -35.41556943  -35.28979253
  -35.19353216  -35.12234482  -35.07963015  -35.06137062  -35.02402835
  -35.01091702  -34.95588623  -34.91427059  -34.88495647  -34.86472602
  -34.63851695  -33.77802335  -33.680383    -33.4779641   -33.26579709
  -33.17157461  -33.04852912  -32.41611773  -32.41053817  -32.02508724
  -31.90686017  -31.64335581  -31.48876757  -31.35230126  -31.21642916
  -31.15850305  -31.06744474  -31.00231541  -30.95117262  -30.772676
  -30.76937686  -30.64081715  -30.63466958  -30.59673213  -30.52049104
  -30.42034344  -30.36982897  -30.36743577  -30.10812976  -29.98083048
  -29.97767593  -29.94526105  -29.66568464  -29.22737048  -29.20595883
  -28.99559546  -28.95484843  -28.5494411   -28.42026397  -28.31342715
  -28.1712167   -28.13409378  -28.11939797  -27.79654204  -27.77638947
  -27.70726756  -27.60103341  -27.27876998  -26.73963502  -26.7058308
  -26.50715502  -26.39763335  -26.36197542  -26.19987885  -25.99907479
  -25.47507675  -25.31217996  -25.29134047  -25.11681575  -24.96039421
  -24.91350015  -24.71538539  -24.58444108  -24.25434383  -24.12244562
  -23.76917367  -23.71252839  -23.48246476  -23.05835519  -22.79553869
  -22.61211306  -22.55283027  -22.53572133  -22.14406511  -21.86139195
  -21.84262733  -21.72370465  -21.7167854   -21.64185366  -21.62657756
  -21.62597106  -21.44290047  -20.76562386  -20.68104247  -20.66287704
  -20.66020817  -20.5100558   -20.35567458  -20.10309328  -19.94369578
  -19.67432429  -19.50391949  -19.30989567  -19.03153444  -19.02121155
  -18.9341265   -18.87189699  -18.71737606  -18.65414818  -18.4634721
  -18.36358054  -18.2465562   -18.08445937  -17.94319772  -17.85124778
  -17.82515236  -17.77364067  -17.67989719  -17.64064603  -17.48133145
  -17.27921081  -17.10217865  -17.06769934  -16.68041488  -16.44066556
  -16.35270212  -16.325741    -16.24688067  -16.23288157  -16.20186147
  -15.4984665   -15.44345121  -15.29527985  -15.26574292  -15.017961
  -14.79738881  -14.47834852  -14.39794675  -14.24892226  -14.15625316
  -13.8953595   -13.73405689  -13.69675016  -13.43873671  -13.16397094
  -12.97987294  -12.6606731   -12.5410027   -12.53347897  -12.51335454
  -12.37040308  -12.29992996  -12.26611853  -12.16816754  -12.13592928
  -11.97407887  -11.96626278  -11.9523875   -11.75626016  -11.04421156
  -10.859881    -10.78272714  -10.76409188  -10.64954177  -10.49423816
   -9.67992171   -9.56750101   -9.4003034    -8.4761537    -8.40276084
   -8.32638751   -8.0269637    -7.70107293   -7.68743448   -7.67259169
   -7.57539849   -7.5455064    -7.54460172   -7.52029309   -7.3740849
   -7.36244313   -7.34546388   -7.1893336    -7.15421432   -7.10832736
   -6.95906356   -6.92008425   -6.77694649   -6.72206384   -6.64795748
   -6.51820418   -6.47884361   -6.29894775   -6.05448903   -5.89467275
   -5.85405865   -5.64485143   -5.61579673   -5.39544196   -5.38326081
   -5.3472021    -5.25793019   -5.24856721   -5.07848501   -5.06486011
   -4.90228293   -4.82757292   -4.63049542   -4.37983153   -4.35856953
   -4.230832     -4.0660223    -4.03104862   -4.00401798   -3.97870856
   -3.65032555   -3.38446715   -3.329867     -3.29356852   -3.07904644
   -2.88591659   -2.83192847   -2.67390706   -2.64166233   -2.49009827
   -2.24005036   -1.91361965]
sorted_val_rewards: [-107.83131135 -102.76285588  -47.96991522  -46.51840523  -43.2041764
  -41.86250044  -41.18200046  -40.98448758  -39.17708227  -38.77785603
  -37.3251261   -37.15321081  -36.99477768  -35.67327882  -34.34266183
  -33.86129846  -33.7629591   -31.71028623  -31.44406811  -31.37649579
  -31.24419956  -30.27374553  -30.06299271  -28.59191373  -27.50815322
  -23.96980649  -18.77655165  -16.25152473  -16.01542464  -13.88057881
  -11.47155848  -11.4307611   -10.01645863   -9.92399886   -7.08431127
   -6.71997062   -5.02795798   -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 703
num val_labels 703
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.13882706295886266, val_acc 0.9445234708392604
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.939640460078016, val_acc 0.9217638691322901
trigger times: 1
end of epoch 2: val_loss 0.1314750920450582, val_acc 0.9516358463726885
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.34476358605641616, val_acc 0.9473684210526315
trigger times: 1
end of epoch 4: val_loss 0.3567684129046844, val_acc 0.9416785206258891
trigger times: 2
end of epoch 5: val_loss 0.3896308042815947, val_acc 0.9075391180654339
trigger times: 3
end of epoch 6: val_loss 0.13790364568223362, val_acc 0.9459459459459459
trigger times: 4
end of epoch 7: val_loss 0.34738876998315577, val_acc 0.9103840682788051
trigger times: 5
end of epoch 8: val_loss 1.0431707602069649, val_acc 0.8492176386913229
trigger times: 6
end of epoch 9: val_loss 0.4870643953227605, val_acc 0.930298719772404
trigger times: 7
end of epoch 10: val_loss 0.12205535252036798, val_acc 0.9473684210526315
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.40386745456462947, val_acc 0.9288762446657184
trigger times: 1
end of epoch 12: val_loss 0.19823920366121256, val_acc 0.9345661450924608
trigger times: 2
end of epoch 13: val_loss 0.47382031391206647, val_acc 0.930298719772404
trigger times: 3
end of epoch 14: val_loss 0.24558942214163726, val_acc 0.9203413940256046
trigger times: 4
end of epoch 15: val_loss 0.43225656524339606, val_acc 0.9260312944523471
trigger times: 5
end of epoch 16: val_loss 0.14861785106044728, val_acc 0.9459459459459459
trigger times: 6
end of epoch 17: val_loss 0.30132220986679525, val_acc 0.9388335704125178
trigger times: 7
end of epoch 18: val_loss 0.3092925829482829, val_acc 0.9431009957325747
trigger times: 8
end of epoch 19: val_loss 0.5760192830246089, val_acc 0.9331436699857752
trigger times: 9
end of epoch 20: val_loss 0.5559703371991819, val_acc 0.9317211948790897
trigger times: 10
Early stopping.
0 -708.4321174621582 -107.83131135261674
1 -710.1641983985901 -102.76285588348956
2 -241.72805666923523 -47.96991522128461
3 -258.58698749542236 -46.51840522660448
4 -243.46490716934204 -43.20417640267866
5 -264.87400794029236 -41.86250043832566
6 -236.7392086982727 -41.182000462242996
7 -261.4683825969696 -40.98448757527241
8 -219.91335082054138 -39.17708227320146
9 -234.9982204437256 -38.77785603121339
10 -230.99182438850403 -37.325126099056185
11 -222.73019194602966 -37.15321080506123
12 -224.3109040260315 -36.99477768281456
13 -221.1096580028534 -35.67327881993891
14 -216.2109818458557 -34.34266182533649
15 -203.24948000907898 -33.861298462169636
16 -212.83339524269104 -33.762959097993274
17 -216.38197112083435 -31.710286229852567
18 -219.32068824768066 -31.444068109180083
19 -203.28189897537231 -31.376495786898342
20 -214.19199991226196 -31.244199560211694
21 -222.72150373458862 -30.27374552587196
22 -209.40936040878296 -30.06299271161229
23 -198.18753576278687 -28.59191372520921
24 -203.7129418849945 -27.50815321826314
25 -191.61258339881897 -23.969806488203698
26 -185.52867007255554 -18.776551647151997
27 -173.92664051055908 -16.251524732424823
28 -168.1210072040558 -16.015424636935276
29 -171.03096175193787 -13.880578810015049
30 -156.39588236808777 -11.47155847649776
31 -167.13668704032898 -11.430761099930217
32 -151.2835817337036 -10.016458630355537
33 -162.77742385864258 -9.923998859009732
34 -150.0118613243103 -7.08431127494343
35 -152.83421778678894 -6.719970621583102
36 -140.11966705322266 -5.027957977402961
37 -137.7296440601349 -3.3322555012187633
train accuracy: 0.9485150785460383
validation accuracy: 0.9317211948790897
sorted_train_rewards: [-110.23395378 -110.0113642  -108.38004168 -107.21337534 -107.08029309
 -106.45562936 -105.45837507 -105.34419894 -104.79090088 -104.0151959
 -103.94856718 -102.76285588 -101.45123452 -100.96305521 -100.90444629
  -99.51009357  -54.46505855  -51.17814647  -49.6099766   -49.13001653
  -49.03866708  -49.0331838   -48.84818037  -48.58850771  -48.19509289
  -47.96991522  -47.53054283  -47.33487662  -47.14538324  -47.10889581
  -46.92815321  -46.7141986   -46.51840523  -46.50762306  -46.46343149
  -46.08033935  -45.96910073  -45.94004954  -45.47189047  -45.23627882
  -44.99494244  -44.99184926  -44.7644634   -44.62310756  -44.53780078
  -44.32699505  -44.29565561  -43.84464734  -43.7920154   -43.22211069
  -43.2041764   -43.14444939  -42.63170505  -42.49903983  -42.18990615
  -42.17868583  -42.08941791  -41.97868948  -41.96459824  -41.91819405
  -41.89252759  -41.86250044  -41.76630373  -41.75643755  -41.50687738
  -41.35020296  -41.32515547  -41.22179983  -41.03992135  -41.03294083
  -40.99991982  -40.98448758  -40.86627436  -40.84937019  -40.63970267
  -40.11576908  -39.97085477  -39.92712396  -39.89750906  -39.72931271
  -39.35357961  -39.17708227  -39.14272956  -39.07321282  -38.91826709
  -38.77785603  -38.52213956  -38.29413029  -38.28199013  -38.28138827
  -38.04661848  -37.94300503  -37.88709355  -37.79694607  -37.70658771
  -37.51412288  -37.47672391  -37.37133772  -37.3251261   -37.15321081
  -37.11841822  -36.99477768  -36.69656462  -36.65639269  -36.58958472
  -36.38667894  -36.33815177  -36.28177521  -36.03378032  -36.02351695
  -35.97994531  -35.89835563  -35.77878287  -35.74239985  -35.67327882
  -35.43726783  -35.41556943  -35.28979253  -35.19353216  -35.12234482
  -35.07963015  -35.06137062  -35.02402835  -35.01091702  -34.95588623
  -34.91427059  -34.88495647  -34.86472602  -34.63851695  -34.34266183
  -33.86129846  -33.77802335  -33.680383    -33.4779641   -33.26579709
  -33.17157461  -32.41611773  -32.41053817  -32.02508724  -31.90686017
  -31.64335581  -31.44406811  -31.37649579  -31.35230126  -31.21642916
  -31.15850305  -31.06744474  -31.00231541  -30.95117262  -30.772676
  -30.76937686  -30.64081715  -30.63466958  -30.59673213  -30.52049104
  -30.42034344  -30.36982897  -30.36743577  -30.27374553  -30.10812976
  -30.06299271  -29.98083048  -29.97767593  -29.94526105  -29.66568464
  -29.22737048  -28.99559546  -28.59191373  -28.42026397  -28.31342715
  -28.1712167   -28.13409378  -28.11939797  -27.79654204  -27.77638947
  -27.70726756  -27.60103341  -27.50815322  -27.27876998  -26.73963502
  -26.7058308   -26.50715502  -26.39763335  -26.36197542  -26.19987885
  -25.99907479  -25.47507675  -25.31217996  -25.29134047  -25.11681575
  -24.96039421  -24.91350015  -24.71538539  -24.58444108  -24.25434383
  -23.76917367  -23.71252839  -23.48246476  -23.05835519  -22.61211306
  -22.55283027  -22.53572133  -21.86139195  -21.72370465  -21.7167854
  -21.64185366  -21.62657756  -21.62597106  -21.44290047  -20.76562386
  -20.68104247  -20.66020817  -20.35567458  -20.10309328  -19.94369578
  -19.67432429  -19.50391949  -19.30989567  -19.02121155  -18.9341265
  -18.87189699  -18.77655165  -18.71737606  -18.65414818  -18.4634721
  -18.36358054  -18.2465562   -18.08445937  -17.94319772  -17.85124778
  -17.82515236  -17.77364067  -17.67989719  -17.64064603  -17.48133145
  -17.27921081  -17.10217865  -17.06769934  -16.68041488  -16.44066556
  -16.39211017  -16.35270212  -16.325741    -16.25152473  -16.24688067
  -16.20186147  -16.01542464  -15.83931859  -15.4984665   -15.44345121
  -15.29527985  -15.26574292  -15.017961    -14.79738881  -14.47834852
  -14.39794675  -14.24892226  -14.15625316  -14.00837258  -13.88057881
  -13.77830513  -13.74340513  -13.73405689  -13.69675016  -13.5472533
  -13.43873671  -13.16397094  -12.81749751  -12.6606731   -12.5410027
  -12.53347897  -12.51335454  -12.37040308  -12.29992996  -12.26611853
  -12.21404647  -12.16816754  -12.13592928  -11.97407887  -11.96626278
  -11.9523875   -11.75626016  -11.70566865  -11.47155848  -11.4307611
  -11.41849202  -11.34119749  -11.10025695  -11.04421156  -10.859881
  -10.78272714  -10.76409188  -10.64954177  -10.49423816  -10.01645863
   -9.67992171   -9.56750101   -9.4003034    -9.33653122   -9.10166191
   -8.4761537    -8.40276084   -8.32638751   -8.0698961    -8.0269637
   -7.70107293   -7.68743448   -7.67259169   -7.6697832    -7.57539849
   -7.5455064    -7.54460172   -7.52029309   -7.47914694   -7.3740849
   -7.36244313   -7.34546388   -7.15421432   -7.10832736   -7.08431127
   -6.95906356   -6.92008425   -6.77694649   -6.72206384   -6.64795748
   -6.51820418   -6.47884361   -6.29894775   -6.05448903   -5.89467275
   -5.85405865   -5.64485143   -5.61579673   -5.38326081   -5.3472021
   -5.25793019   -5.24856721   -5.07848501   -5.06486011   -4.90228293
   -4.63049542   -4.37983153   -4.35856953   -4.230832     -4.0660223
   -4.03104862   -4.00401798   -3.97870856   -3.65032555   -3.38446715
   -3.329867     -3.29356852   -3.07904644   -2.88591659   -2.83192847
   -2.67390706   -2.64166233   -2.49009827   -2.24005036   -1.91361965]
sorted_val_rewards: [-107.83131135 -103.58343946 -100.90629508 -100.52427394  -55.21445752
  -46.92993068  -45.34946654  -45.23541431  -41.18200046  -40.33775477
  -37.10344577  -33.7629591   -33.04852912  -31.71028623  -31.48876757
  -31.24419956  -29.20595883  -28.95484843  -28.5494411   -24.12244562
  -23.96980649  -22.79553869  -22.14406511  -21.84262733  -20.66287704
  -20.5100558   -19.03153444  -16.23288157  -13.8953595   -12.97987294
  -11.39576183   -9.92399886   -7.44805641   -7.1893336    -6.92783809
   -6.71997062   -5.39544196   -5.02795798   -4.82757292   -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 780
num val_labels 780
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.2508530995120781, val_acc 0.9461538461538461
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.35776904454705233, val_acc 0.9538461538461539
trigger times: 1
end of epoch 2: val_loss 0.2571514230120586, val_acc 0.958974358974359
trigger times: 2
end of epoch 3: val_loss 0.23051842107097756, val_acc 0.9576923076923077
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.09612407624728751, val_acc 0.9641025641025641
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.07967908239380257, val_acc 0.9653846153846154
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.5513494087001818, val_acc 0.9230769230769231
trigger times: 1
end of epoch 7: val_loss 0.18551524310905967, val_acc 0.9525641025641025
trigger times: 2
end of epoch 8: val_loss 0.1133757115946393, val_acc 0.9474358974358974
trigger times: 3
end of epoch 9: val_loss 1.0124237269760405, val_acc 0.9384615384615385
trigger times: 4
end of epoch 10: val_loss 0.11473485518459282, val_acc 0.95
trigger times: 5
end of epoch 11: val_loss 0.20907318871113836, val_acc 0.9512820512820512
trigger times: 6
end of epoch 12: val_loss 0.07212236580038252, val_acc 0.9666666666666667
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.2557220703288041, val_acc 0.9294871794871795
trigger times: 1
end of epoch 14: val_loss 0.1503065640728694, val_acc 0.9576923076923077
trigger times: 2
end of epoch 15: val_loss 0.1548174903631054, val_acc 0.9564102564102565
trigger times: 3
end of epoch 16: val_loss 0.07301799026579205, val_acc 0.9692307692307692
trigger times: 4
end of epoch 17: val_loss 0.10913501097626255, val_acc 0.9653846153846154
trigger times: 5
end of epoch 18: val_loss 0.31372251867514767, val_acc 0.9384615384615385
trigger times: 6
end of epoch 19: val_loss 1.0485121727387179, val_acc 0.9461538461538461
trigger times: 7
end of epoch 20: val_loss 0.4696522360527164, val_acc 0.95
trigger times: 8
end of epoch 21: val_loss 0.09432329088704815, val_acc 0.9653846153846154
trigger times: 9
end of epoch 22: val_loss 0.14699048015926633, val_acc 0.9564102564102565
trigger times: 10
Early stopping.
0 -486.916752576828 -107.83131135261674
1 -483.6093773841858 -103.58343946158985
2 -484.836149930954 -100.90629507913533
3 -483.6716322898865 -100.52427394213352
4 -105.77103519439697 -55.21445752462833
5 -89.22261321544647 -46.929930683395
6 -99.45738863945007 -45.3494665431493
7 -86.19331723451614 -45.23541430806973
8 -91.6929577589035 -41.182000462242996
9 -89.2865646481514 -40.337754773394856
10 -84.64559429883957 -37.10344576818814
11 -79.42937308549881 -33.762959097993274
12 -83.22762495279312 -33.048529119631624
13 -84.738889336586 -31.710286229852567
14 -78.27254468202591 -31.488767571491348
15 -81.52325332164764 -31.244199560211694
16 -86.91818404197693 -29.20595883063468
17 -79.25413513183594 -28.954848426086794
18 -78.2291589975357 -28.549441103161325
19 -74.56471353769302 -24.122445621672618
20 -71.18745982646942 -23.969806488203698
21 -66.43222588300705 -22.795538693648083
22 -69.03421181440353 -22.144065107815585
23 -70.25376719236374 -21.842627328989234
24 -67.06945061683655 -20.66287704316175
25 -65.44550389051437 -20.510055800644814
26 -71.30969387292862 -19.031534441908885
27 -60.763453006744385 -16.23288157067845
28 -59.05252677202225 -13.895359503086498
29 -57.03755575418472 -12.979872944709582
30 -55.48719245195389 -11.395761829390118
31 -53.96578085422516 -9.923998859009732
32 -51.73659211397171 -7.448056407813751
33 -50.73555552959442 -7.189333602951256
34 -52.00211161375046 -6.927838094577214
35 -49.1238694190979 -6.719970621583102
36 -50.015691697597504 -5.395441958911243
37 -47.820007503032684 -5.027957977402961
38 -45.62160402536392 -4.827572916892203
39 -44.0746386051178 -3.3322555012187633
train accuracy: 0.9521652715667164
validation accuracy: 0.9564102564102565
sorted_train_rewards: [-110.23395378 -110.0113642  -108.38004168 -107.21337534 -107.08029309
 -106.45562936 -105.45837507 -104.79090088 -104.0151959  -103.94856718
 -103.58343946 -101.45123452 -100.96305521 -100.90444629 -100.52427394
  -99.51009357  -55.21445752  -54.46505855  -49.6099766   -49.13001653
  -48.84818037  -48.58850771  -48.19509289  -47.96991522  -47.53054283
  -47.33487662  -47.14538324  -46.92993068  -46.92815321  -46.7141986
  -46.51840523  -46.50762306  -46.46343149  -46.08033935  -45.96910073
  -45.94004954  -45.47189047  -45.34946654  -45.23627882  -45.23541431
  -44.99494244  -44.99184926  -44.7644634   -44.62310756  -44.53780078
  -44.32699505  -44.29565561  -43.84464734  -43.7920154   -43.22211069
  -43.2041764   -43.14444939  -42.63170505  -42.49903983  -42.18990615
  -42.17868583  -42.08941791  -41.97868948  -41.96459824  -41.91819405
  -41.89252759  -41.76630373  -41.75643755  -41.50687738  -41.35020296
  -41.32515547  -41.22179983  -41.18200046  -41.03992135  -41.03294083
  -40.99991982  -40.98448758  -40.86627436  -40.84937019  -40.63970267
  -40.33775477  -40.11576908  -39.97085477  -39.92712396  -39.89750906
  -39.72931271  -39.35357961  -39.14272956  -39.07321282  -38.91826709
  -38.77785603  -38.52213956  -38.28199013  -38.28138827  -38.04661848
  -37.94300503  -37.88709355  -37.79694607  -37.70658771  -37.51412288
  -37.47672391  -37.37133772  -37.3251261   -37.15321081  -37.11841822
  -36.69656462  -36.65639269  -36.58958472  -36.38667894  -36.33815177
  -36.28177521  -36.03378032  -36.02351695  -35.97994531  -35.89835563
  -35.77878287  -35.74239985  -35.67327882  -35.43726783  -35.28979253
  -35.19353216  -35.12234482  -35.07963015  -35.01091702  -34.91427059
  -34.86472602  -34.63851695  -34.34266183  -33.77802335  -33.7629591
  -33.680383    -33.4779641   -33.26579709  -33.17157461  -33.04852912
  -32.41611773  -32.41053817  -32.02508724  -31.90686017  -31.64335581
  -31.48876757  -31.44406811  -31.37649579  -31.35230126  -31.21642916
  -31.15850305  -31.06744474  -31.00231541  -30.95117262  -30.772676
  -30.76937686  -30.64081715  -30.63466958  -30.52049104  -30.42034344
  -30.36982897  -30.36743577  -30.10812976  -30.06299271  -29.98083048
  -29.97767593  -29.94526105  -29.66568464  -29.22737048  -29.20595883
  -28.99559546  -28.95484843  -28.5494411   -28.42026397  -28.31342715
  -28.1712167   -28.13409378  -28.11939797  -27.79654204  -27.77638947
  -27.70726756  -27.60103341  -27.50815322  -27.27876998  -26.73963502
  -26.7058308   -26.50715502  -26.39763335  -26.36197542  -26.19987885
  -25.47507675  -25.29134047  -25.11681575  -24.96039421  -24.91350015
  -24.71538539  -24.58444108  -24.25434383  -24.12244562  -23.96980649
  -23.76917367  -23.71252839  -23.48246476  -23.05835519  -22.79553869
  -22.61211306  -22.55283027  -22.53572133  -22.14406511  -21.86139195
  -21.72370465  -21.7167854   -21.64185366  -21.62597106  -21.44290047
  -20.76562386  -20.68104247  -20.66287704  -20.66020817  -20.5100558
  -20.35567458  -20.10309328  -19.67432429  -19.50391949  -19.30989567
  -19.03153444  -19.02121155  -18.9341265   -18.87189699  -18.77655165
  -18.71737606  -18.65414818  -18.4634721   -18.36358054  -18.2465562
  -18.08445937  -17.94319772  -17.85124778  -17.82515236  -17.77364067
  -17.67989719  -17.64064603  -17.27921081  -17.10217865  -17.06769934
  -16.68041488  -16.44066556  -16.39211017  -16.35270212  -16.325741
  -16.25152473  -16.24688067  -16.23288157  -16.20186147  -16.01542464
  -15.83931859  -15.4984665   -15.44345121  -15.26574292  -15.017961
  -14.79738881  -14.47834852  -14.24892226  -14.20354965  -14.15625316
  -14.00837258  -13.8953595   -13.88057881  -13.77830513  -13.74340513
  -13.73405689  -13.69675016  -13.43873671  -13.16397094  -12.97987294
  -12.81749751  -12.6606731   -12.5410027   -12.53618001  -12.53347897
  -12.51335454  -12.37040308  -12.29992996  -12.26611853  -12.21404647
  -12.16816754  -12.15764854  -12.13592928  -11.97407887  -11.96626278
  -11.9523875   -11.75626016  -11.70566865  -11.4307611   -11.41849202
  -11.39576183  -11.34119749  -11.18690648  -11.10025695  -11.04421156
  -11.03551002  -10.859881    -10.78272714  -10.76409188  -10.64954177
  -10.49423816  -10.25957145  -10.01645863   -9.92399886   -9.67992171
   -9.61887052   -9.56750101   -9.45622546   -9.4003034    -9.33653122
   -9.10166191   -8.79258536   -8.48219255   -8.4761537    -8.44340964
   -8.40276084   -8.32638751   -8.27625607   -8.0698961    -8.0269637
   -7.99820246   -7.70107293   -7.68743448   -7.67259169   -7.67159088
   -7.6697832    -7.65032422   -7.57539849   -7.5455064    -7.54460172
   -7.52029309   -7.47914694   -7.44805641   -7.3740849    -7.36244313
   -7.34546388   -7.25997846   -7.1893336    -7.15421432   -7.10832736
   -7.08431127   -7.08363896   -6.95906356   -6.92783809   -6.92008425
   -6.77694649   -6.72206384   -6.64795748   -6.51820418   -6.47884361
   -6.29894775   -6.05448903   -5.89467275   -5.85405865   -5.64485143
   -5.61579673   -5.38326081   -5.3472021    -5.24856721   -5.07848501
   -5.06486011   -5.02795798   -4.90228293   -4.63049542   -4.37983153
   -4.35856953   -4.230832     -4.0660223    -4.03104862   -4.00401798
   -3.97870856   -3.65032555   -3.38446715   -3.329867     -3.29356852
   -3.07904644   -2.88591659   -2.83192847   -2.67390706   -2.64166233
   -2.49009827   -2.24005036   -1.91361965]
sorted_val_rewards: [-107.83131135 -105.34419894 -102.76285588 -100.90629508  -51.17814647
  -49.03866708  -49.0331838   -47.10889581  -41.86250044  -39.17708227
  -38.29413029  -37.10344577  -36.99477768  -35.41556943  -35.06137062
  -35.02402835  -34.95588623  -34.88495647  -33.86129846  -31.71028623
  -31.24419956  -30.59673213  -30.27374553  -28.59191373  -25.99907479
  -25.31217996  -21.84262733  -21.62657756  -19.94369578  -17.48133145
  -15.29527985  -14.39794675  -13.5472533   -12.71058295  -12.49482454
  -11.47155848  -10.64073312   -6.71997062   -5.39544196   -5.25793019
   -4.82757292   -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 861
num val_labels 861
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:0
end of epoch 0: val_loss 0.20579583022802653, val_acc 0.9512195121951219
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.09900364419303258, val_acc 0.9605110336817654
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.5203711063955215, val_acc 0.943089430894309
trigger times: 1
end of epoch 3: val_loss 0.1550481742812719, val_acc 0.9454123112659698
trigger times: 2
end of epoch 4: val_loss 0.14470132750811776, val_acc 0.9477351916376306
trigger times: 3
end of epoch 5: val_loss 0.19474167596688496, val_acc 0.9372822299651568
trigger times: 4
end of epoch 6: val_loss 0.253862661061314, val_acc 0.9605110336817654
trigger times: 5
end of epoch 7: val_loss 5.096210180085349, val_acc 0.8966318234610917
trigger times: 6
end of epoch 8: val_loss 0.17161198467114028, val_acc 0.9535423925667829
trigger times: 7
end of epoch 9: val_loss 0.15624437408335595, val_acc 0.9372822299651568
trigger times: 8
end of epoch 10: val_loss 0.5149161928149834, val_acc 0.9372822299651568
trigger times: 9
end of epoch 11: val_loss 0.29152121391771146, val_acc 0.9256678281068524
trigger times: 10
Early stopping.
0 -163.91080617904663 -107.83131135261674
1 -162.32660830020905 -105.34419894017633
2 -162.22952377796173 -102.76285588348956
3 -164.13237369060516 -100.90629507913533
4 -59.70988842844963 -51.178146470411306
5 -59.587595254182816 -49.03866708341239
6 -55.865795493125916 -49.03318379995336
7 -50.72721093893051 -47.10889580853322
8 -63.08897575736046 -41.86250043832566
9 -45.081994600594044 -39.17708227320146
10 -48.66085888445377 -38.29413028930457
11 -54.1019449532032 -37.10344576818814
12 -51.489884197711945 -36.99477768281456
13 -49.095770597457886 -35.41556942545262
14 -47.30906829237938 -35.06137062419995
15 -47.40830883383751 -35.02402834523178
16 -41.02404668927193 -34.95588622511472
17 -49.22497093677521 -34.884956467059645
18 -40.278584361076355 -33.861298462169636
19 -39.68642449378967 -31.710286229852567
20 -43.80892586708069 -31.244199560211694
21 -49.917312145233154 -30.59673213038507
22 -47.599761098623276 -30.27374552587196
23 -37.82973340153694 -28.59191372520921
24 -43.8097442984581 -25.99907478839801
25 -44.74194401502609 -25.312179958453797
26 -40.878072649240494 -21.842627328989234
27 -38.354832381010056 -21.62657756231395
28 -33.556969210505486 -19.943695776581137
29 -37.293441861867905 -17.481331450792165
30 -32.23357072472572 -15.295279851466828
31 -31.123724311590195 -14.397946753443035
32 -26.463686019182205 -13.547253304798181
33 -23.97956457734108 -12.710582951081047
34 -24.53244310617447 -12.494824540255602
35 -23.648808985948563 -11.47155847649776
36 -23.409589409828186 -10.640733123018805
37 -21.682709857821465 -6.719970621583102
38 -21.60635679960251 -5.395441958911243
39 -22.202902257442474 -5.257930188924045
40 -19.657082214951515 -4.827572916892203
41 -19.199440151453018 -3.3322555012187633
train accuracy: 0.9358062913274472
validation accuracy: 0.9256678281068524
sorted_train_rewards: [-110.23395378 -110.0113642  -107.83131135 -107.21337534 -107.08029309
 -106.45562936 -105.45837507 -105.34419894 -104.0151959  -103.58343946
 -102.76285588 -101.45123452 -100.96305521 -100.90629508 -100.90444629
 -100.52427394  -99.51009357  -55.21445752  -54.46505855  -51.17814647
  -49.6099766   -49.13001653  -49.03866708  -49.0331838   -48.84818037
  -48.19509289  -47.96991522  -47.53054283  -47.33487662  -47.14538324
  -47.10889581  -46.92993068  -46.92815321  -46.7141986   -46.51840523
  -46.50762306  -46.46343149  -46.08033935  -45.96910073  -45.94004954
  -45.47189047  -45.34946654  -45.23627882  -45.23541431  -44.99494244
  -44.99184926  -44.7644634   -44.62310756  -44.53780078  -44.32699505
  -44.29565561  -43.84464734  -43.7920154   -43.22211069  -43.2041764
  -43.14444939  -42.63170505  -42.18990615  -42.17868583  -42.08941791
  -41.96459824  -41.91819405  -41.89252759  -41.86250044  -41.76630373
  -41.75643755  -41.50687738  -41.35020296  -41.22179983  -41.18200046
  -41.03992135  -41.03294083  -40.99991982  -40.98448758  -40.84937019
  -40.63970267  -40.33775477  -40.11576908  -39.97085477  -39.92712396
  -39.89750906  -39.72931271  -39.35357961  -39.17708227  -39.14272956
  -39.07321282  -38.91826709  -38.77785603  -38.52213956  -38.29413029
  -38.28199013  -38.28138827  -38.04661848  -37.88709355  -37.70658771
  -37.51412288  -37.47672391  -37.37133772  -37.3251261   -37.15321081
  -37.11841822  -37.10344577  -36.69656462  -36.65639269  -36.38667894
  -36.33815177  -36.28177521  -36.02351695  -35.97994531  -35.89835563
  -35.77878287  -35.74239985  -35.67327882  -35.43726783  -35.28979253
  -35.19353216  -35.12234482  -35.07963015  -35.01091702  -34.95588623
  -34.91427059  -34.86472602  -34.63851695  -34.34266183  -33.86129846
  -33.77802335  -33.7629591   -33.680383    -33.4779641   -33.26579709
  -33.17157461  -33.04852912  -32.41611773  -32.41053817  -32.02508724
  -31.90686017  -31.71028623  -31.64335581  -31.48876757  -31.44406811
  -31.37649579  -31.35230126  -31.21642916  -31.15850305  -31.06744474
  -31.00231541  -30.95117262  -30.772676    -30.76937686  -30.64081715
  -30.63466958  -30.59673213  -30.52049104  -30.42034344  -30.36982897
  -30.36743577  -30.27374553  -30.10812976  -30.06299271  -29.98083048
  -29.97767593  -29.94526105  -29.66568464  -29.22737048  -29.20595883
  -28.99559546  -28.95484843  -28.59191373  -28.5494411   -28.42026397
  -28.31342715  -28.1712167   -28.13409378  -28.11939797  -27.79654204
  -27.70726756  -27.60103341  -27.50815322  -26.73963502  -26.7058308
  -26.50715502  -26.39763335  -26.36197542  -26.19987885  -25.99907479
  -25.47507675  -25.29134047  -24.96039421  -24.91350015  -24.71538539
  -24.58444108  -24.25434383  -24.12244562  -23.76917367  -23.71252839
  -23.48246476  -23.05835519  -22.61211306  -22.55283027  -22.53572133
  -21.86139195  -21.84262733  -21.72370465  -21.7167854   -21.64185366
  -21.62597106  -21.44290047  -20.76562386  -20.68104247  -20.66020817
  -20.5100558   -20.35567458  -20.10309328  -19.67432429  -19.50391949
  -19.30989567  -19.03153444  -19.02121155  -18.9341265   -18.87189699
  -18.77655165  -18.71737606  -18.65414818  -18.4634721   -18.36358054
  -18.28450938  -18.2465562   -18.08445937  -17.94319772  -17.85124778
  -17.82515236  -17.77364067  -17.67989719  -17.64064603  -17.52667397
  -17.48133145  -17.27921081  -17.10217865  -17.06769934  -16.68041488
  -16.44066556  -16.39211017  -16.35270212  -16.325741    -16.25152473
  -16.24688067  -16.23288157  -16.20186147  -16.01542464  -15.94189267
  -15.83931859  -15.4984665   -15.44345121  -15.37530531  -15.29527985
  -15.26574292  -15.017961    -14.79738881  -14.24892226  -14.20354965
  -14.15625316  -14.00837258  -13.8953595   -13.88057881  -13.77830513
  -13.74340513  -13.73405689  -13.69675016  -13.5472533   -13.43873671
  -13.43106007  -13.16397094  -12.97987294  -12.81749751  -12.71058295
  -12.6606731   -12.5410027   -12.53618001  -12.53347897  -12.51335454
  -12.49482454  -12.37040308  -12.30557815  -12.29992996  -12.26611853
  -12.21404647  -12.16816754  -12.15764854  -12.13592928  -11.98821553
  -11.97407887  -11.96626278  -11.9523875   -11.85821444  -11.75626016
  -11.70566865  -11.47155848  -11.4307611   -11.41849202  -11.34119749
  -11.15617037  -11.10025695  -11.04421156  -11.03551002  -10.859881
  -10.78272714  -10.76409188  -10.64954177  -10.49423816  -10.25957145
  -10.07672293  -10.01645863   -9.92399886   -9.67992171   -9.61887052
   -9.6156047    -9.58015876   -9.56750101   -9.4749195    -9.45622546
   -9.4003034    -9.33653122   -9.29416474   -9.10166191   -8.79258536
   -8.60344733   -8.4761537    -8.46496209   -8.44340964   -8.40276084
   -8.32638751   -8.27625607   -8.0698961    -8.0269637    -7.99820246
   -7.70107293   -7.68743448   -7.67259169   -7.6697832    -7.65032422
   -7.57539849   -7.5455064    -7.54460172   -7.52029309   -7.47914694
   -7.44805641   -7.3740849    -7.36244313   -7.34546388   -7.25997846
   -7.1893336    -7.15421432   -7.10832736   -7.08431127   -7.08363896
   -6.95906356   -6.92783809   -6.92008425   -6.77694649   -6.72206384
   -6.64795748   -6.51820418   -6.47884361   -6.29894775   -6.05448903
   -5.89467275   -5.85405865   -5.64485143   -5.61579673   -5.38326081
   -5.3472021    -5.24856721   -5.07848501   -5.06486011   -5.02795798
   -4.90228293   -4.63049542   -4.37983153   -4.35856953   -4.230832
   -4.0660223    -4.03104862   -4.00401798   -3.97870856   -3.65032555
   -3.38446715   -3.329867     -3.29356852   -3.07904644   -2.88591659
   -2.83192847   -2.67390706   -2.64166233   -2.49009827   -2.24005036
   -1.91361965]
sorted_val_rewards: [-108.38004168 -104.79090088 -103.94856718  -48.58850771  -42.49903983
  -41.97868948  -41.32515547  -40.86627436  -37.94300503  -37.79694607
  -36.99477768  -36.58958472  -36.03378032  -35.41556943  -35.06137062
  -35.02402835  -34.88495647  -31.24419956  -27.77638947  -27.27876998
  -25.31217996  -25.11681575  -23.96980649  -22.79553869  -22.14406511
  -21.62657756  -20.66287704  -19.94369578  -14.85974217  -14.47834852
  -14.39794675  -11.39576183  -11.18690648  -11.02229841  -10.64073312
   -9.18143949   -8.73392328   -8.48219255   -7.67159088   -6.71997062
   -5.39544196   -5.25793019   -4.82757292   -3.3322555 ]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 946
num val_labels 946
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.0916975635057816, val_acc 0.9556025369978859
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.09961611892524838, val_acc 0.9503171247357294
trigger times: 1
end of epoch 2: val_loss 0.2174762964351432, val_acc 0.9302325581395349
trigger times: 2
end of epoch 3: val_loss 0.3821884821291769, val_acc 0.9439746300211417
trigger times: 3
end of epoch 4: val_loss 0.32965736714947885, val_acc 0.9323467230443975
trigger times: 4
end of epoch 5: val_loss 0.13619927820981959, val_acc 0.9566596194503171
trigger times: 5
end of epoch 6: val_loss 0.15561576032439342, val_acc 0.9460887949260042
trigger times: 6
end of epoch 7: val_loss 0.2712446238084583, val_acc 0.9344608879492601
trigger times: 7
end of epoch 8: val_loss 0.08542075784841674, val_acc 0.9566596194503171
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.21493121130065712, val_acc 0.9386892177589852
trigger times: 1
end of epoch 10: val_loss 7.718879962670176, val_acc 0.8805496828752643
trigger times: 2
end of epoch 11: val_loss 0.4361422064462304, val_acc 0.9069767441860465
trigger times: 3
end of epoch 12: val_loss 0.24372258988997653, val_acc 0.9397463002114165
trigger times: 4
end of epoch 13: val_loss 0.12591825051241617, val_acc 0.9577167019027484
trigger times: 5
end of epoch 14: val_loss 0.14931533844835815, val_acc 0.9534883720930233
trigger times: 6
end of epoch 15: val_loss 0.4490706360176738, val_acc 0.9302325581395349
trigger times: 7
end of epoch 16: val_loss 0.154278101140793, val_acc 0.9545454545454546
trigger times: 8
end of epoch 17: val_loss 0.2642158417581612, val_acc 0.927061310782241
trigger times: 9
end of epoch 18: val_loss 0.23096705124288747, val_acc 0.9429175475687104
trigger times: 10
Early stopping.
0 -486.87649488449097 -108.38004168270103
1 -488.1156828403473 -104.79090087933874
2 -488.02801752090454 -103.94856718036506
3 -108.14521551132202 -48.58850771418405
4 -113.21088969707489 -42.49903983418174
5 -115.97003841400146 -41.978689481633225
6 -118.16019701957703 -41.325155469676766
7 -108.4740332365036 -40.86627435792861
8 -97.0124739408493 -37.94300503359082
9 -103.24449074268341 -37.79694607206402
10 -96.49504417181015 -36.99477768281456
11 -102.69492220878601 -36.58958472489279
12 -95.64390182495117 -36.03378031573542
13 -99.23551046848297 -35.41556942545262
14 -97.6642462015152 -35.06137062419995
15 -104.84192907810211 -35.02402834523178
16 -101.91787052154541 -34.884956467059645
17 -95.60710382461548 -31.244199560211694
18 -89.78056991100311 -27.776389469559156
19 -93.35936069488525 -27.278769979372363
20 -88.04465645551682 -25.312179958453797
21 -87.915198802948 -25.116815749171092
22 -73.76147770881653 -23.969806488203698
23 -78.79512685537338 -22.795538693648083
24 -77.94728684425354 -22.144065107815585
25 -85.34129858016968 -21.62657756231395
26 -79.8201996088028 -20.66287704316175
27 -75.73368138074875 -19.943695776581137
28 -73.48310017585754 -14.859742166202029
29 -65.28090924024582 -14.478348517871964
30 -70.78665262460709 -14.397946753443035
31 -64.38600879907608 -11.395761829390118
32 -62.572229504585266 -11.18690648333451
33 -63.43781816959381 -11.022298412593859
34 -65.54928052425385 -10.640733123018805
35 -63.58736455440521 -9.181439491492632
36 -64.4002674818039 -8.733923279974388
37 -57.71794855594635 -8.48219254776146
38 -58.56112802028656 -7.671590877597001
39 -57.47333776950836 -6.719970621583102
40 -51.15381848812103 -5.395441958911243
41 -51.628401935100555 -5.257930188924045
42 -49.378312170505524 -4.827572916892203
43 -43.79575490951538 -3.3322555012187633
train accuracy: 0.9463746512250124
validation accuracy: 0.9429175475687104

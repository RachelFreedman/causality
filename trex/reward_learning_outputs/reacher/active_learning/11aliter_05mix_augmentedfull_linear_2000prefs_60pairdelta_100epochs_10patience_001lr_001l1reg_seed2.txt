[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 6.943420111316101e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.7405e-04,  2.1022e-04,  3.8172e-02, -1.9912e-04,  8.5782e-05,
         -1.2818e-05, -1.3982e-03, -1.3848e-04,  7.2770e-06,  7.8730e-05,
         -2.3655e-03, -1.7102e-01, -4.6389e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.0005537333990343995, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.1518e-02,  1.8497e-01,  1.2009e-01, -1.3019e-01,  1.8282e-01,
         -2.1417e-05, -9.1664e-03,  5.3417e-02,  6.6941e-05,  1.0869e-04,
         -2.5089e-03, -1.0015e+00, -1.7319e+00]], device='cuda:0'))])
end of epoch 3: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 2.412138537017938e-06, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 4.237722691513568e-07, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 8.46382134511714e-08, val_acc 1.0
trigger times: 4
end of epoch 7: val_loss 1.4901138456480112e-08, val_acc 1.0
trigger times: 5
end of epoch 8: val_loss 1.2904205303598815e-06, val_acc 1.0
trigger times: 6
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1643e-05,  2.9096e-02, -4.1871e-06, -5.5994e-02, -8.7875e-07,
         -5.6271e-02, -2.7708e-02,  1.8062e-02,  7.6345e-05, -3.4315e-02,
         -5.2126e-04, -1.1071e+00, -2.2586e+00]], device='cuda:0'))])
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.6348e-05,  3.7059e-06, -1.7803e-05, -1.6785e-05, -1.1299e-05,
         -4.7387e-05, -1.5525e-02,  1.8453e-06,  1.7414e-04,  3.4861e-05,
          8.1412e-04, -2.2772e-01, -2.0419e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7115e-05,  1.8128e-05, -4.6754e-05, -3.4639e-05, -2.2275e-05,
          2.0401e-04, -4.5139e-06,  7.2250e-06,  4.0537e-04,  9.1186e-05,
         -2.3633e-03, -1.1957e-04, -1.5087e+00]], device='cuda:0'))])
end of epoch 12: val_loss 0.017987909306803544, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 1.4900939277140423e-07, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.8302e-06, -1.8526e-05,  3.5352e-02,  6.7911e-07,  1.2244e-04,
          1.6036e-04, -2.2810e-06, -4.0307e-02,  5.9828e-05,  1.8765e-04,
          9.7824e-06, -2.7603e-01, -1.9400e+00]], device='cuda:0'))])
end of epoch 15: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 3.5762775496550603e-09, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9701e-02,  1.2966e-01,  1.8581e-03,  5.8649e-02,  4.6158e-05,
         -2.4695e-05, -5.9274e-03,  1.0159e-01,  1.0373e-04, -8.4765e-02,
         -1.2006e-03, -1.6025e+00, -2.6802e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.0012e-05, -1.1758e-05,  2.3761e-05, -1.1082e-05,  9.4815e-05,
          6.7113e-05,  1.9283e-06,  6.2024e-02,  2.7430e-04,  6.0582e-05,
          3.5707e-04, -3.7114e-01, -2.3519e+00]], device='cuda:0'))])
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4057e-04, -3.2878e-05,  4.8020e-05, -4.9799e-06,  1.7134e-04,
          1.5427e-04,  4.2598e-06, -1.7501e-05,  6.8273e-04,  1.7172e-04,
         -3.1727e-04, -3.8140e-04, -1.5449e+00]], device='cuda:0'))])
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9338e-01,  1.5039e-01,  3.2267e-01, -1.1194e-01,  3.2071e-02,
          2.1144e-02, -1.3873e-02, -6.0502e-02,  6.1462e-05, -9.4440e-03,
         -5.1906e-04, -1.4796e+00, -2.0508e+00]], device='cuda:0'))])
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.9559e-02,  5.9032e-02,  2.1357e-01, -4.4788e-02, -1.0061e-04,
         -5.2629e-05, -6.5242e-03, -3.9520e-02, -3.4955e-05, -9.2949e-05,
          8.1632e-04, -8.3041e-01, -1.8949e+00]], device='cuda:0'))])
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.8355e-05,  3.0372e-05, -1.9308e-05,  9.2799e-06, -2.5253e-04,
         -1.2733e-04, -6.8924e-06,  2.0071e-06, -1.6400e-04, -2.5459e-04,
         -2.3611e-03, -2.6490e-05, -1.5103e+00]], device='cuda:0'))])
end of epoch 23: val_loss 2.8096795887542215e-05, val_acc 1.0
trigger times: 1
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6861e-01,  6.6550e-06,  1.6250e-01, -4.0614e-06,  4.2872e-05,
         -8.4123e-02,  1.3281e-02, -1.6784e-02, -5.7437e-05,  7.0322e-06,
         -2.5045e-03, -1.2183e+00, -1.9919e+00]], device='cuda:0'))])
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.3297e-05,  1.3203e-05,  1.9868e-05, -1.0528e-05,  8.1141e-05,
         -2.4752e-04,  2.6537e-03, -6.1401e-06, -7.7997e-05, -1.4075e-04,
          1.1983e-05, -1.2829e-04, -1.7457e+00]], device='cuda:0'))])
end of epoch 26: val_loss 7.748601831281121e-09, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 3.236427437514067e-07, val_acc 1.0
trigger times: 2
end of epoch 28: val_loss 5.233014598715613e-07, val_acc 1.0
trigger times: 3
end of epoch 29: val_loss 2.4437890893125312e-08, val_acc 1.0
trigger times: 4
end of epoch 30: val_loss 1.6093227941382793e-08, val_acc 1.0
trigger times: 5
end of epoch 31: val_loss 2.2649252059636638e-07, val_acc 1.0
trigger times: 6
end of epoch 32: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 7
end of epoch 33: val_loss 0.00038752697544378376, val_acc 1.0
trigger times: 8
end of epoch 34: val_loss 6.381131507247062e-06, val_acc 1.0
trigger times: 9
end of epoch 35: val_loss 1.1444067474997156e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -40.41414362937212 -54.98547503240923
1 -41.9964652210474 -50.492268601198035
2 -37.620819963514805 -50.03933801517046
3 -38.70729626715183 -49.75347184620696
4 -38.31851230561733 -49.72654640753777
5 -40.68625485897064 -46.98011874490918
6 -38.54090116918087 -45.7351542845057
7 -37.013434674590826 -45.670579884154705
8 -38.112616777420044 -44.99030608142343
9 -35.81771668791771 -44.14602409201361
10 -36.21581497788429 -43.81326882122305
11 -36.68721991032362 -43.18878399086166
12 -37.47579425573349 -42.29180714825394
13 -35.457254961133 -42.00401746161006
14 -39.16542911529541 -41.6910044370425
15 -36.149515099823475 -41.68588229294918
16 -36.51821047067642 -41.281777102712205
17 -34.97787253558636 -40.44278203413966
18 -36.10621213540435 -40.34838365523108
19 -34.59966126084328 -39.599701153458774
20 -34.63248562812805 -39.57586365327889
21 -32.981147080659866 -39.31972693233231
22 -31.450982950627804 -39.024610555047154
23 -33.053119875490665 -38.45534493538269
24 -33.02020229399204 -38.41270390343083
25 -34.05021943897009 -38.35634328077039
26 -31.96814513206482 -37.79713616772368
27 -30.29249742627144 -37.741528994987384
28 -35.966767340898514 -37.66475323879293
29 -32.602777786552906 -37.513139380385574
30 -36.28927718102932 -37.1809993033689
31 -33.15782193094492 -37.100703136010694
32 -32.44501197338104 -37.00630588930485
33 -33.81803748011589 -36.821916772458344
34 -33.39070846140385 -36.48799015296732
35 -31.309940174221992 -36.20965269874363
36 -32.242082845419645 -36.19207561676116
37 -33.993977785110474 -36.114459029559086
38 -31.806023940443993 -35.78149902167743
39 -30.00287363678217 -35.394503873250635
40 -33.08896367996931 -35.26282499693737
41 -32.071250446140766 -35.24303541418371
42 -33.47466103732586 -35.209705244501436
43 -32.34889568388462 -35.0654408505187
44 -31.15586301870644 -34.80241747531743
45 -31.13944448530674 -34.64469044638467
46 -29.481297390535474 -33.84284985953318
47 -27.96805714815855 -32.70706485357069
48 -27.712926730513573 -31.969099402548657
49 -28.420823946595192 -31.7109134007892
50 -29.75368081778288 -31.64414355845032
51 -29.17073503881693 -31.392382758954444
52 -30.91452319547534 -31.223196019713853
53 -27.52672104537487 -31.12953085092458
54 -28.63963332772255 -29.39157139549552
55 -29.345067158341408 -29.340125609942326
56 -23.87240409106016 -29.106189988903285
57 -26.48904287442565 -27.41102349748205
58 -26.976649146527052 -27.343722362182305
59 -27.353846538811922 -27.196681629483837
60 -26.32529754191637 -27.07399028854534
61 -23.974578261375427 -26.7047217556024
62 -25.68983132392168 -26.244794902859052
63 -25.97243720293045 -25.548365085275513
64 -23.558004587888718 -25.45878528601009
65 -25.055232387036085 -24.879106999799365
66 -24.48519653454423 -24.828695359328833
67 -25.31390817835927 -24.592745144504722
68 -24.40950644016266 -23.978745577896312
69 -23.509937904775143 -23.57262108435893
70 -22.256913751363754 -23.44970807952351
71 -23.415557213127613 -22.745309160183492
72 -22.421422202140093 -22.60679894414887
73 -22.543633349239826 -22.19891031871716
74 -22.28267419571057 -20.656863763892378
75 -20.37127796933055 -20.444472560731253
76 -20.34056579694152 -20.19699010077007
77 -22.49792281165719 -20.13839114930498
78 -21.432984102517366 -19.63760343800059
79 -21.704547114670277 -19.515598718228343
80 -20.469541527330875 -18.92838809611677
81 -20.331113193184137 -17.994774057192853
82 -18.730032103136182 -17.55742370467821
83 -18.00599910132587 -16.823073927842348
84 -17.258173620328307 -14.855082803515382
85 -16.69456123560667 -14.531424598833084
86 -15.855961738154292 -14.442420089224363
87 -17.17631445452571 -13.596012850960644
88 -12.673675633966923 -12.68135972540495
89 -16.430037649348378 -12.66418205637357
90 -15.576759374234825 -12.30017947419658
91 -14.610585287213326 -12.151904772081672
92 -14.493471316993237 -11.788852141676486
93 -13.944325979799032 -10.869989101210326
94 -14.244751161430031 -10.327681503524177
95 -11.921932093799114 -9.8572159761571
96 -11.568329583620653 -8.330116995310416
97 -11.863247714936733 -8.133195842510668
98 -12.622332982718945 -8.108197691178031
99 -8.685557732358575 -7.57539849177145
100 -8.330878034234047 -7.362443126623615
101 -8.040850743185729 -7.108327355338034
102 -8.548606375698 -6.959063561385431
103 -8.354100037366152 -6.776946485018116
104 -7.7265093482565135 -6.7220638398623045
105 -8.564743960276246 -6.719970621583102
106 -11.908784305676818 -6.535447341844848
107 -9.838587390258908 -6.51820418055673
108 -9.57678809762001 -5.615796733870542
109 -8.15825047250837 -5.34720210027791
110 -7.939522325992584 -5.078485007852753
111 -8.182555967010558 -5.027957977402961
112 -7.447200537659228 -4.827572916892203
113 -7.770173082593828 -4.63049541560991
114 -7.528435409069061 -4.230832004686763
115 -7.728327036369592 -4.031048624093466
116 -7.112279805354774 -3.3844671463622564
117 -7.272659164853394 -3.3322555012187633
118 -7.543556213378906 -2.6416623314910934
119 -6.136416141409427 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-45.73515428 -45.67057988 -44.99030608 -43.18878399 -42.00401746
 -41.69100444 -41.2817771  -39.59970115 -39.57586365 -39.31972693
 -38.45534494 -37.74152899 -37.51313938 -37.1809993  -37.10070314
 -36.82191677 -36.48799015 -35.39450387 -35.20970524 -35.06544085
 -33.84284986 -31.22319602 -31.12953085 -27.4110235  -27.34372236
 -27.19668163 -26.2447949  -25.54836509 -24.879107   -24.82869536
 -24.59274514 -23.57262108 -23.44970808 -22.60679894 -22.19891032
 -20.65686376 -20.1969901  -19.51559872 -18.9283881  -17.72457461
 -17.63997027 -17.5574237  -17.29217499 -16.84573999 -16.84212257
 -16.67777301 -16.55367357 -16.44877666 -16.43587385 -15.90368458
 -15.47061662 -14.95182842 -14.91549969 -14.85756186 -14.8550828
 -14.54928362 -14.44242009 -13.94923417 -13.63228024 -13.38503885
 -13.38040643 -13.24728443 -13.11447076 -13.1056499  -12.66418206
 -12.57278917 -12.33657306 -12.30017947 -12.02735867 -11.9882947
 -11.78885214 -11.74376996 -11.39894516 -11.28874184 -11.28834644
 -11.13649491 -11.07825873 -10.88059287 -10.64626143 -10.5903632
 -10.34264608 -10.3276815  -10.17489689 -10.16814144 -10.05140323
  -9.61625949  -9.50018999  -9.44494494  -9.02123794  -8.99462153
  -8.97104888  -8.5251334   -8.45768839  -8.33677172  -8.32425687
  -8.13781523  -8.13319584  -7.83957492  -7.82124525  -7.78760843
  -7.71423327  -7.64611144  -7.57539849  -7.48460779  -7.16959001
  -7.11236998  -7.05153013  -6.9971333   -6.95906356  -6.77694649
  -6.71997062  -6.53544734  -5.07848501  -5.02795798  -4.82757292
  -4.63049542  -4.230832    -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 6.033332598555319e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.6334e-06, -3.5329e-05, -8.4641e-02, -8.2898e-06,  1.3711e-01,
          2.6868e-05, -7.0876e-03, -5.8253e-02, -2.6963e-01, -3.9931e-01,
          1.7267e-03, -6.6948e-01, -1.8851e+00]], device='cuda:1'))])
end of epoch 1: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.1497e-06,  1.7970e-01, -9.8252e-02,  1.5479e-01,  4.9391e-01,
         -8.9905e-05, -1.8220e-02,  1.1765e-03, -6.8948e-01, -7.2234e-01,
          3.6700e-04, -1.2983e+00, -2.2600e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.002224026873423064, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 4.6073012224923107e-07, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.0003989422678516163, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 4.3159558778071985e-06, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 5.748012245128109e-06, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.0012251621059910177, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 0.001216975616377667, val_acc 1.0
trigger times: 7
end of epoch 9: val_loss 0.0010978878946432502, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 4.0880748381049157e-05, val_acc 1.0
trigger times: 9
end of epoch 11: val_loss 0.00020446880232025678, val_acc 1.0
trigger times: 10
Early stopping.
0 -84.61961948871613 -45.7351542845057
1 -80.8821869045496 -45.670579884154705
2 -80.9921238720417 -44.99030608142343
3 -77.25303000211716 -43.18878399086166
4 -67.51914930343628 -42.00401746161006
5 -93.16697227954865 -41.6910044370425
6 -73.2104613929987 -41.281777102712205
7 -75.37805581092834 -39.599701153458774
8 -71.07629016041756 -39.57586365327889
9 -91.69150120019913 -39.31972693233231
10 -81.18979385495186 -38.45534493538269
11 -73.62258636951447 -37.741528994987384
12 -56.329406678676605 -37.513139380385574
13 -75.78170999884605 -37.1809993033689
14 -72.28133828938007 -37.100703136010694
15 -64.39838007092476 -36.821916772458344
16 -71.04751980304718 -36.48799015296732
17 -54.450427152216434 -35.394503873250635
18 -72.16060642898083 -35.209705244501436
19 -72.78801223635674 -35.0654408505187
20 -78.28756809234619 -33.84284985953318
21 -58.263764798641205 -31.223196019713853
22 -62.60193933546543 -31.12953085092458
23 -59.25811253488064 -27.41102349748205
24 -60.52414643764496 -27.343722362182305
25 -57.64976716041565 -27.196681629483837
26 -63.27340683341026 -26.244794902859052
27 -56.58353406190872 -25.548365085275513
28 -48.06311371922493 -24.879106999799365
29 -52.74319564551115 -24.828695359328833
30 -65.44427278637886 -24.592745144504722
31 -52.64598072320223 -23.57262108435893
32 -37.04550138115883 -23.44970807952351
33 -55.76797857880592 -22.60679894414887
34 -51.52459067106247 -22.19891031871716
35 -39.05880291759968 -20.656863763892378
36 -40.20957101136446 -20.19699010077007
37 -60.35605004429817 -19.515598718228343
38 -41.40089452266693 -18.92838809611677
39 -29.06374355405569 -17.724574614683846
40 -29.42491852119565 -17.6399702651913
41 -51.717849381268024 -17.55742370467821
42 -29.31861412897706 -17.29217499427452
43 -28.6448255777359 -16.845739990710214
44 -28.890009000897408 -16.842122571620603
45 -27.432724475860596 -16.6777730074026
46 -28.970521450042725 -16.553673569918445
47 -24.411809518933296 -16.44877666134057
48 -27.475435696542263 -16.435873850792657
49 -24.986246969550848 -15.90368457906321
50 -24.905116721987724 -15.470616618110476
51 -26.654062181711197 -14.951828418616127
52 -27.639087170362473 -14.915499693849776
53 -26.809485599398613 -14.85756186401059
54 -45.575271122157574 -14.855082803515382
55 -23.79851821064949 -14.549283620049934
56 -29.20751442015171 -14.442420089224363
57 -25.305612593889236 -13.949234174014835
58 -22.012119583785534 -13.632280241885553
59 -25.512756794691086 -13.38503884910182
60 -24.972474321722984 -13.380406433613054
61 -22.844158858060837 -13.247284431658793
62 -23.09852347522974 -13.114470759122947
63 -16.750830300152302 -13.105649897018628
64 -27.364808294922113 -12.66418205637357
65 -14.005205821245909 -12.572789172083947
66 -22.399781361222267 -12.336573062171826
67 -31.838729787617922 -12.30017947419658
68 -14.631595604121685 -12.027358666199445
69 -20.350705325603485 -11.988294699063266
70 -45.938655737787485 -11.788852141676486
71 -21.483911238610744 -11.743769962367569
72 -13.490808762609959 -11.398945159388258
73 -15.168176628649235 -11.288741839843139
74 -19.239211820065975 -11.288346440677055
75 -21.946666553616524 -11.136494912139815
76 -22.271712467074394 -11.078258728161936
77 -20.243853330612183 -10.88059287336969
78 -19.404616782441735 -10.646261428628662
79 -21.420593187212944 -10.590363195585068
80 -20.040834590792656 -10.342646081260657
81 -29.44122089445591 -10.327681503524177
82 -20.493843510746956 -10.174896886158171
83 -19.86310765147209 -10.168141444636143
84 -13.20660650730133 -10.051403228219092
85 -20.812305063009262 -9.616259487211018
86 -19.33684104681015 -9.50018998615456
87 -18.184056475758553 -9.444944938682474
88 -18.75000792182982 -9.021237939095299
89 -10.619096603244543 -8.99462153267818
90 -16.227004073560238 -8.971048882923059
91 -19.04430653527379 -8.525133395248877
92 -18.498639926314354 -8.4576883866055
93 -16.963152952492237 -8.336771716123224
94 -17.241827853024006 -8.324256869336358
95 -17.903531078249216 -8.137815228244785
96 -20.282380670309067 -8.133195842510668
97 -17.858278930187225 -7.839574917707994
98 -15.70496503636241 -7.821245252916755
99 -17.544055111706257 -7.787608427468648
100 -15.324267849326134 -7.714233268875124
101 -14.258474297821522 -7.6461114393525484
102 -23.24486681818962 -7.57539849177145
103 -16.571712836623192 -7.484607785058375
104 -15.485005158931017 -7.169590007095634
105 -16.423633098602295 -7.112369981574578
106 -15.142428133636713 -7.051530126982168
107 -15.662499332800508 -6.997133297274219
108 -18.628194976598024 -6.959063561385431
109 -16.351360395550728 -6.776946485018116
110 -18.192829109728336 -6.719970621583102
111 -23.77784537896514 -6.535447341844848
112 -20.426912173628807 -5.078485007852753
113 -17.363991379737854 -5.027957977402961
114 -9.937088370323181 -4.827572916892203
115 -19.88190395385027 -4.63049541560991
116 -8.046855635941029 -4.230832004686763
117 -10.508717361837626 -3.3322555012187633
118 -12.936952881515026 -2.6416623314910934
119 -5.447661727666855 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-45.73515428 -44.99030608 -43.18878399 -39.57586365 -39.31972693
 -38.45534494 -37.51313938 -37.1809993  -37.10070314 -36.48799015
 -35.20970524 -31.12953085 -27.4110235  -26.2447949  -25.54836509
 -24.59274514 -23.57262108 -22.60679894 -20.65686376 -20.1969901
 -19.51559872 -19.49075159 -18.9283881  -18.2861278  -18.09847823
 -17.63997027 -17.39097709 -17.29202133 -16.97064179 -16.84573999
 -16.84212257 -16.66287321 -16.44877666 -15.97733488 -15.90368458
 -14.95182842 -14.9179952  -14.91549969 -14.85756186 -14.7005576
 -14.54928362 -14.34891002 -14.28818367 -14.23078401 -14.16919496
 -13.99930985 -13.86219634 -13.74455538 -13.46880188 -13.38503885
 -13.11401977 -13.1056499  -12.66418206 -12.62222175 -12.57278917
 -12.30093475 -12.27194113 -12.24482954 -12.221638   -12.11024237
 -11.9882947  -11.91559599 -11.87086254 -11.66265855 -11.28834644
 -11.13649491 -10.93983337 -10.88059287 -10.64626143 -10.34264608
 -10.3276815  -10.19584836 -10.05140323  -9.99234279  -9.58939717
  -9.52912673  -9.50677831  -9.41615718  -9.16052648  -9.15086414
  -9.02123794  -8.99462153  -8.94376518  -8.68039226  -8.65854682
  -8.6550502   -8.5251334   -8.48368872  -8.46120653  -8.33677172
  -8.31621685  -8.24083967  -8.17552792  -8.13319584  -8.11712762
  -7.96960601  -7.95070067  -7.87201075  -7.84929967  -7.78760843
  -7.64611144  -7.63974599  -7.57539849  -7.53546887  -7.50239395
  -7.43569822  -7.18225619  -7.05153013  -6.95906356  -6.77694649
  -6.71997062  -6.53544734  -5.30147413  -5.11942025  -5.07848501
  -5.02795798  -4.82757292  -4.78401348  -4.63049542  -4.1009711 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 1.9643404611180414e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1017e-02,  2.8048e-02,  6.7754e-03,  2.7949e-02,  1.0379e-02,
         -9.7956e-02, -2.2363e-03, -1.0680e-03,  1.0401e-05, -3.3175e-01,
         -1.5603e-03, -2.2027e+00, -1.7252e+00]], device='cuda:2'))])
end of epoch 1: val_loss 1.4197667691462356e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.4624e-06,  2.4300e-06, -8.8565e-06,  5.5703e-06, -1.4172e-05,
         -1.8881e-04, -2.3022e-06,  1.6198e-07,  2.9291e-05, -1.8790e-05,
         -1.5602e-03, -1.6963e+00, -9.8545e-01]], device='cuda:2'))])
end of epoch 2: val_loss 0.00017727287449499586, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 1.907331978046045e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.1309e-03,  1.7904e-01,  8.8792e-02,  2.9966e-06, -2.4296e-05,
          2.6015e-05, -3.2412e-03,  1.8848e-02, -4.2666e-06, -1.9818e-05,
         -1.5599e-03, -2.4444e+00, -1.3830e+00]], device='cuda:2'))])
end of epoch 4: val_loss 1.7535402516344334e-06, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 8.84463587073725e-06, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 9.364079728300112e-06, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 0.000239755652324547, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 1.8696217650671087e-06, val_acc 1.0
trigger times: 5
end of epoch 9: val_loss 0.0004985937416610042, val_acc 1.0
trigger times: 6
end of epoch 10: val_loss 1.2349542201661735e-06, val_acc 1.0
trigger times: 7
end of epoch 11: val_loss 2.7659248143407924e-06, val_acc 1.0
trigger times: 8
end of epoch 12: val_loss 4.211454008071769e-05, val_acc 1.0
trigger times: 9
end of epoch 13: val_loss 0.00014184966481806783, val_acc 1.0
trigger times: 10
Early stopping.
0 -71.85502457618713 -45.7351542845057
1 -69.95524102449417 -44.99030608142343
2 -52.59822218120098 -43.18878399086166
3 -60.58056303858757 -39.57586365327889
4 -59.65161290764809 -39.31972693233231
5 -66.24319770932198 -38.45534493538269
6 -69.45811143517494 -37.513139380385574
7 -60.532593965530396 -37.1809993033689
8 -59.98873281478882 -37.100703136010694
9 -61.0993093252182 -36.48799015296732
10 -46.06031376123428 -35.209705244501436
11 -49.25069986283779 -31.12953085092458
12 -45.868839889764786 -27.41102349748205
13 -44.371984750032425 -26.244794902859052
14 -42.888585567474365 -25.548365085275513
15 -46.306034460663795 -24.592745144504722
16 -38.129190526902676 -23.57262108435893
17 -45.967196732759476 -22.60679894414887
18 -45.26329246163368 -20.656863763892378
19 -28.559112295508385 -20.19699010077007
20 -39.45030263066292 -19.515598718228343
21 -37.188935965299606 -19.490751594693446
22 -26.85546850785613 -18.92838809611677
23 -34.63656637072563 -18.28612780138793
24 -35.462445974349976 -18.098478233514214
25 -37.72078973054886 -17.6399702651913
26 -33.730204641819 -17.390977092867523
27 -31.143093943595886 -17.292021328048623
28 -34.40501797199249 -16.970641792457396
29 -34.21573007106781 -16.845739990710214
30 -35.37660762667656 -16.842122571620603
31 -33.345268189907074 -16.662873211464664
32 -31.23307231068611 -16.44877666134057
33 -30.69643245637417 -15.977334881249668
34 -29.36436140537262 -15.90368457906321
35 -29.014285445213318 -14.951828418616127
36 -26.238257735967636 -14.917995196925245
37 -34.07860964536667 -14.915499693849776
38 -28.56313094496727 -14.85756186401059
39 -25.752787113189697 -14.700557598908077
40 -27.919682770967484 -14.549283620049934
41 -23.7350232899189 -14.348910017218843
42 -23.004465758800507 -14.288183668340137
43 -25.321484372019768 -14.230784008826234
44 -26.30609330534935 -14.16919496415183
45 -25.294810589402914 -13.999309851455987
46 -25.095945984125137 -13.862196340698667
47 -23.810918629169464 -13.74455538188012
48 -22.346304524689913 -13.468801881534166
49 -26.89812633395195 -13.38503884910182
50 -21.758671686053276 -13.114019769179093
51 -22.71070834994316 -13.105649897018628
52 -17.84995475411415 -12.66418205637357
53 -18.688305348157883 -12.622221751577511
54 -19.204780623316765 -12.572789172083947
55 -22.417334377765656 -12.300934751779787
56 -17.273339919745922 -12.271941125574493
57 -17.313415732234716 -12.244829543296502
58 -21.798221707344055 -12.221637998049216
59 -15.357987774536014 -12.110242365517182
60 -21.709299072623253 -11.988294699063266
61 -16.973406434059143 -11.91559598638087
62 -18.051958084106445 -11.87086253961751
63 -19.39808303117752 -11.662658547150777
64 -19.672695741057396 -11.288346440677055
65 -21.544556073844433 -11.136494912139815
66 -16.195259533822536 -10.93983336636122
67 -21.236382015049458 -10.88059287336969
68 -19.02328309416771 -10.646261428628662
69 -19.120021983981133 -10.342646081260657
70 -16.972609892487526 -10.327681503524177
71 -14.274373281747103 -10.195848359972734
72 -14.810792699456215 -10.051403228219092
73 -13.419008783996105 -9.992342792649227
74 -13.701388541609049 -9.589397165050682
75 -10.58175465464592 -9.5291267267524
76 -13.002325154840946 -9.506778313974474
77 -10.54965209774673 -9.416157182615095
78 -9.714544735848904 -9.160526484223647
79 -11.80740612745285 -9.150864143631678
80 -15.416486710309982 -9.021237939095299
81 -13.95995208248496 -8.99462153267818
82 -12.041816640645266 -8.943765182718753
83 -10.237248465418816 -8.68039226263763
84 -8.726371426135302 -8.65854682104942
85 -8.140858694911003 -8.655050202402437
86 -13.767103996127844 -8.525133395248877
87 -8.61761337146163 -8.483688715240277
88 -7.020618781447411 -8.4612065273334
89 -14.11842624284327 -8.336771716123224
90 -6.77245007455349 -8.316216847841357
91 -10.867281407117844 -8.240839670512248
92 -8.384576242417097 -8.175527917323606
93 -10.752121986821294 -8.133195842510668
94 -8.741212956607342 -8.117127623902308
95 -9.068187735974789 -7.969606014422511
96 -8.859379336237907 -7.9507006719455084
97 -9.103540405631065 -7.87201075255148
98 -8.682476844638586 -7.849299669158949
99 -12.957783056423068 -7.787608427468648
100 -11.339135602116585 -7.6461114393525484
101 -6.309082668274641 -7.63974598827021
102 -10.657409466803074 -7.57539849177145
103 -7.034045048058033 -7.535468865573039
104 -7.99458921700716 -7.5023939461068165
105 -9.018709383904934 -7.435698218151715
106 -6.748249746859074 -7.18225618517933
107 -10.001883789896965 -7.051530126982168
108 -10.424580998718739 -6.959063561385431
109 -13.93647962063551 -6.776946485018116
110 -16.11729335039854 -6.719970621583102
111 -13.440817296504974 -6.535447341844848
112 -1.404771015048027 -5.301474126106841
113 -1.5313609391450882 -5.119420248559447
114 -14.958742074668407 -5.078485007852753
115 -7.400285288691521 -5.027957977402961
116 -5.6416445299983025 -4.827572916892203
117 0.2256433181464672 -4.7840134763547155
118 -13.75785693898797 -4.63049541560991
119 0.8130748402327299 -4.100971096690391
train accuracy: 1.0
validation accuracy: 1.0
[-43.18878399 -39.57586365 -39.31972693 -37.51313938 -37.1809993
 -37.10070314 -35.20970524 -27.4110235  -25.54836509 -24.59274514
 -23.57262108 -20.65686376 -18.9283881  -18.2861278  -17.39097709
 -16.84573999 -16.84212257 -14.95182842 -14.9179952  -14.91549969
 -14.85756186 -14.23078401 -13.99930985 -13.74455538 -13.46880188
 -13.1056499  -12.66418206 -12.57278917 -12.27194113 -12.24482954
 -12.221638   -11.9882947  -11.91559599 -11.48478986 -11.28834644
 -11.28269513 -11.13649491 -10.34264608 -10.3276815  -10.31428096
 -10.19584836  -9.99368318  -9.70956522  -9.56652201  -9.47966435
  -9.46733023  -9.41615718  -9.38392244  -9.27584854  -9.19775501
  -9.16526375  -9.16052648  -9.12722127  -9.00691775  -8.99992286
  -8.94376518  -8.94208014  -8.68039226  -8.65854682  -8.5251334
  -8.50091311  -8.48396243  -8.48368872  -8.46120653  -8.24083967
  -8.13597752  -8.11712762  -8.0537226   -7.97128798  -7.96960601
  -7.96137513  -7.85660294  -7.59456334  -7.56120035  -7.53546887
  -7.50239395  -7.43248525  -7.24246497  -7.09892771  -7.08750368
  -7.05153013  -6.98291079  -6.95906356  -6.88328542  -6.77694649
  -6.77292836  -6.73596632  -6.71997062  -6.53544734  -6.48827064
  -6.41935542  -6.37757469  -5.85298954  -5.72817685  -5.72622564
  -5.70633747  -5.42115165  -5.14026892  -5.11942025  -5.02795798
  -4.83092399  -4.82865923  -4.82757292  -4.81694771  -4.8078488
  -4.78401348  -4.76374091  -4.65535134  -4.53206112  -4.38179982
  -4.32729475  -4.12889873  -4.10799057  -4.00232712  -3.85235191
  -3.63587087  -2.99669015  -2.34976316  -2.34620434  -1.6567848 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.005741651063912485, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5019e-01,  3.6752e-01,  5.9029e-06,  5.0647e-04,  1.3562e-01,
         -1.7481e-01,  3.6789e-02,  4.2324e-02,  4.7770e-02,  1.5174e-01,
         -4.1743e-04, -2.1187e+00, -1.0356e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.01765492328707918, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 3.2656341328340945e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6279e-02,  6.8772e-02, -1.0402e-01,  8.3523e-02,  4.8255e-05,
         -9.4002e-06,  4.6315e-02,  7.7148e-03, -1.4349e-05,  5.1049e-06,
         -1.9963e-03, -2.5312e+00, -1.1275e+00]], device='cuda:1'))])
end of epoch 3: val_loss 0.0005427169549490074, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.00866047215916847, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 0.01540938376576058, val_acc 0.99
trigger times: 3
end of epoch 6: val_loss 9.29495605134889e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1992e-01,  2.8256e-01,  3.7245e-02,  5.3486e-03,  1.2391e-01,
          4.1453e-05,  3.9455e-02,  2.7528e-06,  6.8274e-02,  7.3734e-02,
          2.4424e-03, -3.6813e+00, -1.7446e+00]], device='cuda:1'))])
end of epoch 7: val_loss 5.735367225785381e-05, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 1.4343048756444431e-05, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 0.003543196850977317, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.14466400017866193, val_acc 0.98
trigger times: 4
end of epoch 11: val_loss 1.4296412108194544e-05, val_acc 1.0
trigger times: 5
end of epoch 12: val_loss 0.00014896191612784549, val_acc 1.0
trigger times: 6
end of epoch 13: val_loss 4.9880099514183484e-05, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 0.00020130477954321435, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 8.771970649529948e-05, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 1.1086013498129432e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3160e-01,  4.5066e-01,  1.2922e-01,  2.1531e-01,  2.9998e-01,
          1.2065e-01, -1.2991e-02, -1.1861e-01, -1.9204e-05,  1.8392e-05,
         -1.9971e-03, -3.5423e+00, -1.5395e+00]], device='cuda:1'))])
end of epoch 17: val_loss 0.0024370971818804678, val_acc 1.0
trigger times: 1
end of epoch 18: val_loss 8.951892221737979e-06, val_acc 1.0
trigger times: 2
end of epoch 19: val_loss 0.018575266685485873, val_acc 0.995
trigger times: 3
end of epoch 20: val_loss 4.112641773090786e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.5903e-06,  4.5438e-02, -1.6930e-04,  6.9791e-02,  3.2898e-05,
          6.9822e-05,  2.5481e-02,  1.4970e-02, -9.4773e-05,  2.0692e-05,
          2.4416e-03, -3.5511e+00, -1.4781e+00]], device='cuda:1'))])
end of epoch 21: val_loss 0.0039388476848647965, val_acc 1.0
trigger times: 1
end of epoch 22: val_loss 0.0006081267528660206, val_acc 1.0
trigger times: 2
end of epoch 23: val_loss 0.21611764407376322, val_acc 0.965
trigger times: 3
end of epoch 24: val_loss 1.874278324116574e-06, val_acc 1.0
trigger times: 4
end of epoch 25: val_loss 2.087443211344464e-05, val_acc 1.0
trigger times: 5
end of epoch 26: val_loss 0.02888236455418216, val_acc 0.99
trigger times: 6
end of epoch 27: val_loss 0.000269551716079377, val_acc 1.0
trigger times: 7
end of epoch 28: val_loss 2.2660221823755932e-06, val_acc 1.0
trigger times: 8
end of epoch 29: val_loss 9.650543161466629e-05, val_acc 1.0
trigger times: 9
end of epoch 30: val_loss 3.541512510594202e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -73.02970615029335 -43.18878399086166
1 -77.29190391302109 -39.57586365327889
2 -83.63471519947052 -39.31972693233231
3 -80.89737021923065 -37.513139380385574
4 -80.76331925392151 -37.1809993033689
5 -74.75148451328278 -37.100703136010694
6 -59.673675656318665 -35.209705244501436
7 -61.65543404221535 -27.41102349748205
8 -52.84873592853546 -25.548365085275513
9 -60.14233084022999 -24.592745144504722
10 -48.50848279893398 -23.57262108435893
11 -57.13494572043419 -20.656863763892378
12 -35.18265398964286 -18.92838809611677
13 -37.879382729530334 -18.28612780138793
14 -36.989047929644585 -17.390977092867523
15 -40.638869285583496 -16.845739990710214
16 -43.232507079839706 -16.842122571620603
17 -34.96413376927376 -14.951828418616127
18 -30.16789138317108 -14.917995196925245
19 -40.707761347293854 -14.915499693849776
20 -34.2746596634388 -14.85756186401059
21 -27.125355929136276 -14.230784008826234
22 -28.30884788930416 -13.999309851455987
23 -28.394676595926285 -13.74455538188012
24 -25.264114651829004 -13.468801881534166
25 -25.94090437144041 -13.105649897018628
26 -21.859743237495422 -12.66418205637357
27 -21.004533242434263 -12.572789172083947
28 -23.014931213110685 -12.271941125574493
29 -22.476980343461037 -12.244829543296502
30 -25.257911443710327 -12.221637998049216
31 -28.182930290699005 -11.988294699063266
32 -22.64753507077694 -11.91559598638087
33 -20.23339305073023 -11.484789856129499
34 -24.70228224992752 -11.288346440677055
35 -21.122605681419373 -11.282695130699953
36 -25.983982980251312 -11.136494912139815
37 -23.361383736133575 -10.342646081260657
38 -20.529130816459656 -10.327681503524177
39 -15.503315199166536 -10.314280961861064
40 -19.766229905188084 -10.195848359972734
41 -19.98611119389534 -9.993683175918981
42 -17.959121122956276 -9.709565222894039
43 -15.905192948877811 -9.566522011125985
44 -15.225049316883087 -9.479664350332623
45 -15.98234986513853 -9.467330230991891
46 -15.897791244089603 -9.416157182615095
47 -14.34280639886856 -9.383922435306305
48 -14.252477664500475 -9.275848540913927
49 -13.84242795035243 -9.197755013048635
50 -12.754901215434074 -9.165263753724206
51 -15.347209066152573 -9.160526484223647
52 -12.799885503947735 -9.127221274521968
53 -13.771196397021413 -9.006917752489057
54 -14.397700684145093 -8.99992285725172
55 -17.999076392501593 -8.943765182718753
56 -14.642277088016272 -8.942080144233044
57 -12.730078637599945 -8.68039226263763
58 -13.767032157629728 -8.65854682104942
59 -17.724904464557767 -8.525133395248877
60 -9.47689988464117 -8.50091310919901
61 -9.629392502829432 -8.483962431858552
62 -14.228247057646513 -8.483688715240277
63 -12.791767157614231 -8.4612065273334
64 -13.244574815034866 -8.240839670512248
65 -11.405894719064236 -8.135977524374193
66 -11.906611196696758 -8.117127623902308
67 -11.84770430624485 -8.053722600064578
68 -11.491599299013615 -7.971287984123293
69 -10.138401575386524 -7.969606014422511
70 -6.595487825572491 -7.961375133586069
71 -12.57728437334299 -7.8566029368279295
72 -7.570534490048885 -7.594563342123137
73 -10.652299866080284 -7.5612003475252285
74 -12.86478653550148 -7.535468865573039
75 -9.892561465501785 -7.5023939461068165
76 -4.470818310976028 -7.4324852482721395
77 -3.908075414597988 -7.24246496502205
78 -9.311049871146679 -7.098927705306281
79 -9.44837587326765 -7.087503681635319
80 -12.083365611732006 -7.051530126982168
81 -7.149717595428228 -6.982910787968871
82 -14.093306005001068 -6.959063561385431
83 -5.68148548156023 -6.883285423240972
84 -20.595814764499664 -6.776946485018116
85 -5.310125648975372 -6.772928361827614
86 -5.261111818253994 -6.735966317464037
87 -23.04711139202118 -6.719970621583102
88 -15.342203620821238 -6.535447341844848
89 -5.806158881634474 -6.488270639651921
90 -3.41737699508667 -6.41935542455863
91 -0.9147781059145927 -6.377574687151854
92 -1.9320393800735474 -5.8529895436154
93 0.9489477649331093 -5.728176854145034
94 0.5203867182135582 -5.726225642041838
95 -1.71251580119133 -5.706337465753805
96 -0.501482754945755 -5.421151654154866
97 2.894808314740658 -5.140268920955735
98 -5.825385272502899 -5.119420248559447
99 -9.273751102387905 -5.027957977402961
100 0.43752820789813995 -4.830923991814134
101 3.042928494513035 -4.828659232520133
102 -7.889974348247051 -4.827572916892203
103 2.3096002638339996 -4.816947714108353
104 1.6792766600847244 -4.8078487960416725
105 -3.0232465639710426 -4.7840134763547155
106 -2.278489664196968 -4.763740909772624
107 3.5429514050483704 -4.655351343005467
108 -0.4014224708080292 -4.532061118432481
109 1.0299336314201355 -4.381799820527099
110 -1.1168570071458817 -4.327294748896761
111 4.027506694197655 -4.128898725123693
112 -0.550283208489418 -4.107990571731516
113 5.288697585463524 -4.002327117806079
114 4.511704578995705 -3.8523519110867634
115 6.104815527796745 -3.635870867271763
116 8.334892898797989 -2.9966901466378366
117 10.94569517672062 -2.3497631595734494
118 10.322198569774628 -2.346204335167675
119 10.40986368060112 -1.6567848028390555
train accuracy: 0.9994444444444445
validation accuracy: 1.0
[-39.57586365 -39.31972693 -37.51313938 -37.1809993  -37.10070314
 -25.54836509 -24.59274514 -20.65686376 -18.9283881  -17.39097709
 -16.84573999 -14.95182842 -14.85756186 -13.99930985 -13.74455538
 -13.1056499  -12.66418206 -12.27194113 -12.24482954 -11.9882947
 -11.48478986 -11.28834644 -11.13649491 -10.71292076 -10.67926152
 -10.60830184 -10.34264608 -10.3276815  -10.06814053  -9.96368506
  -9.92370916  -9.78428197  -9.63942346  -9.47966435  -9.46733023
  -9.38392244  -9.36071003  -9.33586787  -9.27636048  -9.16526375
  -9.12722127  -9.00691775  -8.99992286  -8.95198227  -8.94273722
  -8.94208014  -8.71465611  -8.68039226  -8.4878537   -8.48368872
  -8.34544476  -8.24083967  -8.0537226   -8.04696644  -7.96960601
  -7.85660294  -7.65131563  -7.61031459  -7.59456334  -7.58319182
  -7.53546887  -7.50239395  -7.50056678  -7.42809319  -7.29842105
  -7.27615454  -7.24246497  -7.19750622  -7.08750368  -7.06774301
  -7.05276943  -6.98482533  -6.92605293  -6.92271647  -6.88328542
  -6.77694649  -6.29209904  -6.11635258  -5.93878638  -5.82691873
  -5.8092524   -5.72817685  -5.71252364  -5.70633747  -5.48763026
  -5.42115165  -5.41477254  -5.35302172  -5.11942025  -5.07567919
  -5.02449742  -4.86752615  -4.82865923  -4.82757292  -4.81694771
  -4.79029746  -4.78401348  -4.78020035  -4.76374091  -4.73741949
  -4.73097189  -4.6372362   -4.53206112  -4.48167559  -4.37609874
  -4.26153564  -4.12277038  -4.06763239  -4.02358943  -4.01919151
  -3.85235191  -3.81372308  -3.13652655  -2.99669015  -2.95657118
  -2.34620434  -2.11415247  -2.00680625  -1.6567848   -1.31062678]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.00015469165710403133, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4272e-01,  1.7408e-01,  1.9502e-04,  3.4267e-02,  4.9653e-05,
          4.3697e-05, -2.8481e-02,  4.9946e-02, -2.4308e-04,  3.9694e-04,
         -2.7486e-03, -1.7419e+00, -4.1186e-01]], device='cuda:1'))])
end of epoch 1: val_loss 0.00027621378190815447, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 2.4399042294831474e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.9023e-01,  4.0669e-01, -1.0331e-01,  3.5064e-01,  4.4109e-02,
         -3.9899e-02, -1.9363e-03,  6.9495e-02, -2.5828e-05, -3.3796e-05,
          1.5442e-03, -1.5931e+00, -9.3401e-01]], device='cuda:1'))])
end of epoch 3: val_loss 0.2266384185763269, val_acc 0.99
trigger times: 1
end of epoch 4: val_loss 0.00017219297788557242, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.00012012661826062754, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 0.04250199637371438, val_acc 0.995
trigger times: 4
end of epoch 7: val_loss 0.04213446899666451, val_acc 0.985
trigger times: 5
end of epoch 8: val_loss 2.7977870786344283e-05, val_acc 1.0
trigger times: 6
end of epoch 9: val_loss 0.03204632043803031, val_acc 0.99
trigger times: 7
end of epoch 10: val_loss 8.940644818267174e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.1159e-01,  2.6175e-01, -9.5253e-04,  2.0736e-01,  6.8010e-05,
          9.1800e-05,  2.8889e-06,  2.0934e-01, -2.8255e-04, -1.3897e-05,
         -1.2748e-03, -1.7648e+00, -1.9465e+00]], device='cuda:1'))])
end of epoch 11: val_loss 0.03886057192779727, val_acc 0.99
trigger times: 1
end of epoch 12: val_loss 0.0001220541266467734, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.10230606256932105, val_acc 0.99
trigger times: 3
end of epoch 14: val_loss 8.312487661893186e-05, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 2.029582006283448e-05, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 6.465146769180308e-05, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.8609e-01,  2.3276e-01, -9.1093e-02,  6.8474e-01,  9.9988e-02,
         -9.9709e-02,  4.0148e-02,  1.3933e-01,  1.0612e-01,  1.4592e-01,
         -1.2746e-03, -2.0086e+00, -1.7453e+00]], device='cuda:1'))])
end of epoch 18: val_loss 1.5437508405824473e-07, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.004849400816983618, val_acc 0.995
trigger times: 2
end of epoch 20: val_loss 7.094487805684225e-05, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 0.012561076581478118, val_acc 0.995
trigger times: 4
end of epoch 22: val_loss 3.5762659535976126e-08, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 0.003593559684057759, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.0006927061208488539, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 8.106201800472945e-08, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 0.00013771869172558127, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 1.1924179729980722e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -82.77478983998299 -39.57586365327889
1 -86.32129839062691 -39.31972693233231
2 -43.06290249526501 -37.513139380385574
3 -39.979296535253525 -37.1809993033689
4 -94.70335945487022 -37.100703136010694
5 -44.17490570247173 -25.548365085275513
6 -58.73143869638443 -24.592745144504722
7 -32.529324911534786 -20.656863763892378
8 -62.01369899511337 -18.92838809611677
9 -14.123329371213913 -17.390977092867523
10 -41.30678506195545 -16.845739990710214
11 -37.40158635377884 -14.951828418616127
12 -39.21306023001671 -14.85756186401059
13 -20.395483426749706 -13.999309851455987
14 -14.72116057574749 -13.74455538188012
15 -38.677955627441406 -13.105649897018628
16 -12.76239351183176 -12.66418205637357
17 -27.476663053035736 -12.271941125574493
18 -19.25662360340357 -12.244829543296502
19 -44.283715918660164 -11.988294699063266
20 -35.29011823236942 -11.484789856129499
21 -40.33619764447212 -11.288346440677055
22 -39.87812654674053 -11.136494912139815
23 -22.64675608277321 -10.71292075806092
24 -26.570451572537422 -10.679261519778208
25 -22.986175268888474 -10.608301839030222
26 -39.51192130893469 -10.342646081260657
27 -43.79234044253826 -10.327681503524177
28 -26.8125751465559 -10.068140528026715
29 -25.078862071037292 -9.963685057799404
30 -24.813541248440742 -9.92370916151865
31 -19.342149168252945 -9.784281971774401
32 -20.37671760097146 -9.63942345577694
33 -31.987750850617886 -9.479664350332623
34 -18.83865438401699 -9.467330230991891
35 -33.29422743991017 -9.383922435306305
36 -17.631508499383926 -9.360710031925452
37 -21.500656872987747 -9.335867874632077
38 -20.858114153146744 -9.276360483410961
39 -31.42258992791176 -9.165263753724206
40 15.79847925901413 -9.127221274521968
41 -23.61010479927063 -9.006917752489057
42 -22.503968343138695 -8.99992285725172
43 -22.715415954589844 -8.95198226647749
44 -11.598030716180801 -8.942737217742144
45 -28.955081671476364 -8.942080144233044
46 -8.371760800480843 -8.714656107015536
47 10.022182680666447 -8.68039226263763
48 16.64762207865715 -8.487853702555677
49 -15.276344187557697 -8.483688715240277
50 -29.9323710501194 -8.345444758180122
51 11.526447601616383 -8.240839670512248
52 -28.289810970425606 -8.053722600064578
53 -26.019496604800224 -8.046966435041158
54 16.46794380992651 -7.969606014422511
55 -22.121119007468224 -7.8566029368279295
56 24.67236962914467 -7.6513156339067905
57 -2.558647759258747 -7.610314593242017
58 -14.470938369631767 -7.594563342123137
59 -27.08164671063423 -7.583191818896811
60 -16.08214644342661 -7.535468865573039
61 12.409659281373024 -7.5023939461068165
62 18.729573227465153 -7.500566776021941
63 13.970956545323133 -7.42809318613815
64 -19.297232624143362 -7.298421051278368
65 9.676463805139065 -7.276154542888324
66 20.04109823703766 -7.24246496502205
67 32.9826278090477 -7.197506224774837
68 -25.435666501522064 -7.087503681635319
69 16.33642040938139 -7.067743005639877
70 28.705618381500244 -7.052769430539495
71 5.666711360216141 -6.9848253278488555
72 26.112545639276505 -6.9260529252990795
73 1.640227235853672 -6.922716470685388
74 -8.56075382232666 -6.883285423240972
75 -13.90871287882328 -6.776946485018116
76 31.751742213964462 -6.2920990363656415
77 25.56004849076271 -6.116352576782218
78 -6.265478774905205 -5.9387863847340485
79 0.0165545791387558 -5.826918734440358
80 10.192022524774075 -5.80925239990972
81 28.642727732658386 -5.728176854145034
82 8.966963112354279 -5.712523641419758
83 9.645390346646309 -5.706337465753805
84 11.506480678915977 -5.487630257454451
85 24.56837648153305 -5.421151654154866
86 29.101286441087723 -5.414772538397276
87 -2.666283890604973 -5.353021722031865
88 4.507894694805145 -5.119420248559447
89 13.776140481233597 -5.075679192045811
90 35.505918085575104 -5.0244974241294456
91 -2.7626043260097504 -4.867526151985875
92 30.27160581946373 -4.828659232520133
93 -5.269958570599556 -4.827572916892203
94 31.675339967012405 -4.816947714108353
95 28.12551647424698 -4.790297460383462
96 8.952647089958191 -4.7840134763547155
97 36.9125412106514 -4.780200351233374
98 9.45265045762062 -4.763740909772624
99 28.350717186927795 -4.737419488474228
100 15.497979030013084 -4.7309718867790815
101 17.85783737897873 -4.637236196629255
102 10.686967805027962 -4.532061118432481
103 34.245201379060745 -4.481675593569466
104 23.281663566827774 -4.376098740081638
105 39.079467087984085 -4.261535641183115
106 13.461072772741318 -4.122770377386251
107 28.95622369647026 -4.067632394173196
108 31.945075273513794 -4.023589428000606
109 37.65188452601433 -4.0191915116036085
110 25.91692167520523 -3.8523519110867634
111 28.693980991840363 -3.8137230844621905
112 38.15323907136917 -3.136526552869496
113 28.513011127710342 -2.9966901466378366
114 33.14611494541168 -2.956571177126045
115 34.49452042579651 -2.346204335167675
116 38.0787116587162 -2.1141524746995115
117 39.59452188014984 -2.0068062515641367
118 32.23773869872093 -1.6567848028390555
119 36.50707942247391 -1.3106267826654534
train accuracy: 1.0
validation accuracy: 1.0
[-39.57586365 -39.31972693 -37.51313938 -20.65686376 -19.24281621
 -18.50495743 -17.82597492 -17.76586403 -17.02032142 -16.84573999
 -16.75748247 -16.58177731 -16.52278569 -16.49582992 -16.40441549
 -15.03109677 -15.00664469 -14.99718477 -14.85756186 -14.61579056
 -13.44330445 -13.16695933 -13.07604389 -12.94438122 -12.66418206
 -12.63058145 -12.62193841 -12.59616817 -12.48940709 -12.33056089
 -12.27194113 -12.24482954 -11.9882947  -11.84778374 -11.28834644
 -11.01452778 -11.00038576 -10.96438787 -10.71292076 -10.67926152
 -10.60830184 -10.4212543  -10.34264608 -10.3276815  -10.09007701
  -9.96368506  -9.85956043  -9.83736061  -9.78428197  -9.48597715
  -9.48565858  -9.47966435  -9.36108742  -9.33586787  -9.27636048
  -9.16526375  -9.12722127  -9.08428009  -8.99992286  -8.95198227
  -8.94273722  -8.68039226  -8.48726621  -8.48368872  -8.24083967
  -7.96960601  -7.85660294  -7.78689344  -7.66111727  -7.61031459
  -7.58319182  -7.53546887  -7.50239395  -7.50056678  -7.31887209
  -7.30977618  -7.29842105  -7.06537994  -7.03150017  -7.009279
  -6.92605293  -6.92271647  -6.88328542  -6.77694649  -6.43259238
  -6.36275735  -6.06885328  -5.86389867  -5.72817685  -5.71252364
  -5.48763026  -5.3993896   -5.38065914  -5.07567919  -5.02449742
  -4.82757292  -4.81694771  -4.76374091  -4.73097189  -4.72326706
  -4.55753616  -4.55103873  -4.53206112  -4.34920943  -4.26153564
  -4.09368062  -4.04055533  -4.01919151  -3.87481118  -3.85235191
  -3.81372308  -3.79779216  -3.40486485  -2.95657118  -2.89516895
  -2.74634983  -2.34620434  -2.00680625  -1.6567848   -1.31062678]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 5.960462665655087e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1214e-01,  5.0676e-01,  7.3739e-03,  2.7005e-01,  3.2979e-01,
          1.0997e-01, -1.6977e-02, -5.7708e-02, -5.6041e-01, -4.9244e-01,
         -1.6950e-03, -2.6247e+00, -9.2497e-01]], device='cuda:3'))])
end of epoch 1: val_loss 4.45838085063599e-07, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.001516493046656251, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 5.954898142057629e-06, val_acc 1.0
trigger times: 3
end of epoch 4: val_loss 0.03067148447036743, val_acc 0.995
trigger times: 4
end of epoch 5: val_loss 5.960462061693761e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7774e-01,  4.6282e-01, -1.0455e-01,  5.1106e-02,  4.5991e-01,
          4.9712e-05, -4.2086e-02,  3.8712e-02, -5.9441e-01, -3.9500e-01,
         -2.0084e-03, -2.9995e+00, -1.3736e+00]], device='cuda:3'))])
end of epoch 6: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.3565e-02,  3.7914e-01, -2.2997e-03,  6.3461e-02,  1.8849e-01,
         -6.6892e-05, -5.8192e-02,  7.1510e-02, -4.3430e-01, -3.5826e-01,
          1.1618e-03, -2.8561e+00, -1.1316e+00]], device='cuda:3'))])
end of epoch 7: val_loss 1.9811429953264793e-06, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.000312911092754824, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 6.06142639227869e-07, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 2.1301411292569126e-05, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.0197652959717896, val_acc 0.99
trigger times: 5
end of epoch 12: val_loss 0.3711665221832011, val_acc 0.975
trigger times: 6
end of epoch 13: val_loss 6.318072561128929e-08, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 1.0609592631993792e-07, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 9.081596745090793e-06, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 2.3470665206239973e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -67.42637273669243 -39.57586365327889
1 -94.40320664644241 -39.31972693233231
2 -41.04814948141575 -37.513139380385574
3 -30.800475120544434 -20.656863763892378
4 -29.410353139042854 -19.242816212294166
5 -28.142372827976942 -18.50495742645877
6 -26.704241879284382 -17.82597491863555
7 -27.21641756594181 -17.765864030964657
8 -26.677088379859924 -17.02032142295731
9 -46.09850314259529 -16.845739990710214
10 -26.26258922368288 -16.757482471422303
11 -26.887913927435875 -16.58177730572604
12 -24.00054805353284 -16.522785692263998
13 -25.58786415308714 -16.495829924269778
14 -25.958472803235054 -16.404415493360194
15 -23.626529805362225 -15.031096774378039
16 -22.18758959695697 -15.006644685654399
17 -20.256325490772724 -14.9971847688964
18 -40.92910027503967 -14.85756186401059
19 -20.499594174325466 -14.615790560581251
20 -15.387822441756725 -13.443304451257804
21 -18.878013093024492 -13.166959328690657
22 -12.566292904317379 -13.076043892286735
23 -10.57991024851799 -12.944381219757929
24 -12.833787590265274 -12.66418205637357
25 -18.43679467961192 -12.6305814509087
26 -8.860188119113445 -12.621938408693614
27 -8.850731022655964 -12.596168169170387
28 -17.91914987191558 -12.489407089135321
29 -10.5387521982193 -12.33056089286093
30 -7.988907590508461 -12.271941125574493
31 -6.275147574022412 -12.244829543296502
32 -34.94108684360981 -11.988294699063266
33 -12.461580283939838 -11.847783738851842
34 -31.231384605169296 -11.288346440677055
35 -8.479280330240726 -11.014527779242215
36 -14.189688075333834 -11.000385759725312
37 -10.013436198234558 -10.964387869212313
38 -23.90719611942768 -10.71292075806092
39 -19.07308131828904 -10.679261519778208
40 -26.555565021932125 -10.608301839030222
41 -13.11468842625618 -10.42125430417041
42 -30.732833325862885 -10.342646081260657
43 -25.755709692835808 -10.327681503524177
44 -12.099816411733627 -10.09007701495914
45 -18.238838206976652 -9.963685057799404
46 -6.647534608840942 -9.859560432480492
47 -9.193404477089643 -9.837360609929247
48 -33.547284211963415 -9.784281971774401
49 -4.804008845239878 -9.485977148997533
50 -11.53050135076046 -9.485658579002585
51 -0.13643959164619446 -9.479664350332623
52 -8.72512636333704 -9.361087416250541
53 -13.463945750147104 -9.335867874632077
54 -12.084836650639772 -9.276360483410961
55 -10.754873923957348 -9.165263753724206
56 -1.5118351206183434 -9.127221274521968
57 -3.9659854117780924 -9.084280094191366
58 0.9986795894801617 -8.99992285725172
59 -8.854745626449585 -8.95198226647749
60 -26.723468706011772 -8.942737217742144
61 3.3029724694788456 -8.68039226263763
62 -7.712887566536665 -8.487266213439751
63 1.8474198319017887 -8.483688715240277
64 4.658508442342281 -8.240839670512248
65 7.123954764567316 -7.969606014422511
66 3.1211084201931953 -7.8566029368279295
67 0.994909904897213 -7.786893443501367
68 0.3782815709710121 -7.661117274367613
69 -2.3599950447678566 -7.610314593242017
70 5.63871856033802 -7.583191818896811
71 3.6111827343702316 -7.535468865573039
72 7.192175924777985 -7.5023939461068165
73 -19.802284695208073 -7.500566776021941
74 -3.939855620265007 -7.318872092639314
75 -5.567437306046486 -7.3097761834225805
76 -0.8808881789445877 -7.298421051278368
77 2.4150201082229614 -7.065379943961902
78 -3.570288136601448 -7.031500173288191
79 -0.8221057765185833 -7.009278999353541
80 1.6798149719834328 -6.9260529252990795
81 -0.8546318411827087 -6.922716470685388
82 -0.8735852167010307 -6.883285423240972
83 -0.8496298491954803 -6.776946485018116
84 2.6758625842630863 -6.432592382671392
85 0.2720562629401684 -6.362757354472518
86 3.822045087814331 -6.0688532769174985
87 3.239392224699259 -5.863898665005999
88 12.636059582233429 -5.728176854145034
89 -0.5466040968894958 -5.712523641419758
90 7.285552062094212 -5.487630257454451
91 0.09524417668581009 -5.399389602843481
92 4.355449704453349 -5.3806591375491255
93 13.964696645736694 -5.075679192045811
94 14.119143813848495 -5.0244974241294456
95 -0.8332668170332909 -4.827572916892203
96 15.93150545656681 -4.816947714108353
97 14.956820487976074 -4.763740909772624
98 15.317951798439026 -4.7309718867790815
99 4.745764575898647 -4.723267062048994
100 7.732324995100498 -4.557536158109129
101 9.848801366984844 -4.551038727121267
102 15.67658020555973 -4.532061118432481
103 11.638152055442333 -4.349209429446324
104 16.572811871767044 -4.261535641183115
105 6.280394077301025 -4.093680619749477
106 12.492371588945389 -4.040555331823342
107 16.08776119351387 -4.0191915116036085
108 10.988700911402702 -3.874811178801939
109 14.401658304035664 -3.8523519110867634
110 14.378255672752857 -3.8137230844621905
111 8.570811718702316 -3.797792162769874
112 11.254799604415894 -3.404864847186596
113 18.619341105222702 -2.956571177126045
114 13.90305832028389 -2.895168948237943
115 15.782426118850708 -2.74634982776156
116 22.87233078479767 -2.346204335167675
117 23.633852779865265 -2.0068062515641367
118 23.79851344227791 -1.6567848028390555
119 24.745689868927002 -1.3106267826654534
train accuracy: 1.0
validation accuracy: 1.0
[-39.31972693 -37.51313938 -20.65686376 -19.8874124  -19.24281621
 -19.03500682 -18.65075207 -18.16866712 -17.89855028 -17.42603426
 -17.4203458  -17.02032142 -16.84573999 -16.75748247 -16.61371714
 -16.58177731 -16.52278569 -16.40441549 -15.5459922  -15.34186813
 -15.03109677 -14.99718477 -14.61579056 -14.54696489 -13.47516616
 -13.44330445 -13.30772345 -13.23213079 -13.16695933 -12.94438122
 -12.66418206 -12.63058145 -12.62193841 -12.48940709 -12.27194113
 -12.24482954 -12.03637125 -11.28834644 -10.60830184 -10.46655183
 -10.34264608 -10.3276815  -10.21739421  -9.84873861  -9.83736061
  -9.47966435  -9.33586787  -9.30230337  -9.28153805  -9.16526375
  -8.99992286  -8.95198227  -8.83470073  -8.75513435  -8.67944075
  -8.65569585  -8.29468755  -8.24083967  -7.96960601  -7.89483782
  -7.85660294  -7.82134685  -7.78689344  -7.75020229  -7.56527074
  -7.53546887  -7.50056678  -7.31887209  -7.29842105  -7.13303588
  -7.0064268   -6.92271647  -6.86957315  -6.80564121  -6.65212406
  -6.51617954  -6.44693422  -6.43259238  -6.07932374  -6.06885328
  -5.72817685  -5.55632243  -5.41567432  -5.3993896   -5.26724028
  -5.21633873  -5.12210783  -5.07567919  -5.02449742  -4.81694771
  -4.80983222  -4.7542862   -4.7517755   -4.73882191  -4.73097189
  -4.72326706  -4.70029532  -4.62683722  -4.46263826  -4.44260589
  -4.39671052  -4.35994573  -4.34920943  -4.32156518  -4.23088841
  -4.04055533  -4.01919151  -4.00702913  -3.81372308  -3.79779216
  -3.5309077   -3.31961177  -2.89516895  -2.40637188  -2.34620434
  -2.30542912  -2.00680625  -1.86499956  -1.6567848   -1.31062678]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 6.945928887677866e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2812e-01,  1.0834e-01,  3.0454e-04,  6.4681e-03, -2.2816e-05,
          3.3663e-04,  4.3789e-03,  1.8954e-04, -8.1626e-06, -1.2258e-04,
         -2.9030e-04, -1.7409e+00, -4.0329e-01]], device='cuda:3'))])
end of epoch 1: val_loss 7.883742498858481e-05, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 1.045410790538881e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.7322e-01,  8.4227e-01, -9.4071e-02,  7.8531e-02,  6.9158e-01,
          2.6333e-01,  7.8163e-02,  2.0612e-01, -3.7691e-01, -8.9599e-02,
          1.6589e-03, -2.1076e+00, -8.4691e-01]], device='cuda:3'))])
end of epoch 3: val_loss 0.0030742211523912388, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 2.2827104818325948e-05, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.07517871578988142, val_acc 0.98
trigger times: 3
end of epoch 6: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.0750e-01,  5.5901e-01, -2.3157e-01,  1.6948e-05,  6.0783e-01,
          1.1488e-04,  3.0908e-02,  1.7113e-01, -7.9837e-01, -3.3538e-01,
         -1.5988e-03, -2.0584e+00, -6.7811e-01]], device='cuda:3'))])
end of epoch 7: val_loss 2.4113045318365777e-05, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 2.5333926414532472e-05, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 1.7881391656260348e-09, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.00017445919694974776, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.000118785662280807, val_acc 1.0
trigger times: 5
end of epoch 12: val_loss 6.727089482012616e-06, val_acc 1.0
trigger times: 6
end of epoch 13: val_loss 1.5675823988203775e-07, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 2.753694388601957e-07, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 4.768351914208324e-08, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 4.573786215225084e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -28.391334354877472 -39.31972693233231
1 -20.541957706212997 -37.513139380385574
2 12.190893918275833 -20.656863763892378
3 -77.61423668265343 -19.887412397462857
4 10.323815271258354 -19.242816212294166
5 -59.432349395006895 -19.035006817302474
6 -66.87165395915508 -18.65075207110132
7 -61.25915724039078 -18.168667120089104
8 -62.92084043100476 -17.898550275370233
9 -45.52032786607742 -17.426034263633998
10 -41.908058762550354 -17.42034579713426
11 9.779809072613716 -17.02032142295731
12 -39.24801520444453 -16.845739990710214
13 12.6264179199934 -16.757482471422303
14 -55.15790082886815 -16.613717142989117
15 11.596800930798054 -16.58177730572604
16 10.836533725261688 -16.522785692263998
17 12.093673974275589 -16.404415493360194
18 -36.80606357753277 -15.545992202687383
19 -35.47529951110482 -15.341868134619814
20 12.993194550275803 -15.031096774378039
21 13.384413659572601 -14.9971847688964
22 13.7785025537014 -14.615790560581251
23 -30.880876444280148 -14.546964893968843
24 -10.223632290959358 -13.475166163414174
25 22.214686376973987 -13.443304451257804
26 17.257353991270065 -13.307723454813354
27 29.451189398765564 -13.23213079007415
28 17.619032204151154 -13.166959328690657
29 25.470908869057894 -12.944381219757929
30 -8.980047471821308 -12.66418205637357
31 17.1290597692132 -12.6305814509087
32 23.59179005213082 -12.621938408693614
33 15.693904221057892 -12.489407089135321
34 1.1533393487334251 -12.271941125574493
35 5.369423126801848 -12.244829543296502
36 22.79424872249365 -12.036371254560509
37 -11.24274642392993 -11.288346440677055
38 20.53591039776802 -10.608301839030222
39 25.224038016051054 -10.466551834330692
40 -18.464721016585827 -10.342646081260657
41 -28.83409357443452 -10.327681503524177
42 18.454097498208284 -10.217394209451738
43 9.971589440479875 -9.848738614195986
44 25.367122242227197 -9.837360609929247
45 1.1935975663363934 -9.479664350332623
46 26.839581875130534 -9.335867874632077
47 7.682337544858456 -9.302303368012318
48 19.605634946376085 -9.281538049322021
49 23.748094592243433 -9.165263753724206
50 5.523931093513966 -8.99992285725172
51 26.45990252494812 -8.95198226647749
52 17.093491729348898 -8.83470073254567
53 12.033359669148922 -8.755134347476751
54 37.74124042503536 -8.679440754598561
55 38.0235635638237 -8.655695848821567
56 28.934686541557312 -8.294687546259297
57 28.685700342059135 -8.240839670512248
58 28.713994227349758 -7.969606014422511
59 8.530796751379967 -7.894837817135773
60 9.868090014904737 -7.8566029368279295
61 28.292796805500984 -7.821346849958816
62 36.88057750836015 -7.786893443501367
63 27.915417417883873 -7.750202285886992
64 32.514383256435394 -7.5652707376973725
65 12.419804999604821 -7.535468865573039
66 18.19387459754944 -7.500566776021941
67 31.94797057658434 -7.318872092639314
68 7.815111085772514 -7.298421051278368
69 17.57373071461916 -7.133035879764897
70 26.510496750473976 -7.006426800551302
71 26.236369418911636 -6.922716470685388
72 27.569208124652505 -6.869573150644068
73 30.722990304231644 -6.805641205812142
74 32.84519078209996 -6.652124057298917
75 35.84917908906937 -6.516179539581962
76 24.896274611353874 -6.4469342153005655
77 34.59468870609999 -6.432592382671392
78 37.78174986317754 -6.079323740279211
79 38.89514988474548 -6.0688532769174985
80 33.1794697791338 -5.728176854145034
81 35.5257999189198 -5.556322434550843
82 34.29304042458534 -5.415674324505799
83 35.352472769096494 -5.399389602843481
84 37.27730539441109 -5.267240283324333
85 39.414048075675964 -5.2163387297308175
86 37.8878473341465 -5.122107825021334
87 37.07514816522598 -5.075679192045811
88 41.070892974734306 -5.0244974241294456
89 40.903782695531845 -4.816947714108353
90 28.30862958729267 -4.809832222418352
91 41.80307447910309 -4.754286204233377
92 38.20892533659935 -4.751775496705465
93 36.31765026971698 -4.738821908511513
94 34.57800728082657 -4.7309718867790815
95 34.26833677850664 -4.723267062048994
96 41.27104842662811 -4.700295321782677
97 40.63122034072876 -4.626837219134058
98 36.91961534321308 -4.4626382597630245
99 43.81748205423355 -4.442605889097515
100 26.28082351386547 -4.396710518473766
101 29.928404992446303 -4.35994573201175
102 45.311501014977694 -4.349209429446324
103 39.470152616500854 -4.321565182843197
104 41.535998322069645 -4.230888413289276
105 43.38722373172641 -4.040555331823342
106 47.88006454706192 -4.0191915116036085
107 32.287584099918604 -4.007029127474384
108 38.53538167476654 -3.8137230844621905
109 42.379681669175625 -3.797792162769874
110 44.400437623262405 -3.530907695702055
111 41.860450118780136 -3.319611765338025
112 43.35334189981222 -2.895168948237943
113 44.37642550468445 -2.4063718848256643
114 44.16806352138519 -2.346204335167675
115 47.06041640043259 -2.30542911864898
116 48.251744240522385 -2.0068062515641367
117 45.42185682058334 -1.8649995618588944
118 46.659994304180145 -1.6567848028390555
119 50.67460650205612 -1.3106267826654534
train accuracy: 0.9977777777777778
validation accuracy: 1.0
[-37.51313938 -19.8874124  -18.81165001 -18.65075207 -17.89855028
 -17.42603426 -16.76767903 -16.61371714 -16.52278569 -16.40441549
 -16.26169222 -16.04433874 -15.5459922  -15.4253252  -15.03109677
 -14.99718477 -14.54696489 -13.98415638 -13.97030842 -13.41531169
 -13.33599308 -12.94438122 -12.67982048 -12.66418206 -12.62193841
 -12.48940709 -12.25708183 -12.18633816 -11.93748166 -11.90118285
 -11.28834644 -11.22902791 -11.01175628 -10.97961354 -10.60830184
 -10.46655183 -10.34264608 -10.3276815  -10.21739421 -10.07556014
  -9.93826961  -9.8781815   -9.84873861  -9.47966435  -9.47657475
  -9.37423791  -9.33586787  -9.30230337  -9.28153805  -9.16703086
  -9.16526375  -9.0647761   -8.95198227  -8.75513435  -8.6468868
  -8.52409123  -8.45576134  -8.30775998  -8.28809454  -8.24083967
  -8.16434786  -7.96960601  -7.89483782  -7.85660294  -7.83857333
  -7.79811816  -7.72055676  -7.55237688  -7.53546887  -7.49829866
  -7.31887209  -7.29842105  -7.25633005  -7.2414014   -7.23425694
  -7.20956735  -6.80866786  -6.65212406  -6.53294841  -6.43259238
  -6.32008782  -6.2622371   -6.07932374  -6.06885328  -6.02054342
  -5.98526631  -5.86660664  -5.82281904  -5.57302471  -5.41567432
  -5.3101156   -5.26724028  -5.1465156   -5.08785182  -5.08463898
  -5.02449742  -4.81694771  -4.81412086  -4.80983222  -4.72326706
  -4.71860623  -4.70029532  -4.62683722  -4.46263826  -4.39671052
  -4.34920943  -4.25157744  -4.19817622  -4.04055533  -4.01919151
  -3.9168888   -3.31961177  -3.22516015  -2.93980808  -2.40637188
  -2.34620434  -2.00680625  -2.00392601  -1.86499956  -1.31062678]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.009354548407923033, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.0085e-02,  3.7304e-01, -1.3085e-01,  1.5246e-01,  4.5104e-01,
         -6.6079e-02,  9.4319e-02,  1.2874e-01, -4.8049e-01, -4.2841e-01,
         -4.5634e-04, -2.3246e+00, -2.5817e-01]], device='cuda:2'))])
end of epoch 1: val_loss 5.060942243311217e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1145e-01,  4.4561e-01, -1.9724e-01,  6.5784e-02,  2.4452e-01,
          1.9473e-01,  4.5015e-03,  1.0093e-01, -1.4112e-01, -3.3895e-01,
          6.7387e-04, -2.2583e+00, -1.4917e-01]], device='cuda:2'))])
end of epoch 2: val_loss 0.0010977007441642782, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.036967344933797615, val_acc 0.985
trigger times: 2
end of epoch 4: val_loss 0.0006642899531642499, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 1.2253473583356821e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3515e-01,  5.9233e-01, -1.0805e-01,  2.4374e-01,  8.3573e-01,
         -2.9453e-05,  1.2878e-02, -6.2234e-06, -5.2322e-01, -5.0032e-05,
          1.1030e-03, -1.8661e+00, -3.6265e-01]], device='cuda:2'))])
end of epoch 6: val_loss 0.0006206277890674983, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 1.311301275563892e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1457e-01,  5.6931e-01, -2.6403e-01,  1.0120e-01,  1.0111e+00,
          5.4308e-05,  5.1961e-02,  7.3892e-02, -1.0946e+00, -7.0128e-02,
         -4.5672e-04, -2.5463e+00, -3.0962e-01]], device='cuda:2'))])
end of epoch 8: val_loss 2.8148879298690587e-05, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.0008925592235319257, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 1.1998068140428587e-06, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.0012371700224313642, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 1.966951966636543e-08, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.004406359953850014, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 6.393580455217318e-06, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 3.457064099876561e-08, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.7153941193538788, val_acc 0.925
trigger times: 9
end of epoch 17: val_loss 1.901370947621217e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -44.94890024619235 -37.513139380385574
1 -85.1494248509407 -19.887412397462857
2 -15.202258687466383 -18.81165000686687
3 -75.61138087511063 -18.65075207110132
4 -69.64555230736732 -17.898550275370233
5 -55.10431179404259 -17.426034263633998
6 -10.23568702727789 -16.767679028026983
7 -58.44331793487072 -16.613717142989117
8 -11.856804514303803 -16.522785692263998
9 -18.9637537477538 -16.404415493360194
10 -10.220969009707915 -16.261692222719844
11 -6.835039953235537 -16.044338737927326
12 -43.589139649644494 -15.545992202687383
13 -2.7113371583400294 -15.425325195598626
14 -16.550832092761993 -15.031096774378039
15 -8.596505917608738 -14.9971847688964
16 -34.26328478846699 -14.546964893968843
17 1.7226397847989574 -13.984156379712648
18 -5.673725634813309 -13.97030841563213
19 3.4247658474487253 -13.415311692661081
20 1.798294973676093 -13.33599308034852
21 7.667448880150914 -12.944381219757929
22 -2.79768358648289 -12.679820482449692
23 10.40821653418243 -12.66418205637357
24 6.158636208623648 -12.621938408693614
25 -10.015812467318028 -12.489407089135321
26 3.7723638413008302 -12.257081830633139
27 -1.9571002116426826 -12.186338157947947
28 5.870183348655701 -11.93748165517242
29 -0.28375066444277763 -11.90118285392665
30 -5.169704890809953 -11.288346440677055
31 0.8334499597549438 -11.22902790822328
32 8.19265592098236 -11.01175628438104
33 0.05932584963738918 -10.979613542333484
34 4.564062465084135 -10.608301839030222
35 9.0016481988132 -10.466551834330692
36 -5.27929912135005 -10.342646081260657
37 -8.276854800060391 -10.327681503524177
38 -5.693840842694044 -10.217394209451738
39 9.30895772203803 -10.07556014260397
40 4.28925796598196 -9.93826960962467
41 10.916361562907696 -9.878181496454127
42 3.4943110211752355 -9.848738614195986
43 -12.294887045420182 -9.479664350332623
44 6.88310069590807 -9.47657474596931
45 -2.2056916039437056 -9.374237905647544
46 2.881977928802371 -9.335867874632077
47 -0.27984701562672853 -9.302303368012318
48 -4.010629625990987 -9.281538049322021
49 3.8059967570006847 -9.167030858294677
50 5.800474488176405 -9.165263753724206
51 12.257887966930866 -9.064776095001887
52 4.862100799800828 -8.95198226647749
53 -2.285034153610468 -8.755134347476751
54 16.727243974106386 -8.64688680288017
55 -1.105013219639659 -8.524091228769816
56 4.915541863068938 -8.45576133807848
57 17.49505029246211 -8.307759983687033
58 16.911045029759407 -8.28809453873509
59 18.598895877599716 -8.240839670512248
60 7.141313347965479 -8.164347858675805
61 21.643740445375443 -7.969606014422511
62 5.462300779297948 -7.894837817135773
63 3.1280697504989803 -7.8566029368279295
64 18.86597876250744 -7.838573326324209
65 8.001596009824425 -7.798118157884665
66 5.366899503394961 -7.72055675549343
67 17.242862537503242 -7.552376881387534
68 2.6309195407666266 -7.535468865573039
69 17.981301557272673 -7.498298658560492
70 10.83822598378174 -7.318872092639314
71 -6.055416326969862 -7.298421051278368
72 12.181976589374244 -7.256330047799693
73 20.719183888286352 -7.2414014012340795
74 15.067330867052078 -7.23425693640025
75 19.329812470823526 -7.209567346389686
76 19.80150213558227 -6.808667863785255
77 9.196627254248597 -6.652124057298917
78 19.07857645303011 -6.532948408503871
79 20.27385751903057 -6.432592382671392
80 22.432763546705246 -6.320087823856232
81 23.10473307967186 -6.262237098879159
82 15.986563384532928 -6.079323740279211
83 22.95712737739086 -6.0688532769174985
84 23.9205419421196 -6.020543422830673
85 25.078411877155304 -5.985266311681195
86 19.219457864761353 -5.866606640396992
87 23.466182440519333 -5.82281904480234
88 27.614885330200195 -5.573024711429679
89 25.024130076169968 -5.415674324505799
90 26.374831333756447 -5.3101156046247855
91 25.85602405667305 -5.267240283324333
92 24.14683422446251 -5.14651560272713
93 27.171578645706177 -5.087851824435376
94 26.27008866891265 -5.084638976604463
95 27.126696437597275 -5.0244974241294456
96 27.395340085029602 -4.816947714108353
97 25.038499165326357 -4.814120858549001
98 25.337881276849657 -4.809832222418352
99 18.60418774187565 -4.723267062048994
100 28.141422770917416 -4.718606231781894
101 26.92967289686203 -4.700295321782677
102 28.385491281747818 -4.626837219134058
103 21.483172327280045 -4.4626382597630245
104 23.92268088273704 -4.396710518473766
105 32.62969955801964 -4.349209429446324
106 29.137283354997635 -4.251577437319087
107 28.257005026564002 -4.198176224690014
108 29.696539729833603 -4.040555331823342
109 30.28225764632225 -4.0191915116036085
110 27.480520576238632 -3.9168887999688566
111 28.550706148147583 -3.319611765338025
112 31.573088884353638 -3.2251601549983135
113 31.8488826751709 -2.93980808482853
114 33.91176134347916 -2.4063718848256643
115 33.69217127561569 -2.346204335167675
116 35.71292155981064 -2.0068062515641367
117 36.709301114082336 -2.003926014547661
118 37.83370494842529 -1.8649995618588944
119 40.43367797136307 -1.3106267826654534
train accuracy: 1.0
validation accuracy: 1.0
[-55.37610793 -53.90188864 -51.07717037 -37.51313938 -29.25859467
 -28.43126029 -19.8874124  -18.81165001 -17.89855028 -16.76767903
 -16.52278569 -16.40441549 -16.26169222 -15.4253252  -15.03109677
 -14.99718477 -14.42815141 -14.11047136 -14.06543262 -12.79259228
 -12.76219823 -12.74433431 -12.67982048 -12.57034127 -12.40075932
 -11.90118285 -11.6095396  -11.28834644 -11.22902791 -11.10015409
 -11.01175628 -10.7325968  -10.46655183 -10.44403281 -10.44095611
 -10.4060726  -10.3276815  -10.24511943 -10.19361073 -10.07556014
  -9.93826961  -9.8781815   -9.8721322   -9.86092515  -9.85221251
  -9.85021299  -9.78029122  -9.72196612  -9.47657475  -9.37423791
  -9.33586787  -9.31729612  -9.30230337  -9.28153805  -8.95198227
  -8.52409123  -8.48884667  -8.29613802  -8.27829568  -8.24083967
  -8.16434786  -7.9957411   -7.96960601  -7.89483782  -7.88983516
  -7.85660294  -7.84559203  -7.82718113  -7.79811816  -7.3822393
  -7.29842105  -7.09995349  -7.09646408  -6.43259238  -6.28897675
  -6.24782044  -6.06885328  -6.02054342  -6.00793675  -5.98526631
  -5.86660664  -5.8405177   -5.83483435  -5.82281904  -5.71599806
  -5.67369819  -5.59800322  -5.59198713  -5.53730194  -5.4822199
  -5.44445443  -5.41567432  -5.11347915  -5.08785182  -5.08463898
  -5.00770933  -4.81694771  -4.81412086  -4.80983222  -4.71691312
  -4.65159606  -4.62683722  -4.46263826  -4.39671052  -4.34920943
  -4.04907881  -4.04055533  -4.01919151  -3.9168888   -3.90255684
  -2.93980808  -2.88641938  -2.71495748  -2.56813094  -2.40637188
  -2.34723703  -2.34620434  -2.00680625  -2.00392601  -1.86499956]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.00028542812529174454, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.4108e-02, -2.1740e-05, -2.7488e-03,  2.5608e-03,  9.5196e-02,
          9.0026e-05, -2.5639e-04,  3.8270e-03,  1.9673e-04,  1.0094e-04,
          2.5225e-03, -1.5058e+00, -5.3577e-02]], device='cuda:2'))])
end of epoch 1: val_loss 0.1152546357607568, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5816e-01,  3.2575e-01,  6.8262e-06,  1.7363e-01,  8.2106e-01,
         -2.6729e-02,  4.9972e-03, -1.5939e-01, -9.6213e-01,  5.8038e-02,
         -1.0526e-03, -2.6235e+00, -1.0929e+00]], device='cuda:2'))])
end of epoch 3: val_loss 1.2737525112278547e-05, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.013932263851165772, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.8218e-01, -5.5118e-06, -1.4529e-01,  2.1641e-01,  1.4136e+00,
         -4.6088e-01, -6.2743e-03, -2.3004e-06, -1.4581e+00,  5.2272e-05,
          9.3962e-04, -2.8508e+00, -1.4855e+00]], device='cuda:2'))])
end of epoch 6: val_loss 1.1920921245689441e-08, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0023998261201914417, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 7.510128853027709e-08, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 1.4265076024457813e-05, val_acc 1.0
trigger times: 4
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.4426e-01,  2.9695e-01, -1.8107e-01,  1.9806e-01,  1.6342e+00,
         -9.8123e-02, -2.3870e-02, -5.7078e-02, -1.2460e+00,  1.9981e-05,
         -6.4246e-04, -2.9837e+00, -2.1512e+00]], device='cuda:2'))])
end of epoch 11: val_loss 7.816996076144278e-06, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.00034125054410630187, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 0.11134670563388621, val_acc 0.995
trigger times: 3
end of epoch 14: val_loss 3.9935002789093234e-08, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 7.441621320062098e-05, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 0.02092522940505603, val_acc 0.995
trigger times: 6
end of epoch 17: val_loss 0.0005152441369925498, val_acc 1.0
trigger times: 7
end of epoch 18: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 8
end of epoch 19: val_loss 1.4535994102971018e-06, val_acc 1.0
trigger times: 9
end of epoch 20: val_loss 3.297064049121445e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -96.8275785446167 -55.37610793226023
1 -94.934330701828 -53.90188864178995
2 -91.81778740882874 -51.07717037294617
3 -152.4241613149643 -37.513139380385574
4 -78.02535405755043 -29.258594665064997
5 -90.6965982913971 -28.431260291249906
6 -135.90490472316742 -19.887412397462857
7 -83.73808860778809 -18.81165000686687
8 -118.62952828407288 -17.898550275370233
9 -71.15266340970993 -16.767679028026983
10 -58.904667139053345 -16.522785692263998
11 -68.19220244884491 -16.404415493360194
12 -76.04938578605652 -16.261692222719844
13 -59.678070425987244 -15.425325195598626
14 -62.276613891124725 -15.031096774378039
15 -52.26765584945679 -14.9971847688964
16 -53.998690724372864 -14.42815140979679
17 -67.03805804252625 -14.110471364968028
18 -56.45426791906357 -14.065432621703204
19 -69.65551948547363 -12.792592280947487
20 -72.41234713792801 -12.762198225523013
21 -37.12976914644241 -12.744334308078406
22 -64.29587376117706 -12.679820482449692
23 -76.14752078056335 -12.570341265070416
24 -68.80462366342545 -12.400759321113764
25 -59.91206967830658 -11.90118285392665
26 -32.17272888123989 -11.609539600001554
27 -21.096544161438942 -11.288346440677055
28 -62.79057937860489 -11.22902790822328
29 -63.81010973453522 -11.100154092786427
30 -27.69124060124159 -11.01175628438104
31 -44.81042143702507 -10.732596800125306
32 -25.29710452631116 -10.466551834330692
33 -47.120823472738266 -10.444032809059108
34 -61.97620636224747 -10.44095611401804
35 -59.86989241838455 -10.406072597127395
36 -13.217376723885536 -10.327681503524177
37 -51.557100504636765 -10.245119429410915
38 -38.70257729291916 -10.193610725482145
39 -39.788630574941635 -10.07556014260397
40 -46.89638590812683 -9.93826960962467
41 -20.372586600482464 -9.878181496454127
42 -48.42629960179329 -9.872132202442446
43 -57.13032442331314 -9.860925148825789
44 -31.189361982047558 -9.852212514868041
45 -58.888388246297836 -9.850212988395915
46 -52.31922376155853 -9.780291223991883
47 -40.58533804118633 -9.721966118216129
48 -43.359197556972504 -9.47657474596931
49 -45.75658471882343 -9.374237905647544
50 -61.39505696296692 -9.335867874632077
51 -24.924111969769 -9.317296122828937
52 -58.19387638568878 -9.302303368012318
53 -38.19650959968567 -9.281538049322021
54 -55.914551079273224 -8.95198226647749
55 -40.26296180486679 -8.524091228769816
56 -32.619656912982464 -8.488846673399044
57 -17.413322657346725 -8.29613802008782
58 -24.663423091173172 -8.278295683174203
59 -29.387584447860718 -8.240839670512248
60 -44.538055539131165 -8.164347858675805
61 -27.099381260573864 -7.995741100669417
62 -18.89010800421238 -7.969606014422511
63 -46.51146024465561 -7.894837817135773
64 -18.704178676009178 -7.889835163057102
65 -53.729979276657104 -7.8566029368279295
66 -22.76689851284027 -7.845592030486049
67 -31.87752116844058 -7.827181125082897
68 -28.042221641168 -7.798118157884665
69 -15.54208304733038 -7.382239297457808
70 -66.67989152669907 -7.298421051278368
71 -28.024408161640167 -7.099953494210567
72 -10.942626796662807 -7.096464081883708
73 -10.29363888502121 -6.432592382671392
74 -18.58654147386551 -6.288976754362249
75 -9.06324788928032 -6.247820440734541
76 -10.274560004472733 -6.0688532769174985
77 -1.8978899419307709 -6.020543422830673
78 -14.040223866701126 -6.007936750422423
79 -7.424065243452787 -5.985266311681195
80 -5.304676420986652 -5.866606640396992
81 -7.058093249797821 -5.840517696561822
82 -9.625059813261032 -5.834834353138624
83 -0.681210346519947 -5.82281904480234
84 -2.875601977109909 -5.715998064302696
85 -9.62317943572998 -5.67369818920163
86 -11.696326598525047 -5.598003221637787
87 -7.768037170171738 -5.591987127704268
88 -2.499565653502941 -5.5373019351081885
89 -10.312152776867151 -5.482219898969396
90 -2.70330086350441 -5.4444544346305275
91 0.44250065460801125 -5.415674324505799
92 -1.6472753956913948 -5.113479152524089
93 0.8745168969035149 -5.087851824435376
94 -10.974078860133886 -5.084638976604463
95 -1.0136041045188904 -5.007709327857718
96 0.6063977889716625 -4.816947714108353
97 -8.192643575370312 -4.814120858549001
98 -9.821804642677307 -4.809832222418352
99 -4.944921985268593 -4.716913123923519
100 -1.3835967667400837 -4.651596055680976
101 0.6068464145064354 -4.626837219134058
102 -2.6659233570098877 -4.4626382597630245
103 -6.3430140838027 -4.396710518473766
104 -0.2452089488506317 -4.349209429446324
105 -0.387491837143898 -4.049078813840524
106 1.09274435415864 -4.040555331823342
107 -6.314271878451109 -4.0191915116036085
108 5.817738048732281 -3.9168887999688566
109 3.564435161650181 -3.9025568395730055
110 13.671265378594398 -2.93980808482853
111 10.958223447203636 -2.8864193843797725
112 6.516518652439117 -2.7149574846016904
113 12.710931420326233 -2.5681309408059074
114 13.536656677722931 -2.4063718848256643
115 14.111050188541412 -2.347237031572228
116 14.346292242407799 -2.346204335167675
117 11.489241778850555 -2.0068062515641367
118 17.531617403030396 -2.003926014547661
119 16.687776029109955 -1.8649995618588944
train accuracy: 0.9994444444444445
validation accuracy: 1.0
[-55.37610793 -53.90188864 -51.07717037 -19.8874124  -18.93757193
 -18.81165001 -18.73967651 -18.07395417 -18.04092782 -17.89855028
 -17.54426724 -17.49490215 -17.15689213 -17.10877605 -16.85620135
 -16.76767903 -16.5761576  -16.52278569 -16.40441549 -15.99392486
 -15.90284153 -15.69647403 -15.26454026 -15.03109677 -14.94326536
 -14.11047136 -12.96221766 -12.76219823 -12.57034127 -12.4786264
 -11.6095396  -11.28834644 -11.10015409 -11.06385736 -11.01175628
 -10.92957546 -10.7325968  -10.62437351 -10.44403281 -10.4060726
 -10.3276815  -10.24511943 -10.14102883  -9.8721322   -9.85021299
  -9.49027343  -9.47657475  -9.30230337  -8.94911896  -8.80146926
  -8.52409123  -8.46888743  -8.24083967  -8.16434786  -7.89483782
  -7.86176529  -7.39970516  -7.3822393   -7.29842105  -7.27061466
  -7.09995349  -7.00133307  -6.81863225  -6.80069904  -6.75658205
  -6.6963547   -6.55902615  -6.45900634  -6.39396507  -6.24782044
  -6.23697289  -6.06885328  -6.00793675  -5.86660664  -5.82281904
  -5.78078874  -5.71599806  -5.4822199   -5.41567432  -5.11347915
  -5.08785182  -5.08463898  -5.00770933  -4.95170556  -4.81694771
  -4.81412086  -4.81368248  -4.74960199  -4.71691312  -4.69065502
  -4.62683722  -4.20794658  -4.15852816  -4.12911264  -4.12215872
  -4.05165242  -4.04055533  -4.01919151  -3.98542908  -3.93238597
  -3.84690509  -3.81366454  -3.74727939  -3.74559856  -3.70822008
  -3.14164929  -2.93980808  -2.71495748  -2.61464118  -2.56813094
  -2.43783781  -2.40637188  -2.34723703  -2.34620434  -2.00680625
  -2.00392601  -1.86499956  -1.74944139  -1.60420926  -0.81901038]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 5.459237134939343e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2822e-01,  4.4752e-05, -1.1434e-05,  2.2377e-05,  1.1861e-02,
         -3.0975e-06, -3.4160e-03,  4.8557e-06,  3.4733e-05, -2.3458e-05,
         -5.9238e-04, -1.1238e+00, -3.8039e-01]], device='cuda:2'))])
end of epoch 1: val_loss 0.0004071021823619247, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 3.647344186894941e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.3773e-01,  3.6692e-02,  8.9476e-03,  3.1904e-03,  9.4606e-01,
          1.8819e-04, -3.4350e-04,  7.7772e-02, -3.9685e-01,  3.6557e-05,
          1.6036e-03, -1.6635e+00, -6.8612e-01]], device='cuda:2'))])
end of epoch 3: val_loss 3.719245520272807e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.1211e-01,  1.7256e-01,  1.0532e-05,  2.7434e-06,  7.0652e-01,
         -2.8217e-05, -9.0241e-07, -1.5992e-05, -3.3050e-01, -3.4975e-05,
         -1.9596e-03, -1.5995e+00, -3.2699e-01]], device='cuda:2'))])
end of epoch 4: val_loss 0.1473619220975877, val_acc 0.985
trigger times: 1
end of epoch 5: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.1529e-01,  3.8912e-01,  2.1490e-04, -5.2982e-06,  1.0014e+00,
         -3.0899e-05, -3.0647e-02,  5.8103e-02, -9.4013e-01, -2.5112e-05,
          1.1629e-03, -2.0635e+00, -8.3839e-01]], device='cuda:2'))])
end of epoch 6: val_loss 3.5762777983450177e-09, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 1.5982411699858468, val_acc 0.915
trigger times: 2
end of epoch 8: val_loss 1.1503586165417801e-07, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.3315616597503716, val_acc 0.97
trigger times: 4
end of epoch 10: val_loss 3.814690384018604e-08, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 5.841221533131602e-08, val_acc 1.0
trigger times: 6
end of epoch 12: val_loss 0.0007111443401072393, val_acc 1.0
trigger times: 7
end of epoch 13: val_loss 4.11270366384997e-08, val_acc 1.0
trigger times: 8
end of epoch 14: val_loss 4.708744654635666e-08, val_acc 1.0
trigger times: 9
end of epoch 15: val_loss 1.4066505151788534e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -17.76772554218769 -55.37610793226023
1 -16.79080404341221 -53.90188864178995
2 -19.228296294808388 -51.07717037294617
3 -46.75370419025421 -19.887412397462857
4 -14.600376039743423 -18.93757193410032
5 -24.699316650629044 -18.81165000686687
6 -10.902559906244278 -18.73967650878228
7 -14.679161414504051 -18.073954169457647
8 -11.724221095442772 -18.04092782181678
9 -39.29276296496391 -17.898550275370233
10 -6.211627252399921 -17.54426724084796
11 -10.05635355040431 -17.494902152943794
12 -20.038276098668575 -17.15689213042813
13 -24.00231274217367 -17.108776051385746
14 -23.725315757095814 -16.856201351024367
15 -14.977521240711212 -16.767679028026983
16 -20.641056537628174 -16.576157599151664
17 -6.722955621778965 -16.522785692263998
18 -11.899509653449059 -16.404415493360194
19 -27.039645448327065 -15.993924863697691
20 -12.490148888435215 -15.902841531489633
21 0.5530294901691377 -15.696474031408975
22 -1.5782532882876694 -15.264540263000683
23 -8.407930605113506 -15.031096774378039
24 1.2912481622770429 -14.943265362343569
25 -19.493860229849815 -14.110471364968028
26 7.196568572893739 -12.962217660095831
27 -27.796451836824417 -12.762198225523013
28 -31.47061276435852 -12.570341265070416
29 1.0961856450885534 -12.478626404764997
30 16.496745720505714 -11.609539600001554
31 -9.353716377168894 -11.288346440677055
32 -21.90388932824135 -11.100154092786427
33 15.098561361432076 -11.063857360754957
34 17.60428613424301 -11.01175628438104
35 6.522443451453 -10.929575458283075
36 -3.8155390229076147 -10.732596800125306
37 10.219063490629196 -10.624373510774404
38 -8.943297699093819 -10.444032809059108
39 -18.762887209653854 -10.406072597127395
40 1.4046029280871153 -10.327681503524177
41 -13.46205858886242 -10.245119429410915
42 10.031882181763649 -10.141028831117373
43 -9.946369864046574 -9.872132202442446
44 -17.974630504846573 -9.850212988395915
45 -8.16005701199174 -9.490273434452003
46 -4.545958870090544 -9.47657474596931
47 -19.32886315509677 -9.302303368012318
48 13.8097632676363 -8.94911896212173
49 14.690173480659723 -8.801469257265925
50 -0.9637240683659911 -8.524091228769816
51 -5.618623085319996 -8.468887430651794
52 10.279196816496551 -8.240839670512248
53 -7.208578541874886 -8.164347858675805
54 -11.417543739080429 -7.894837817135773
55 24.4397511780262 -7.86176529337025
56 6.3726632203906775 -7.39970516340022
57 23.85141870379448 -7.382239297457808
58 -28.525629207491875 -7.298421051278368
59 24.02705729007721 -7.270614656568981
60 9.84426099061966 -7.099953494210567
61 20.746350318193436 -7.001333068828838
62 25.303724706172943 -6.818632246960502
63 21.297000885009766 -6.800699037226459
64 0.28610089235007763 -6.756582045858895
65 -0.46794735826551914 -6.696354698544381
66 17.090523779392242 -6.559026149385048
67 3.618651505559683 -6.459006344822167
68 21.9673093855381 -6.3939650700050334
69 28.95977196097374 -6.247820440734541
70 26.685233414173126 -6.236972892000211
71 27.900229454040527 -6.0688532769174985
72 21.771916806697845 -6.007936750422423
73 30.188495874404907 -5.866606640396992
74 32.31029713153839 -5.82281904480234
75 29.383703261613846 -5.78078873940941
76 32.23145776987076 -5.715998064302696
77 22.182804584503174 -5.482219898969396
78 32.986098289489746 -5.415674324505799
79 34.88348966836929 -5.113479152524089
80 34.7707622051239 -5.087851824435376
81 22.795295923948288 -5.084638976604463
82 34.70773357152939 -5.007709327857718
83 32.3952052295208 -4.951705559235491
84 35.287045896053314 -4.816947714108353
85 23.08854115009308 -4.814120858549001
86 32.24464589357376 -4.813682484052973
87 34.62949860095978 -4.749601985629164
88 31.216191560029984 -4.716913123923519
89 21.619004547595978 -4.690655017967139
90 34.95588767528534 -4.626837219134058
91 34.29555916786194 -4.207946575217971
92 35.82879465818405 -4.158528159978416
93 25.593795359134674 -4.129112638200174
94 32.77084136009216 -4.122158719072504
95 34.92045569419861 -4.05165242209985
96 35.65325927734375 -4.040555331823342
97 31.65692138671875 -4.0191915116036085
98 31.10746495425701 -3.9854290816836992
99 37.25674331188202 -3.9323859717113923
100 36.77461361885071 -3.84690509410518
101 25.979682058095932 -3.8136645449734625
102 28.19739854335785 -3.7472793926063215
103 36.83404183387756 -3.7455985584343083
104 26.464370727539062 -3.7082200816236273
105 37.73624449968338 -3.141649290633311
106 42.44538640975952 -2.93980808482853
107 39.291636407375336 -2.7149574846016904
108 41.48249399662018 -2.614641181549277
109 42.86673456430435 -2.5681309408059074
110 40.361012041568756 -2.4378378068901383
111 42.84091401100159 -2.4063718848256643
112 44.50762301683426 -2.347237031572228
113 43.23803246021271 -2.346204335167675
114 42.011831641197205 -2.0068062515641367
115 45.18855029344559 -2.003926014547661
116 44.810003995895386 -1.8649995618588944
117 44.57535183429718 -1.7494413941026523
118 44.75150316953659 -1.6042092623434276
119 47.26506221294403 -0.8190103835290985
train accuracy: 1.0
validation accuracy: 1.0
[-53.90188864 -51.07717037 -19.8874124  -19.85186184 -19.30773161
 -18.81165001 -18.78544957 -18.73967651 -18.61316294 -18.42187904
 -18.292595   -18.27809313 -18.07395417 -17.89855028 -17.79028705
 -17.70655449 -17.62356285 -17.49490215 -17.10877605 -17.09587584
 -16.85620135 -16.76767903 -16.52278569 -16.11740336 -15.99392486
 -15.94696455 -15.90284153 -15.69647403 -15.28940833 -15.28128558
 -15.26454026 -14.94326536 -14.88516077 -14.2853697  -13.16245316
 -13.0221113  -12.96221766 -12.77643776 -12.76219823 -12.33682288
 -12.23992194 -12.06028857 -11.6095396  -11.2269195  -11.11992467
 -10.44403281 -10.4396805  -10.3276815   -9.97080441  -9.8721322
  -9.76118774  -9.49027343  -9.47657475  -9.26717423  -9.06199067
  -8.80146926  -8.78110922  -8.46888743  -8.31085701  -8.26700355
  -8.24083967  -8.16434786  -7.92494924  -7.89483782  -7.52064665
  -7.48682895  -7.34459549  -7.00133307  -6.85806447  -6.45900634
  -6.39239458  -6.30601309  -6.17018657  -6.06885328  -6.00793675
  -5.86126589  -5.82281904  -5.71599806  -5.70065347  -5.67046298
  -5.50593441  -5.4822199   -5.30938938  -5.27590481  -5.2327211
  -5.08785182  -5.08696021  -5.00770933  -4.9880361   -4.8537929
  -4.81694771  -4.81368248  -4.79264055  -4.74960199  -4.71691312
  -4.62683722  -4.35413095  -4.12911264  -4.12215872  -4.0904788
  -4.04055533  -4.01919151  -3.99305833  -3.93238597  -3.86239827
  -3.84690509  -3.81366454  -3.01404293  -2.93980808  -2.69493855
  -2.56813094  -2.43783781  -2.42441354  -2.40637188  -2.34723703
  -2.26717111  -2.00680625  -2.00392601  -1.74944139  -0.81901038]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 6.464439168851754e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.7478e-01, -1.0342e-05, -2.6666e-05, -1.0424e-05,  1.6823e-01,
          4.1672e-05, -3.7723e-03, -1.7890e-02, -7.3923e-03, -9.6516e-06,
         -2.8924e-04, -1.1601e+00, -2.0019e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.030184237807359297, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.0017989048198455747, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.3033e-01,  6.1232e-02,  5.9372e-02, -1.7857e-01,  1.3715e+00,
         -1.4232e-05, -8.0876e-02, -7.4500e-02, -1.3742e+00,  1.1321e-04,
          6.0656e-04, -2.2827e+00, -1.1824e+00]], device='cuda:0'))])
end of epoch 4: val_loss 3.4570573461678575e-08, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 8.344643447344424e-09, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 1.311300479756028e-08, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 5.1359901814151955e-06, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 0.0005096009586108607, val_acc 1.0
trigger times: 5
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0188e+00,  5.2433e-02, -2.0045e-06, -1.6337e-01,  1.5051e+00,
          1.4534e-01, -2.2796e-02, -1.5619e-01, -1.2513e+00,  1.4489e-01,
          1.6604e-03, -2.0222e+00, -1.0858e+00]], device='cuda:0'))])
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.0557e-01, -3.1598e-06,  2.4914e-02, -1.3758e-05,  1.1423e+00,
         -4.0415e-05, -5.8938e-03, -5.7841e-03, -9.4615e-01,  1.3512e-04,
          6.0696e-04, -1.6871e+00, -9.0864e-01]], device='cuda:0'))])
end of epoch 11: val_loss 2.384185506798531e-09, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 2.384181410519659e-08, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 2.4748322738332717e-06, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 3.8563826578297265e-07, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 4.768369308294495e-09, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 6.348052982616537e-06, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.9598e-01, -3.1380e-03,  7.2863e-02, -3.2591e-02,  1.4095e+00,
          7.9954e-02, -2.9546e-02, -2.0841e-01, -1.3913e+00, -8.6202e-02,
          6.0738e-04, -2.4993e+00, -1.3131e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.2218e-01, -7.1743e-04,  8.0027e-03,  3.5861e-03,  7.4438e-01,
          8.2094e-03, -1.4961e-02, -3.9222e-02, -7.1549e-01, -7.6471e-03,
         -8.7585e-04, -1.8376e+00, -1.0046e+00]], device='cuda:0'))])
end of epoch 19: val_loss 0.0010876055929517037, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 1.9984252956462e-06, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 9.536734069115482e-09, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.5375e-01,  1.3797e-01, -7.9283e-05, -1.2151e-02,  1.3105e+00,
          3.5333e-05, -2.4857e-02, -2.3327e-01, -1.3543e+00,  2.0135e-04,
          6.0780e-04, -2.0563e+00, -1.1421e+00]], device='cuda:0'))])
end of epoch 25: val_loss 6.079608663966951e-07, val_acc 1.0
trigger times: 1
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0200e+00,  1.6288e-01,  2.1119e-06, -1.2314e-01,  1.3410e+00,
          5.8625e-02,  2.4964e-02, -7.6718e-02, -1.0176e+00,  9.0327e-05,
          9.1081e-06, -1.9710e+00, -1.1358e+00]], device='cuda:0'))])
end of epoch 27: val_loss 4.1723234289747776e-09, val_acc 1.0
trigger times: 1
end of epoch 28: val_loss 4.86947097471102e-07, val_acc 1.0
trigger times: 2
end of epoch 29: val_loss 9.655858775658998e-08, val_acc 1.0
trigger times: 3
end of epoch 30: val_loss 7.152556058542814e-09, val_acc 1.0
trigger times: 4
end of epoch 31: val_loss 1.0073102725272065e-07, val_acc 1.0
trigger times: 5
end of epoch 32: val_loss 1.668929506593031e-08, val_acc 1.0
trigger times: 6
end of epoch 33: val_loss 0.00038467789742238525, val_acc 1.0
trigger times: 7
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2687e-01, -5.5210e-05, -5.0040e-02, -9.5722e-06,  5.0236e-01,
          5.7159e-05, -2.9047e-03,  3.5433e-02, -6.5409e-01,  2.0421e-05,
         -1.5961e-03, -1.5820e+00, -2.9003e-01]], device='cuda:0'))])
end of epoch 35: val_loss 4.0292044481304854e-07, val_acc 1.0
trigger times: 1
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.0087e-01,  1.3212e-01, -7.0014e-06, -1.0797e-01,  1.2841e+00,
         -1.1816e-05, -8.6229e-03, -7.7717e-02, -1.0395e+00, -2.9105e-05,
          5.3969e-04, -2.1303e+00, -8.6513e-01]], device='cuda:0'))])
end of epoch 37: val_loss 1.3113005898901519e-08, val_acc 1.0
trigger times: 1
end of epoch 38: val_loss 1.1324877320362248e-08, val_acc 1.0
trigger times: 2
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.8879e-01, -6.3177e-05, -6.1657e-02,  1.8399e-02,  1.2323e+00,
          7.2278e-06,  2.1377e-02,  8.1648e-03, -7.2323e-01,  2.6372e-06,
         -8.7459e-04, -1.9801e+00, -7.8913e-01]], device='cuda:0'))])
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.5291e-01,  2.4738e-05,  1.2543e-04, -3.5011e-05,  7.8838e-01,
         -4.2180e-06, -1.2181e-02, -1.1846e-02, -5.4965e-01,  1.3330e-04,
          9.9463e-06, -1.7145e+00, -5.9240e-01]], device='cuda:0'))])
end of epoch 41: val_loss 0.000696723931874601, val_acc 1.0
trigger times: 1
end of epoch 42: val_loss 1.9111617960376747e-06, val_acc 1.0
trigger times: 2
end of epoch 43: val_loss 5.060381921495605e-07, val_acc 1.0
trigger times: 3
end of epoch 44: val_loss 1.0641250962457604e-05, val_acc 1.0
trigger times: 4
end of epoch 45: val_loss 4.899304754246714e-07, val_acc 1.0
trigger times: 5
end of epoch 46: val_loss 1.1026821418624877e-07, val_acc 1.0
trigger times: 6
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.1201e-01,  2.5201e-01,  8.7395e-02, -2.4584e-03,  9.3865e-01,
          5.0257e-03, -2.5666e-02,  9.8955e-07, -8.6298e-01,  5.2032e-05,
          1.0365e-05, -2.1201e+00, -1.0980e+00]], device='cuda:0'))])
end of epoch 48: val_loss 1.1026836059357947e-07, val_acc 1.0
trigger times: 1
end of epoch 49: val_loss 0.02372884878769778, val_acc 0.995
trigger times: 2
end of epoch 50: val_loss 1.6093026214747396e-07, val_acc 1.0
trigger times: 3
end of epoch 51: val_loss 3.5169995164470698, val_acc 0.93
trigger times: 4
end of epoch 52: val_loss 4.172324636897429e-09, val_acc 1.0
trigger times: 5
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.5362e-01,  4.3834e-01, -6.4853e-02, -3.5367e-02,  1.3678e+00,
          2.2013e-05,  3.3466e-02, -4.1138e-02, -7.9855e-01, -3.8208e-05,
         -8.7376e-04, -1.8164e+00, -7.2725e-01]], device='cuda:0'))])
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.7631e-01,  4.0640e-01,  2.1358e-02, -2.2684e-01,  1.8940e+00,
          3.6000e-01, -4.1419e-02, -2.2216e-01, -1.2820e+00, -4.7627e-01,
          1.0784e-05, -2.3777e+00, -1.1657e+00]], device='cuda:0'))])
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.8177e-01,  1.3763e-01,  3.2179e-02, -5.5067e-02,  1.5763e+00,
         -3.3011e-05, -2.9753e-02, -1.1111e-01, -9.0693e-01,  2.6529e-04,
         -1.5948e-03, -2.0234e+00, -1.0054e+00]], device='cuda:0'))])
end of epoch 56: val_loss 2.3484067074974745e-07, val_acc 1.0
trigger times: 1
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.8322e-01,  7.9519e-02,  8.2838e-02, -1.0375e-01,  1.4059e+00,
          1.7079e-01, -2.5184e-02, -1.1115e-01, -1.2200e+00,  7.7633e-02,
          5.4095e-04, -1.7975e+00, -8.2924e-01]], device='cuda:0'))])
end of epoch 58: val_loss 9.846067587204744e-07, val_acc 1.0
trigger times: 1
end of epoch 59: val_loss 1.9192411709667567e-07, val_acc 1.0
trigger times: 2
end of epoch 60: val_loss 8.61874225961401e-07, val_acc 1.0
trigger times: 3
end of epoch 61: val_loss 3.576272078475995e-08, val_acc 1.0
trigger times: 4
end of epoch 62: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 5
end of epoch 63: val_loss 3.111738717066714e-05, val_acc 1.0
trigger times: 6
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.1482e-01,  1.3739e-02, -5.3175e-02, -4.0898e-02,  1.0118e+00,
         -4.8132e-05,  1.6792e-02, -5.9271e-02, -8.6323e-01, -5.4868e-05,
          5.4136e-04, -1.7527e+00, -8.5393e-01]], device='cuda:0'))])
end of epoch 65: val_loss 4.1723234289747776e-09, val_acc 1.0
trigger times: 1
end of epoch 66: val_loss 2.980231883498163e-09, val_acc 1.0
trigger times: 2
end of epoch 67: val_loss 1.2460380046377396e-05, val_acc 1.0
trigger times: 3
end of epoch 68: val_loss 3.981217116653113e-06, val_acc 1.0
trigger times: 4
end of epoch 69: val_loss 8.624209809227068e-07, val_acc 1.0
trigger times: 5
end of epoch 70: val_loss 3.1490991533900115e-06, val_acc 1.0
trigger times: 6
end of epoch 71: val_loss 3.397453156139818e-08, val_acc 1.0
trigger times: 7
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.1745e-01,  2.3050e-01,  2.8162e-02,  8.3133e-06,  6.3094e-01,
          1.3246e-01, -1.5328e-02,  3.7136e-02, -7.9687e-01, -8.1210e-03,
          1.6642e-03, -1.8835e+00, -6.3881e-01]], device='cuda:0'))])
end of epoch 73: val_loss 0.0006044135192722422, val_acc 1.0
trigger times: 1
end of epoch 74: val_loss 3.5762775496550603e-09, val_acc 1.0
trigger times: 2
end of epoch 75: val_loss 5.185591987810767e-08, val_acc 1.0
trigger times: 3
end of epoch 76: val_loss 1.1268185979460554e-05, val_acc 1.0
trigger times: 4
end of epoch 77: val_loss 1.5882146464463176e-06, val_acc 1.0
trigger times: 5
end of epoch 78: val_loss 5.4998639867775975e-06, val_acc 1.0
trigger times: 6
end of epoch 79: val_loss 3.939198382145292e-05, val_acc 1.0
trigger times: 7
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.3020e-01,  5.5686e-02, -8.6432e-06,  1.5122e-05,  1.0498e+00,
          1.0641e-01,  3.0056e-02, -1.1914e-01, -1.1650e+00,  1.1414e-04,
          6.1116e-04, -2.2522e+00, -1.3273e+00]], device='cuda:0'))])
end of epoch 81: val_loss 1.7642701806153126e-07, val_acc 1.0
trigger times: 1
end of epoch 82: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 83: val_loss 8.523405849558685e-08, val_acc 1.0
trigger times: 3
end of epoch 84: val_loss 5.1259739812792305e-08, val_acc 1.0
trigger times: 4
end of epoch 85: val_loss 0.00016480649340504526, val_acc 1.0
trigger times: 5
end of epoch 86: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 6
end of epoch 87: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 7
end of epoch 88: val_loss 7.76062510567499e-06, val_acc 1.0
trigger times: 8
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.8711e-01,  2.8964e-01, -8.1369e-06, -2.4445e-05,  8.7888e-01,
          1.3991e-01, -4.0427e-02, -5.8480e-02, -8.8584e-01,  1.8001e-04,
          1.2880e-05, -1.5855e+00, -7.2213e-01]], device='cuda:0'))])
end of epoch 90: val_loss 2.518618157054675e-06, val_acc 1.0
trigger times: 1
end of epoch 91: val_loss 1.1324872133400277e-08, val_acc 1.0
trigger times: 2
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.4409e-01,  6.1014e-06,  1.0372e-01, -8.7571e-02,  1.3361e+00,
          2.8619e-02, -2.7680e-02, -1.0761e-01, -1.2192e+00,  6.0068e-05,
          5.4304e-04, -2.1421e+00, -9.6967e-01]], device='cuda:0'))])
end of epoch 93: val_loss 4.172324530316018e-09, val_acc 1.0
trigger times: 1
end of epoch 94: val_loss 5.823137213468499e-05, val_acc 1.0
trigger times: 2
end of epoch 95: val_loss 2.69406117254789e-07, val_acc 1.0
trigger times: 3
end of epoch 96: val_loss 6.556510214750233e-09, val_acc 1.0
trigger times: 4
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.9022e-01,  1.4398e-01,  6.6824e-02, -8.4868e-02,  1.2270e+00,
          1.6181e-01,  1.6237e-02, -1.6208e-01, -1.3417e+00, -5.2322e-05,
         -1.5923e-03, -2.2682e+00, -8.9076e-01]], device='cuda:0'))])
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.1277e-01, -4.3794e-05, -1.5765e-05,  3.2603e-05,  5.8910e-01,
         -1.3836e-04,  7.6830e-03, -2.7507e-03, -6.7823e-01, -9.9848e-05,
         -2.8342e-04, -1.6123e+00, -5.3472e-01]], device='cuda:0'))])
end of epoch 99: val_loss 1.364928172620239e-07, val_acc 1.0
trigger times: 1
Finished training.
0 -63.858303874731064 -53.90188864178995
1 -72.89682495594025 -51.07717037294617
2 -29.720688201487064 -19.887412397462857
3 -22.489589750766754 -19.851861844595003
4 -21.87794315814972 -19.307731610606336
5 -43.79208165407181 -18.81165000686687
6 -24.975491881370544 -18.78544957182699
7 -21.73519578576088 -18.73967650878228
8 -19.33051973581314 -18.613162935332625
9 -24.534687757492065 -18.421879044378116
10 -17.447711542248726 -18.292595002995714
11 -20.255396157503128 -18.278093125363213
12 -22.538027226924896 -18.073954169457647
13 -27.755010530352592 -17.898550275370233
14 -16.08785079419613 -17.79028704716345
15 -20.076284497976303 -17.706554486191603
16 -20.227622032165527 -17.623562851408806
17 -18.984615355730057 -17.494902152943794
18 -31.269066244363785 -17.108776051385746
19 -19.647138595581055 -17.09587583636458
20 -30.856784850358963 -16.856201351024367
21 -33.66707617044449 -16.767679028026983
22 -13.466642215847969 -16.522785692263998
23 -10.862278491258621 -16.11740335635787
24 -34.87887763977051 -15.993924863697691
25 -7.58981841430068 -15.946964554138912
26 -19.758388951420784 -15.902841531489633
27 -10.426314413547516 -15.696474031408975
28 -14.416594192385674 -15.289408334091076
29 -5.2118657529354095 -15.28128558081877
30 -10.228535935282707 -15.264540263000683
31 -7.272677928209305 -14.943265362343569
32 -11.314287632703781 -14.885160767579233
33 -9.288401491940022 -14.285369700722367
34 0.9776403978466988 -13.16245315641086
35 0.0692547895014286 -13.02211130240287
36 -0.9406727289315313 -12.962217660095831
37 -3.1453205160796642 -12.776437758130028
38 -42.72076776623726 -12.762198225523013
39 5.090767860412598 -12.336822875274512
40 4.326838441193104 -12.239921942166841
41 2.6954730190336704 -12.060288566696768
42 5.171125568449497 -11.609539600001554
43 7.532522710040212 -11.226919503805776
44 3.0129048749804497 -11.119924666866373
45 -28.00470571964979 -10.444032809059108
46 10.726682774722576 -10.439680498818209
47 -0.32657016068696976 -10.327681503524177
48 12.18006619811058 -9.97080440542322
49 -29.912076205015182 -9.872132202442446
50 12.26287080720067 -9.761187742606833
51 -8.664908230304718 -9.490273434452003
52 -23.215784929692745 -9.47657474596931
53 14.216771319508553 -9.267174228605466
54 10.951974615454674 -9.061990671861954
55 10.857073731720448 -8.801469257265925
56 8.595802374184132 -8.781109219338152
57 -5.4584521939978 -8.468887430651794
58 12.924286097288132 -8.310857012603629
59 1.334540594369173 -8.26700355117661
60 -1.7903533577919006 -8.240839670512248
61 -26.167257726192474 -8.164347858675805
62 8.205804452300072 -7.924949243713371
63 -29.056887939572334 -7.894837817135773
64 14.110862761735916 -7.520646645774171
65 20.41665644943714 -7.486828953992419
66 19.88073080778122 -7.344595490381647
67 17.775629617273808 -7.001333068828838
68 7.6887203603982925 -6.858064470043165
69 5.51469523832202 -6.459006344822167
70 9.236981064081192 -6.392394575135553
71 23.975823551416397 -6.30601308838187
72 10.151280388236046 -6.170186571327494
73 23.628881961107254 -6.0688532769174985
74 2.5240918323397636 -6.007936750422423
75 7.723470829427242 -5.861265887111263
76 20.19850367307663 -5.82281904480234
77 20.899359196424484 -5.715998064302696
78 19.763953626155853 -5.70065346584489
79 23.036162316799164 -5.670462984151192
80 25.732297837734222 -5.505934413980964
81 3.5204788967967033 -5.482219898969396
82 21.947969883680344 -5.309389384255599
83 27.54088044166565 -5.275904812440864
84 28.885381281375885 -5.232721097967916
85 22.996844083070755 -5.087851824435376
86 22.37683656811714 -5.0869602108051915
87 23.718498319387436 -5.007709327857718
88 25.643008559942245 -4.988036101679123
89 28.97141882777214 -4.8537928958288665
90 26.732928693294525 -4.816947714108353
91 20.88929656147957 -4.813682484052973
92 30.890895545482635 -4.792640552302889
93 26.455738484859467 -4.749601985629164
94 31.5822065025568 -4.716913123923519
95 24.352227181196213 -4.626837219134058
96 32.052566796541214 -4.3541309480947845
97 9.57943058013916 -4.129112638200174
98 33.488478660583496 -4.122158719072504
99 31.36780095100403 -4.090478803011665
100 31.59340798854828 -4.040555331823342
101 23.17198619246483 -4.0191915116036085
102 32.488873451948166 -3.993058331453574
103 29.121834576129913 -3.9323859717113923
104 34.83985996246338 -3.8623982743742875
105 31.924103379249573 -3.84690509410518
106 9.641714811325073 -3.8136645449734625
107 36.86685460805893 -3.014042932541162
108 33.13694876432419 -2.93980808482853
109 35.13337290287018 -2.694938548866412
110 34.449036717414856 -2.5681309408059074
111 39.67545568943024 -2.4378378068901383
112 37.99978989362717 -2.424413537069022
113 34.63994896411896 -2.4063718848256643
114 33.084240078926086 -2.347237031572228
115 34.9188089966774 -2.2671711144414792
116 34.823974609375 -2.0068062515641367
117 34.10692447423935 -2.003926014547661
118 37.91968238353729 -1.7494413941026523
119 38.313336968421936 -0.8190103835290985
train accuracy: 1.0
validation accuracy: 1.0

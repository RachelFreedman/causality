[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 2.3668525744469093e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.1147e-05,  1.0287e-02,  4.9879e-02, -3.9535e-02, -4.8162e-06,
         -5.4131e-05, -2.6469e-03,  8.3328e-03, -1.6730e-05, -9.6156e-05,
         -1.4320e-03, -3.1577e-01, -4.4573e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.0005483991693182588, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4040e-03,  8.3545e-06,  2.3820e-01, -4.0498e-01,  6.1708e-01,
         -3.8418e-01, -1.7700e-02,  8.2190e-02,  5.5750e-06,  2.2582e-01,
         -1.4320e-03, -1.5216e+00, -2.2380e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0988e-06,  2.0700e-05,  1.3492e-01, -3.1241e-01,  9.8734e-02,
         -1.8887e-05, -9.7833e-03,  6.1603e-02, -1.0153e-05,  1.0519e-04,
         -1.4320e-03, -7.8990e-01, -2.0750e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.9284e-07,  4.3789e-05, -8.5689e-05, -8.4482e-02,  1.4320e-04,
          7.3387e-06,  2.0202e-06,  1.0905e-02, -7.0696e-06,  2.5003e-04,
         -1.4320e-03, -1.0760e-04, -1.6736e+00]], device='cuda:0'))])
end of epoch 5: val_loss 3.608318161383295e-06, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7660e-02,  5.3817e-01,  2.1330e-01, -2.2923e-01,  8.5501e-02,
         -5.3820e-03, -1.6160e-02,  2.1847e-02, -9.9022e-02,  2.3629e-05,
         -1.4320e-03, -1.5457e+00, -2.3056e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0844e-05,  3.8350e-01,  8.3527e-02, -1.3325e-01, -3.6519e-05,
          4.2199e-05, -6.7449e-03,  1.0488e-05,  1.5884e-05, -1.2523e-04,
         -1.4320e-03, -5.9810e-01, -2.0773e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.5299e-05,  4.8203e-03, -1.7317e-05, -3.0083e-07,  2.7796e-04,
         -5.1552e-05, -1.4219e-06, -3.8273e-06,  1.3127e-04, -1.2803e-04,
         -1.4320e-03,  2.2320e-04, -1.5148e+00]], device='cuda:0'))])
end of epoch 9: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.5684e-02,  1.3603e-01, -4.8847e-06, -2.7198e-03,  9.8646e-02,
          3.3617e-05, -2.5005e-02,  9.4508e-03, -2.2571e-02,  8.3683e-06,
         -1.4321e-03, -1.6532e+00, -3.0604e+00]], device='cuda:0'))])
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.7047e-05,  9.7347e-06, -8.5313e-06,  5.2924e-06, -9.8109e-05,
          7.6902e-05, -1.5529e-02, -5.2856e-06,  2.9940e-05,  7.8832e-05,
         -1.4321e-03, -3.8861e-01, -2.7249e+00]], device='cuda:0'))])
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6113e-05, -1.5504e-05, -1.7499e-05,  1.6178e-05, -2.0610e-04,
          1.8151e-04, -5.6360e-06, -1.1398e-05,  2.0151e-04,  4.0048e-04,
         -1.4321e-03, -1.7192e-04, -1.9002e+00]], device='cuda:0'))])
end of epoch 14: val_loss 0.08981173086452272, val_acc 0.96
trigger times: 1
end of epoch 15: val_loss 8.225374582337964e-08, val_acc 1.0
trigger times: 2
end of epoch 16: val_loss 2.2053692418921854e-08, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.5106e-06,  5.6795e-02,  3.9854e-05,  3.5685e-06, -8.1430e-06,
         -1.0093e-04,  2.2925e-02,  7.1748e-02, -5.5406e-05, -1.6209e-04,
         -1.4323e-03, -2.9608e-01, -2.6054e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.9911e-05, -3.6522e-05,  1.9762e-05,  1.2718e-05, -1.4608e-05,
         -2.7691e-04,  6.0784e-04,  5.3642e-04, -1.4644e-04, -3.5925e-04,
         -1.4323e-03,  8.9219e-05, -1.9277e+00]], device='cuda:0'))])
end of epoch 19: val_loss 2.102788572364958, val_acc 0.85
trigger times: 1
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1725e-05,  1.9059e-01,  3.6855e-01, -5.8872e-02,  2.1622e-01,
         -7.9350e-05, -5.7932e-03,  8.2091e-02, -4.2825e-01,  6.6474e-05,
         -1.4324e-03, -1.3524e+00, -2.3856e+00]], device='cuda:0'))])
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6943e-05,  3.6280e-05,  2.0753e-01, -1.2136e-05, -2.9952e-05,
         -1.3688e-04,  3.5060e-07,  4.2350e-02, -6.5783e-07,  4.1244e-05,
         -1.4324e-03, -3.2434e-01, -2.1295e+00]], device='cuda:0'))])
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.4219e-05,  4.3889e-05,  5.4138e-05, -2.9934e-05,  4.6472e-04,
         -3.8251e-04,  1.0288e-06, -3.7701e-06, -2.6587e-04,  1.2030e-04,
         -1.4325e-03,  6.9807e-05, -1.4997e+00]], device='cuda:0'))])
end of epoch 23: val_loss 0.03075372579822062, val_acc 0.985
trigger times: 1
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1584e-01,  4.3894e-02,  1.6855e-01, -1.4356e-01, -9.9014e-06,
          1.5560e-04, -4.2192e-02,  3.7708e-02, -1.5234e-04,  4.6119e-05,
         -1.4325e-03, -1.2183e+00, -2.3014e+00]], device='cuda:0'))])
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.2524e-02,  4.8907e-01,  4.1426e-01, -8.3814e-02,  2.4538e-01,
         -3.2643e-02,  4.3322e-03,  4.1232e-02,  8.9822e-04,  1.1942e-02,
         -1.4326e-03, -1.2022e+00, -3.2146e+00]], device='cuda:0'))])
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1595e-05,  2.9657e-01,  2.6755e-01,  1.2050e-05,  2.4671e-05,
         -1.0239e-04,  1.7260e-06,  1.7533e-02,  2.2051e-05,  4.6714e-05,
         -1.4326e-03, -2.3339e-01, -2.9986e+00]], device='cuda:0'))])
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2855e-05,  4.7340e-05,  1.6303e-06,  4.0708e-06,  2.5982e-04,
         -2.6966e-04,  4.0960e-06, -1.3201e-07,  9.3285e-05,  6.2905e-05,
         -1.4326e-03, -3.5568e-04, -2.4673e+00]], device='cuda:0'))])
end of epoch 28: val_loss 4.1723246724245655e-09, val_acc 1.0
trigger times: 1
end of epoch 29: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 2
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4274e-05,  9.7896e-02,  1.3196e-01,  5.4745e-06,  1.6627e-04,
         -2.3347e-06, -1.3491e-02,  6.9627e-02,  1.0285e-04, -4.1865e-06,
         -1.4327e-03, -6.7897e-01, -2.0835e+00]], device='cuda:0'))])
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1704e-05, -9.2018e-05, -1.6207e-05,  8.2772e-06, -3.4784e-05,
          1.1172e-04,  1.0011e-08,  5.2927e-03,  2.1746e-04,  4.1731e-04,
         -1.4328e-03,  2.1803e-04, -1.5460e+00]], device='cuda:0'))])
end of epoch 32: val_loss 6.407270240018193e-05, val_acc 1.0
trigger times: 1
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.1589e-07,  2.4981e-01,  1.5243e-01, -5.4566e-02,  4.5808e-01,
         -4.2505e-02,  2.6600e-02,  2.9102e-02,  1.2013e-04, -1.4210e-05,
         -1.4328e-03, -1.3054e+00, -2.2952e+00]], device='cuda:0'))])
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.0983e-06,  8.4182e-02,  8.8252e-03,  1.0928e-05, -1.4750e-05,
          1.6845e-05,  1.7512e-02,  7.3520e-03,  1.1672e-04,  2.8532e-05,
         -1.4329e-03, -4.0162e-01, -2.0929e+00]], device='cuda:0'))])
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0544e-05, -6.8591e-05,  1.0840e-05,  5.2372e-06,  1.4923e-04,
          9.5799e-05, -3.3858e-06, -1.6763e-06,  6.6604e-05,  1.1799e-04,
         -1.4329e-03,  3.2904e-04, -1.5952e+00]], device='cuda:0'))])
end of epoch 36: val_loss 2.873794116002366, val_acc 0.745
trigger times: 1
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5688e-01,  2.2317e-02,  2.2130e-01, -9.0510e-02, -3.3149e-07,
         -2.6302e-05,  1.5703e-02,  1.9013e-02,  1.2813e-05,  9.9558e-06,
         -1.4330e-03, -1.1613e+00, -2.1067e+00]], device='cuda:0'))])
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.5320e-05,  9.2262e-07,  6.1668e-02,  1.0795e-05,  4.5736e-05,
          8.1600e-06,  5.2208e-03, -3.3037e-06,  2.5209e-05,  3.8428e-05,
         -1.4330e-03, -6.0352e-02, -1.8584e+00]], device='cuda:0'))])
end of epoch 39: val_loss 1.7881392366803084e-09, val_acc 1.0
trigger times: 1
end of epoch 40: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 41: val_loss 8.940692985959231e-09, val_acc 1.0
trigger times: 3
end of epoch 42: val_loss 7.181248951120267e-06, val_acc 1.0
trigger times: 4
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.0784e-06,  1.7316e-01,  2.2411e-01, -1.4323e-02, -1.0025e-05,
          1.2954e-05, -2.7064e-02,  1.8149e-06, -7.4973e-05,  1.0876e-04,
         -1.4332e-03, -9.7374e-01, -1.9159e+00]], device='cuda:0'))])
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.0393e-06, -4.1212e-05,  3.0642e-02,  3.0994e-06,  4.3537e-05,
         -2.8296e-05, -1.5993e-02,  2.5688e-06, -5.3915e-05, -4.8613e-05,
         -1.4332e-03, -2.1293e-03, -1.6675e+00]], device='cuda:0'))])
end of epoch 45: val_loss 2.0265570270794342e-08, val_acc 1.0
trigger times: 1
end of epoch 46: val_loss 5.693304103111529e-06, val_acc 1.0
trigger times: 2
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.1168e-06,  2.3896e-01,  7.9159e-02, -3.7393e-01,  4.1529e-05,
         -6.2683e-02, -1.1126e-02,  5.4995e-03, -2.4292e-05,  7.6845e-05,
         -1.4333e-03, -1.1986e+00, -2.2488e+00]], device='cuda:0'))])
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.2626e-06, -2.5814e-05, -4.7539e-05, -1.8389e-01,  1.2664e-04,
         -1.7766e-04, -2.4904e-06,  7.3424e-06, -2.2877e-05,  3.7077e-04,
         -1.4334e-03,  1.8923e-04, -1.9009e+00]], device='cuda:0'))])
end of epoch 49: val_loss 2.3245799063431605e-08, val_acc 1.0
trigger times: 1
end of epoch 50: val_loss 0.00023753375146043254, val_acc 1.0
trigger times: 2
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.3765e-01,  3.7923e-01,  3.4925e-01, -1.8928e-01, -3.7132e-06,
          3.8649e-04, -2.1180e-07,  1.0935e-01,  9.3521e-06,  2.9272e-05,
         -1.4335e-03, -1.6213e+00, -3.0639e+00]], device='cuda:0'))])
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.1352e-05,  1.8824e-01,  1.9377e-01, -5.0498e-02,  8.5843e-06,
          3.6014e-05,  1.0459e-08,  7.7415e-02,  4.1814e-05,  9.4605e-05,
         -1.4335e-03, -2.7506e-01, -2.7586e+00]], device='cuda:0'))])
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.9072e-05,  1.4686e-06,  5.7935e-05,  6.3552e-05, -1.3232e-04,
          1.9797e-04, -1.8067e-06,  5.9527e-05,  1.2087e-04,  2.4829e-04,
         -1.4335e-03,  3.1416e-04, -2.0081e+00]], device='cuda:0'))])
end of epoch 54: val_loss 0.07483139003276303, val_acc 0.98
trigger times: 1
end of epoch 55: val_loss 1.2516971139575617e-08, val_acc 1.0
trigger times: 2
end of epoch 56: val_loss 5.9604633051435486e-09, val_acc 1.0
trigger times: 3
end of epoch 57: val_loss 1.2283160114208158e-05, val_acc 1.0
trigger times: 4
end of epoch 58: val_loss 3.7668799109269456e-07, val_acc 1.0
trigger times: 5
end of epoch 59: val_loss 2.205369778351951e-08, val_acc 1.0
trigger times: 6
end of epoch 60: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 7
end of epoch 61: val_loss 5.3430895763995065e-06, val_acc 1.0
trigger times: 8
end of epoch 62: val_loss 2.9802313861182485e-09, val_acc 1.0
trigger times: 9
end of epoch 63: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 10
Early stopping.
0 -108.69324445724487 -54.98547503240923
1 -105.11925798654556 -50.492268601198035
2 -103.52027025818825 -50.03933801517046
3 -107.66399222612381 -49.75347184620696
4 -107.69682535529137 -49.72654640753777
5 -105.08911192417145 -46.98011874490918
6 -84.64378969371319 -45.7351542845057
7 -77.75266215205193 -45.670579884154705
8 -95.14238995313644 -44.99030608142343
9 -71.72027635574341 -44.14602409201361
10 -77.23692208528519 -43.81326882122305
11 -75.58329030871391 -43.18878399086166
12 -92.24349404871464 -42.29180714825394
13 -72.83684879541397 -42.00401746161006
14 -95.92297548055649 -41.6910044370425
15 -93.48424273729324 -41.68588229294918
16 -77.34315693378448 -41.281777102712205
17 -73.20881414413452 -40.44278203413966
18 -64.94966834783554 -40.34838365523108
19 -94.01573067903519 -39.599701153458774
20 -71.5796474814415 -39.57586365327889
21 -90.48330673575401 -39.31972693233231
22 -81.98524402081966 -39.024610555047154
23 -91.9717449247837 -38.45534493538269
24 -91.71874752640724 -38.41270390343083
25 -85.74109864234924 -38.35634328077039
26 -95.78658270835876 -37.79713616772368
27 -76.82246808707714 -37.741528994987384
28 -83.70407837629318 -37.66475323879293
29 -58.61306348443031 -37.513139380385574
30 -68.2987812757492 -37.1809993033689
31 -83.01873587071896 -37.100703136010694
32 -90.42929376661777 -37.00630588930485
33 -66.42786002159119 -36.821916772458344
34 -89.95915907621384 -36.48799015296732
35 -91.09116195142269 -36.20965269874363
36 -81.08624172210693 -36.19207561676116
37 -73.34663239121437 -36.114459029559086
38 -82.72268974781036 -35.78149902167743
39 -66.60315805673599 -35.394503873250635
40 -74.96327024698257 -35.26282499693737
41 -63.35341680049896 -35.24303541418371
42 -79.88761475682259 -35.209705244501436
43 -69.39163407683372 -35.0654408505187
44 -76.81688173115253 -34.80241747531743
45 -67.88447335362434 -34.64469044638467
46 -78.45347487926483 -33.84284985953318
47 -72.00421011447906 -32.70706485357069
48 -77.07238361239433 -31.969099402548657
49 -69.45864161849022 -31.7109134007892
50 -50.56331957876682 -31.64414355845032
51 -73.17571793496609 -31.392382758954444
52 -67.9981023594737 -31.223196019713853
53 -67.99594776332378 -31.12953085092458
54 -75.31451398134232 -29.39157139549552
55 -62.08556741476059 -29.340125609942326
56 -57.22312796115875 -29.106189988903285
57 -56.97680255025625 -27.41102349748205
58 -64.66593031585217 -27.343722362182305
59 -57.847092896699905 -27.196681629483837
60 -68.82460063695908 -27.07399028854534
61 -66.22309049963951 -26.7047217556024
62 -66.58256068825722 -26.244794902859052
63 -59.348439529538155 -25.548365085275513
64 -54.40119116008282 -25.45878528601009
65 -42.13117937743664 -24.879106999799365
66 -58.08731085062027 -24.828695359328833
67 -67.93237014859915 -24.592745144504722
68 -70.09193286299706 -23.978745577896312
69 -50.897057831287384 -23.57262108435893
70 -46.634804517030716 -23.44970807952351
71 -54.21048058569431 -22.745309160183492
72 -50.198618054389954 -22.60679894414887
73 -57.47872096300125 -22.19891031871716
74 -39.3074609041214 -20.656863763892378
75 -49.809714429080486 -20.444472560731253
76 -42.350797817111015 -20.19699010077007
77 -56.07584124803543 -20.13839114930498
78 -42.027474496513605 -19.63760343800059
79 -61.63827720284462 -19.515598718228343
80 -43.872219294309616 -18.92838809611677
81 -44.38255712389946 -17.994774057192853
82 -50.41053268313408 -17.55742370467821
83 -42.23354071378708 -16.823073927842348
84 -46.676077120006084 -14.855082803515382
85 -42.4594846740365 -14.531424598833084
86 -34.29494059830904 -14.442420089224363
87 -39.787105947732925 -13.596012850960644
88 -22.2466392070055 -12.68135972540495
89 -34.35756875574589 -12.66418205637357
90 -35.0222524702549 -12.30017947419658
91 -34.49526661634445 -12.151904772081672
92 -46.00190206617117 -11.788852141676486
93 -27.159931614995003 -10.869989101210326
94 -31.517057694494724 -10.327681503524177
95 -25.51777818799019 -9.8572159761571
96 -25.54142537713051 -8.330116995310416
97 -24.52917356789112 -8.133195842510668
98 -24.02998771518469 -8.108197691178031
99 -23.449326433241367 -7.57539849177145
100 -22.594495743513107 -7.362443126623615
101 -11.45279447734356 -7.108327355338034
102 -19.034470714628696 -6.959063561385431
103 -22.892043381929398 -6.776946485018116
104 -20.70944382250309 -6.7220638398623045
105 -24.0656343922019 -6.719970621583102
106 -25.477195776998997 -6.535447341844848
107 -32.68273621797562 -6.51820418055673
108 -31.461327455937862 -5.615796733870542
109 -23.34820333868265 -5.34720210027791
110 -18.90961404144764 -5.078485007852753
111 -18.388607248663902 -5.027957977402961
112 -17.78299516439438 -4.827572916892203
113 -18.732182875275612 -4.63049541560991
114 -17.76281586661935 -4.230832004686763
115 -18.708428226411343 -4.031048624093466
116 -19.083601862192154 -3.3844671463622564
117 -15.412392131984234 -3.3322555012187633
118 -13.659665316343307 -2.6416623314910934
119 -10.534840039908886 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -49.75347185 -49.72654641 -46.98011874
 -45.67057988 -44.14602409 -43.81326882 -41.68588229 -40.34838366
 -39.31972693 -39.02461056 -38.4127039  -38.35634328 -37.79713617
 -37.51313938 -37.00630589 -36.82191677 -35.78149902 -35.262825
 -34.80241748 -34.64469045 -33.84284986 -31.9690994  -31.7109134
 -31.39238276 -31.12953085 -29.34012561 -27.4110235  -26.70472176
 -24.879107   -24.59274514 -23.57262108 -20.65686376 -20.44447256
 -20.1969901  -20.13839115 -19.63760344 -19.31195542 -18.78995588
 -18.65141068 -17.5574237  -17.37127767 -16.98434074 -16.47404353
 -16.19220619 -15.98992773 -15.93178374 -15.88427434 -15.86494761
 -15.69009874 -15.46512868 -15.43907186 -15.3762895  -14.8550828
 -14.5314246  -14.50357614 -14.50151669 -14.44242009 -14.17107073
 -14.08872036 -13.96284566 -13.59601285 -13.23889484 -12.93192763
 -12.83470558 -12.68135973 -12.66418206 -12.65388613 -12.65219061
 -12.52174906 -12.44855141 -12.21288548 -12.20156251 -12.15190477
 -12.10007385 -11.93081941 -11.85500902 -11.49303066 -11.23642026
 -10.99452215 -10.90937378 -10.76377279 -10.71236081 -10.59059751
 -10.50003222 -10.2468219  -10.07834075  -9.53946712  -9.42973971
  -9.2437806   -9.03520409  -8.79637123  -8.13319584  -7.86677913
  -7.76995836  -7.57539849  -7.54534603  -7.44362022  -7.36244313
  -7.10832736  -6.95906356  -6.94390739  -6.82437929  -6.77694649
  -6.72206384  -6.63051532  -6.53544734  -6.20502842  -5.61579673
  -5.45723971  -5.42516253  -5.39140804  -5.1064791   -5.07848501
  -5.02795798  -4.230832    -4.00401567  -3.3322555   -2.64166233]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.00949496590267188, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.3111e-02, -1.6859e-01, -1.6705e-02, -2.4027e-02,  1.5278e-01,
         -1.1348e-04,  9.1404e-03,  5.0142e-02, -1.5060e-01, -9.9246e-02,
         -4.8476e-04, -5.8264e-01, -1.4310e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.00036664190585721456, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.1053e-02, -3.8480e-02, -2.0085e-01, -1.6230e-01,  3.1217e-01,
         -3.6474e-02,  3.4587e-02, -2.0956e-02, -8.2583e-02, -3.2865e-01,
          8.0826e-04, -1.6736e+00, -2.3447e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.00015102178850437298, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.7412e-02,  7.3459e-06, -7.0416e-02, -4.9617e-02,  4.9348e-06,
         -2.3753e-05,  2.7829e-02,  2.5668e-07, -3.1567e-05, -6.0228e-05,
          5.5592e-05, -1.3535e+00, -2.1419e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.00016350489903516063, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 1.67462247190997e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.2503e-03,  1.3192e-01,  1.3800e-01, -8.4035e-02,  4.6623e-01,
          1.7395e-01, -1.7585e-02, -5.5451e-02, -3.9974e-01, -4.6533e-01,
          7.5204e-04, -1.7179e+00, -1.9762e+00]], device='cuda:0'))])
end of epoch 5: val_loss 1.1164733634458911e-05, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 2.205367081842269e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.3047e-02, -2.5358e-04,  3.9058e-02, -5.3829e-02,  2.4409e-01,
          5.0184e-01, -8.5036e-03,  6.1626e-02, -4.2930e-01, -8.9037e-01,
         -5.9108e-04, -1.7879e+00, -1.9914e+00]], device='cuda:0'))])
end of epoch 7: val_loss 9.071556029738304e-07, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.0002483533294344653, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 1.2159216307594535e-07, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.00010697443953148422, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.0013522995508983727, val_acc 1.0
trigger times: 5
end of epoch 12: val_loss 3.2901286623143733e-07, val_acc 1.0
trigger times: 6
end of epoch 13: val_loss 1.2534574187768044e-05, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 0.0017780428138069837, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 1.1008244264587575e-06, val_acc 1.0
trigger times: 9
end of epoch 16: val_loss 0.00034490948707340154, val_acc 1.0
trigger times: 10
Early stopping.
0 -82.92327928543091 -50.492268601198035
1 -81.30526587367058 -50.03933801517046
2 -75.51058998703957 -49.75347184620696
3 -79.24279335141182 -49.72654640753777
4 -67.3757646381855 -46.98011874490918
5 -73.14326068758965 -45.670579884154705
6 -78.31784296035767 -44.14602409201361
7 -70.22256174683571 -43.81326882122305
8 -71.36770844459534 -41.68588229294918
9 -69.90644124150276 -40.34838365523108
10 -73.28882819414139 -39.31972693233231
11 -67.51627358794212 -39.024610555047154
12 -66.80254137516022 -38.41270390343083
13 -65.19890254735947 -38.35634328077039
14 -66.82693365216255 -37.79713616772368
15 -60.67735731601715 -37.513139380385574
16 -66.52776300907135 -37.00630588930485
17 -64.10637855529785 -36.821916772458344
18 -66.36328092217445 -35.78149902167743
19 -64.6879131346941 -35.26282499693737
20 -59.982183553278446 -34.80241747531743
21 -59.18161964416504 -34.64469044638467
22 -62.7227186858654 -33.84284985953318
23 -54.98686358332634 -31.969099402548657
24 -58.96464741230011 -31.7109134007892
25 -56.43938872218132 -31.392382758954444
26 -54.299221098423004 -31.12953085092458
27 -55.848457396030426 -29.340125609942326
28 -53.98806685209274 -27.41102349748205
29 -51.79251828789711 -26.7047217556024
30 -50.23922771215439 -24.879106999799365
31 -50.015318900346756 -24.592745144504722
32 -46.681913286447525 -23.57262108435893
33 -40.25068978127092 -20.656863763892378
34 -41.14181520789862 -20.444472560731253
35 -41.35154254734516 -20.19699010077007
36 -42.44257266819477 -20.13839114930498
37 -41.28724516928196 -19.63760343800059
38 -27.96900177001953 -19.31195541503258
39 -27.21590980887413 -18.789955875501278
40 -28.04912379384041 -18.651410680500383
41 -37.620738446712494 -17.55742370467821
42 -25.984360605478287 -17.37127767078158
43 -24.740342497825623 -16.984340740864216
44 -25.238226890563965 -16.474043528511853
45 -24.007664501667023 -16.192206193594494
46 -25.336459949612617 -15.98992773021787
47 -23.804723277688026 -15.931783735950784
48 -24.881882458925247 -15.884274344349825
49 -24.718124136328697 -15.86494760790249
50 -24.123097151517868 -15.69009874026936
51 -24.574100732803345 -15.465128678873123
52 -23.35183671116829 -15.43907186058345
53 -25.09576365351677 -15.376289504800605
54 -33.525422655045986 -14.855082803515382
55 -31.89109916985035 -14.531424598833084
56 -23.2719683945179 -14.503576136807254
57 -22.94704608619213 -14.501516688290415
58 -30.747098181396723 -14.442420089224363
59 -22.86692675948143 -14.17107072526127
60 -22.57082213461399 -14.088720357165514
61 -23.3948854804039 -13.962845663378395
62 -32.59593082591891 -13.596012850960644
63 -22.89365464448929 -13.238894840717434
64 -21.744464367628098 -12.931927626703619
65 -21.38603588938713 -12.834705577238381
66 -27.741685643792152 -12.68135972540495
67 -29.83035981655121 -12.66418205637357
68 -21.769416198134422 -12.653886125125771
69 -21.657068192958832 -12.652190612170207
70 -20.633747711777687 -12.52174906400413
71 -22.30538859963417 -12.448551412706754
72 -20.946806699037552 -12.212885479706735
73 -21.263472989201546 -12.201562509199652
74 -30.144964523613453 -12.151904772081672
75 -20.243150904774666 -12.100073847455203
76 -21.480852112174034 -11.930819411722847
77 -21.371774792671204 -11.855009024590505
78 -20.543627083301544 -11.493030663563674
79 -21.14683574438095 -11.236420262857893
80 -18.217684969305992 -10.994522150997422
81 -18.498905017971992 -10.909373775687042
82 -18.639386862516403 -10.763772787655057
83 -20.340866938233376 -10.712360811983652
84 -19.50448364019394 -10.590597511454334
85 -18.305089190602303 -10.500032216901475
86 -18.14815227687359 -10.246821899009882
87 -19.051088273525238 -10.078340749132316
88 -17.71025498956442 -9.539467116745772
89 -17.991478711366653 -9.429739708537976
90 -19.04877705872059 -9.243780604295752
91 -15.902210883796215 -9.035204094334185
92 -16.862329468131065 -8.79637122896176
93 -22.55111040920019 -8.133195842510668
94 -18.266434744000435 -7.866779132361029
95 -15.55990095436573 -7.769958361285778
96 -21.243649419397116 -7.57539849177145
97 -17.57978443801403 -7.54534603313117
98 -16.054591119289398 -7.443620221774227
99 -20.518778927624226 -7.362443126623615
100 -19.440982058644295 -7.108327355338034
101 -20.495624877512455 -6.959063561385431
102 -14.097525641322136 -6.943907387922596
103 -14.540022909641266 -6.82437929018563
104 -17.728072553873062 -6.776946485018116
105 -18.931558813899755 -6.7220638398623045
106 -15.98377987742424 -6.630515321441343
107 -22.763296090066433 -6.535447341844848
108 -14.134730502963066 -6.2050284204911375
109 -19.466201186180115 -5.615796733870542
110 -12.632533840835094 -5.457239714920104
111 -13.718780487775803 -5.42516253051715
112 -12.796212494373322 -5.3914080391198675
113 -13.763576216995716 -5.106479098285975
114 -17.467777971178293 -5.078485007852753
115 -18.045762550085783 -5.027957977402961
116 -14.803301207721233 -4.230832004686763
117 -12.779006533324718 -4.00401566602188
118 -14.232656605541706 -3.3322555012187633
119 -14.257492490112782 -2.6416623314910934
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -46.98011874 -45.67057988 -44.14602409
 -38.4127039  -38.35634328 -36.82191677 -35.262825   -34.64469045
 -33.84284986 -31.9690994  -27.4110235  -26.70472176 -24.879107
 -24.59274514 -20.65686376 -20.13839115 -19.31195542 -17.5574237
 -17.37127767 -16.98434074 -15.98992773 -15.86494761 -15.46512868
 -15.3762895  -14.8550828  -14.5314246  -14.44242009 -14.08872036
 -13.23889484 -12.93192763 -12.83470558 -12.68135973 -12.66418206
 -12.65388613 -12.44855141 -12.21288548 -12.20156251 -12.15190477
 -12.10007385 -11.93081941 -11.85500902 -10.99452215 -10.90937378
 -10.85809971 -10.71236081 -10.68284148 -10.50003222 -10.43362977
 -10.38307558 -10.2468219  -10.24301497 -10.23741402  -9.95213624
  -9.91562534  -9.61341383  -9.60058186  -9.53946712  -9.461679
  -9.36583769  -9.36282533  -9.33826373  -9.3204038   -9.03885952
  -9.03503631  -9.03213832  -9.01913844  -9.00826184  -8.91120511
  -8.88785669  -8.82594775  -8.78264071  -8.77669269  -8.60682171
  -8.52993643  -8.50908773  -8.50818542  -8.50594657  -8.48521769
  -8.21132132  -8.13319584  -8.05223899  -7.89620449  -7.87085652
  -7.86677913  -7.83888593  -7.63743471  -7.57539849  -7.54534603
  -7.51952996  -7.31767288  -7.26183842  -7.123849    -6.97596103
  -6.93393055  -6.86294037  -6.86078373  -6.84968074  -6.82437929
  -6.81642205  -6.80721785  -6.80464164  -6.77694649  -6.75449075
  -6.72206384  -6.70154907  -6.63051532  -6.62261373  -6.59168692
  -6.55472023  -6.52571473  -6.33290711  -6.29331089  -6.26835296
  -6.23484939  -6.20502842  -5.8221793   -5.1064791   -5.02795798]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.0007632497016379248, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.3164e-02, -5.1055e-02, -1.5728e-01,  3.0423e-02,  4.2056e-02,
         -3.4113e-02,  3.7106e-02, -4.4137e-03,  2.2270e-05, -5.7036e-05,
         -1.0599e-03, -1.7889e+00, -1.7265e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.00031740119156893344, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.3560e-01, -3.3040e-05, -1.5059e-02,  4.2998e-02,  4.7633e-01,
          1.6243e-01,  3.1871e-02, -2.0338e-02, -1.4035e-01,  5.0071e-05,
         -1.0599e-03, -2.8435e+00, -2.3548e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.0014738065386172394, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.0002989182775153054, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.8814e-01, -6.9759e-02, -3.1756e-01,  2.0601e-03,  3.3413e-01,
         -4.7860e-01,  9.3892e-02,  3.2881e-03, -1.4411e-01,  2.3527e-01,
         -1.0600e-03, -2.6391e+00, -2.5953e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.0006930122388549975, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.0001509487723465952, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0282e-01, -3.4728e-05, -8.9867e-02,  4.6368e-02,  2.2762e-04,
          3.7472e-05,  2.2825e-02, -2.9136e-02, -1.9578e-01,  4.9499e-05,
         -1.0600e-03, -2.0812e+00, -1.8675e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.00015391696451342085, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.1151802003457442, val_acc 0.985
trigger times: 2
end of epoch 8: val_loss 8.60554797679569e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.7643e-05,  4.0680e-05, -1.5218e-02,  5.2802e-02,  1.4297e-02,
         -2.1351e-04,  1.4734e-02,  3.7443e-04,  1.5504e-04, -2.4530e-05,
         -1.0601e-03, -2.4914e+00, -2.2831e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.00922297726489969, val_acc 0.995
trigger times: 1
end of epoch 10: val_loss 0.00013175977731020084, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.0002476610360297471, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 1.2665540738439063e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.2652e-02, -3.8253e-03, -4.8394e-02, -5.2773e-03,  5.2261e-02,
         -8.5081e-03,  1.2554e-02, -2.0799e-06,  1.6300e-06,  1.7540e-01,
         -1.0602e-03, -2.4684e+00, -1.6228e+00]], device='cuda:0'))])
end of epoch 13: val_loss 0.0004689964165308069, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 8.277964952014827e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.3339e-01, -6.6760e-02, -6.5123e-02,  1.4968e-02,  3.4060e-01,
         -9.2502e-02,  1.3179e-03, -2.7109e-02, -1.5621e-06, -3.6348e-06,
         -1.0602e-03, -2.6893e+00, -2.2249e+00]], device='cuda:0'))])
end of epoch 15: val_loss 0.00010321008783250818, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 0.21012252718963367, val_acc 0.965
trigger times: 2
end of epoch 17: val_loss 0.00021271095634535443, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 6.804245696599764e-05, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 8.994375516095943e-06, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 0.6721580306752165, val_acc 0.95
trigger times: 6
end of epoch 21: val_loss 2.458438900976745e-05, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 9.996885736104134e-05, val_acc 1.0
trigger times: 8
end of epoch 23: val_loss 0.00030151628382999005, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.000683631375311613, val_acc 1.0
trigger times: 10
Early stopping.
0 -115.6409747004509 -50.492268601198035
1 -141.55071103572845 -50.03933801517046
2 -128.59894233942032 -46.98011874490918
3 -138.16181933879852 -45.670579884154705
4 -130.42652598023415 -44.14602409201361
5 -110.98110884428024 -38.41270390343083
6 -129.18334978818893 -38.35634328077039
7 -127.08337724208832 -36.821916772458344
8 -97.29603776335716 -35.26282499693737
9 -110.58887708187103 -34.64469044638467
10 -86.7321136444807 -33.84284985953318
11 -126.69431686401367 -31.969099402548657
12 -77.98780553042889 -27.41102349748205
13 -102.32015722990036 -26.7047217556024
14 -96.57531154155731 -24.879106999799365
15 -70.88177208602428 -24.592745144504722
16 -64.27832208573818 -20.656863763892378
17 -80.7638051211834 -20.13839114930498
18 -95.87087285518646 -19.31195541503258
19 -56.47879612445831 -17.55742370467821
20 -89.53421771526337 -17.37127767078158
21 -83.75808823108673 -16.984340740864216
22 -89.51172351837158 -15.98992773021787
23 -81.33527481555939 -15.86494760790249
24 -78.73284089565277 -15.465128678873123
25 -78.5503956079483 -15.376289504800605
26 -50.81362769752741 -14.855082803515382
27 -56.05930469930172 -14.531424598833084
28 -70.6211076527834 -14.442420089224363
29 -78.97055351734161 -14.088720357165514
30 -64.82818448543549 -13.238894840717434
31 -74.84643757343292 -12.931927626703619
32 -74.94520646333694 -12.834705577238381
33 -62.772282510995865 -12.68135972540495
34 -54.794889852404594 -12.66418205637357
35 -72.45452219247818 -12.653886125125771
36 -64.51198387145996 -12.448551412706754
37 -66.67046463489532 -12.212885479706735
38 -60.70051795244217 -12.201562509199652
39 -72.23911380767822 -12.151904772081672
40 -75.12729460000992 -12.100073847455203
41 -65.16587480902672 -11.930819411722847
42 -61.23242771625519 -11.855009024590505
43 -59.4770290851593 -10.994522150997422
44 -57.10130840539932 -10.909373775687042
45 -65.76431956887245 -10.858099711614404
46 -60.61603683233261 -10.712360811983652
47 -55.791365802288055 -10.682841476337513
48 -63.818833351135254 -10.500032216901475
49 -57.51745444536209 -10.433629772221371
50 -55.914426267147064 -10.38307557588936
51 -53.46674633026123 -10.246821899009882
52 -54.283804193139076 -10.243014974013878
53 -57.79249280691147 -10.237414016504284
54 -50.28093512356281 -9.952136243613072
55 -62.66750580072403 -9.915625344666225
56 -63.95753267407417 -9.613413827917123
57 -54.09103836119175 -9.600581856673752
58 -51.19974184036255 -9.539467116745772
59 -40.05479560792446 -9.461678996226073
60 -50.903684198856354 -9.365837693916959
61 -59.92909514904022 -9.362825330334665
62 -60.140538930892944 -9.338263730002462
63 -54.11748491972685 -9.320403797692478
64 -52.37169539183378 -9.038859517967621
65 -44.84318596124649 -9.035036311097683
66 -53.53664189577103 -9.032138321897921
67 -56.64193406701088 -9.019138444493374
68 -38.56719879806042 -9.00826184473765
69 -38.11568148061633 -8.91120510557969
70 -40.34117614477873 -8.887856690456726
71 -42.67398266494274 -8.825947745499777
72 -56.014208897948265 -8.782640708932549
73 -44.02253493666649 -8.77669268950041
74 -46.86360861361027 -8.606821706638067
75 -44.15848844498396 -8.529936431914402
76 -44.28206194937229 -8.509087731683886
77 -57.16253352165222 -8.508185416654529
78 -46.54689083248377 -8.505946573574441
79 -47.14227918535471 -8.48521769025379
80 -40.419545508921146 -8.21132132358506
81 -49.89528802037239 -8.133195842510668
82 -38.8874050155282 -8.052238985281663
83 -37.6539371907711 -7.896204488576646
84 -34.33186061680317 -7.87085652085273
85 -47.104829370975494 -7.866779132361029
86 -39.28529779613018 -7.8388859293032205
87 -39.384389758110046 -7.637434708337033
88 -59.02632850408554 -7.57539849177145
89 -49.748123586177826 -7.54534603313117
90 -34.136298418045044 -7.51952996356267
91 -42.04663063585758 -7.3176728825465
92 -43.71133791655302 -7.261838415984296
93 -30.009756192564964 -7.123848998133276
94 -32.17920347303152 -6.975961025657157
95 -40.00748190283775 -6.933930553634297
96 -30.760836571455002 -6.862940368843278
97 -35.05342976003885 -6.860783725003083
98 -35.607925809919834 -6.84968074158862
99 -36.289148181676865 -6.82437929018563
100 -39.080485977232456 -6.816422052849038
101 -36.62103020399809 -6.8072178516168425
102 -30.27193532884121 -6.804641638829289
103 -44.302544586360455 -6.776946485018116
104 -32.733285300433636 -6.754490746334454
105 -55.180564641952515 -6.7220638398623045
106 -30.712675534188747 -6.70154906780897
107 -48.04372537136078 -6.630515321441343
108 -37.257134199142456 -6.6226137251955555
109 -36.27971975505352 -6.591686918758855
110 -34.57277147471905 -6.554720227695469
111 -34.505003809928894 -6.525714725590517
112 -31.640373803675175 -6.332907111332838
113 -33.44444793835282 -6.2933108931500765
114 -28.235957115888596 -6.268352959625601
115 -33.300476871430874 -6.234849386475081
116 -34.08977399021387 -6.2050284204911375
117 -30.220903486013412 -5.822179296497748
118 -34.23761895298958 -5.106479098285975
119 -38.74662555754185 -5.027957977402961
train accuracy: 0.9994444444444445
validation accuracy: 1.0
[-50.4922686  -50.03933802 -45.67057988 -44.14602409 -38.4127039
 -31.9690994  -26.70472176 -24.879107   -20.13839115 -19.31195542
 -17.5574237  -15.86494761 -15.3762895  -14.5314246  -14.44242009
 -13.23889484 -12.93192763 -12.83470558 -12.68135973 -12.65388613
 -12.21288548 -12.20156251 -12.15190477 -10.71236081 -10.68284148
 -10.24301497 -10.23741402  -9.95213624  -9.91562534  -9.61341383
  -9.53946712  -9.4114321   -9.36583769  -9.3204038   -9.03213832
  -9.00826184  -8.78767738  -8.77669269  -8.77140354  -8.73442715
  -8.69799836  -8.60682171  -8.57176638  -8.50908773  -8.39503439
  -8.21132132  -8.1871554   -8.17730947  -8.14504731  -8.13319584
  -8.08385873  -7.8925977   -7.87085652  -7.86677913  -7.83888593
  -7.80687588  -7.78313555  -7.74814189  -7.74091195  -7.64370801
  -7.52208801  -7.46074446  -7.45461548  -7.44992307  -7.42236696
  -7.33588812  -7.31767288  -7.26183842  -7.2605308   -7.26044772
  -7.25764963  -7.20072394  -7.18702184  -7.06833942  -6.97596103
  -6.91641149  -6.89105631  -6.8824015   -6.86078373  -6.84968074
  -6.8329252   -6.82437929  -6.80637329  -6.79503811  -6.78275526
  -6.75449075  -6.70154907  -6.61386724  -6.59168692  -6.5505446
  -6.52571473  -6.44012287  -6.33290711  -6.29331089  -6.26835296
  -6.2623833   -6.23484939  -6.11489017  -6.1076302   -5.94907623
  -5.92510533  -5.8221793   -5.81563372  -5.619283    -5.47822502
  -5.34672977  -5.28473069  -5.23560202  -5.1064791   -5.02795798
  -4.94798829  -4.91146799  -4.86871724  -4.73967842  -4.66517688
  -4.55224494  -4.33888524  -2.98295572  -2.08936513  -2.08798829]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.000347943025866897, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.8706e-02,  2.3755e-02,  2.7980e-02,  1.4543e-02,  2.5182e-01,
          1.2505e-04,  1.1612e-02,  3.7933e-03,  3.7346e-04, -4.7825e-05,
         -2.0634e-03, -2.5489e+00, -1.0057e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.00031655222069105094, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.2624e-01,  6.4426e-02, -1.5108e-01,  8.7019e-02,  1.6872e+00,
         -5.0574e-01,  3.7206e-02, -3.6473e-02, -3.6810e-01,  5.3633e-02,
         -1.0927e-03, -4.0118e+00, -1.6014e+00]], device='cuda:1'))])
end of epoch 2: val_loss 4.370373752458079e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0735e-01,  2.2961e-02, -9.7075e-02, -6.5612e-02,  1.3819e+00,
         -2.0234e-01,  1.3474e-02,  4.9143e-03,  1.6533e-02,  2.0653e-01,
          1.5915e-04, -3.4554e+00, -1.1156e+00]], device='cuda:1'))])
end of epoch 3: val_loss 0.0003949774290862962, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 5.486686902571591e-06, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.18674668034211536, val_acc 0.98
trigger times: 3
end of epoch 6: val_loss 8.752419816531187e-06, val_acc 1.0
trigger times: 4
end of epoch 7: val_loss 0.04928335453968152, val_acc 0.985
trigger times: 5
end of epoch 8: val_loss 1.0727864647464003e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.2964e-01,  2.1093e-01, -7.0547e-02,  1.3406e-01,  1.8764e+00,
          2.4156e-05,  2.5734e-02, -4.3826e-02, -4.6068e-01,  9.9661e-06,
         -2.0647e-03, -4.2662e+00, -1.8320e+00]], device='cuda:1'))])
end of epoch 9: val_loss 2.4120226968094015e-06, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 4.361918244732976e-06, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 1.73160128639438e-05, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 2.5847109628429623e-06, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 1.0972582262169795e-06, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 3.933896341123955e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0083e-01,  1.3656e-01, -5.4963e-02,  1.3308e-01,  1.4348e+00,
         -2.9743e-01,  2.6367e-02, -2.8711e-02,  4.6100e-05,  1.1567e-01,
          1.6541e-03, -4.0767e+00, -1.9200e+00]], device='cuda:1'))])
end of epoch 15: val_loss 4.656656306814e-06, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 1.7821683535146348e-07, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 5.687865704295803e-06, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 6.318083713097167e-08, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 2.3778313493956206e-05, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 7.226245741023263e-06, val_acc 1.0
trigger times: 6
end of epoch 21: val_loss 1.1699954960775471e-06, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 4.6013396506339175e-07, val_acc 1.0
trigger times: 8
end of epoch 23: val_loss 2.89077600577059e-07, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.0029997204753646044, val_acc 1.0
trigger times: 10
Early stopping.
0 -56.48393738269806 -50.492268601198035
1 -79.46009767055511 -50.03933801517046
2 -71.28071027994156 -45.670579884154705
3 -64.35446505248547 -44.14602409201361
4 -55.76901948451996 -38.41270390343083
5 -68.0540640950203 -31.969099402548657
6 -55.18898642063141 -26.7047217556024
7 -36.47645227611065 -24.879106999799365
8 -32.50046882033348 -20.13839114930498
9 -61.70411956310272 -19.31195541503258
10 -33.26925067603588 -17.55742370467821
11 -59.169775664806366 -15.86494760790249
12 -40.52384090423584 -15.376289504800605
13 -34.07776319980621 -14.531424598833084
14 -41.931652665138245 -14.442420089224363
15 -35.09151503443718 -13.238894840717434
16 -41.079790115356445 -12.931927626703619
17 -43.914318561553955 -12.834705577238381
18 -30.588909447193146 -12.68135972540495
19 -44.447696298360825 -12.653886125125771
20 -32.91717302799225 -12.212885479706735
21 -28.73350962996483 -12.201562509199652
22 -29.97782066464424 -12.151904772081672
23 -31.099580466747284 -10.712360811983652
24 -28.789117362350225 -10.682841476337513
25 -34.88881307840347 -10.243014974013878
26 -30.969350308179855 -10.237414016504284
27 -33.22770157456398 -9.952136243613072
28 -41.21214336156845 -9.915625344666225
29 -39.59112012386322 -9.613413827917123
30 -36.5109720826149 -9.539467116745772
31 -38.47572219371796 -9.411432104623874
32 -21.49527657777071 -9.365837693916959
33 -35.22053521871567 -9.320403797692478
34 -25.843865640461445 -9.032138321897921
35 -21.637823402881622 -9.00826184473765
36 -38.97243458032608 -8.787677382440757
37 -19.722309209406376 -8.77669268950041
38 -23.730193629860878 -8.771403544589875
39 -27.822582334280014 -8.734427145563672
40 -30.465460538864136 -8.697998357644375
41 -29.985929414629936 -8.606821706638067
42 -27.96288886666298 -8.571766375923568
43 -28.574587419629097 -8.509087731683886
44 -36.41462862491608 -8.395034386541155
45 -18.588223300874233 -8.21132132358506
46 -30.43812234699726 -8.18715539982755
47 -31.08408510684967 -8.177309469852489
48 -31.80695252120495 -8.14504730530435
49 -14.905563252046704 -8.133195842510668
50 -22.771238937973976 -8.083858734159596
51 -30.947738975286484 -7.89259769544321
52 -18.967044189572334 -7.87085652085273
53 -16.710251063108444 -7.866779132361029
54 -19.72197588905692 -7.8388859293032205
55 -27.357521548867226 -7.806875877421669
56 -28.16181279718876 -7.783135549451343
57 -28.84586726129055 -7.748141886862429
58 -32.36753508448601 -7.74091194733651
59 -29.094981625676155 -7.643708012588531
60 -33.95132575929165 -7.5220880133088865
61 -33.03733766078949 -7.460744459941412
62 -23.779293477535248 -7.454615479759885
63 -23.565134838223457 -7.449923071761981
64 -31.614064931869507 -7.42236696228996
65 -21.717195600271225 -7.335888123526957
66 -27.15610098838806 -7.3176728825465
67 -28.70747435092926 -7.261838415984296
68 -22.354052059352398 -7.260530804009704
69 -18.51259197294712 -7.260447723331225
70 -22.54646334797144 -7.257649626366425
71 -22.26075080037117 -7.200723940745014
72 -13.421640615910292 -7.187021842392776
73 -11.555171802639961 -7.068339420262656
74 -20.060524811968207 -6.975961025657157
75 -20.577429689466953 -6.916411493886011
76 -24.356205755844712 -6.89105631221384
77 -24.65110670775175 -6.8824015040531314
78 -20.294322926551104 -6.860783725003083
79 -20.28550375252962 -6.84968074158862
80 -25.079277716577053 -6.832925196826282
81 -20.952502518892288 -6.82437929018563
82 -21.769198121502995 -6.806373286750884
83 -16.48978853970766 -6.795038107128534
84 -21.150127410888672 -6.782755260434519
85 -16.81341851502657 -6.754490746334454
86 -15.426408164203167 -6.70154906780897
87 -18.545714423060417 -6.6138672355107495
88 -21.169344007968903 -6.591686918758855
89 -20.350637143477798 -6.550544598138377
90 -12.02434054017067 -6.525714725590517
91 -18.16890649497509 -6.440122873488353
92 -15.806472301483154 -6.332907111332838
93 -17.027917087078094 -6.2933108931500765
94 -14.303849279880524 -6.268352959625601
95 -10.306608645245433 -6.262383298052851
96 -16.831927478313446 -6.234849386475081
97 -15.045626796782017 -6.114890165876514
98 -15.379359908401966 -6.1076302039584585
99 -10.594635833054781 -5.949076232223302
100 -19.50964508485049 -5.9251053339710795
101 -18.009558144956827 -5.822179296497748
102 -19.533161237835884 -5.815633719981554
103 -8.449643351137638 -5.6192830006500385
104 -16.39093716815114 -5.478225015141782
105 -15.263091821223497 -5.346729766280054
106 -11.09608443081379 -5.284730694668094
107 -12.73143908008933 -5.235602024100907
108 -15.063686735928059 -5.106479098285975
109 -14.491339456290007 -5.027957977402961
110 -9.221825361251831 -4.947988286227357
111 -13.861087117344141 -4.91146799266931
112 -8.465562604367733 -4.868717238112522
113 -11.114740425720811 -4.739678419174161
114 -7.661070443689823 -4.665176876338204
115 -9.804599196650088 -4.5522449364103235
116 -8.05153938382864 -4.338885239595814
117 -4.742561783641577 -2.9829557234889226
118 -1.3773633912205696 -2.089365132553529
119 -1.623802898451686 -2.087988288572641
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -38.4127039  -31.9690994  -24.879107   -20.13839115
 -19.31195542 -17.5574237  -15.3762895  -14.44242009 -12.93192763
 -12.83470558 -12.68135973 -12.21288548 -12.20156251 -10.23741402
 -10.15787243  -9.85344864  -9.61341383  -9.61207605  -9.49480305
  -9.4324156   -9.4114321   -9.25897447  -9.12736692  -9.10365571
  -9.04373401  -9.03213832  -9.00826184  -8.94019661  -8.87031787
  -8.82251889  -8.78767738  -8.75573811  -8.73442715  -8.69799836
  -8.60682171  -8.60440336  -8.60365983  -8.53879682  -8.50908773
  -8.13319584  -8.12731487  -8.11367876  -7.87085652  -7.80790338
  -7.80687588  -7.79546853  -7.78313555  -7.74814189  -7.68041575
  -7.64370801  -7.52465352  -7.4537144   -7.44992307  -7.42236696
  -7.31767288  -7.26734525  -7.26044772  -7.25764963  -7.20072394
  -7.06833942  -7.06192876  -7.00482833  -6.97596103  -6.91509528
  -6.90682209  -6.8824015   -6.79503811  -6.78275526  -6.76487272
  -6.75272261  -6.70154907  -6.44971335  -6.44012287  -6.43471928
  -6.36082757  -6.33290711  -6.29331089  -6.28733754  -6.2623833
  -6.23484939  -6.12029399  -6.1076302   -6.00581574  -5.99594331
  -5.99169392  -5.96780874  -5.8221793   -5.75133868  -5.47822502
  -5.46942129  -5.42600947  -5.35747625  -5.29825089  -5.23560202
  -5.1064791   -5.10258537  -4.94798829  -4.91146799  -4.86871724
  -4.78555666  -4.73967842  -4.66517688  -4.61082554  -4.55605447
  -4.55224494  -4.36755907  -4.33888524  -4.31024202  -4.27568753
  -3.9675774   -3.93741379  -3.91306779  -3.90833852  -3.37247845
  -3.16231135  -2.50493442  -2.43676513  -2.08936513  -1.63961111]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.000441434629802977, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.3446e-03,  1.9802e-03, -2.1475e-02, -1.1448e-02,  8.4435e-01,
          4.4841e-05,  1.9433e-02,  2.8033e-02, -1.8615e-04,  1.9883e-05,
          4.7205e-04, -2.8039e+00, -8.8959e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.0046047716823555615, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 4.735715007484487e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.5583e-06,  5.7278e-02, -5.3592e-02,  9.3674e-02,  1.0891e+00,
          7.8399e-06, -2.1350e-06, -2.7676e-02, -3.6002e-06,  4.4242e-05,
         -2.4109e-03, -3.0639e+00, -1.7059e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.15542908821505613, val_acc 0.985
trigger times: 1
end of epoch 4: val_loss 0.0020925585892751643, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.0030092651799046165, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 0.0018150591610125843, val_acc 1.0
trigger times: 4
end of epoch 7: val_loss 0.14267306701512872, val_acc 0.975
trigger times: 5
end of epoch 8: val_loss 1.182454317145698e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5384e-02,  1.5379e-01, -1.5336e-01, -2.4448e-06,  9.1205e-01,
         -1.1876e-04,  1.6366e-02, -1.0949e-02,  6.4499e-05, -4.1776e-05,
         -1.5391e-03, -2.6619e+00, -1.8714e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.004851155440901849, val_acc 0.995
trigger times: 1
end of epoch 10: val_loss 0.00023811803452748138, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 3.360809185579683e-06, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 0.008726837547437433, val_acc 0.995
trigger times: 4
end of epoch 13: val_loss 5.6191835284309376e-05, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 0.15066483479881757, val_acc 0.955
trigger times: 6
end of epoch 15: val_loss 3.1034479620828393e-06, val_acc 1.0
trigger times: 7
end of epoch 16: val_loss 0.0663624308973088, val_acc 0.975
trigger times: 8
end of epoch 17: val_loss 3.437196532409814e-06, val_acc 1.0
trigger times: 9
end of epoch 18: val_loss 0.004971778030301657, val_acc 0.995
trigger times: 10
Early stopping.
0 -92.42149758338928 -50.492268601198035
1 -84.24488490819931 -38.41270390343083
2 -99.66404378414154 -31.969099402548657
3 -59.82460531592369 -24.879106999799365
4 -48.113779321312904 -20.13839114930498
5 -85.1465208530426 -19.31195541503258
6 -56.33350211381912 -17.55742370467821
7 -60.10639727115631 -15.376289504800605
8 -58.7655388712883 -14.442420089224363
9 -55.913538098335266 -12.931927626703619
10 -57.00635975599289 -12.834705577238381
11 -51.86018180847168 -12.68135972540495
12 -46.671499848365784 -12.212885479706735
13 -41.556728303432465 -12.201562509199652
14 -48.13285630941391 -10.237414016504284
15 -44.1426702439785 -10.157872429279685
16 -43.118636548519135 -9.853448639155308
17 -48.888421803712845 -9.613413827917123
18 -45.53723528981209 -9.61207604859686
19 -41.598114639520645 -9.494803050150548
20 -41.875151455402374 -9.43241559619315
21 -41.22785297036171 -9.411432104623874
22 -43.73575112223625 -9.258974465411482
23 -44.42989465594292 -9.127366920639858
24 -40.46834334731102 -9.103655708884856
25 -38.37072730064392 -9.043734007025918
26 -41.10482946038246 -9.032138321897921
27 -27.294930934906006 -9.00826184473765
28 -44.25701814889908 -8.94019661450874
29 -44.21978959441185 -8.870317865569552
30 -38.185712702572346 -8.822518894155467
31 -41.31714445352554 -8.787677382440757
32 -39.82477582991123 -8.755738108715551
33 -36.24578692018986 -8.734427145563672
34 -37.34135961532593 -8.697998357644375
35 -40.10335302352905 -8.606821706638067
36 -33.437400624156 -8.604403355518787
37 -33.964614763855934 -8.603659829199897
38 -36.34115946292877 -8.538796823137261
39 -38.88659244775772 -8.509087731683886
40 -22.346167474985123 -8.133195842510668
41 -34.419075071811676 -8.12731487164362
42 -33.15929798036814 -8.113678756388452
43 -25.41351194679737 -7.87085652085273
44 -34.567548990249634 -7.807903384839565
45 -33.122247368097305 -7.806875877421669
46 -36.07925674319267 -7.795468530225197
47 -31.490877613425255 -7.783135549451343
48 -33.330118745565414 -7.748141886862429
49 -33.02393947541714 -7.680415749299637
50 -40.61563563346863 -7.643708012588531
51 -34.57817332446575 -7.524653523945226
52 -29.07683628425002 -7.453714404038217
53 -29.603639990091324 -7.449923071761981
54 -32.739181861281395 -7.42236696228996
55 -38.328318759799004 -7.3176728825465
56 -31.557916551828384 -7.2673452516659385
57 -27.196867287158966 -7.260447723331225
58 -27.976074017584324 -7.257649626366425
59 -30.11847484856844 -7.200723940745014
60 -23.969086796045303 -7.068339420262656
61 -22.69452053308487 -7.061928755861928
62 -27.704316429793835 -7.0048283252024754
63 -28.83985234797001 -6.975961025657157
64 -26.91683328151703 -6.91509528361517
65 -28.353146955370903 -6.906822092413673
66 -35.232695147395134 -6.8824015040531314
67 -24.80118841677904 -6.795038107128534
68 -27.48596751689911 -6.782755260434519
69 -27.256908431649208 -6.764872722555186
70 -33.840016797184944 -6.752722605708495
71 -22.325560584664345 -6.70154906780897
72 -30.287777200341225 -6.449713347520261
73 -26.955338910222054 -6.440122873488353
74 -30.348343029618263 -6.434719281747585
75 -27.36616812646389 -6.360827570707348
76 -23.54072729125619 -6.332907111332838
77 -24.844224974513054 -6.2933108931500765
78 -30.685279183089733 -6.287337537107321
79 -18.931728333234787 -6.262383298052851
80 -24.926448233425617 -6.234849386475081
81 -26.23074108362198 -6.1202939866803
82 -22.29149254411459 -6.1076302039584585
83 -25.7484123185277 -6.005815735391963
84 -27.19459468126297 -5.995943312113342
85 -26.818689107894897 -5.99169391883529
86 -27.88901449739933 -5.967808740946718
87 -27.800622537732124 -5.822179296497748
88 -20.72596375644207 -5.751338682941158
89 -25.139015093445778 -5.478225015141782
90 -24.485952015966177 -5.469421285315224
91 -21.034880578517914 -5.426009468592621
92 -23.204405024647713 -5.357476254142403
93 -23.506089732050896 -5.298250886694488
94 -17.78817270696163 -5.235602024100907
95 -22.834095910191536 -5.106479098285975
96 -23.399140253663063 -5.10258536899766
97 -16.616467740386724 -4.947988286227357
98 -23.041545443236828 -4.91146799266931
99 -15.343114629387856 -4.868717238112522
100 -19.15928726270795 -4.785556656903412
101 -18.88913687132299 -4.739678419174161
102 -13.807924672961235 -4.665176876338204
103 -16.28158864378929 -4.610825535937228
104 -18.93056431412697 -4.5560544664847225
105 -17.39550255611539 -4.5522449364103235
106 -15.564418502151966 -4.3675590655329115
107 -15.064715020358562 -4.338885239595814
108 -15.052322253584862 -4.310242015420162
109 -16.83937705308199 -4.275687526844279
110 -13.627373486757278 -3.9675774015799345
111 -15.995478797703981 -3.9374137914382588
112 -13.645735941827297 -3.9130677902883444
113 -16.005909215658903 -3.9083385163550144
114 -11.293455354869366 -3.372478449846485
115 -11.659068197011948 -3.1623113531496707
116 -6.277020499110222 -2.5049344166441636
117 -4.1556647308170795 -2.4367651295022275
118 -2.3511923775076866 -2.089365132553529
119 -0.6434772685170174 -1.6396111071788084
train accuracy: 1.0
validation accuracy: 0.995
[-38.4127039  -20.13839115 -19.31195542 -17.5574237  -15.3762895
 -12.83470558 -12.68135973 -12.20156251 -10.15787243  -9.85344864
  -9.65864135  -9.65535076  -9.6046481   -9.4114321   -9.40805152
  -9.20264686  -9.12736692  -9.10365571  -9.04373401  -9.03213832
  -9.00751386  -8.99408723  -8.99273722  -8.96674577  -8.90101705
  -8.85809344  -8.82760102  -8.75573811  -8.73442715  -8.730148
  -8.69799836  -8.60365983  -8.50908773  -8.49280469  -8.26623343
  -8.19675801  -8.13319584  -8.12731487  -8.11367876  -7.90428498
  -7.87085652  -7.83721949  -7.80687588  -7.78839562  -7.72589861
  -7.52465352  -7.51671871  -7.4391712   -7.41449106  -7.39168934
  -7.34286852  -7.31767288  -7.26734525  -7.25764963  -7.22680925
  -7.20072394  -7.17974052  -7.06836821  -7.06833942  -7.00482833
  -6.97388964  -6.90682209  -6.8824015   -6.76487272  -6.75272261
  -6.75184459  -6.70154907  -6.51063975  -6.50260159  -6.2751656
  -6.2623833   -6.23484939  -6.22326004  -6.12029399  -6.09677229
  -6.07387604  -6.00581574  -5.96780874  -5.93044068  -5.85664895
  -5.85623643  -5.84873259  -5.83962418  -5.8221793   -5.79859834
  -5.43590821  -5.35747625  -5.25061782  -5.20433167  -5.10258537
  -4.94648548  -4.94345112  -4.91146799  -4.86871724  -4.79100676
  -4.78555666  -4.74171638  -4.73967842  -4.6250402   -4.61082554
  -4.55605447  -4.55224494  -4.4785664   -4.36755907  -4.33888524
  -4.31024202  -4.27568753  -4.033256    -4.01774431  -4.01266445
  -3.91306779  -3.90833852  -3.87622447  -3.59892155  -3.49307323
  -3.37247845  -3.09428415  -2.4180376   -1.79496041  -1.63961111]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.046867942067530206, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.1441e-03, -6.7364e-03, -2.5530e-02,  6.1206e-02,  2.7205e-01,
          1.5135e-01, -2.0812e-02,  1.6571e-02, -6.2528e-03, -3.0690e-03,
         -3.9664e-04, -1.7563e+00, -6.4152e-01]], device='cuda:2'))])
end of epoch 1: val_loss 3.6000227606081124e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6614e-02,  5.5969e-02,  4.0809e-03,  7.9089e-02,  1.1269e+00,
          6.2258e-01, -3.2653e-02, -3.5535e-04,  3.5520e-05,  1.3128e-04,
          7.3357e-04, -2.7825e+00, -1.6473e+00]], device='cuda:2'))])
end of epoch 2: val_loss 5.464028331296333e-05, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 6.890081987265262e-07, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 1.5764834103784153e-05, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.00032091554112852807, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 5.539790011432899e-06, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.5093246104982234, val_acc 0.96
trigger times: 6
end of epoch 8: val_loss 1.460308792289311e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.9833e-02,  1.1639e-01, -5.2721e-05,  5.2639e-02,  1.0118e+00,
          6.3876e-01, -6.5173e-03,  1.6105e-02, -1.6188e-04, -9.0752e-02,
          7.3322e-04, -2.8169e+00, -1.8784e+00]], device='cuda:2'))])
end of epoch 9: val_loss 0.003125171958344204, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 5.98801144936445e-05, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.05278902997408231, val_acc 0.995
trigger times: 3
end of epoch 12: val_loss 5.731575974238012e-05, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 0.00031381678548875414, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 2.4914756772851378e-05, val_acc 1.0
trigger times: 6
end of epoch 15: val_loss 1.7288000234394475e-05, val_acc 1.0
trigger times: 7
end of epoch 16: val_loss 0.014733473546889399, val_acc 0.995
trigger times: 8
end of epoch 17: val_loss 0.0005271870642820886, val_acc 1.0
trigger times: 9
end of epoch 18: val_loss 0.0003136417397777791, val_acc 1.0
trigger times: 10
Early stopping.
0 -112.41111055016518 -38.41270390343083
1 -37.48557013273239 -20.13839114930498
2 -84.68508100509644 -19.31195541503258
3 -81.66230937838554 -17.55742370467821
4 -77.93497014045715 -15.376289504800605
5 -54.05007356405258 -12.834705577238381
6 -68.11116276681423 -12.68135972540495
7 -48.86242491006851 -12.201562509199652
8 -72.07975372672081 -10.157872429279685
9 -81.77404969930649 -9.853448639155308
10 -77.4147764146328 -9.658641351762821
11 -73.62021094560623 -9.655350758594011
12 -75.77061250805855 -9.604648102507884
13 -35.15376669168472 -9.411432104623874
14 -78.19319695234299 -9.408051516044287
15 -79.42685657739639 -9.202646861176186
16 -75.75343132019043 -9.127366920639858
17 -73.83465433120728 -9.103655708884856
18 -79.28852516412735 -9.043734007025918
19 -60.87511366605759 -9.032138321897921
20 -68.58290416002274 -9.007513856698463
21 -77.36624121665955 -8.994087233756813
22 -27.396118842065334 -8.992737224962683
23 -77.47697669267654 -8.966745767291004
24 -76.10010659694672 -8.901017048154072
25 -74.6767930984497 -8.858093436775409
26 -76.19609877467155 -8.827601022025194
27 -74.19466441869736 -8.755738108715551
28 -70.47095346450806 -8.734427145563672
29 -72.08781450986862 -8.730147999204457
30 -23.057490050792694 -8.697998357644375
31 -72.925990909338 -8.603659829199897
32 -29.67298198118806 -8.509087731683886
33 -77.72967255115509 -8.492804685757212
34 -70.25974178314209 -8.266233430525494
35 -32.35698202252388 -8.196758005562865
36 -12.505079209804535 -8.133195842510668
37 -25.057677090168 -8.12731487164362
38 -70.62914061546326 -8.113678756388452
39 -63.10597139596939 -7.904284979076667
40 -17.50949054956436 -7.87085652085273
41 -22.621443759649992 -7.837219491176332
42 -19.66309578716755 -7.806875877421669
43 -64.72982746362686 -7.788395620745407
44 -18.869129590690136 -7.7258986133705365
45 -63.19745844602585 -7.524653523945226
46 -23.670187942683697 -7.516718710973199
47 -62.13562834262848 -7.43917120161514
48 -59.5491678416729 -7.414491061606524
49 -58.686761260032654 -7.391689338548364
50 -62.489189356565475 -7.342868522116522
51 -31.992267452180386 -7.3176728825465
52 -54.62212598323822 -7.2673452516659385
53 -12.742386639118195 -7.257649626366425
54 -24.29707307368517 -7.226809246458072
55 -58.62507364153862 -7.200723940745014
56 -20.869916072115302 -7.179740515233901
57 -52.37979567050934 -7.068368208264248
58 -48.67809098958969 -7.068339420262656
59 -58.60250699520111 -7.0048283252024754
60 -57.364769250154495 -6.97388963577068
61 -22.951430574059486 -6.906822092413673
62 -29.261758759617805 -6.8824015040531314
63 -20.609999358654022 -6.764872722555186
64 -27.969230458140373 -6.752722605708495
65 -22.35061015188694 -6.751844591700733
66 -12.355171419680119 -6.70154906780897
67 -42.982662349939346 -6.510639752980461
68 -23.296930104494095 -6.50260158507656
69 -25.911237478256226 -6.2751656016398085
70 -39.1242610514164 -6.262383298052851
71 -15.747841536998749 -6.234849386475081
72 -35.63666695356369 -6.223260044568572
73 -20.018695812672377 -6.1202939866803
74 -37.95606953650713 -6.096772287835152
75 -22.31653843820095 -6.073876037129521
76 -47.980884075164795 -6.005815735391963
77 -32.684837974607944 -5.967808740946718
78 -32.87675429135561 -5.93044068320723
79 -25.20929125137627 -5.856648951280896
80 -16.125742219388485 -5.856236434055476
81 -8.180930633097887 -5.848732590103763
82 -14.348684415221214 -5.839624178425524
83 -21.349252231419086 -5.822179296497748
84 -32.805748607963324 -5.798598341357546
85 -10.191808223724365 -5.435908209068999
86 -30.084392607212067 -5.357476254142403
87 -13.539553336799145 -5.2506178238974535
88 -12.394461005926132 -5.204331672977582
89 -17.2459933757782 -5.10258536899766
90 -2.26362032443285 -4.946485480590224
91 -3.166619800031185 -4.943451117293054
92 -17.44481748342514 -4.91146799266931
93 -5.933070585131645 -4.868717238112522
94 -5.965178236365318 -4.791006764007675
95 -8.54654998332262 -4.785556656903412
96 -2.70009995251894 -4.741716383084015
97 -10.380848094820976 -4.739678419174161
98 -4.090248174965382 -4.625040202528297
99 -4.5220115929841995 -4.610825535937228
100 -10.10869524627924 -4.5560544664847225
101 -10.512634709477425 -4.5522449364103235
102 -0.4791606143116951 -4.4785663972795335
103 -3.0804454758763313 -4.3675590655329115
104 -6.729065030813217 -4.338885239595814
105 -4.455414295196533 -4.310242015420162
106 -8.05199383944273 -4.275687526844279
107 0.09241610020399094 -4.0332560028107505
108 -3.964230455458164 -4.0177443066186225
109 -9.602970138192177 -4.012664454404856
110 -0.5505044609308243 -3.9130677902883444
111 -11.513332948088646 -3.9083385163550144
112 -7.682170893996954 -3.876224473277615
113 3.65152145922184 -3.598921545070444
114 5.773646652698517 -3.493073232614087
115 -0.17160437256097794 -3.372478449846485
116 10.28661473095417 -3.094284145851846
117 14.986387059092522 -2.418037602502445
118 18.53863325715065 -1.7949604098549072
119 18.607241958379745 -1.6396111071788084
train accuracy: 0.9972222222222222
validation accuracy: 1.0
[-38.4127039  -20.13839115 -19.31195542 -12.83470558 -12.68135973
  -9.65864135  -9.65535076  -9.14706149  -9.11980224  -9.10365571
  -9.04373401  -9.03213832  -8.96674577  -8.90101705  -8.80256609
  -8.75573811  -8.73443055  -8.730148    -8.66630332  -8.52025495
  -8.32658981  -8.32560648  -8.31950964  -8.29259794  -8.26623343
  -8.24746774  -8.19675801  -8.13913081  -8.12731487  -8.05604444
  -7.92438012  -7.90428498  -7.87640195  -7.87033693  -7.83721949
  -7.80687588  -7.72589861  -7.65848991  -7.59142333  -7.52721323
  -7.52465352  -7.47107281  -7.4391712   -7.41489467  -7.41029665
  -7.35967473  -7.31927217  -7.31767288  -7.26734525  -7.25904301
  -7.23655556  -7.22680925  -7.20072394  -7.17974052  -7.09046892
  -7.08226137  -7.06836821  -7.06833942  -6.8824015   -6.88050986
  -6.83776572  -6.79164458  -6.75272261  -6.72251962  -6.70154907
  -6.51063975  -6.44532969  -6.43927706  -6.2751656   -6.18132443
  -6.04267498  -5.94413328  -5.91294489  -5.85664895  -5.85623643
  -5.84873259  -5.83962418  -5.8221793   -5.79859834  -5.79148527
  -5.63169056  -5.62958402  -5.60160762  -5.45230038  -5.11018365
  -5.10258537  -5.0824288   -4.94648548  -4.94345112  -4.91146799
  -4.90263371  -4.86871724  -4.8214024   -4.79100676  -4.78555666
  -4.74171638  -4.61875499  -4.61082554  -4.59132428  -4.55224494
  -4.4785664   -4.36442799  -4.31024202  -4.29063844  -4.27568753
  -4.26401389  -4.21478551  -4.033256    -4.01266445  -4.00148557
  -3.97897365  -3.91306779  -3.90833852  -3.59892155  -3.50461066
  -3.25656246  -2.77810919  -1.92068026  -1.82842212  -1.79496041]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.10669916076254463, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7707e-01,  8.7338e-02,  9.1740e-02,  1.5397e-01,  5.8386e-01,
         -2.4751e-02, -6.0970e-02,  4.4209e-02, -1.1970e-04, -2.4042e-03,
         -2.2910e-04, -2.3904e+00, -1.2369e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.0922867825803047, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4460e-01,  2.2815e-01,  1.2687e-01,  3.5516e-01,  1.3233e+00,
          3.0564e-05, -6.0160e-02, -4.5962e-03, -2.2734e-01, -1.6158e-01,
          5.9776e-04, -3.1034e+00, -1.3625e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.03880671945944112, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.8892e-02,  1.2633e-01,  9.6184e-02,  2.4589e-01,  7.4163e-01,
         -3.5801e-05, -5.0586e-02, -3.6569e-02,  4.6412e-05,  3.0119e-04,
          1.7201e-03, -2.6483e+00, -5.7936e-01]], device='cuda:0'))])
end of epoch 3: val_loss 7.105783474131044e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2687e-02,  2.4332e-01,  1.2912e-01,  2.5847e-01,  9.2255e-01,
          3.2658e-02, -3.3088e-02, -4.4374e-03,  5.6251e-05, -2.2141e-05,
          6.6670e-04, -2.9034e+00, -1.1867e+00]], device='cuda:0'))])
end of epoch 4: val_loss 1.0463711808411358, val_acc 0.95
trigger times: 1
end of epoch 5: val_loss 0.08622851032513978, val_acc 0.985
trigger times: 2
end of epoch 6: val_loss 0.0002630942417366455, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 0.4365882992954241, val_acc 0.98
trigger times: 4
end of epoch 8: val_loss 0.032899568996550846, val_acc 0.985
trigger times: 5
end of epoch 9: val_loss 0.00029500488482014475, val_acc 1.0
trigger times: 6
end of epoch 10: val_loss 0.06629136111221207, val_acc 0.995
trigger times: 7
end of epoch 11: val_loss 0.0059203825223425, val_acc 0.995
trigger times: 8
end of epoch 12: val_loss 0.06269855172633526, val_acc 0.985
trigger times: 9
end of epoch 13: val_loss 8.28987287295746e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -70.44583186507225 -38.41270390343083
1 -27.536972120404243 -20.13839114930498
2 -57.44789183139801 -19.31195541503258
3 -40.49167877435684 -12.834705577238381
4 -54.15755930542946 -12.68135972540495
5 -45.96877697110176 -9.658641351762821
6 -43.746880769729614 -9.655350758594011
7 -42.244387075304985 -9.147061489753503
8 -34.78547176718712 -9.119802238248868
9 -43.69168081879616 -9.103655708884856
10 -51.806471168994904 -9.043734007025918
11 -44.453206688165665 -9.032138321897921
12 -45.41017389297485 -8.966745767291004
13 -46.82816098630428 -8.901017048154072
14 -30.66497364640236 -8.802566091249483
15 -43.59575144946575 -8.755738108715551
16 -29.170669578015804 -8.734430550491137
17 -42.090426713228226 -8.730147999204457
18 -33.60507944226265 -8.666303321247662
19 -30.61391019821167 -8.520254950088233
20 -30.24899235367775 -8.326589812889171
21 -21.60688526928425 -8.325606477429643
22 -30.59017948806286 -8.319509642015275
23 -28.10519577935338 -8.292597941603981
24 -41.356714844703674 -8.266233430525494
25 -27.155398927628994 -8.247467736949476
26 -25.35521499812603 -8.196758005562865
27 -27.73042743653059 -8.13913080908403
28 -26.868744120001793 -8.12731487164362
29 -21.188274770975113 -8.056044444148919
30 -25.09399577975273 -7.924380115403088
31 -37.19984617829323 -7.904284979076667
32 -26.35146912187338 -7.876401954072012
33 -32.31717166304588 -7.870336925700266
34 -26.540697395801544 -7.837219491176332
35 -19.05548732727766 -7.806875877421669
36 -17.456723272800446 -7.7258986133705365
37 -34.29083640128374 -7.658489907304371
38 -25.436353400349617 -7.591423329545319
39 -17.372618846595287 -7.5272132296143965
40 -37.12373098731041 -7.524653523945226
41 -34.75032480061054 -7.471072810429692
42 -36.6988328397274 -7.43917120161514
43 -28.660782873630524 -7.414894668727581
44 -25.97928486019373 -7.410296652361681
45 -25.033347636461258 -7.359674727801493
46 -28.03650203347206 -7.31927217477425
47 -25.612117052078247 -7.3176728825465
48 -31.96082440018654 -7.2673452516659385
49 -24.296515598893166 -7.259043009386958
50 -33.994111984968185 -7.2365555642158395
51 -27.405906528234482 -7.226809246458072
52 -35.24249383807182 -7.200723940745014
53 -24.10428111255169 -7.179740515233901
54 -23.421279601752758 -7.090468916754366
55 -24.867561742663383 -7.082261372106341
56 -30.219074226915836 -7.068368208264248
57 -19.33357697725296 -7.068339420262656
58 -19.92270465567708 -6.8824015040531314
59 -14.311774611473083 -6.880509855368357
60 -19.42457687854767 -6.83776572313227
61 -19.646947890520096 -6.791644584866051
62 -19.19365579634905 -6.752722605708495
63 -14.50196310132742 -6.72251961967784
64 -16.402093894779682 -6.70154906780897
65 -23.337911285459995 -6.510639752980461
66 -15.22194505110383 -6.445329694416962
67 -18.402679357677698 -6.439277061674403
68 -17.95508124306798 -6.2751656016398085
69 -12.322060041129589 -6.181324425895096
70 -14.38689647987485 -6.042674981665909
71 -19.573403738439083 -5.944133276796924
72 -18.57795951515436 -5.912944888094787
73 -13.781746424734592 -5.856648951280896
74 -9.574188649654388 -5.856236434055476
75 -7.99986557662487 -5.848732590103763
76 -9.740749139338732 -5.839624178425524
77 -19.90178808569908 -5.822179296497748
78 -19.52523770928383 -5.798598341357546
79 -18.697001218795776 -5.791485270977389
80 -9.650419570505619 -5.6316905560166655
81 -7.535383202135563 -5.629584019035365
82 -15.344546385109425 -5.601607622606792
83 -10.833511009812355 -5.452300379776242
84 -6.656110502779484 -5.110183648287911
85 -13.00985505245626 -5.10258536899766
86 -17.537499990314245 -5.082428796500795
87 -1.4422657415270805 -4.946485480590224
88 -1.4798078685998917 -4.943451117293054
89 -13.600327618420124 -4.91146799266931
90 -1.8327909111976624 -4.902633708410038
91 -7.533386260271072 -4.868717238112522
92 -1.8360940366983414 -4.8214024049459745
93 -4.457850784063339 -4.791006764007675
94 -8.619657773524523 -4.785556656903412
95 -9.349684838205576 -4.741716383084015
96 -9.679804837331176 -4.618754989193665
97 -11.76165372505784 -4.610825535937228
98 -4.931992493569851 -4.591324283259181
99 -8.689192172139883 -4.5522449364103235
100 -3.184185814112425 -4.4785663972795335
101 -0.49183420836925507 -4.364427992827658
102 -3.2267444133758545 -4.310242015420162
103 -3.387039966881275 -4.290638435684295
104 -6.780899219214916 -4.275687526844279
105 -4.3382655791938305 -4.264013889545308
106 -8.674080103635788 -4.214785508603272
107 -2.2056453451514244 -4.0332560028107505
108 -3.2763843312859535 -4.012664454404856
109 -9.614823825657368 -4.0014855703628225
110 -3.4389783293008804 -3.9789736482271096
111 -4.150691851973534 -3.9130677902883444
112 -2.31985105201602 -3.9083385163550144
113 -3.768967315554619 -3.598921545070444
114 -2.5669944137334824 -3.5046106584449706
115 -0.08764437586069107 -3.25656245614231
116 7.354310140013695 -2.7781091942523632
117 9.483769692480564 -1.9206802602455646
118 14.59083566069603 -1.8284221231808335
119 15.281220749020576 -1.7949604098549072
train accuracy: 1.0
validation accuracy: 1.0
[-38.4127039  -19.31195542 -13.86718658 -13.75504213 -13.65326057
 -13.27065953 -13.01830004 -12.83470558 -11.65663553 -11.44502796
 -11.36801234 -11.2887089  -10.90363351 -10.84591401 -10.80698363
 -10.78072561 -10.71741343 -10.35406488 -10.24904322  -9.65535076
  -9.14706149  -9.11980224  -9.04373401  -9.03298769  -9.03213832
  -8.90101705  -8.84297262  -8.81176083  -8.80256609  -8.73506922
  -8.62189712  -8.5688124   -8.47763349  -8.38883728  -8.29101253
  -8.13913081  -8.05604444  -7.98399747  -7.92438012  -7.89415877
  -7.83721949  -7.65848991  -7.52721323  -7.4391712   -7.42882384
  -7.41489467  -7.41029665  -7.35967473  -7.33985562  -7.31927217
  -7.31767288  -7.26734525  -7.25904301  -7.22680925  -7.20072394
  -7.09046892  -7.08226137  -7.06836821  -6.982023    -6.91823995
  -6.8824015   -6.8645202   -6.86304753  -6.80386539  -6.7806778
  -6.75272261  -6.65882426  -6.43927706  -6.2751656   -6.25356454
  -6.23067868  -6.18132443  -6.03773866  -5.87197342  -5.87188269
  -5.84873259  -5.83962418  -5.83725652  -5.82738806  -5.79148527
  -5.73198373  -5.72480064  -5.63169056  -5.62958402  -5.46457201
  -5.45230038  -5.3836752   -5.26908705  -5.17505053  -5.10258537
  -5.04765746  -4.94345112  -4.93449215  -4.91699158  -4.91146799
  -4.90263371  -4.86871724  -4.85944384  -4.8214024   -4.79100676
  -4.74171638  -4.61875499  -4.61082554  -4.59132428  -4.4785664
  -4.29063844  -4.27568753  -4.26401389  -4.21364954  -4.02868362
  -3.97897365  -3.91306779  -3.85792415  -3.83948931  -3.59892155
  -3.50461066  -3.16589402  -1.89499529  -1.83777886  -1.82842212]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.07507380958881538, val_acc 0.965
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.1890e-02,  2.0698e-01, -1.2980e-01,  3.1949e-02,  5.8217e-01,
         -5.2308e-03, -3.9983e-02,  4.4482e-02, -6.2788e-02, -3.1674e-02,
         -2.9562e-04, -2.6490e+00, -1.8688e-01]], device='cuda:0'))])
end of epoch 1: val_loss 2.580864812529171e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2067e-01,  2.3706e-01, -1.2222e-01,  1.2886e-01,  1.0754e+00,
         -3.1807e-03,  4.4321e-02,  6.7796e-02, -5.8630e-03,  8.4788e-03,
          5.3124e-04, -3.0623e+00, -1.2357e-01]], device='cuda:0'))])
end of epoch 2: val_loss 0.001689257416087564, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 1.7986660871116556e-06, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 1.5925437777042362e-06, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.0008273755499544677, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 2.5783646443855446e-06, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.0007465722970227873, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 4.398768808044906e-07, val_acc 1.0
trigger times: 7
end of epoch 9: val_loss 0.00020155759041912803, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 7.973760021222631e-06, val_acc 1.0
trigger times: 9
end of epoch 11: val_loss 2.193602104956227e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -58.05836749076843 -38.41270390343083
1 -70.98771142959595 -19.31195541503258
2 -27.83626177161932 -13.867186575297486
3 -27.984054386615753 -13.755042126034148
4 -26.93657049909234 -13.653260574695521
5 -25.925350926816463 -13.270659532356545
6 -27.78842591494322 -13.018300036841936
7 -53.10766476392746 -12.834705577238381
8 -25.780341375619173 -11.656635527193743
9 -27.985766917467117 -11.445027960343468
10 -26.510967254638672 -11.368012339728596
11 -26.367370299994946 -11.288708897673244
12 -25.781708888709545 -10.90363351238908
13 -26.403786927461624 -10.845914011461558
14 -25.391410306096077 -10.806983627911146
15 -26.36212718486786 -10.780725607900552
16 -26.16207278519869 -10.717413434172826
17 -26.221801981329918 -10.354064881933361
18 -26.872666984796524 -10.249043217806056
19 -44.20057138800621 -9.655350758594011
20 -43.39527577161789 -9.147061489753503
21 -41.52552539110184 -9.119802238248868
22 -40.852110385894775 -9.043734007025918
23 -10.739446893334389 -9.0329876863419
24 -39.06235718727112 -9.032138321897921
25 -44.14121276140213 -8.901017048154072
26 -15.775584433227777 -8.842972622225393
27 -32.07464453577995 -8.811760828190003
28 -28.37853291630745 -8.802566091249483
29 -20.391169391572475 -8.735069215767
30 -26.537291705608368 -8.621897119618861
31 -7.515839753672481 -8.568812404680017
32 -5.456149749457836 -8.47763349125231
33 -17.145158246159554 -8.388837276450653
34 -17.580072432756424 -8.291012533606596
35 -24.413177847862244 -8.13913080908403
36 -24.886433720588684 -8.056044444148919
37 -16.335928425192833 -7.983997466690604
38 -19.986160829663277 -7.924380115403088
39 -33.71095612645149 -7.8941587686517956
40 -20.09195016697049 -7.837219491176332
41 -36.39784994721413 -7.658489907304371
42 -4.421351172961295 -7.5272132296143965
43 -40.603711277246475 -7.43917120161514
44 -31.69009891152382 -7.428823839494483
45 -7.489055245183408 -7.414894668727581
46 -20.317075729370117 -7.410296652361681
47 -17.133942395448685 -7.359674727801493
48 -26.75565242767334 -7.33985562019908
49 -31.26462671160698 -7.31927217477425
50 -16.881644401699305 -7.3176728825465
51 -37.62770038843155 -7.2673452516659385
52 -6.298623660579324 -7.259043009386958
53 -8.130925205536187 -7.226809246458072
54 -41.0926530957222 -7.200723940745014
55 -15.87351431697607 -7.090468916754366
56 -18.801215574145317 -7.082261372106341
57 -36.54951739311218 -7.068368208264248
58 -12.418631818145514 -6.982023000386
59 -9.5213624201715 -6.918239950890848
60 -12.329647466540337 -6.8824015040531314
61 -6.113999109715223 -6.864520203180165
62 -11.54360419139266 -6.863047528779784
63 0.34121036157011986 -6.803865394679868
64 2.748463351279497 -6.780677798910882
65 -9.803683262318373 -6.752722605708495
66 -5.658520633354783 -6.658824258147932
67 -9.345919637009501 -6.439277061674403
68 -8.641244009137154 -6.2751656016398085
69 -8.183708379045129 -6.253564541974518
70 -3.938627813011408 -6.230678679558844
71 -13.788937337696552 -6.181324425895096
72 5.39934079349041 -6.037738664103451
73 -0.8323498331010342 -5.871973423632955
74 0.4806344248354435 -5.871882685178712
75 4.6482425183057785 -5.848732590103763
76 0.9950189180672169 -5.839624178425524
77 -17.23245370388031 -5.837256517234489
78 -16.58743559010327 -5.82738805975381
79 -19.77972422540188 -5.791485270977389
80 -14.319400859996676 -5.731983727777117
81 6.697079181671143 -5.7248006359550025
82 -1.0238138157874346 -5.6316905560166655
83 5.177809931337833 -5.629584019035365
84 3.436060070991516 -5.464572013562832
85 0.7009954191744328 -5.452300379776242
86 10.501576453447342 -5.383675204536576
87 10.448931634426117 -5.26908704643081
88 8.635928243398666 -5.175050528032114
89 -0.5725531587377191 -5.10258536899766
90 10.820867821574211 -5.047657461568441
91 10.574731856584549 -4.943451117293054
92 6.2254371754825115 -4.934492151233675
93 9.390688754618168 -4.916991577439874
94 -1.2765528354793787 -4.91146799266931
95 10.687193043529987 -4.902633708410038
96 5.858757793903351 -4.868717238112522
97 11.089560620486736 -4.859443841746001
98 10.640551425516605 -4.8214024049459745
99 8.458633311092854 -4.791006764007675
100 8.858954899013042 -4.741716383084015
101 9.29973927885294 -4.618754989193665
102 7.457672044634819 -4.610825535937228
103 8.750466503202915 -4.591324283259181
104 11.591277301311493 -4.4785663972795335
105 -3.7632492408156395 -4.290638435684295
106 8.28524723649025 -4.275687526844279
107 11.703983336687088 -4.264013889545308
108 12.16136246919632 -4.213649540024929
109 2.75932458601892 -4.028683623146895
110 12.029693640768528 -3.9789736482271096
111 12.057085633277893 -3.9130677902883444
112 12.851715788245201 -3.857924150517684
113 -0.1654120311141014 -3.839489305056573
114 14.141729548573494 -3.598921545070444
115 14.291548810899258 -3.5046106584449706
116 18.95711389183998 -3.1658940222657144
117 23.648042365908623 -1.8949952888463266
118 27.438004165887833 -1.8377788555257153
119 27.233100652694702 -1.8284221231808335
train accuracy: 1.0
validation accuracy: 1.0
[-106.33878536 -103.44829346 -102.68439261 -100.90279129 -100.88648225
 -100.76921336 -100.74026993 -100.67071901 -100.52052024 -100.50694617
 -100.32187549 -100.12017118 -100.00215204  -99.9931141   -99.506683
  -99.43354767  -99.41058255  -99.38632741  -99.37487795  -99.21152191
  -99.10032012  -99.01224837  -98.75283225  -97.78190344  -97.45529183
  -97.13608582  -96.84296697  -96.66622575  -92.52509415  -91.83829981
  -81.31590436  -79.38039416  -78.62150161  -13.86718658  -13.75504213
  -13.27065953  -12.83470558  -11.65663553  -11.44502796  -11.2887089
  -10.24904322   -9.65535076   -9.14706149   -9.03298769   -9.03213832
   -8.80019188   -8.73506922   -8.62189712   -8.59057809   -8.5688124
   -8.47763349   -8.16842242   -7.98399747   -7.9347791    -7.89415877
   -7.85709309   -7.75269054   -7.66855473   -7.65848991   -7.56992219
   -7.41489467   -7.3873344    -7.35967473   -7.33985562   -7.31927217
   -7.31767288   -7.26734525   -7.21882843   -7.20072394   -7.13502217
   -7.09046892   -7.07274188   -7.06836821   -6.92123174   -6.91823995
   -6.8824015    -6.86230871   -6.75272261   -6.43927706   -6.29977825
   -6.2751656    -6.25356454   -6.03773866   -5.99228436   -5.87197342
   -5.87188269   -5.85465472   -5.83962418   -5.82738806   -5.79281466
   -5.72480064   -5.62958402   -5.61593021   -5.45230038   -5.3836752
   -5.26908705   -5.26765966   -5.23859397   -5.17505053   -5.10258537
   -5.04765746   -4.85944384   -4.8214024    -4.79100676   -4.74171638
   -4.61875499   -4.59132428   -4.4785664    -4.26401389   -4.21364954
   -3.91306779   -3.89155962   -3.83948931   -3.83736398   -3.50461066
   -3.35112215   -3.22513528   -1.91776093   -1.83777886   -1.83414213]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 6.597176536757843e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4566e-01,  2.2484e-01, -1.6122e-02,  7.5683e-03,  4.9037e-01,
         -9.3956e-05, -5.6539e-02,  2.4000e-06,  1.9095e-04, -8.4136e-06,
         -7.5252e-04, -2.5085e+00, -1.6496e-01]], device='cuda:0'))])
end of epoch 1: val_loss 0.10526616461615017, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.13218291580676408, val_acc 0.99
trigger times: 2
end of epoch 3: val_loss 0.006233398169251813, val_acc 0.995
trigger times: 3
end of epoch 4: val_loss 1.8894421032200627e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.9812e-01,  1.4970e-01,  2.2855e-02,  8.8329e-02,  8.5084e-01,
          8.0187e-05, -1.1787e-01, -4.4864e-02,  2.9984e-05, -1.1819e-04,
         -1.1816e-03, -2.2427e+00, -2.9507e-02]], device='cuda:0'))])
end of epoch 5: val_loss 0.24070879551774851, val_acc 0.985
trigger times: 1
end of epoch 6: val_loss 0.05376501210663264, val_acc 0.99
trigger times: 2
end of epoch 7: val_loss 0.1410404825126818, val_acc 0.99
trigger times: 3
end of epoch 8: val_loss 0.17408114186783333, val_acc 0.99
trigger times: 4
end of epoch 9: val_loss 3.404255476191054e-05, val_acc 1.0
trigger times: 5
end of epoch 10: val_loss 3.153587680515102e-05, val_acc 1.0
trigger times: 6
end of epoch 11: val_loss 4.1723234289747776e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.1149e-01,  1.4401e-01, -6.2423e-02,  8.0337e-02,  4.4437e-01,
          1.9579e-05, -3.2391e-02, -1.2151e-01, -1.4647e-01, -1.2375e-04,
         -1.1812e-03, -3.1733e+00, -4.1428e-04]], device='cuda:0'))])
end of epoch 12: val_loss 0.1850963914245949, val_acc 0.99
trigger times: 1
end of epoch 13: val_loss 0.014632115364074707, val_acc 0.99
trigger times: 2
end of epoch 14: val_loss 1.3410961287263489e-07, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 0.1670203441374997, val_acc 0.99
trigger times: 4
end of epoch 16: val_loss 0.09494603633880615, val_acc 0.99
trigger times: 5
end of epoch 17: val_loss 3.884108736883718e-05, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 0.08007207155227661, val_acc 0.99
trigger times: 7
end of epoch 19: val_loss 0.009445311874150661, val_acc 0.995
trigger times: 8
end of epoch 20: val_loss 8.618196061149774e-07, val_acc 1.0
trigger times: 9
end of epoch 21: val_loss 6.419171108973387e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -987.4775319099426 -106.3387853584976
1 -973.5034003257751 -103.44829346119795
2 -974.859654545784 -102.68439261481575
3 -973.1943451166153 -100.90279129177834
4 -984.5115348696709 -100.88648224948467
5 -971.3955001831055 -100.76921336385534
6 -988.9276667237282 -100.7402699270995
7 -967.9043249487877 -100.6707190051482
8 -988.8704680800438 -100.52052024441564
9 -986.5782983899117 -100.50694616506347
10 -977.657284617424 -100.32187549355092
11 -985.5779471993446 -100.12017118253488
12 -982.4595177769661 -100.00215204175382
13 -968.4482007026672 -99.99311410267318
14 -965.5493509173393 -99.50668300330008
15 -980.1202117800713 -99.43354767487739
16 -986.1372753381729 -99.41058254524987
17 -967.1249235868454 -99.38632740502334
18 -931.3822667598724 -99.37487794659387
19 -974.8744280338287 -99.2115219139335
20 -974.155798971653 -99.10032011850174
21 -980.4611659049988 -99.01224836989934
22 -967.4517275691032 -98.7528322457548
23 -973.780647277832 -97.78190343874087
24 -965.4439684152603 -97.45529183325944
25 -963.438650727272 -97.13608582116728
26 -972.4572725892067 -96.84296697205971
27 -882.6004348397255 -96.66622574915127
28 -727.9886455535889 -92.52509414832849
29 -882.1611924171448 -91.83829981170625
30 -669.3059270083904 -81.3159043611833
31 -634.9470607638359 -79.38039415651993
32 -611.8630602955818 -78.62150160550247
33 4.152632929384708 -13.867186575297486
34 -7.1831407099962234 -13.755042126034148
35 12.258579134941101 -13.270659532356545
36 -28.610225096344948 -12.834705577238381
37 -17.760011300444603 -11.656635527193743
38 -7.9987276047468185 -11.445027960343468
39 -9.5113013535738 -11.288708897673244
40 -11.176467031240463 -10.249043217806056
41 -77.68729037046432 -9.655350758594011
42 -81.83488965034485 -9.147061489753503
43 9.579986959695816 -9.0329876863419
44 -63.8333540558815 -9.032138321897921
45 -1.4900841936469078 -8.800191880129875
46 -20.455783918499947 -8.735069215767
47 -25.332685831934214 -8.621897119618861
48 -60.3048614859581 -8.59057808951432
49 13.498254477977753 -8.568812404680017
50 12.400692652910948 -8.47763349125231
51 14.796731412410736 -8.168422419518215
52 -2.2254141084849834 -7.983997466690604
53 16.87676353752613 -7.934779103555965
54 -57.516844153404236 -7.8941587686517956
55 5.056356981396675 -7.857093089476009
56 -45.77720066905022 -7.7526905428972785
57 8.410109928809106 -7.66855472620954
58 -62.841177344322205 -7.658489907304371
59 11.007563967257738 -7.56992219346592
60 3.8603004664182663 -7.414894668727581
61 -27.32014242839068 -7.387334398144209
62 22.978091828525066 -7.359674727801493
63 -34.053613767027855 -7.33985562019908
64 -44.58551946282387 -7.31927217477425
65 -20.88751769065857 -7.3176728825465
66 -70.52149629592896 -7.2673452516659385
67 -1.7350831925868988 -7.218828428696178
68 -78.67267289757729 -7.200723940745014
69 5.132724776864052 -7.135022169812883
70 24.33947964012623 -7.090468916754366
71 1.882018469274044 -7.072741877061801
72 -65.03626650571823 -7.068368208264248
73 36.89962059259415 -6.9212317401256005
74 -8.003276616334915 -6.918239950890848
75 -11.81533158943057 -6.8824015040531314
76 41.06413081288338 -6.862308709354992
77 -9.361922852694988 -6.752722605708495
78 -5.419697463512421 -6.439277061674403
79 18.122513622045517 -6.299778249208555
80 -5.667438708245754 -6.2751656016398085
81 10.341439381241798 -6.253564541974518
82 27.406169150955975 -6.037738664103451
83 26.81280167400837 -5.992284360924977
84 3.5743898153305054 -5.871973423632955
85 4.086523003876209 -5.871882685178712
86 21.442082196474075 -5.854654721093232
87 10.252385836094618 -5.839624178425524
88 -10.938390472903848 -5.82738805975381
89 8.233265310525894 -5.792814664499287
90 41.14472818374634 -5.7248006359550025
91 22.569501914083958 -5.629584019035365
92 34.162247866392136 -5.615930212812628
93 6.5927844196558 -5.452300379776242
94 31.368981689214706 -5.383675204536576
95 29.43094652891159 -5.26908704643081
96 39.00344759225845 -5.267659655976192
97 39.698424987494946 -5.238593974358965
98 26.12631229683757 -5.175050528032114
99 -4.705903172492981 -5.10258536899766
100 40.65603268146515 -5.047657461568441
101 39.348841428756714 -4.859443841746001
102 26.160180002450943 -4.8214024049459745
103 23.499972235411406 -4.791006764007675
104 32.29276657104492 -4.741716383084015
105 33.34134903550148 -4.618754989193665
106 21.250112049281597 -4.591324283259181
107 34.18071123957634 -4.4785663972795335
108 32.19690689444542 -4.264013889545308
109 48.30816851556301 -4.213649540024929
110 27.76810062304139 -3.9130677902883444
111 52.03251752257347 -3.891559618073184
112 22.763874851167202 -3.839489305056573
113 51.7346418723464 -3.837363980411749
114 48.28384310007095 -3.5046106584449706
115 53.765068888664246 -3.3511221472018096
116 44.29683291912079 -3.225135283090763
117 60.932557851076126 -1.917760927514272
118 59.665161073207855 -1.8377788555257153
119 61.73376727104187 -1.834142128245275
train accuracy: 0.9983333333333333
validation accuracy: 1.0
[-100.90279129 -100.76921336 -100.52052024  -99.506683    -99.43354767
  -99.41058255  -99.38632741  -99.37487795  -99.10032012  -97.78190344
  -96.84296697  -96.66622575  -92.52509415  -88.8287523   -88.61836059
  -81.31590436  -79.38039416  -78.62150161  -71.91567602  -71.16977413
  -70.35725018  -69.85390693  -69.1483668   -69.11291682  -68.48000858
  -67.18702772  -65.92760066  -65.25654038  -63.76375114  -62.66889479
  -62.21546648  -61.28478752  -60.49919675  -60.43674646  -60.40831172
  -60.34433908  -60.21527617  -59.56658931  -59.45821235  -59.07749771
  -58.82882462  -58.47396223  -58.2478618   -57.84271549  -57.81737276
  -57.60360849  -57.57914547  -56.48068419  -55.69543327  -55.20052925
  -34.63082662  -22.33153102  -16.55373263  -15.40317898  -13.75504213
  -13.38603096  -12.8557193   -12.57190636  -11.79933778  -11.65663553
  -11.44502796  -10.20175545   -9.76842627   -9.16255995   -9.11916729
   -9.03298769   -9.03213832   -8.96058      -8.9488529    -8.59057809
   -8.57234003   -8.31014218   -8.16842242   -7.962695     -7.89415877
   -7.85709309   -7.82316132   -7.76656941   -7.66855473   -7.3873344
   -7.33985562   -7.31927217   -7.30676982   -7.26734525   -7.21882843
   -7.20072394   -7.09969483   -7.09046892   -7.06836821   -6.92123174
   -6.91823995   -6.8824015    -6.86230871   -6.75272261   -6.43927706
   -6.29977825   -6.2751656    -6.03773866   -5.99228436   -5.97429577
   -5.87188269   -5.83962418   -5.79281466   -5.72480064   -5.61593021
   -5.45230038   -5.26908705   -5.17505053   -5.08058213   -5.05074454
   -5.04765746   -4.61875499   -4.59132428   -4.4785664    -4.26401389
   -4.21364954   -3.83736398   -3.42098603   -1.83414213   -1.53436475]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 1.4997251340638229e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.7744e-01,  2.9437e-01, -6.1588e-02,  3.1608e-02, -9.7341e-05,
          6.3483e-05, -3.5423e-02,  1.0743e-01, -1.6956e-04, -4.5005e-05,
         -2.4596e-05, -2.8380e+00, -2.4987e-01]], device='cuda:3'))])
end of epoch 1: val_loss 0.00596180125144194, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 2.2781869574828305e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.0033e-01,  6.2072e-01, -1.9216e-01, -1.5705e-01,  5.1487e-01,
          2.3674e-04, -2.4740e-01,  1.5868e-01,  1.0665e-01, -5.6611e-05,
          2.7213e-04, -8.1846e-01, -8.9244e-01]], device='cuda:3'))])
end of epoch 3: val_loss 7.003059718613258e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.4952e-01,  6.6144e-01, -2.2314e-05, -1.6170e-01,  3.3538e-01,
          3.8692e-05, -1.6686e-01,  2.5831e-01, -1.9654e-05, -1.6788e-04,
         -5.5473e-04, -5.5013e-01, -1.0504e+00]], device='cuda:3'))])
end of epoch 4: val_loss 0.002548306873510455, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.006203915472724475, val_acc 0.995
trigger times: 2
end of epoch 6: val_loss 1.5437418468877695e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.4934e-01,  5.7820e-01, -3.0687e-05, -6.1430e-02,  6.1667e-05,
         -5.5712e-05, -2.2421e-01,  2.5781e-01,  4.2274e-05, -6.6429e-04,
          8.5956e-04, -8.8276e-05, -1.2524e+00]], device='cuda:3'))])
end of epoch 7: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.5562e-01,  4.0677e-01, -4.9524e-01, -2.2858e-01,  2.6053e-01,
         -2.2815e-01, -7.2001e-02,  1.7287e-01,  8.6604e-02,  3.4924e-04,
         -2.4972e-05, -2.6462e-01, -1.4893e+00]], device='cuda:3'))])
end of epoch 8: val_loss 3.0316932587624024e-05, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 6.437270979375853e-08, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2140e-01,  3.0687e-01,  9.7104e-03, -3.3046e-01,  4.2492e-01,
          2.0077e-01, -1.4572e-01,  2.0964e-01, -1.1106e-01, -2.4561e-05,
         -1.6775e-03, -5.2646e-01, -1.5798e+00]], device='cuda:3'))])
end of epoch 12: val_loss 0.007547661066014371, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.8518e-01,  2.7310e-01,  1.5524e-05,  1.8838e-05,  1.7915e-01,
          2.2837e-01, -2.2575e-01,  4.9483e-02, -3.3101e-05,  1.3946e-05,
          8.5915e-04, -1.6782e-01, -1.8121e+00]], device='cuda:3'))])
end of epoch 14: val_loss 3.913903487244852e-06, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 6.139256129245042e-08, val_acc 1.0
trigger times: 2
end of epoch 16: val_loss 0.06331714806219679, val_acc 0.99
trigger times: 3
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5182e-01,  4.4439e-01, -6.0358e-01, -9.7886e-02,  5.8928e-01,
          1.0631e-05, -2.4149e-01,  1.3034e-01, -1.3985e-05,  4.0132e-04,
         -5.5555e-04, -8.7045e-01, -1.4570e+00]], device='cuda:3'))])
end of epoch 18: val_loss 5.513326264463103e-05, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 2.7656060410663483e-07, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 2.0265538296371232e-08, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 6.324224523233113e-06, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 1.0192317290602659e-07, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 1.895409239338619e-07, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.6395e-01,  3.3138e-01, -2.2313e-01,  2.7963e-02,  6.3262e-01,
          4.3700e-01, -2.0604e-01,  7.9032e-02,  1.4300e-02, -3.9427e-04,
         -1.6783e-03, -1.0588e+00, -1.5604e+00]], device='cuda:3'))])
end of epoch 26: val_loss 1.7881391656260348e-09, val_acc 1.0
trigger times: 1
end of epoch 27: val_loss 0.014781089722428078, val_acc 0.995
trigger times: 2
end of epoch 28: val_loss 4.17232396188183e-09, val_acc 1.0
trigger times: 3
end of epoch 29: val_loss 3.291444372280239e-05, val_acc 1.0
trigger times: 4
end of epoch 30: val_loss 1.3291770965651038e-07, val_acc 1.0
trigger times: 5
end of epoch 31: val_loss 0.0021372603232511266, val_acc 1.0
trigger times: 6
end of epoch 32: val_loss 0.00032272100448523135, val_acc 1.0
trigger times: 7
end of epoch 33: val_loss 9.238643414732905e-08, val_acc 1.0
trigger times: 8
end of epoch 34: val_loss 1.0907530850090553e-07, val_acc 1.0
trigger times: 9
end of epoch 35: val_loss 6.437261159675245e-08, val_acc 1.0
trigger times: 10
Early stopping.
0 -872.9147417545319 -100.90279129177834
1 -876.9407439231873 -100.76921336385534
2 -873.1472851037979 -100.52052024441564
3 -874.0017248392105 -99.50668300330008
4 -875.6335347294807 -99.43354767487739
5 -868.1095340847969 -99.41058254524987
6 -872.8649117946625 -99.38632740502334
7 -833.5613614916801 -99.37487794659387
8 -875.4726495742798 -99.10032011850174
9 -874.5601191520691 -97.78190343874087
10 -874.4564731121063 -96.84296697205971
11 -777.1466456651688 -96.66622574915127
12 -628.0308585166931 -92.52509414832849
13 -88.06626315414906 -88.82875230477262
14 -90.90300816297531 -88.61836059163966
15 -594.4746919870377 -81.3159043611833
16 -565.3959904909134 -79.38039415651993
17 -549.270209133625 -78.62150160550247
18 -77.58579403162003 -71.91567601738892
19 -75.81007844209671 -71.16977413212614
20 -73.25641876459122 -70.35725018418154
21 -73.89280450344086 -69.85390692712409
22 -72.5187486410141 -69.14836680294484
23 -78.40983366966248 -69.11291681778481
24 -74.75298696756363 -68.48000857546204
25 -71.76817101240158 -67.187027723147
26 -72.7676078081131 -65.9276006589106
27 -70.15117055177689 -65.25654037927377
28 -70.13800525665283 -63.76375114133969
29 -71.34460151195526 -62.66889478750643
30 -68.82368296384811 -62.21546647550949
31 -70.77133721113205 -61.284787518807505
32 -71.5757228732109 -60.49919675306591
33 -68.54518049955368 -60.43674646406117
34 -72.2086792588234 -60.40831171818687
35 -72.37558472156525 -60.344339081081145
36 -72.51427912712097 -60.21527616837919
37 -72.08614563941956 -59.5665893116803
38 -69.82846021652222 -59.458212348780634
39 -70.15425699949265 -59.07749770962377
40 -70.14144361019135 -58.82882462317006
41 -68.6507898569107 -58.47396223424336
42 -73.6021916270256 -58.247861802674144
43 -75.02886152267456 -57.84271548525784
44 -73.1132926940918 -57.817372758890606
45 -69.30028879642487 -57.60360849063046
46 -69.82608580589294 -57.57914546530296
47 -70.25517350435257 -56.480684187620824
48 -70.10474729537964 -55.69543327490694
49 -72.12473177909851 -55.20052925013773
50 -3.161937415599823 -34.63082662039489
51 0.15780949592590332 -22.331531017919122
52 24.272417724132538 -16.55373263077397
53 24.48055139183998 -15.403178979444155
54 35.98036120086908 -13.755042126034148
55 10.16095069795847 -13.386030955271975
56 9.461361676454544 -12.85571930348937
57 8.028515376150608 -12.571906359458604
58 10.931991651654243 -11.79933777595667
59 29.148382261395454 -11.656635527193743
60 34.91900044679642 -11.445027960343468
61 34.80700019001961 -10.201755445956442
62 22.231049925088882 -9.768426270622168
63 30.25718981027603 -9.162559945329809
64 23.118410423398018 -9.119167287261256
65 26.104250125586987 -9.0329876863419
66 -28.36948212981224 -9.032138321897921
67 31.745075583457947 -8.960579996084245
68 25.844883918762207 -8.948852903553306
69 -23.641249971464276 -8.59057808951432
70 32.563673317432404 -8.572340030270306
71 28.254891008138657 -8.310142180170574
72 13.582694754004478 -8.168422419518215
73 39.87506213784218 -7.962694997594596
74 -21.745627347379923 -7.8941587686517956
75 29.721568785607815 -7.857093089476009
76 28.949330002069473 -7.823161319731097
77 34.32478603720665 -7.766569408572078
78 31.410601273179054 -7.66855472620954
79 -5.6178992837667465 -7.387334398144209
80 -7.176470883190632 -7.33985562019908
81 -12.47493021376431 -7.31927217477425
82 42.15416842699051 -7.306769815952803
83 -43.754573345184326 -7.2673452516659385
84 7.047378420829773 -7.218828428696178
85 -58.03294235467911 -7.200723940745014
86 39.88192638754845 -7.099694825696307
87 0.20511285215616226 -7.090468916754366
88 -32.40771076083183 -7.068368208264248
89 44.90530490875244 -6.9212317401256005
90 25.66623479127884 -6.918239950890848
91 21.67111910879612 -6.8824015040531314
92 45.21661841869354 -6.862308709354992
93 24.19237733259797 -6.752722605708495
94 28.326643727719784 -6.439277061674403
95 10.336382731795311 -6.299778249208555
96 27.815719593316317 -6.2751656016398085
97 37.033724933862686 -6.037738664103451
98 21.672653511166573 -5.992284360924977
99 33.649268478155136 -5.974295765090471
100 26.411511780694127 -5.871882685178712
101 33.44824876636267 -5.839624178425524
102 12.723555147647858 -5.792814664499287
103 39.53571829199791 -5.7248006359550025
104 42.42706698179245 -5.615930212812628
105 30.79884245991707 -5.452300379776242
106 36.11334240436554 -5.26908704643081
107 32.30931732058525 -5.175050528032114
108 26.87669961154461 -5.080582132973166
109 35.79724740982056 -5.050744544518496
110 37.72900140285492 -5.047657461568441
111 30.82384531199932 -4.618754989193665
112 31.121274664998055 -4.591324283259181
113 32.67326224222779 -4.4785663972795335
114 29.75596135854721 -4.264013889545308
115 31.721117317676544 -4.213649540024929
116 35.751861453056335 -3.837363980411749
117 43.09945157170296 -3.4209860296878745
118 44.60708010196686 -1.834142128245275
119 45.2364741563797 -1.5343647478041849
train accuracy: 0.9988888888888889
validation accuracy: 1.0

sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.58850771 -48.19509289
 -47.96991522 -47.53054283 -47.33487662 -47.14538324 -47.10889581
 -46.92993068 -46.92815321 -46.7141986  -46.51840523 -46.50762306
 -46.46343149 -46.08033935 -45.96910073 -45.94004954 -45.47189047
 -45.34946654 -45.23627882 -45.23541431 -44.99494244 -44.99184926
 -44.7644634  -44.62310756 -44.53780078 -44.32699505 -43.84464734
 -43.7920154  -43.22211069 -43.2041764  -43.14444939 -42.63170505
 -42.49903983 -42.18990615 -42.17868583 -42.08941791 -41.96459824
 -41.91819405 -41.89252759 -41.86250044 -41.76630373 -41.75643755
 -41.50687738 -41.35020296 -41.32515547 -41.22179983 -41.18200046
 -41.03992135 -41.03294083 -40.99991982 -40.98448758 -40.86627436
 -40.84937019 -40.63970267 -40.33775477 -40.11576908 -39.92712396
 -39.89750906 -39.72931271 -39.35357961 -39.17708227 -39.14272956
 -39.07321282 -38.91826709 -38.77785603 -38.52213956 -38.29413029
 -38.28199013 -38.28138827 -38.04661848 -37.88709355 -37.79694607
 -37.70658771 -37.51412288 -37.37133772 -37.3251261  -37.15321081
 -37.11841822 -37.10344577 -36.99477768 -36.69656462 -36.65639269
 -36.58958472 -36.38667894 -36.33815177 -36.03378032 -36.02351695
 -35.97994531 -35.89835563 -35.77878287 -35.74239985 -35.67327882
 -35.43726783 -35.41556943 -35.28979253 -35.12234482 -35.07963015
 -35.06137062 -35.02402835 -35.01091702 -34.95588623 -34.91427059
 -34.88495647 -34.86472602 -34.34266183 -33.86129846 -33.77802335
 -33.680383   -33.4779641  -33.26579709 -33.17157461 -33.04852912
 -32.41611773 -32.41053817 -32.02508724 -31.71028623 -31.64335581
 -31.48876757 -31.44406811 -31.37649579 -31.35230126 -31.24419956
 -31.21642916 -31.15850305 -31.06744474 -31.00231541 -30.95117262
 -30.76937686 -30.64081715 -30.63466958 -30.59673213 -30.52049104
 -30.42034344 -30.36982897 -30.36743577 -30.27374553 -30.10812976
 -30.06299271 -29.98083048 -29.97767593 -29.94526105 -29.66568464
 -29.22737048 -29.20595883 -28.99559546 -28.95484843 -28.59191373
 -28.5494411  -28.31342715 -28.1712167  -28.13409378 -28.11939797
 -27.79654204 -27.77638947 -27.70726756 -27.60103341 -27.50815322
 -27.27876998 -26.73963502 -26.7058308  -26.50715502 -26.36197542
 -26.19987885 -25.99907479 -25.47507675 -25.29134047 -25.11681575
 -24.96039421 -24.91350015 -24.71538539 -24.58444108 -24.25434383
 -24.12244562 -23.96980649 -23.76917367 -23.71252839 -23.48246476
 -23.05835519 -22.79553869 -22.61211306 -22.55283027 -22.53572133
 -21.86139195 -21.84262733 -21.7167854  -21.64185366 -21.62657756
 -21.62597106 -21.44290047 -20.68104247 -20.66287704 -20.66020817
 -20.5100558  -20.35567458 -20.10309328 -19.94369578 -19.67432429
 -19.50391949 -19.30989567 -19.03153444 -18.9341265  -18.87189699
 -18.77655165 -18.71737606 -18.65414818 -18.4634721  -18.36358054
 -18.2465562  -18.08445937 -17.85124778 -17.82515236 -17.77364067
 -17.67989719 -17.64064603 -17.48133145 -17.27921081 -17.10217865
 -17.06769934 -16.68041488 -16.44066556 -16.35270212 -16.325741
 -16.24688067 -16.20186147 -16.01542464 -15.4984665  -15.44345121
 -15.29527985 -15.26574292 -15.017961   -14.79738881 -14.47834852
 -14.39794675 -14.24892226 -14.15625316 -13.88057881 -13.73405689
 -13.69675016 -13.43873671 -13.16397094 -12.97987294 -12.6606731
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.9523875  -11.75626016 -11.47155848
 -11.04421156 -10.859881   -10.76409188 -10.64954177 -10.49423816
  -9.92399886  -9.67992171  -9.56750101  -9.4003034   -8.4761537
  -8.40276084  -8.32638751  -8.0269637   -7.68743448  -7.57539849
  -7.5455064   -7.54460172  -7.52029309  -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.10832736  -7.08431127  -6.95906356
  -6.92008425  -6.72206384  -6.71997062  -6.64795748  -6.51820418
  -6.47884361  -6.05448903  -5.85405865  -5.64485143  -5.61579673
  -5.39544196  -5.38326081  -5.3472021   -5.25793019  -5.24856721
  -5.07848501  -5.06486011  -4.90228293  -4.82757292  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.3322555
  -3.329867    -3.29356852  -3.07904644  -2.88591659  -2.83192847
  -2.67390706  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-44.29565561 -41.97868948 -39.97085477 -37.94300503 -37.47672391
 -36.28177521 -35.19353216 -34.63851695 -33.7629591  -31.90686017
 -30.772676   -28.42026397 -26.39763335 -25.31217996 -22.14406511
 -21.72370465 -20.76562386 -19.02121155 -17.94319772 -16.25152473
 -16.23288157 -13.8953595  -12.5410027  -11.97407887 -11.96626278
 -11.4307611  -10.78272714 -10.01645863  -7.70107293  -7.67259169
  -7.3740849   -6.77694649  -6.29894775  -5.89467275  -5.02795798
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 630
num val_labels 630
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:0
end of epoch 0: val_loss 0.10494377862077822, val_acc 0.9619047619047619
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.11110501408626017, val_acc 0.9603174603174603
trigger times: 1
end of epoch 2: val_loss 0.10832529237203305, val_acc 0.9650793650793651
trigger times: 2
end of epoch 3: val_loss 0.5811278113622595, val_acc 0.9047619047619048
trigger times: 3
end of epoch 4: val_loss 0.08830718972456562, val_acc 0.9619047619047619
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.08214326440924136, val_acc 0.9634920634920635
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.07451128349484676, val_acc 0.9666666666666667
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.056137206414544484, val_acc 0.9793650793650793
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.13107411594781415, val_acc 0.9507936507936507
trigger times: 1
end of epoch 9: val_loss 0.06219901224547024, val_acc 0.973015873015873
trigger times: 2
end of epoch 10: val_loss 0.08145090419746233, val_acc 0.9714285714285714
trigger times: 3
end of epoch 11: val_loss 0.07373305341285112, val_acc 0.9666666666666667
trigger times: 4
end of epoch 12: val_loss 0.059926806432584384, val_acc 0.9746031746031746
trigger times: 5
end of epoch 13: val_loss 0.17179829733332516, val_acc 0.9428571428571428
trigger times: 6
end of epoch 14: val_loss 0.0735062651856176, val_acc 0.9650793650793651
trigger times: 7
end of epoch 15: val_loss 0.12113189722682968, val_acc 0.9476190476190476
trigger times: 8
end of epoch 16: val_loss 0.09749068178775339, val_acc 0.9555555555555556
trigger times: 9
end of epoch 17: val_loss 0.07870283975675509, val_acc 0.9682539682539683
trigger times: 10
Early stopping.
0 -8.228560451418161 -44.29565561256425
1 -8.872416471363977 -41.978689481633225
2 -7.762930771801621 -39.97085476609623
3 -4.460255046840757 -37.94300503359082
4 -8.902931061107665 -37.47672390906202
5 -5.405720918439329 -36.28177521453294
6 -5.550753851421177 -35.19353215618221
7 -3.472651867661625 -34.63851695272603
8 -4.094442049972713 -33.762959097993274
9 -2.3610591678880155 -31.906860172355092
10 -1.729686276987195 -30.772676001251654
11 -1.345306159928441 -28.42026396583319
12 -0.61122175026685 -26.397633350086274
13 0.9424077812582254 -25.312179958453797
14 1.7007514666765928 -22.144065107815585
15 1.849253530614078 -21.723704649211356
16 2.4535824870690703 -20.7656238590111
17 2.9449559915810823 -19.02121154786651
18 4.064238240942359 -17.943197718572
19 6.4948752066120505 -16.251524732424823
20 6.454937368631363 -16.23288157067845
21 7.02581549435854 -13.895359503086498
22 8.410372563637793 -12.541002696898001
23 7.520429566502571 -11.974078865914656
24 8.686314595863223 -11.966262779359818
25 8.250380158424377 -11.430761099930217
26 8.285861030220985 -10.782727135522462
27 9.763337600976229 -10.016458630355537
28 9.715573850087821 -7.70107292515296
29 11.167784731835127 -7.67259168928286
30 10.986784962937236 -7.374084902530061
31 11.904402592219412 -6.776946485018116
32 13.306382719427347 -6.298947752982548
33 12.779240854084492 -5.894672754152052
34 12.77406968921423 -5.027957977402961
35 14.067177847027779 -2.49009826539426
train accuracy: 0.9462982073921187
validation accuracy: 0.9682539682539683
sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.58850771 -48.19509289
 -47.33487662 -47.14538324 -47.10889581 -46.92993068 -46.92815321
 -46.7141986  -46.51840523 -46.50762306 -46.46343149 -46.08033935
 -45.96910073 -45.94004954 -45.47189047 -45.34946654 -45.23627882
 -45.23541431 -44.99494244 -44.99184926 -44.7644634  -44.62310756
 -44.53780078 -44.32699505 -44.29565561 -43.84464734 -43.22211069
 -43.2041764  -43.14444939 -42.49903983 -42.18990615 -42.17868583
 -42.08941791 -41.97868948 -41.96459824 -41.91819405 -41.89252759
 -41.86250044 -41.76630373 -41.50687738 -41.35020296 -41.32515547
 -41.22179983 -41.18200046 -41.03992135 -41.03294083 -40.98448758
 -40.86627436 -40.84937019 -40.63970267 -40.33775477 -40.11576908
 -39.97085477 -39.92712396 -39.89750906 -39.72931271 -39.35357961
 -39.17708227 -39.14272956 -39.07321282 -38.91826709 -38.77785603
 -38.52213956 -38.29413029 -38.28199013 -38.28138827 -38.04661848
 -37.88709355 -37.79694607 -37.70658771 -37.51412288 -37.47672391
 -37.37133772 -37.3251261  -37.15321081 -37.11841822 -37.10344577
 -36.99477768 -36.58958472 -36.38667894 -36.28177521 -36.03378032
 -36.02351695 -35.97994531 -35.89835563 -35.77878287 -35.74239985
 -35.67327882 -35.41556943 -35.28979253 -35.19353216 -35.12234482
 -35.07963015 -35.06137062 -35.02402835 -35.01091702 -34.95588623
 -34.91427059 -34.88495647 -34.63851695 -34.34266183 -33.86129846
 -33.77802335 -33.7629591  -33.680383   -33.4779641  -33.26579709
 -33.17157461 -32.41611773 -32.41053817 -32.02508724 -31.90686017
 -31.71028623 -31.64335581 -31.48876757 -31.44406811 -31.37649579
 -31.35230126 -31.24419956 -31.21642916 -31.06744474 -31.00231541
 -30.95117262 -30.772676   -30.76937686 -30.64081715 -30.59673213
 -30.52049104 -30.36743577 -30.27374553 -30.06299271 -29.98083048
 -29.97767593 -29.94526105 -29.66568464 -29.22737048 -29.20595883
 -28.99559546 -28.95484843 -28.59191373 -28.42026397 -28.1712167
 -28.13409378 -28.11939797 -27.79654204 -27.77638947 -27.70726756
 -27.50815322 -27.27876998 -26.73963502 -26.7058308  -26.50715502
 -26.36197542 -26.19987885 -25.99907479 -25.47507675 -25.31217996
 -25.29134047 -25.11681575 -24.96039421 -24.91350015 -24.58444108
 -24.25434383 -24.12244562 -23.96980649 -23.76917367 -23.71252839
 -23.48246476 -23.05835519 -22.79553869 -22.61211306 -22.55283027
 -22.53572133 -22.14406511 -21.86139195 -21.84262733 -21.72370465
 -21.7167854  -21.64185366 -21.62657756 -21.62597106 -21.44290047
 -20.76562386 -20.68104247 -20.66287704 -20.66020817 -20.35567458
 -20.10309328 -19.94369578 -19.67432429 -19.50391949 -19.30989567
 -19.03153444 -19.02121155 -18.9341265  -18.87189699 -18.77655165
 -18.71737606 -18.65414818 -18.4634721  -18.36358054 -18.2465562
 -18.08445937 -17.94319772 -17.85124778 -17.77364067 -17.64064603
 -17.48133145 -17.27921081 -17.10217865 -17.06769934 -16.68041488
 -16.44066556 -16.35270212 -16.325741   -16.25152473 -16.24688067
 -16.01542464 -15.4984665  -15.44345121 -15.29527985 -15.26574292
 -15.017961   -14.79738881 -14.47834852 -14.39794675 -14.24892226
 -14.15625316 -14.06734668 -13.91417163 -13.8953595  -13.88057881
 -13.7939212  -13.73405689 -13.69675016 -13.43873671 -13.16397094
 -13.16386337 -12.97987294 -12.94677751 -12.9406564  -12.92457626
 -12.79058052 -12.6606731  -12.5410027  -12.53347897 -12.51335454
 -12.40468089 -12.37040308 -12.29992996 -12.26611853 -12.16816754
 -12.13592928 -11.97407887 -11.96626278 -11.9523875  -11.89104433
 -11.77766704 -11.75626016 -11.47155848 -11.04421156 -11.01208752
 -10.859881   -10.78272714 -10.76409188 -10.64954177 -10.49423816
 -10.01645863  -9.92399886  -9.67992171  -9.56750101  -9.4003034
  -9.32139356  -8.97244586  -8.4761537   -8.40276084  -8.32638751
  -8.0269637   -7.84254125  -7.68743448  -7.57539849  -7.5455064
  -7.54460172  -7.52029309  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.10832736  -7.08431127  -6.95906356
  -6.92008425  -6.77694649  -6.72206384  -6.71997062  -6.64795748
  -6.60654569  -6.51820418  -6.47884361  -6.05448903  -5.89467275
  -5.85405865  -5.61579673  -5.39544196  -5.38326081  -5.3472021
  -5.25793019  -5.24856721  -5.23608532  -5.07848501  -5.06486011
  -4.90228293  -4.82757292  -4.63049542  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.00401798  -3.97870856
  -3.65032555  -3.38446715  -3.3322555   -3.329867    -3.29356852
  -3.07904644  -2.88591659  -2.83192847  -2.67390706  -2.49009827
  -2.24005036  -1.91361965]
sorted_val_rewards: [-47.96991522 -47.53054283 -43.7920154  -42.63170505 -41.75643755
 -40.99991982 -37.94300503 -36.69656462 -36.65639269 -36.33815177
 -35.43726783 -34.86472602 -33.04852912 -31.15850305 -30.63466958
 -30.42034344 -30.36982897 -30.10812976 -28.5494411  -28.31342715
 -27.60103341 -26.39763335 -24.71538539 -20.5100558  -17.82515236
 -17.67989719 -16.23288157 -16.20186147 -14.80364551 -12.82166814
 -11.4307611   -7.70107293  -7.67259169  -7.53907027  -6.29894775
  -5.64485143  -5.02795798  -2.64166233]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 703
num val_labels 703
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.252658405845666, val_acc 0.9416785206258891
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.13633103236328742, val_acc 0.9345661450924608
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.10037615048103929, val_acc 0.9587482219061166
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1287875472296147, val_acc 0.9516358463726885
trigger times: 1
end of epoch 4: val_loss 0.15202573565747318, val_acc 0.9459459459459459
trigger times: 2
end of epoch 5: val_loss 0.09226066270131478, val_acc 0.9530583214793741
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.11232872214663844, val_acc 0.9487908961593172
trigger times: 1
end of epoch 7: val_loss 0.12538293938539913, val_acc 0.9530583214793741
trigger times: 2
end of epoch 8: val_loss 0.1819712274290401, val_acc 0.9374110953058321
trigger times: 3
end of epoch 9: val_loss 0.2011441402975055, val_acc 0.9473684210526315
trigger times: 4
end of epoch 10: val_loss 0.10075181831325071, val_acc 0.9544807965860598
trigger times: 5
end of epoch 11: val_loss 0.0907628260811747, val_acc 0.957325746799431
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.14653896812220182, val_acc 0.9459459459459459
trigger times: 1
end of epoch 13: val_loss 0.14391622425682177, val_acc 0.9374110953058321
trigger times: 2
end of epoch 14: val_loss 0.14154097921722303, val_acc 0.9416785206258891
trigger times: 3
end of epoch 15: val_loss 3.3461441092826636, val_acc 0.8421052631578947
trigger times: 4
end of epoch 16: val_loss 0.11635541072306295, val_acc 0.9587482219061166
trigger times: 5
end of epoch 17: val_loss 0.46242418897267945, val_acc 0.9174964438122333
trigger times: 6
end of epoch 18: val_loss 0.08497332645885014, val_acc 0.9615931721194879
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.15223694058445045, val_acc 0.9388335704125178
trigger times: 1
end of epoch 20: val_loss 0.20296888948162148, val_acc 0.9331436699857752
trigger times: 2
end of epoch 21: val_loss 0.10878651778411746, val_acc 0.9502133712660028
trigger times: 3
end of epoch 22: val_loss 0.13149618828932655, val_acc 0.9431009957325747
trigger times: 4
end of epoch 23: val_loss 0.19609394608034236, val_acc 0.9246088193456614
trigger times: 5
end of epoch 24: val_loss 0.1834867014674272, val_acc 0.9459459459459459
trigger times: 6
end of epoch 25: val_loss 0.0924938343176674, val_acc 0.9630156472261735
trigger times: 7
end of epoch 26: val_loss 0.11442408930963342, val_acc 0.9601706970128022
trigger times: 8
end of epoch 27: val_loss 0.26618144737367444, val_acc 0.9075391180654339
trigger times: 9
end of epoch 28: val_loss 0.1163329055513537, val_acc 0.9530583214793741
trigger times: 10
Early stopping.
0 -33.03983882069588 -47.96991522128461
1 -32.57556036859751 -47.53054283339663
2 -31.28259602934122 -43.792015395413316
3 -30.50814066082239 -42.631705052377264
4 -29.844425328075886 -41.756437546020415
5 -33.27495610713959 -40.999919815235636
6 -28.171384796500206 -37.94300503359082
7 -27.063730537891388 -36.69656461923256
8 -30.63786990940571 -36.65639269257871
9 -28.962150119245052 -36.33815176682742
10 -26.61792665719986 -35.437267827701675
11 -28.00942900031805 -34.86472601743658
12 -28.353401377797127 -33.048529119631624
13 -26.079410932958126 -31.158503045607585
14 -24.66177788376808 -30.63466958360597
15 -23.947883494198322 -30.42034343549779
16 -25.632138565182686 -30.36982897332624
17 -26.581037402153015 -30.10812975899098
18 -24.459517396986485 -28.549441103161325
19 -25.630407586693764 -28.313427148940267
20 -20.94115799665451 -27.6010334141087
21 -22.48581187427044 -26.397633350086274
22 -19.70277751982212 -24.715385385378266
23 -14.748916633427143 -20.510055800644814
24 -16.797430634498596 -17.825152358862276
25 -15.26892664283514 -17.679897189647424
26 -14.685056060552597 -16.23288157067845
27 -14.308340594172478 -16.201861466781004
28 -13.727247796952724 -14.803645513170105
29 -11.799853935837746 -12.821668138028135
30 -12.370857194066048 -11.430761099930217
31 -9.805915392935276 -7.70107292515296
32 -8.246029883623123 -7.67259168928286
33 -7.037937484681606 -7.539070271915578
34 -6.1466410756111145 -6.298947752982548
35 -6.061725065112114 -5.644851434365073
36 -6.800790518522263 -5.027957977402961
37 -5.711082004010677 -2.6416623314910934
train accuracy: 0.9441386691128694
validation accuracy: 0.9530583214793741
sorted_train_rewards: [-55.21445752 -54.46505855 -49.6099766  -49.13001653 -49.03866708
 -49.0331838  -48.58850771 -48.19509289 -47.96991522 -47.53054283
 -47.33487662 -47.14538324 -47.10889581 -46.92993068 -46.7141986
 -46.51840523 -46.50762306 -46.46343149 -46.08033935 -45.96910073
 -45.94004954 -45.47189047 -45.34946654 -45.23627882 -45.23541431
 -44.99494244 -44.99184926 -44.7644634  -44.62310756 -44.32699505
 -44.29565561 -43.84464734 -43.7920154  -43.22211069 -43.2041764
 -43.14444939 -42.63170505 -42.49903983 -42.18990615 -42.17868583
 -42.08941791 -41.97868948 -41.96459824 -41.91819405 -41.89252759
 -41.86250044 -41.76630373 -41.75643755 -41.50687738 -41.35020296
 -41.22179983 -41.18200046 -41.03992135 -41.03294083 -40.99991982
 -40.98448758 -40.86627436 -40.84937019 -40.33775477 -40.11576908
 -39.97085477 -39.92712396 -39.89750906 -39.72931271 -39.35357961
 -39.17708227 -39.14272956 -39.07321282 -38.91826709 -38.77785603
 -38.29413029 -38.28199013 -38.28138827 -38.04661848 -37.94300503
 -37.88709355 -37.79694607 -37.70658771 -37.47672391 -37.37133772
 -37.3251261  -37.15321081 -37.11841822 -37.10344577 -36.69656462
 -36.65639269 -36.38667894 -36.28177521 -36.03378032 -35.97994531
 -35.89835563 -35.77878287 -35.74239985 -35.43726783 -35.41556943
 -35.28979253 -35.19353216 -35.12234482 -35.07963015 -35.06137062
 -35.02402835 -35.01091702 -34.95588623 -34.91427059 -34.88495647
 -34.86472602 -34.34266183 -33.86129846 -33.77802335 -33.7629591
 -33.680383   -33.4779641  -33.17157461 -33.04852912 -32.41611773
 -32.41053817 -32.02508724 -31.90686017 -31.71028623 -31.48876757
 -31.44406811 -31.37649579 -31.35230126 -31.24419956 -31.21642916
 -31.15850305 -31.06744474 -31.00231541 -30.95117262 -30.772676
 -30.76937686 -30.64081715 -30.59673213 -30.52049104 -30.36982897
 -30.36743577 -30.27374553 -30.10812976 -30.06299271 -29.98083048
 -29.97767593 -29.94526105 -29.66568464 -29.22737048 -29.20595883
 -28.99559546 -28.59191373 -28.5494411  -28.31342715 -28.1712167
 -28.13409378 -28.11939797 -27.79654204 -27.77638947 -27.70726756
 -27.60103341 -27.50815322 -26.73963502 -26.7058308  -26.50715502
 -26.39763335 -26.36197542 -26.19987885 -25.99907479 -25.47507675
 -25.31217996 -25.29134047 -25.11681575 -24.96039421 -24.91350015
 -24.71538539 -24.25434383 -24.12244562 -23.96980649 -23.76917367
 -23.48246476 -23.05835519 -22.79553869 -22.61211306 -22.55283027
 -22.53572133 -22.14406511 -21.86139195 -21.84262733 -21.72370465
 -21.7167854  -21.64185366 -21.62657756 -21.62597106 -21.44290047
 -20.76562386 -20.68104247 -20.66287704 -20.66020817 -20.35567458
 -20.10309328 -19.50391949 -19.30989567 -19.03153444 -19.02121155
 -18.9341265  -18.87189699 -18.77655165 -18.71737606 -18.65414818
 -18.4634721  -18.36358054 -18.2465562  -18.08445937 -17.94319772
 -17.85124778 -17.77364067 -17.77158541 -17.67989719 -17.64064603
 -17.48133145 -17.27921081 -17.10217865 -17.06769934 -16.68041488
 -16.61753067 -16.44066556 -16.35270212 -16.325741   -16.25152473
 -16.24688067 -16.20186147 -16.01542464 -15.8535005  -15.65380823
 -15.60339789 -15.4984665  -15.44345121 -15.29527985 -15.26574292
 -15.017961   -14.80364551 -14.47834852 -14.39794675 -14.24892226
 -14.15625316 -14.06734668 -14.05092615 -14.04015398 -13.99249522
 -13.91417163 -13.8953595  -13.88057881 -13.7939212  -13.73405689
 -13.69675016 -13.43873671 -13.27768447 -13.16397094 -13.16386337
 -12.97987294 -12.94677751 -12.9406564  -12.92457626 -12.82166814
 -12.79058052 -12.6606731  -12.5410027  -12.53347897 -12.40468089
 -12.38996869 -12.37040308 -12.29992996 -12.26611853 -12.18283802
 -12.16816754 -12.13592928 -11.9523875  -11.89104433 -11.77766704
 -11.75626016 -11.74367847 -11.63863136 -11.59932628 -11.51164274
 -11.47155848 -11.07847378 -11.04421156 -11.01208752 -10.94613422
 -10.89242091 -10.859881   -10.76409188 -10.64954177 -10.50774226
 -10.49423816 -10.01645863  -9.92399886  -9.67992171  -9.56750101
  -9.32139356  -8.97244586  -8.4761537   -8.40276084  -8.32638751
  -8.0269637   -7.84254125  -7.70107293  -7.68743448  -7.67259169
  -7.57539849  -7.5455064   -7.54460172  -7.53907027  -7.52029309
  -7.3740849   -7.36244313  -7.34546388  -7.1893336   -7.15421432
  -7.10832736  -7.08431127  -6.95906356  -6.92008425  -6.72206384
  -6.71997062  -6.64795748  -6.60654569  -6.47884361  -6.05448903
  -5.89467275  -5.85405865  -5.61579673  -5.39544196  -5.38326081
  -5.3472021   -5.25793019  -5.24856721  -5.23608532  -5.07848501
  -5.06486011  -5.02795798  -4.90228293  -4.82757292  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.3322555
  -3.329867    -3.29356852  -3.07904644  -2.88591659  -2.83192847
  -2.67390706  -2.64166233  -2.49009827  -2.24005036  -1.91361965]
sorted_val_rewards: [-51.17814647 -48.84818037 -46.92815321 -44.53780078 -41.32515547
 -40.63970267 -38.52213956 -37.51412288 -36.99477768 -36.58958472
 -36.33815177 -36.02351695 -35.67327882 -34.63851695 -33.26579709
 -31.64335581 -30.63466958 -30.42034344 -28.95484843 -28.42026397
 -27.27876998 -24.58444108 -23.71252839 -20.5100558  -19.94369578
 -19.67432429 -18.35047025 -17.82515236 -16.23288157 -14.79738881
 -12.51335454 -11.97407887 -11.96626278 -11.4307611  -10.78272714
  -9.4003034   -6.77694649  -6.51820418  -6.29894775  -5.64485143]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 780
num val_labels 780
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.16611638510551247, val_acc 0.9333333333333333
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.16030237514672263, val_acc 0.9461538461538461
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.25711605189191633, val_acc 0.9230769230769231
trigger times: 1
end of epoch 3: val_loss 0.15780628381444253, val_acc 0.9487179487179487
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.20481015130626234, val_acc 0.9333333333333333
trigger times: 1
end of epoch 5: val_loss 0.12276561998433591, val_acc 0.9435897435897436
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.18502801413486786, val_acc 0.9397435897435897
trigger times: 1
end of epoch 7: val_loss 0.29153618102963713, val_acc 0.9294871794871795
trigger times: 2
end of epoch 8: val_loss 0.438648579765877, val_acc 0.9153846153846154
trigger times: 3
end of epoch 9: val_loss 0.1602580153854646, val_acc 0.9371794871794872
trigger times: 4
end of epoch 10: val_loss 0.11853306844931247, val_acc 0.9487179487179487
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.10507478341472241, val_acc 0.9551282051282052
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.11210572627562111, val_acc 0.95
trigger times: 1
end of epoch 13: val_loss 0.3196401569065865, val_acc 0.9384615384615385
trigger times: 2
end of epoch 14: val_loss 0.1259245161561333, val_acc 0.95
trigger times: 3
end of epoch 15: val_loss 0.09468174650249628, val_acc 0.9551282051282052
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.14181933092925494, val_acc 0.9333333333333333
trigger times: 1
end of epoch 17: val_loss 1.5702689635036895, val_acc 0.8756410256410256
trigger times: 2
end of epoch 18: val_loss 0.14605026926631537, val_acc 0.9320512820512821
trigger times: 3
end of epoch 19: val_loss 0.11214165573051367, val_acc 0.95
trigger times: 4
end of epoch 20: val_loss 0.7290904446923022, val_acc 0.9153846153846154
trigger times: 5
end of epoch 21: val_loss 0.22165041417422293, val_acc 0.9474358974358974
trigger times: 6
end of epoch 22: val_loss 0.10415489131479894, val_acc 0.95
trigger times: 7
end of epoch 23: val_loss 0.16613661792814552, val_acc 0.9397435897435897
trigger times: 8
end of epoch 24: val_loss 0.15351759434825354, val_acc 0.941025641025641
trigger times: 9
end of epoch 25: val_loss 0.15637054504350417, val_acc 0.9282051282051282
trigger times: 10
Early stopping.
0 -26.44799245055765 -51.178146470411306
1 -28.3522384557873 -48.84818037054459
2 -23.154570491053164 -46.92815320989692
3 -21.347397812642157 -44.5378007768349
4 -16.376717045903206 -41.325155469676766
5 -17.42737060971558 -40.63970267477056
6 -19.412721872329712 -38.52213955510221
7 -18.066620499826968 -37.514122881621645
8 -20.396983951330185 -36.99477768281456
9 -20.437919474206865 -36.58958472489279
10 -20.179750308394432 -36.33815176682742
11 -18.54985587578267 -36.02351694770323
12 -19.553802829235792 -35.67327881993891
13 -18.795831178314984 -34.63851695272603
14 -19.277324037626386 -33.26579709040458
15 -14.659818915650249 -31.64335580631245
16 -14.119676137343049 -30.63466958360597
17 -13.714070858433843 -30.42034343549779
18 -12.497473244555295 -28.954848426086794
19 -12.579100450966507 -28.42026396583319
20 -14.495567475445569 -27.278769979372363
21 -11.853231794200838 -24.58444107753506
22 -7.590473929420114 -23.71252838726811
23 -4.693861882202327 -20.510055800644814
24 -7.3446618635207415 -19.943695776581137
25 -5.103926934301853 -19.674324289009657
26 -2.22937897965312 -18.350470254570325
27 -3.6748574525117874 -17.825152358862276
28 -3.5836220616474748 -16.23288157067845
29 -1.8751968322321773 -14.797388813532848
30 1.1602558828890324 -12.513354538352278
31 2.155208417214453 -11.974078865914656
32 0.819249652326107 -11.966262779359818
33 -0.023357892408967018 -11.430761099930217
34 1.3510028370656073 -10.782727135522462
35 3.720868520438671 -9.400303400301372
36 5.20841571316123 -6.776946485018116
37 5.408358208835125 -6.51820418055673
38 5.15169938839972 -6.298947752982548
39 4.696533500216901 -5.644851434365073
train accuracy: 0.9372778351106524
validation accuracy: 0.9282051282051282
sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.19509289 -47.96991522
 -47.53054283 -47.33487662 -47.14538324 -47.10889581 -46.92993068
 -46.7141986  -46.51840523 -46.50762306 -46.46343149 -45.96910073
 -45.94004954 -45.47189047 -45.34946654 -45.23627882 -44.99494244
 -44.99184926 -44.7644634  -44.62310756 -44.32699505 -44.29565561
 -43.84464734 -43.7920154  -43.22211069 -43.14444939 -42.63170505
 -42.49903983 -42.18990615 -42.17868583 -42.08941791 -41.97868948
 -41.96459824 -41.91819405 -41.89252759 -41.86250044 -41.76630373
 -41.75643755 -41.50687738 -41.35020296 -41.32515547 -41.22179983
 -41.18200046 -41.03992135 -41.03294083 -40.99991982 -40.98448758
 -40.86627436 -40.84937019 -40.33775477 -40.11576908 -39.97085477
 -39.92712396 -39.89750906 -39.72931271 -39.35357961 -39.17708227
 -39.07321282 -38.91826709 -38.77785603 -38.52213956 -38.29413029
 -38.28199013 -38.28138827 -38.04661848 -37.94300503 -37.88709355
 -37.79694607 -37.51412288 -37.47672391 -37.37133772 -37.3251261
 -37.11841822 -37.10344577 -36.99477768 -36.69656462 -36.65639269
 -36.58958472 -36.38667894 -36.28177521 -36.03378032 -36.02351695
 -35.97994531 -35.89835563 -35.74239985 -35.67327882 -35.43726783
 -35.41556943 -35.28979253 -35.19353216 -35.12234482 -35.07963015
 -35.06137062 -35.02402835 -35.01091702 -34.95588623 -34.91427059
 -34.88495647 -34.86472602 -34.63851695 -33.77802335 -33.7629591
 -33.680383   -33.4779641  -33.26579709 -33.17157461 -33.04852912
 -32.41611773 -32.41053817 -32.02508724 -31.90686017 -31.71028623
 -31.64335581 -31.48876757 -31.44406811 -31.37649579 -31.35230126
 -31.24419956 -31.21642916 -31.15850305 -31.06744474 -30.95117262
 -30.772676   -30.76937686 -30.64081715 -30.63466958 -30.52049104
 -30.42034344 -30.36982897 -30.36743577 -30.27374553 -30.10812976
 -30.06299271 -29.98083048 -29.97767593 -29.94526105 -29.66568464
 -29.22737048 -29.20595883 -28.99559546 -28.95484843 -28.59191373
 -28.5494411  -28.42026397 -28.1712167  -28.13409378 -28.11939797
 -27.79654204 -27.77638947 -27.70726756 -27.60103341 -27.50815322
 -27.27876998 -26.73963502 -26.7058308  -26.50715502 -26.39763335
 -26.36197542 -26.19987885 -25.47507675 -25.31217996 -25.29134047
 -25.11681575 -24.96039421 -24.91350015 -24.71538539 -24.58444108
 -24.12244562 -23.96980649 -23.76917367 -23.48246476 -23.05835519
 -22.79553869 -22.61211306 -22.55283027 -22.53572133 -21.86139195
 -21.84262733 -21.72370465 -21.7167854  -21.64185366 -21.62657756
 -21.62597106 -21.44290047 -20.78375363 -20.76562386 -20.68104247
 -20.66287704 -20.66020817 -20.5100558  -20.10309328 -19.94369578
 -19.30989567 -19.03153444 -19.02121155 -18.9341265  -18.87189699
 -18.77655165 -18.73611103 -18.71737606 -18.65414818 -18.4634721
 -18.36358054 -18.35047025 -18.2465562  -18.08445937 -17.94319772
 -17.85124778 -17.82515236 -17.77364067 -17.77158541 -17.67989719
 -17.64064603 -17.48133145 -17.27921081 -17.06769934 -16.68041488
 -16.61753067 -16.44066556 -16.35270212 -16.325741   -16.25152473
 -16.24688067 -16.20186147 -16.01542464 -15.8535005  -15.65380823
 -15.60339789 -15.4984665  -15.44345121 -15.29527985 -15.26574292
 -15.017961   -14.80364551 -14.79738881 -14.47834852 -14.41662641
 -14.39794675 -14.24892226 -14.06734668 -14.05092615 -14.04015398
 -13.99249522 -13.91417163 -13.8953595  -13.88057881 -13.7939212
 -13.73405689 -13.69675016 -13.45720676 -13.43873671 -13.27768447
 -13.16397094 -13.16386337 -12.97987294 -12.94677751 -12.92457626
 -12.82166814 -12.79058052 -12.6606731  -12.5410027  -12.53347897
 -12.40468089 -12.38996869 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.97407887 -11.9523875  -11.89104433
 -11.77766704 -11.75626016 -11.74367847 -11.73746586 -11.63863136
 -11.59932628 -11.51164274 -11.47155848 -11.06694367 -11.04421156
 -11.01208752 -10.94613422 -10.89242091 -10.859881   -10.85010911
 -10.78272714 -10.76409188 -10.70992158 -10.64954177 -10.50774226
 -10.49423816 -10.01645863  -9.99493371  -9.95712771  -9.92399886
  -9.67992171  -9.56750101  -9.4003034   -9.32139356  -8.97244586
  -8.49497785  -8.4761537   -8.46838135  -8.40276084  -8.32638751
  -8.0269637   -8.01868294  -7.70107293  -7.57539849  -7.5455064
  -7.54460172  -7.53907027  -7.52029309  -7.3740849   -7.36244313
  -7.34546388  -7.1893336   -7.15421432  -7.08431127  -7.07404409
  -6.95906356  -6.92008425  -6.83697432  -6.72206384  -6.71997062
  -6.64795748  -6.60654569  -6.47884361  -6.2891636   -6.05448903
  -5.89467275  -5.85405865  -5.61579673  -5.39544196  -5.38326081
  -5.3472021   -5.25793019  -5.24856721  -5.23608532  -5.16571156
  -5.07848501  -5.06486011  -5.02795798  -4.90228293  -4.82757292
  -4.71135754  -4.63049542  -4.37983153  -4.35856953  -4.230832
  -4.0660223   -4.03104862  -4.00401798  -3.97870856  -3.65032555
  -3.38446715  -3.3322555   -3.329867    -3.29356852  -3.12579961
  -3.07904644  -2.88591659  -2.83192847  -2.67390706  -2.64166233
  -2.49009827  -2.24005036  -1.91361965]
sorted_val_rewards: [-48.58850771 -46.92815321 -46.08033935 -45.23541431 -44.53780078
 -43.2041764  -40.63970267 -39.14272956 -37.70658771 -37.15321081
 -36.33815177 -35.77878287 -34.34266183 -33.86129846 -31.00231541
 -30.59673213 -28.31342715 -25.99907479 -24.25434383 -23.71252839
 -22.14406511 -20.35567458 -20.10754588 -19.67432429 -19.50391949
 -17.10217865 -16.23288157 -14.15625316 -12.9406564  -12.51335454
 -12.18283802 -11.96626278 -11.4307611  -11.07847378  -7.84254125
  -7.68743448  -7.67259169  -7.10832736  -6.77694649  -6.51820418
  -6.29894775  -5.64485143]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 861
num val_labels 861
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.1261321490482829, val_acc 0.9488966318234611
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.12017420854204601, val_acc 0.9465737514518002
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.15996748367397384, val_acc 0.9337979094076655
trigger times: 1
end of epoch 3: val_loss 0.3220124664193599, val_acc 0.9419279907084785
trigger times: 2
end of epoch 4: val_loss 0.2658451986635184, val_acc 0.9210220673635308
trigger times: 3
end of epoch 5: val_loss 0.2236855340766336, val_acc 0.9372822299651568
trigger times: 4
end of epoch 6: val_loss 0.20680464602202284, val_acc 0.9454123112659698
trigger times: 5
end of epoch 7: val_loss 0.09786987130872288, val_acc 0.9488966318234611
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.16263739604708335, val_acc 0.943089430894309
trigger times: 1
end of epoch 9: val_loss 0.1304901331951638, val_acc 0.9442508710801394
trigger times: 2
end of epoch 10: val_loss 0.8242206117212674, val_acc 0.9070847851335656
trigger times: 3
end of epoch 11: val_loss 0.1280048256274657, val_acc 0.9477351916376306
trigger times: 4
end of epoch 12: val_loss 0.21244031130392702, val_acc 0.9384436701509872
trigger times: 5
end of epoch 13: val_loss 0.1439181033502074, val_acc 0.943089430894309
trigger times: 6
end of epoch 14: val_loss 0.21108176374863857, val_acc 0.9233449477351916
trigger times: 7
end of epoch 15: val_loss 0.10390010465739487, val_acc 0.9616724738675958
trigger times: 8
end of epoch 16: val_loss 0.1163376322960805, val_acc 0.9477351916376306
trigger times: 9
end of epoch 17: val_loss 0.271674188491198, val_acc 0.9291521486643438
trigger times: 10
Early stopping.
0 -51.95647466927767 -48.58850771418405
1 -55.31895255856216 -46.92815320989692
2 -48.38233317434788 -46.080339345825905
3 -49.51712630689144 -45.23541430806973
4 -58.01626961119473 -44.5378007768349
5 -60.43118903040886 -43.20417640267866
6 -44.84012542851269 -40.63970267477056
7 -50.44834801927209 -39.142729555970334
8 -50.78837411850691 -37.706587708128204
9 -49.03799127880484 -37.15321080506123
10 -52.467456333339214 -36.33815176682742
11 -48.947185195982456 -35.77878286603585
12 -43.93157885968685 -34.34266182533649
13 -40.87865260615945 -33.861298462169636
14 -45.49895730242133 -31.002315411370166
15 -38.87011471949518 -30.59673213038507
16 -39.95958966575563 -28.313427148940267
17 -39.551930356770754 -25.99907478839801
18 -36.25502315815538 -24.254343826223852
19 -30.822591688483953 -23.71252838726811
20 -34.12986387917772 -22.144065107815585
21 -33.35689600324258 -20.35567457658451
22 -33.90216761827469 -20.107545880322675
23 -28.379834272898734 -19.674324289009657
24 -33.249678472056985 -19.50391949269807
25 -22.02960720937699 -17.102178647036034
26 -21.499737228266895 -16.23288157067845
27 -25.07002041535452 -14.156253158328367
28 -19.603471003472805 -12.940656397029304
29 -18.82264889869839 -12.513354538352278
30 -12.00297586247325 -12.182838020530454
31 -20.343415562994778 -11.966262779359818
32 -21.116464731283486 -11.430761099930217
33 -10.501247748266906 -11.078473784311472
34 -14.012598991394043 -7.842541250546751
35 -8.628480517771095 -7.687434481862833
36 -8.16767852101475 -7.67259168928286
37 -4.333715776447207 -7.108327355338034
38 -5.98321360303089 -6.776946485018116
39 -12.3483203263022 -6.51820418055673
40 -6.422820913605392 -6.298947752982548
41 -5.166768758092076 -5.644851434365073
train accuracy: 0.9371440584030883
validation accuracy: 0.9291521486643438
sorted_train_rewards: [-55.21445752 -51.17814647 -49.6099766  -49.13001653 -49.03866708
 -49.0331838  -48.84818037 -48.58850771 -48.19509289 -47.96991522
 -47.53054283 -47.33487662 -47.14538324 -47.10889581 -46.92993068
 -46.92815321 -46.7141986  -46.51840523 -46.50762306 -46.46343149
 -46.08033935 -45.96910073 -45.94004954 -45.47189047 -45.34946654
 -44.99184926 -44.7644634  -44.62310756 -44.53780078 -44.32699505
 -44.29565561 -43.22211069 -43.2041764  -43.14444939 -42.63170505
 -42.49903983 -42.18990615 -42.17868583 -42.08941791 -41.97868948
 -41.96459824 -41.91819405 -41.89252759 -41.86250044 -41.75643755
 -41.50687738 -41.35020296 -41.32515547 -41.22179983 -41.18200046
 -41.03992135 -40.99991982 -40.86627436 -40.84937019 -40.63970267
 -40.33775477 -40.11576908 -39.97085477 -39.92712396 -39.89750906
 -39.72931271 -39.35357961 -39.17708227 -39.14272956 -39.07321282
 -38.77785603 -38.52213956 -38.28199013 -38.28138827 -38.04661848
 -37.94300503 -37.88709355 -37.70658771 -37.51412288 -37.47672391
 -37.37133772 -37.3251261  -37.15321081 -37.11841822 -37.10344577
 -36.99477768 -36.69656462 -36.65639269 -36.38667894 -36.33815177
 -36.28177521 -36.03378032 -36.02351695 -35.97994531 -35.74239985
 -35.67327882 -35.43726783 -35.41556943 -35.28979253 -35.19353216
 -35.12234482 -35.07963015 -35.06137062 -35.02402835 -35.01091702
 -34.95588623 -34.91427059 -34.88495647 -34.86472602 -34.63851695
 -34.34266183 -33.86129846 -33.77802335 -33.7629591  -33.680383
 -33.4779641  -33.26579709 -33.17157461 -33.04852912 -32.41611773
 -32.41053817 -32.02508724 -31.90686017 -31.71028623 -31.64335581
 -31.48876757 -31.44406811 -31.37649579 -31.35230126 -31.24419956
 -31.21642916 -31.15850305 -31.06744474 -31.00231541 -30.76937686
 -30.64081715 -30.63466958 -30.59673213 -30.52049104 -30.42034344
 -30.36982897 -30.36743577 -30.27374553 -30.10812976 -30.06299271
 -29.98083048 -29.97767593 -29.66568464 -29.20595883 -28.99559546
 -28.95484843 -28.59191373 -28.5494411  -28.31342715 -28.1712167
 -28.13409378 -28.11939797 -27.79654204 -27.77638947 -27.70726756
 -27.60103341 -27.50815322 -27.27876998 -26.73963502 -26.7058308
 -26.50715502 -26.39763335 -26.36197542 -25.99907479 -25.47507675
 -25.31217996 -25.29134047 -25.11681575 -24.96039421 -24.91350015
 -24.58444108 -24.12244562 -23.96980649 -23.76917367 -23.71252839
 -23.48246476 -23.05835519 -22.79553869 -22.61211306 -22.55283027
 -22.53572133 -22.14406511 -21.86139195 -21.84262733 -21.72370465
 -21.7167854  -21.64185366 -21.62657756 -21.62597106 -21.44290047
 -20.78375363 -20.76562386 -20.68104247 -20.66287704 -20.66020817
 -20.5100558  -20.35567458 -20.10754588 -20.10309328 -19.94369578
 -19.90282899 -19.67432429 -19.50391949 -19.30989567 -19.03153444
 -19.02121155 -18.9341265  -18.87189699 -18.77655165 -18.73611103
 -18.71737606 -18.65414818 -18.4634721  -18.35047025 -18.2465562
 -18.08445937 -17.94319772 -17.85124778 -17.77364067 -17.77158541
 -17.64064603 -17.48133145 -17.27921081 -17.06769934 -16.68041488
 -16.61753067 -16.44066556 -16.35270212 -16.325741   -16.25152473
 -16.24688067 -16.23288157 -16.20186147 -16.01542464 -15.8535005
 -15.65380823 -15.4984665  -15.44345121 -15.29527985 -15.26574292
 -15.017961   -14.95736212 -14.80364551 -14.79738881 -14.47834852
 -14.41662641 -14.34836492 -14.24892226 -14.15625316 -14.06734668
 -14.05092615 -14.04015398 -13.99249522 -13.91417163 -13.8953595
 -13.88057881 -13.7939212  -13.73405689 -13.69675016 -13.45720676
 -13.43873671 -13.27768447 -13.1684561  -13.16397094 -13.16386337
 -12.99945969 -12.98525644 -12.97987294 -12.94677751 -12.9406564
 -12.92457626 -12.82166814 -12.79058052 -12.6606731  -12.5410027
 -12.53347897 -12.51335454 -12.40468089 -12.38996869 -12.37040308
 -12.29992996 -12.26611853 -12.18283802 -12.16816754 -12.13592928
 -12.04555617 -11.9523875  -11.89211294 -11.89104433 -11.77766704
 -11.75626016 -11.73746586 -11.63863136 -11.59932628 -11.53852267
 -11.51164274 -11.47155848 -11.4307611  -11.37941677 -11.34446877
 -11.22568273 -11.137381   -11.07847378 -11.06694367 -11.04421156
 -11.01208752 -10.89242091 -10.859881   -10.85010911 -10.78272714
 -10.76409188 -10.70992158 -10.64954177 -10.50774226 -10.49423816
 -10.37922657 -10.01645863  -9.99493371  -9.95712771  -9.92399886
  -9.67992171  -9.56750101  -9.44077216  -9.4003034   -9.32139356
  -8.97244586  -8.87472007  -8.49497785  -8.4761537   -8.46838135
  -8.40276084  -8.32638751  -8.09147894  -8.0269637   -8.01868294
  -7.84254125  -7.70107293  -7.57539849  -7.5455064   -7.54460172
  -7.53907027  -7.52029309  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.15421432  -7.08431127  -7.07404409  -6.95906356
  -6.92008425  -6.83697432  -6.77694649  -6.72206384  -6.71997062
  -6.64795748  -6.60654569  -6.47884361  -6.2891636   -6.05448903
  -5.89467275  -5.85405865  -5.64485143  -5.61579673  -5.39544196
  -5.38326081  -5.3472021   -5.25793019  -5.24856721  -5.23608532
  -5.16571156  -5.07848501  -5.06486011  -5.02795798  -4.90228293
  -4.82757292  -4.63049542  -4.35856953  -4.230832    -4.0660223
  -4.03104862  -4.00401798  -3.97870856  -3.65032555  -3.38446715
  -3.3322555   -3.329867    -3.29356852  -3.12579961  -3.07904644
  -2.88591659  -2.83192847  -2.67390706  -2.49009827  -2.24005036
  -1.91361965]
sorted_val_rewards: [-54.46505855 -45.23627882 -45.23541431 -44.99494244 -43.84464734
 -43.7920154  -41.76630373 -41.03294083 -40.98448758 -38.91826709
 -38.29413029 -37.79694607 -36.58958472 -35.89835563 -35.77878287
 -30.95117262 -30.772676   -29.94526105 -29.22737048 -28.42026397
 -26.19987885 -24.71538539 -24.25434383 -18.36358054 -17.82515236
 -17.67989719 -17.10217865 -15.60339789 -14.71689733 -14.39794675
 -13.75007868 -13.66581557 -11.97407887 -11.96626278 -11.74367847
 -10.94613422  -7.68743448  -7.67259169  -7.10832736  -6.51820418
  -6.29894775  -4.71135754  -4.37983153  -2.64166233]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 946
num val_labels 946
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.1115971576935789, val_acc 0.952431289640592
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.12830208503886048, val_acc 0.9492600422832981
trigger times: 1
end of epoch 2: val_loss 0.14551049948734376, val_acc 0.9439746300211417
trigger times: 2
end of epoch 3: val_loss 0.15783016985068815, val_acc 0.9471458773784355
trigger times: 3
end of epoch 4: val_loss 0.06996914561220688, val_acc 0.9682875264270613
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.11797057933844593, val_acc 0.9545454545454546
trigger times: 1
end of epoch 6: val_loss 0.244429367643463, val_acc 0.9228329809725159
trigger times: 2
end of epoch 7: val_loss 0.1283959126086745, val_acc 0.9513742071881607
trigger times: 3
end of epoch 8: val_loss 0.12037480438830979, val_acc 0.9534883720930233
trigger times: 4
end of epoch 9: val_loss 0.09910976389483844, val_acc 0.9566596194503171
trigger times: 5
end of epoch 10: val_loss 0.1191713944373788, val_acc 0.9460887949260042
trigger times: 6
end of epoch 11: val_loss 0.10821346579922768, val_acc 0.9460887949260042
trigger times: 7
end of epoch 12: val_loss 0.20225960085510467, val_acc 0.9482029598308668
trigger times: 8
end of epoch 13: val_loss 0.5064137919289102, val_acc 0.9323467230443975
trigger times: 9
end of epoch 14: val_loss 0.1630292283578341, val_acc 0.9482029598308668
trigger times: 10
Early stopping.
0 -43.99315968155861 -54.46505855143417
1 -32.89344434067607 -45.23627882417037
2 -30.007932394742966 -45.23541430806973
3 -37.92200087662786 -44.99494243924764
4 -31.134273250587285 -43.844647337720424
5 -33.29927144944668 -43.792015395413316
6 -29.91661206074059 -41.76630372811994
7 -34.41323020402342 -41.03294083119757
8 -28.04637487884611 -40.98448757527241
9 -30.163159929215908 -38.91826708792281
10 -31.418351378291845 -38.29413028930457
11 -36.706684527918696 -37.79694607206402
12 -28.990944735705853 -36.58958472489279
13 -28.83398763462901 -35.89835562511604
14 -26.7177924066782 -35.77878286603585
15 -30.7655457155779 -30.9511726207529
16 -23.381428462453187 -30.772676001251654
17 -24.974686469882727 -29.945261049542818
18 -20.737287647090852 -29.227370478407224
19 -22.732503678649664 -28.42026396583319
20 -17.40701301395893 -26.199878851984913
21 -18.796208541840315 -24.715385385378266
22 -20.554846493527293 -24.254343826223852
23 -12.293635568581522 -18.363580535271076
24 -10.232886429876089 -17.825152358862276
25 -9.032167695462704 -17.679897189647424
26 -7.058113872073591 -17.102178647036034
27 -8.588980888016522 -15.60339788815964
28 -7.428482932038605 -14.716897331906482
29 -5.917066362686455 -14.397946753443035
30 -6.346664294600487 -13.750078682646253
31 -5.918940119445324 -13.665815568204538
32 -3.5977895595133305 -11.974078865914656
33 -5.096748070791364 -11.966262779359818
34 -2.659359418787062 -11.743678465997743
35 -1.5692389057949185 -10.94613421613846
36 -1.001112598925829 -7.687434481862833
37 -0.23080865386873484 -7.67259168928286
38 2.1968436101451516 -7.108327355338034
39 1.4776011370122433 -6.51820418055673
40 1.7752923853695393 -6.298947752982548
41 4.605939719825983 -4.71135753915452
42 4.795051305554807 -4.379831534481121
43 5.740124896168709 -2.6416623314910934
train accuracy: 0.9526048236058556
validation accuracy: 0.9482029598308668

[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.02871814446808255, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.2966e-03,  4.7246e-02, -7.6213e-02,  2.1310e-03, -1.6260e-02,
         -2.0544e-01, -9.7125e-04,  7.7913e-03,  1.6073e-01, -2.1930e-01,
         -2.3655e-03, -1.4551e+00, -2.1550e+00]], device='cuda:2'))])
end of epoch 1: val_loss 0.005569825032318825, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1889,  0.0879,  0.0447,  0.0885,  0.1855, -0.1929, -0.0253, -0.0310,
         -0.0431, -0.0853,  0.0038, -2.3301, -3.4546]], device='cuda:2'))])
end of epoch 2: val_loss 0.21549306709836524, val_acc 0.98
trigger times: 1
end of epoch 3: val_loss 3.292332866404735e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.2466e-02,  4.2557e-02,  4.3073e-02, -7.9736e-02,  1.2642e-01,
         -3.2291e-05,  1.4137e-02,  1.9987e-02, -1.2698e-01, -9.7696e-02,
          7.5657e-06, -3.0098e+00, -5.3585e+00]], device='cuda:2'))])
end of epoch 4: val_loss 0.001015659989748201, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 1.2253456469863977e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.9363e-02, -5.5603e-02,  1.1972e-01, -3.6395e-02,  6.1036e-01,
          2.5406e-02, -4.7024e-03,  2.3269e-02, -2.5392e-01, -1.2504e-01,
          1.5752e-03, -3.8798e+00, -7.3857e+00]], device='cuda:2'))])
end of epoch 6: val_loss 0.0002377141911667735, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0037282227579271067, val_acc 0.995
trigger times: 2
end of epoch 8: val_loss 4.410731417436864e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5753e-01, -8.9581e-02, -9.2440e-02,  7.4903e-02,  1.3662e-01,
          1.1394e-01,  4.6440e-03,  2.2631e-02, -1.3018e-01, -2.6239e-01,
         -3.1946e-04, -5.2430e+00, -8.9884e+00]], device='cuda:2'))])
end of epoch 9: val_loss 0.005012116672605025, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.0352346660330656, val_acc 0.985
trigger times: 2
end of epoch 11: val_loss 0.20909793881649358, val_acc 0.985
trigger times: 3
end of epoch 12: val_loss 9.1658381097659e-06, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 1.4126153359939054e-07, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 5.3047909034376065e-08, val_acc 1.0
trigger times: 6
end of epoch 15: val_loss 0.04196566712111206, val_acc 0.995
trigger times: 7
end of epoch 16: val_loss 1.1896395420762928e-06, val_acc 1.0
trigger times: 8
end of epoch 17: val_loss 2.2162563424998893e-06, val_acc 1.0
trigger times: 9
end of epoch 18: val_loss 0.02440227863607788, val_acc 0.99
trigger times: 10
Early stopping.
0 -488.80921626091003 -54.98547503240923
1 -432.1281797885895 -50.492268601198035
2 -436.41050481796265 -50.03933801517046
3 -457.55739879608154 -49.75347184620696
4 -443.3829493522644 -49.72654640753777
5 -451.4467239379883 -46.98011874490918
6 -424.7105133533478 -45.7351542845057
7 -427.7127125263214 -45.670579884154705
8 -434.7791385650635 -44.99030608142343
9 -406.12717854976654 -44.14602409201361
10 -424.37517189979553 -43.81326882122305
11 -399.37458431720734 -43.18878399086166
12 -385.3205101490021 -42.29180714825394
13 -413.99838185310364 -42.00401746161006
14 -397.1950750350952 -41.6910044370425
15 -400.61925196647644 -41.68588229294918
16 -397.888062953949 -41.281777102712205
17 -396.1606397628784 -40.44278203413966
18 -386.5427861213684 -40.34838365523108
19 -400.93813705444336 -39.599701153458774
20 -375.7895722389221 -39.57586365327889
21 -376.3786323070526 -39.31972693233231
22 -370.29129219055176 -39.024610555047154
23 -386.2981791496277 -38.45534493538269
24 -362.9135020971298 -38.41270390343083
25 -390.22779607772827 -38.35634328077039
26 -359.31952542066574 -37.79713616772368
27 -367.79921984672546 -37.741528994987384
28 -369.5398027896881 -37.66475323879293
29 -382.1046407222748 -37.513139380385574
30 -374.1399110555649 -37.1809993033689
31 -368.43332147598267 -37.100703136010694
32 -350.81459081172943 -37.00630588930485
33 -370.17766869068146 -36.821916772458344
34 -383.64964485168457 -36.48799015296732
35 -350.71543323993683 -36.20965269874363
36 -349.796826004982 -36.19207561676116
37 -363.26069235801697 -36.114459029559086
38 -346.80663657188416 -35.78149902167743
39 -362.5589550733566 -35.394503873250635
40 -364.2479838132858 -35.26282499693737
41 -379.9033284187317 -35.24303541418371
42 -349.6264593601227 -35.209705244501436
43 -350.64170026779175 -35.0654408505187
44 -341.4190163612366 -34.80241747531743
45 -359.13357985019684 -34.64469044638467
46 -319.60734140872955 -33.84284985953318
47 -323.68002355098724 -32.70706485357069
48 -344.0518231391907 -31.969099402548657
49 -322.82865130901337 -31.7109134007892
50 -316.3935774564743 -31.64414355845032
51 -326.53178465366364 -31.392382758954444
52 -323.79674702882767 -31.223196019713853
53 -321.2659692764282 -31.12953085092458
54 -318.56013798713684 -29.39157139549552
55 -314.65512347221375 -29.340125609942326
56 -313.42278814315796 -29.106189988903285
57 -284.5514486730099 -27.41102349748205
58 -284.5244197845459 -27.343722362182305
59 -287.0625196695328 -27.196681629483837
60 -283.19436717033386 -27.07399028854534
61 -289.1316375732422 -26.7047217556024
62 -281.36342561244965 -26.244794902859052
63 -274.46224451065063 -25.548365085275513
64 -286.3707740306854 -25.45878528601009
65 -283.47070479393005 -24.879106999799365
66 -267.5392736196518 -24.828695359328833
67 -267.27100878953934 -24.592745144504722
68 -277.96916806697845 -23.978745577896312
69 -255.90481877326965 -23.57262108435893
70 -268.56243896484375 -23.44970807952351
71 -278.87857484817505 -22.745309160183492
72 -249.39293026924133 -22.60679894414887
73 -249.93127351999283 -22.19891031871716
74 -252.3182363808155 -20.656863763892378
75 -261.56352412700653 -20.444472560731253
76 -235.04774606227875 -20.19699010077007
77 -246.20866721868515 -20.13839114930498
78 -242.08748579025269 -19.63760343800059
79 -221.65383341908455 -19.515598718228343
80 -212.14542186260223 -18.92838809611677
81 -231.19929909706116 -17.994774057192853
82 -196.82422137260437 -17.55742370467821
83 -208.0590586066246 -16.823073927842348
84 -171.29301863908768 -14.855082803515382
85 -186.9345887005329 -14.531424598833084
86 -198.72203102707863 -14.442420089224363
87 -166.68881741166115 -13.596012850960644
88 -154.9344772696495 -12.68135972540495
89 -172.7835473716259 -12.66418205637357
90 -157.78596925735474 -12.30017947419658
91 -163.23443281650543 -12.151904772081672
92 -150.88842423260212 -11.788852141676486
93 -157.78691399097443 -10.869989101210326
94 -138.79056803882122 -10.327681503524177
95 -145.6797575354576 -9.8572159761571
96 -116.79208110272884 -8.330116995310416
97 -121.24850165843964 -8.133195842510668
98 -121.76351629197598 -8.108197691178031
99 -113.88404470682144 -7.57539849177145
100 -109.09414726495743 -7.362443126623615
101 -101.88405260816216 -7.108327355338034
102 -97.14740233123302 -6.959063561385431
103 -126.36778527498245 -6.776946485018116
104 -100.66720091551542 -6.7220638398623045
105 -126.33689773082733 -6.719970621583102
106 -113.11178171634674 -6.535447341844848
107 -97.93842594325542 -6.51820418055673
108 -91.85584597289562 -5.615796733870542
109 -97.28707680106163 -5.34720210027791
110 -79.45103684067726 -5.078485007852753
111 -82.79712533205748 -5.027957977402961
112 -98.02867457270622 -4.827572916892203
113 -75.37501785904169 -4.63049541560991
114 -98.28348010778427 -4.230832004686763
115 -87.72230621799827 -4.031048624093466
116 -65.81160046160221 -3.3844671463622564
117 -79.85431222617626 -3.3322555012187633
118 -66.35265947878361 -2.6416623314910934
119 -61.43255515396595 -1.9136196540088464
train accuracy: 0.9933333333333333
validation accuracy: 0.99
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.27957967 -12.68135973
 -12.66418206 -12.30017947 -12.15190477 -11.98389944 -11.78885214
 -11.27494741 -11.11801877 -10.8699891  -10.3276815   -9.85721598
  -9.36478387  -9.1139336   -8.89521187  -8.56256514  -8.53973786
  -8.49091398  -8.330117    -8.27510634  -8.22175307  -8.13319584
  -8.1168431   -8.10819769  -8.05253613  -7.80208868  -7.73509476
  -7.57539849  -7.51214588  -7.36244313  -7.32542828  -7.10832736
  -6.97360612  -6.95906356  -6.92631889  -6.85054872  -6.84697174
  -6.83778929  -6.77694649  -6.72206384  -6.71997062  -6.68648227
  -6.53544734  -6.51820418  -6.36649112  -6.33271049  -6.30050379
  -5.72427553  -5.63547795  -5.61579673  -5.52294917  -5.47572888
  -5.45033634  -5.37711661  -5.3472021   -5.20169858  -5.19697957
  -5.12500222  -5.07848501  -5.02795798  -4.87298764  -4.82757292
  -4.69362289  -4.68417408  -4.63049542  -4.59812978  -4.50440842
  -4.42388744  -4.230832    -4.03104862  -4.02904373  -4.01338114
  -3.97642389  -3.8837253   -3.63878996  -3.63007491  -3.50064848
  -3.4506545   -3.3867875   -3.38446715  -3.3468113   -3.3322555
  -3.3194082   -3.20672797  -3.06518528  -2.7090979   -2.64166233
  -2.42617852  -2.13898073  -1.91361965  -1.78799419  -1.67945101]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.007171664445616486, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0884,  0.0183, -0.0526,  0.0622,  0.4191,  0.0441, -0.0126, -0.0183,
         -0.1004, -0.2090, -0.0024, -1.6217, -1.5800]], device='cuda:1'))])
end of epoch 1: val_loss 0.018029865011076146, val_acc 0.985
trigger times: 1
end of epoch 2: val_loss 0.026229312349121514, val_acc 0.99
trigger times: 2
end of epoch 3: val_loss 0.004714984758985103, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1822e-01, -4.3432e-02,  7.0472e-02,  9.2942e-02,  8.7807e-01,
          3.1909e-02,  1.4153e-02, -1.9590e-02,  8.8301e-02,  1.3418e-01,
          7.5657e-06, -3.7652e+00, -4.4990e+00]], device='cuda:1'))])
end of epoch 4: val_loss 5.384429632435684e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4624e-01, -8.7821e-03,  4.4744e-03, -1.1682e-01,  8.2976e-01,
         -1.8722e-01,  3.2757e-03,  2.0875e-02, -1.5363e-01,  3.5349e-01,
         -2.2278e-05, -3.5503e+00, -4.9918e+00]], device='cuda:1'))])
end of epoch 5: val_loss 5.571824994312635e-06, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.09572293433435021, val_acc 0.995
trigger times: 2
end of epoch 7: val_loss 4.632176873684557e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1892e-01,  8.4301e-04,  7.1901e-02, -1.4011e-01,  9.5745e-01,
         -1.5577e-01,  2.1153e-02,  3.8858e-02, -1.6457e-01,  1.6592e-01,
          3.5488e-04, -4.7656e+00, -6.4048e+00]], device='cuda:1'))])
end of epoch 8: val_loss 4.1733072002259064e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9330e-01, -5.8007e-02, -3.0908e-02, -6.5027e-06,  6.8051e-01,
         -2.0505e-01,  4.0326e-03, -1.7928e-02,  7.3664e-02,  1.2658e-01,
         -3.1946e-04, -4.6936e+00, -6.1958e+00]], device='cuda:1'))])
end of epoch 9: val_loss 0.20871145979815645, val_acc 0.975
trigger times: 1
end of epoch 10: val_loss 7.05102365195387e-05, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.000674118659072569, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 0.005147437721325297, val_acc 0.995
trigger times: 4
end of epoch 13: val_loss 2.384185471271394e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7376e-02, -4.4496e-03,  1.3163e-01, -6.5833e-03,  9.0849e-01,
          1.6965e-02, -3.5141e-02,  5.2209e-03, -9.3581e-02,  9.8687e-04,
         -2.5067e-03, -5.3804e+00, -7.1685e+00]], device='cuda:1'))])
end of epoch 14: val_loss 1.0430737884092878e-07, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.04525367734259305, val_acc 0.995
trigger times: 2
end of epoch 16: val_loss 7.748601227319796e-09, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.3255e-01,  2.3257e-01,  1.8965e-01, -9.1285e-02,  1.2469e+00,
         -2.0489e-01, -2.4063e-02,  1.6538e-02, -4.6057e-01,  1.1276e-01,
         -1.2006e-03, -6.4630e+00, -7.8466e+00]], device='cuda:1'))])
end of epoch 18: val_loss 0.18834039322667878, val_acc 0.985
trigger times: 1
end of epoch 19: val_loss 0.0030653275431955862, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 0.15403103479578475, val_acc 0.99
trigger times: 3
end of epoch 21: val_loss 1.1634423016104734e-06, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 2.9162255484038726e-06, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 1.1937684745078059e-06, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.0001664943480625425, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.0422140392433192, val_acc 0.99
trigger times: 8
end of epoch 26: val_loss 7.521540101151913e-07, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 0.05912488580580486, val_acc 0.99
trigger times: 10
Early stopping.
0 -466.9859836101532 -54.98547503240923
1 -445.55085253715515 -50.03933801517046
2 -397.3803298473358 -49.75347184620696
3 -319.4390007853508 -46.98011874490918
4 -430.21917700767517 -45.7351542845057
5 -374.08656120300293 -44.99030608142343
6 -478.71323108673096 -44.14602409201361
7 -421.765264749527 -43.18878399086166
8 -444.358624458313 -42.29180714825394
9 -401.4893972873688 -41.6910044370425
10 -383.51606822013855 -41.68588229294918
11 -396.93029856681824 -40.44278203413966
12 -404.4648199081421 -40.34838365523108
13 -374.40956807136536 -39.57586365327889
14 -408.00948095321655 -39.31972693233231
15 -334.60659551620483 -38.45534493538269
16 -348.28468215465546 -38.41270390343083
17 -369.7353689670563 -37.79713616772368
18 -385.3806893825531 -37.741528994987384
19 -341.73069989681244 -37.513139380385574
20 -410.32495987415314 -37.1809993033689
21 -351.1486932039261 -37.00630588930485
22 -355.0957872867584 -36.821916772458344
23 -329.87329518795013 -36.20965269874363
24 -336.97696340084076 -36.19207561676116
25 -354.3660509586334 -35.78149902167743
26 -311.7419309616089 -35.394503873250635
27 -328.4224603176117 -35.24303541418371
28 -325.504292845726 -35.209705244501436
29 -296.2784053981304 -34.80241747531743
30 -309.5152596831322 -34.64469044638467
31 -331.6697814464569 -32.70706485357069
32 -284.9226381778717 -31.969099402548657
33 -326.2649402618408 -31.64414355845032
34 -281.2201095223427 -31.392382758954444
35 -291.12131732702255 -31.12953085092458
36 -288.19603431224823 -29.39157139549552
37 -301.9820069074631 -29.106189988903285
38 -301.03112357854843 -27.41102349748205
39 -259.9961529970169 -27.196681629483837
40 -293.3918044567108 -27.07399028854534
41 -263.01990938186646 -26.244794902859052
42 -244.86547130346298 -25.548365085275513
43 -277.7869279384613 -24.879106999799365
44 -239.76196986436844 -24.828695359328833
45 -233.8746089041233 -23.978745577896312
46 -266.2422807812691 -23.57262108435893
47 -251.7560669183731 -22.745309160183492
48 -282.27332305908203 -22.60679894414887
49 -243.3374606668949 -20.656863763892378
50 -229.91554182767868 -20.444472560731253
51 -202.53838378190994 -20.13839114930498
52 -214.9110176563263 -19.63760343800059
53 -198.32780861854553 -18.92838809611677
54 -230.02538591623306 -17.994774057192853
55 -216.17300242185593 -16.823073927842348
56 -165.4354903101921 -14.855082803515382
57 -180.59215760231018 -14.442420089224363
58 -154.4563768208027 -13.596012850960644
59 -164.2463320493698 -12.68135972540495
60 -136.30886386334896 -12.66418205637357
61 -170.6124497950077 -12.151904772081672
62 -138.79676175117493 -11.983899438286592
63 -135.02315068244934 -11.27494740992659
64 -133.03258150815964 -11.118018771528469
65 -125.37403655052185 -10.327681503524177
66 -114.30860802531242 -9.8572159761571
67 -115.94886988401413 -9.113933599858365
68 -111.25138503313065 -8.895211870845777
69 -91.98380488157272 -8.539737855317396
70 -91.0599895119667 -8.490913981197433
71 -89.37573152780533 -8.275106338530883
72 -108.86156666278839 -8.221753066471264
73 -81.54412543773651 -8.116843101163562
74 -97.46385908126831 -8.108197691178031
75 -97.60360538959503 -7.80208867511953
76 -81.89946687221527 -7.735094760301914
77 -82.47790223360062 -7.512145882355178
78 -117.29291927814484 -7.362443126623615
79 -123.44493174552917 -7.108327355338034
80 -95.50668981671333 -6.973606122501844
81 -112.22323822975159 -6.926318887747202
82 -78.79914127290249 -6.850548715270908
83 -82.05764955282211 -6.837789294669928
84 -110.15661364793777 -6.776946485018116
85 -113.17197227478027 -6.719970621583102
86 -68.28880523145199 -6.686482268732197
87 -97.95962619781494 -6.51820418055673
88 -97.69382324814796 -6.366491116001723
89 -97.53633671998978 -6.300503794045418
90 -87.78875660896301 -5.724275531291021
91 -88.62200665473938 -5.615796733870542
92 -88.6170619726181 -5.522949169588691
93 -83.17294871807098 -5.450336337955995
94 -83.90708231925964 -5.3771166066367835
95 -80.10588997602463 -5.201698579635878
96 -85.4034651517868 -5.196979565892167
97 -74.84689703583717 -5.078485007852753
98 -78.05525888502598 -5.027957977402961
99 -68.84101309627295 -4.827572916892203
100 -70.26744332909584 -4.693622885576568
101 -66.50528978556395 -4.63049541560991
102 -69.60546088218689 -4.598129780154474
103 -85.199587225914 -4.423887437667808
104 -66.43842417746782 -4.230832004686763
105 -66.82416488230228 -4.029043726175636
106 -65.64304488897324 -4.013381141636067
107 -71.06338971853256 -3.88372529647515
108 -69.26252022385597 -3.6387899596012976
109 -66.88856959342957 -3.500648479903634
110 -64.83669079840183 -3.4506544990909047
111 -49.06594493985176 -3.3844671463622564
112 -72.58760499954224 -3.346811297710865
113 -69.20554175972939 -3.3194081996857228
114 -66.64252626895905 -3.2067279692300117
115 -54.32483082264662 -2.7090979049729076
116 -46.626899272203445 -2.6416623314910934
117 -45.436127945780754 -2.1389807342856253
118 -32.28615853190422 -1.9136196540088464
119 -48.430387541651726 -1.6794510066809267
train accuracy: 0.9766666666666667
validation accuracy: 0.99
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.27957967 -12.68135973
 -12.66418206 -12.30017947 -12.15190477 -11.98389944 -11.78885214
 -11.27494741 -11.11801877 -10.8699891  -10.3276815   -9.85721598
  -9.7720712   -9.74146416  -9.68952853  -9.36478387  -9.17446256
  -9.1139336   -8.89521187  -8.68551512  -8.61749742  -8.56704868
  -8.56256514  -8.53973786  -8.49091398  -8.330117    -8.27510634
  -8.26704963  -8.22175307  -8.19850526  -8.16621769  -8.13319584
  -8.1168431   -8.10819769  -8.10547793  -8.05253613  -8.03779943
  -7.99327536  -7.80208868  -7.73509476  -7.72997027  -7.72795966
  -7.57539849  -7.56791213  -7.51362985  -7.51214588  -7.46574771
  -7.41328518  -7.36244313  -7.32542828  -7.20727587  -7.17164584
  -7.10832736  -7.05559771  -7.00610293  -6.97755703  -6.97360612
  -6.95906356  -6.92631889  -6.92397264  -6.86246718  -6.8583624
  -6.85054872  -6.84697174  -6.83778929  -6.77694649  -6.72206384
  -6.71997062  -6.68648227  -6.61460987  -6.55735147  -6.54176568
  -6.53544734  -6.51820418  -6.47179286  -6.36649112  -6.33271049
  -6.30394682  -6.30065638  -6.30050379  -6.27109572  -6.01474705
  -5.72427553  -5.71443789  -5.63547795  -5.61579673  -5.52294917
  -5.47572888  -5.45414406  -5.45033634  -5.37711661  -5.3472021
  -5.30416879  -5.20169858  -5.19697957  -5.12500222  -5.07848501
  -5.02795798  -5.01103558  -4.88626095  -4.87298764  -4.82757292
  -4.74732721  -4.69362289  -4.69279796  -4.68417408  -4.63049542
  -4.59812978  -4.52383165  -4.5070808   -4.50440842  -4.44167567
  -4.42388744  -4.36689695  -4.32438653  -4.25415639  -4.230832
  -4.06856479  -4.03104862  -4.02904373  -4.0223706   -4.01338114
  -3.97642389  -3.95013088  -3.8837253   -3.79054117  -3.63878996
  -3.63659967  -3.63007491  -3.62507593  -3.50064848  -3.4506545
  -3.3867875   -3.38446715  -3.3553423   -3.3468113   -3.3322555
  -3.3194082   -3.20672797  -3.1164616   -3.06518528  -2.73871505
  -2.7090979   -2.64166233  -2.42617852  -2.13898073  -2.06325843
  -2.04424411  -1.91361965  -1.78799419  -1.73565254  -1.67945101]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.11401674769003729, val_acc 0.975
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0741,  0.1559,  0.0055,  0.0039,  0.4229, -0.0904, -0.0116,  0.0025,
          0.0989,  0.0772, -0.0024, -1.6988, -1.5062]], device='cuda:0'))])
end of epoch 1: val_loss 0.06266472442787852, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2494e-01, -2.2672e-04, -1.1083e-02,  8.7543e-02,  3.1743e-01,
          1.7790e-03, -8.8351e-03, -8.7321e-03,  2.5247e-01,  4.1089e-02,
          3.7959e-03, -2.0588e+00, -2.5921e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.29036716349655667, val_acc 0.965
trigger times: 1
end of epoch 3: val_loss 0.033884323177126315, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.9652e-03, -1.4202e-01,  3.8823e-02, -1.4423e-02,  4.0184e-01,
          1.1549e-01, -1.5992e-03, -4.9816e-02,  1.6608e-01, -3.0846e-01,
          7.5657e-06, -3.5754e+00, -3.6356e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.02925507711118815, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.3324e-02, -1.0433e-01, -1.8996e-01, -5.7782e-02,  6.3208e-01,
         -2.1744e-01,  3.7132e-02, -4.6241e-03,  5.8156e-03, -1.5604e-01,
         -2.2278e-05, -4.1182e+00, -4.4408e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.019439487968325047, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.2312e-02, -2.3659e-01, -8.0824e-02,  1.4552e-01,  5.2871e-01,
         -1.1908e-02,  1.2683e-03, -8.8414e-02,  1.0352e-02,  2.7964e-02,
          1.5752e-03, -4.4870e+00, -4.9978e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.09791839797760574, val_acc 0.985
trigger times: 1
end of epoch 7: val_loss 0.01041103912768886, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.6879e-02, -7.7829e-02, -7.8222e-02,  1.1851e-01,  6.6869e-01,
          6.2640e-04, -7.9472e-06, -3.2409e-02,  4.5493e-02,  2.1062e-02,
          3.5488e-04, -4.6902e+00, -5.4815e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.015078873519563985, val_acc 0.99
trigger times: 1
end of epoch 9: val_loss 0.000538000952427069, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.2418e-07, -4.3569e-02,  2.8997e-05,  7.7432e-02,  3.2517e-01,
         -4.4469e-06, -4.1392e-03, -6.6735e-02,  5.8131e-02, -1.2131e-04,
         -5.2126e-04, -4.6794e+00, -5.4914e+00]], device='cuda:0'))])
end of epoch 10: val_loss 0.1041133403735698, val_acc 0.995
trigger times: 1
end of epoch 11: val_loss 0.004805871282544203, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 0.009554948319607562, val_acc 0.995
trigger times: 3
end of epoch 13: val_loss 0.00012073711814320376, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.0219e-06, -7.1804e-02, -1.1821e-03, -1.9160e-03,  2.6063e-05,
          4.0383e-05, -6.6891e-03, -2.5768e-02,  6.7616e-05,  3.9410e-05,
         -2.5067e-03, -4.8242e+00, -5.0009e+00]], device='cuda:0'))])
end of epoch 14: val_loss 0.005003414420078158, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.05787742961529968, val_acc 0.985
trigger times: 2
end of epoch 16: val_loss 0.048447505613852936, val_acc 0.995
trigger times: 3
end of epoch 17: val_loss 0.02851331197604978, val_acc 0.995
trigger times: 4
end of epoch 18: val_loss 0.011132952292086244, val_acc 0.995
trigger times: 5
end of epoch 19: val_loss 0.07043978839317784, val_acc 0.99
trigger times: 6
end of epoch 20: val_loss 0.009875274920302672, val_acc 0.995
trigger times: 7
end of epoch 21: val_loss 0.0032056844586474042, val_acc 1.0
trigger times: 8
end of epoch 22: val_loss 0.00027842904871636873, val_acc 1.0
trigger times: 9
end of epoch 23: val_loss 4.816446545735431e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.6596e-03, -1.5693e-01, -2.1789e-02,  4.9775e-03,  2.2344e-01,
         -2.0442e-04,  1.9422e-02, -3.9822e-02,  1.8351e-01, -3.4888e-04,
          3.8004e-03, -5.2726e+00, -6.7300e+00]], device='cuda:0'))])
end of epoch 24: val_loss 0.0404244559161657, val_acc 0.99
trigger times: 1
end of epoch 25: val_loss 5.404581426624589e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0435e-02, -1.3318e-01, -7.1326e-02,  7.7153e-02,  7.1861e-01,
         -3.7781e-02,  1.9061e-02, -4.6514e-02,  1.9130e-01,  5.4984e-02,
          1.1983e-05, -5.7063e+00, -6.5015e+00]], device='cuda:0'))])
end of epoch 26: val_loss 0.10414260157515515, val_acc 0.985
trigger times: 1
end of epoch 27: val_loss 0.00017663298180099928, val_acc 1.0
trigger times: 2
end of epoch 28: val_loss 0.028943559357013236, val_acc 0.99
trigger times: 3
end of epoch 29: val_loss 3.8945761246722555e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.3844e-06, -2.2309e-01, -2.4448e-02, -3.2158e-02,  4.1665e-01,
          1.3570e-01,  4.9754e-03, -1.9527e-02, -5.4371e-05,  1.6773e-07,
          3.5927e-04, -5.7672e+00, -6.3803e+00]], device='cuda:0'))])
end of epoch 30: val_loss 0.03403756332975391, val_acc 0.995
trigger times: 1
end of epoch 31: val_loss 0.021165561153554755, val_acc 0.995
trigger times: 2
end of epoch 32: val_loss 0.060862968678919174, val_acc 0.985
trigger times: 3
end of epoch 33: val_loss 2.5088274537985455e-05, val_acc 1.0
trigger times: 4
end of epoch 34: val_loss 0.07076903118875563, val_acc 0.985
trigger times: 5
end of epoch 35: val_loss 0.0018968808348916967, val_acc 1.0
trigger times: 6
end of epoch 36: val_loss 0.09264949820771527, val_acc 0.99
trigger times: 7
end of epoch 37: val_loss 0.0001117639241283186, val_acc 1.0
trigger times: 8
end of epoch 38: val_loss 0.6140669840615067, val_acc 0.935
trigger times: 9
end of epoch 39: val_loss 0.03461637262340666, val_acc 0.995
trigger times: 10
Early stopping.
0 -354.5521686077118 -54.98547503240923
1 -338.5320255756378 -50.03933801517046
2 -323.6052072048187 -49.72654640753777
3 -310.56586837768555 -45.7351542845057
4 -292.5131754875183 -44.99030608142343
5 -294.7536326646805 -43.81326882122305
6 -316.14535880088806 -42.29180714825394
7 -292.2190239429474 -41.6910044370425
8 -282.57100439071655 -41.281777102712205
9 -294.1224197149277 -40.34838365523108
10 -286.3193151950836 -39.57586365327889
11 -287.8282587528229 -39.024610555047154
12 -273.9733740091324 -38.41270390343083
13 -270.359824180603 -37.79713616772368
14 -268.63915061950684 -37.66475323879293
15 -285.8838469982147 -37.1809993033689
16 -266.7426053285599 -37.00630588930485
17 -244.5118910074234 -36.48799015296732
18 -266.2861528992653 -36.19207561676116
19 -266.31871914863586 -35.78149902167743
20 -264.68504798412323 -35.26282499693737
21 -254.55256420373917 -35.209705244501436
22 -243.74512207508087 -34.80241747531743
23 -249.40467357635498 -33.84284985953318
24 -235.93585419654846 -31.969099402548657
25 -240.7356082201004 -31.64414355845032
26 -228.6423777937889 -31.223196019713853
27 -218.63823127746582 -29.39157139549552
28 -223.79215288162231 -29.106189988903285
29 -213.7898155003786 -27.343722362182305
30 -215.12626034021378 -27.07399028854534
31 -214.0417041182518 -26.244794902859052
32 -206.9007419347763 -25.45878528601009
33 -188.041786134243 -24.828695359328833
34 -175.9297048151493 -23.978745577896312
35 -194.0200160741806 -23.44970807952351
36 -194.9413827061653 -22.60679894414887
37 -169.61381554603577 -20.656863763892378
38 -179.20511043071747 -20.19699010077007
39 -173.72478026151657 -19.63760343800059
40 -168.86465829610825 -18.92838809611677
41 -148.3699642419815 -17.55742370467821
42 -131.02885055541992 -14.855082803515382
43 -142.56814095377922 -14.442420089224363
44 -114.9407331943512 -13.27957966807184
45 -117.86783128976822 -12.66418205637357
46 -132.8565037548542 -12.151904772081672
47 -114.11464561522007 -11.788852141676486
48 -103.02125644683838 -11.118018771528469
49 -113.96989983320236 -10.327681503524177
50 -94.83765959739685 -9.77207119978166
51 -95.80940014123917 -9.689528533421145
52 -94.83264541625977 -9.174462558852495
53 -87.80160862207413 -8.895211870845777
54 -99.39031773805618 -8.617497422248727
55 -79.69574570655823 -8.562565135830942
56 -84.68271619081497 -8.490913981197433
57 -82.88856446743011 -8.275106338530883
58 -87.92924159765244 -8.221753066471264
59 -88.0894450545311 -8.166217686314063
60 -95.75897127389908 -8.108197691178031
61 -83.62684899568558 -8.0525361308728
62 -79.87911933660507 -7.99327535653492
63 -75.8710697889328 -7.735094760301914
64 -82.87547272443771 -7.727959660645908
65 -91.21913146972656 -7.5679121290375235
66 -77.95933347940445 -7.512145882355178
67 -84.6203984618187 -7.413285178359452
68 -72.19765128195286 -7.3254282803317174
69 -80.75056239962578 -7.171645844959185
70 -79.75730416178703 -7.055597710392153
71 -85.92514351010323 -6.977557034812622
72 -94.4479700922966 -6.959063561385431
73 -79.57473874092102 -6.923972641968443
74 -82.02192956209183 -6.85836239896135
75 -70.03799107670784 -6.846971739801642
76 -85.70297008752823 -6.776946485018116
77 -83.1499690413475 -6.719970621583102
78 -79.0069169998169 -6.614609866811899
79 -72.6107332110405 -6.541765681017651
80 -77.04808533191681 -6.51820418055673
81 -68.02826645970345 -6.366491116001723
82 -67.48037427663803 -6.303946824004978
83 -66.56431075930595 -6.300503794045418
84 -82.54197654128075 -6.014747046722244
85 -64.37440025806427 -5.71443788791496
86 -71.03351072967052 -5.615796733870542
87 -67.88968671113253 -5.475728882640741
88 -71.02844870090485 -5.450336337955995
89 -62.79821974039078 -5.34720210027791
90 -59.23985409736633 -5.201698579635878
91 -58.67640940845013 -5.125002219072155
92 -79.03473842144012 -5.027957977402961
93 -63.95322918891907 -4.8862609534974535
94 -65.48372294008732 -4.827572916892203
95 -65.79787570238113 -4.693622885576568
96 -57.30669150874019 -4.684174080585208
97 -64.70272866636515 -4.598129780154474
98 -70.85126024484634 -4.507080795614342
99 -61.35090836882591 -4.441675674817799
100 -54.32650876045227 -4.366896951069967
101 -55.18473246693611 -4.254156394916765
102 -57.09543430805206 -4.0685647855578635
103 -55.32624101638794 -4.029043726175636
104 -54.40875631570816 -4.013381141636067
105 -57.73929017782211 -3.9501308795677033
106 -50.73566573858261 -3.790541172385446
107 -43.7653011828661 -3.6365996744825937
108 -57.40640327334404 -3.6250759257183156
109 -51.623911306262016 -3.4506544990909047
110 -51.63797365128994 -3.3844671463622564
111 -50.329145804047585 -3.346811297710865
112 -49.196954533457756 -3.3194081996857228
113 -49.432343393564224 -3.116461602948011
114 -49.32893976569176 -2.7387150511750744
115 -54.17359633743763 -2.6416623314910934
116 -40.40941569954157 -2.1389807342856253
117 -39.828888684511185 -2.0442441132914695
118 -35.02626186609268 -1.7879941944360636
119 -40.04473713040352 -1.6794510066809267
train accuracy: 0.9994444444444445
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.27957967 -12.68135973
 -12.66418206 -12.30017947 -12.15190477 -11.98389944 -11.78885214
 -11.27494741 -11.11801877 -10.8699891  -10.3276815   -9.85721598
  -9.7720712   -9.74146416  -9.72444275  -9.68952853  -9.36478387
  -9.17446256  -9.1139336   -8.89521187  -8.8785385   -8.68551512
  -8.62690943  -8.61749742  -8.56704868  -8.56256514  -8.53973786
  -8.49091398  -8.330117    -8.27677489  -8.27510634  -8.26704963
  -8.23937512  -8.22175307  -8.19850526  -8.16621769  -8.13319584
  -8.1168431   -8.10819769  -8.10547793  -8.05253613  -8.03779943
  -7.99327536  -7.93802161  -7.84382527  -7.80208868  -7.73509476
  -7.72997027  -7.72795966  -7.64301755  -7.57539849  -7.56791213
  -7.51801355  -7.51362985  -7.51214588  -7.48447793  -7.47105633
  -7.46574771  -7.41328518  -7.36917692  -7.36244313  -7.32542828
  -7.25639815  -7.20727587  -7.17984693  -7.17164584  -7.10832736
  -7.05559771  -7.00610293  -6.98016532  -6.97755703  -6.97360612
  -6.96433162  -6.95906356  -6.92631889  -6.92397264  -6.89322056
  -6.86246718  -6.8583624   -6.85054872  -6.84697174  -6.83778929
  -6.77694649  -6.72206384  -6.71997062  -6.68648227  -6.61460987
  -6.59277381  -6.56603026  -6.55735147  -6.54176568  -6.53544734
  -6.51820418  -6.47179286  -6.36649112  -6.33271049  -6.30394682
  -6.30065638  -6.30050379  -6.27109572  -6.11071342  -6.07813013
  -6.06942898  -6.01474705  -5.72427553  -5.71443789  -5.69109376
  -5.63547795  -5.61964966  -5.61863421  -5.61579673  -5.59698209
  -5.52294917  -5.47572888  -5.46244731  -5.45414406  -5.45033634
  -5.37711661  -5.3472021   -5.30416879  -5.28748613  -5.20169858
  -5.19697957  -5.12500222  -5.07848501  -5.07846038  -5.03919135
  -5.02795798  -5.01103558  -4.99684237  -4.99452028  -4.88626095
  -4.87298764  -4.84190182  -4.82757292  -4.74732721  -4.73042961
  -4.69362289  -4.69279796  -4.68417408  -4.6802591   -4.67870145
  -4.63049542  -4.61266982  -4.59812978  -4.55558374  -4.52383165
  -4.50773655  -4.5070808   -4.50440842  -4.49486487  -4.44167567
  -4.42388744  -4.38040165  -4.36689695  -4.32438653  -4.25415639
  -4.230832    -4.10684711  -4.07211922  -4.06856479  -4.0326245
  -4.03104862  -4.02904373  -4.0223706   -4.01470312  -4.01338114
  -3.97642389  -3.95013088  -3.8837253   -3.79054117  -3.64055388
  -3.63878996  -3.63659967  -3.63007491  -3.62507593  -3.59305256
  -3.55063774  -3.50064848  -3.4506545   -3.42389989  -3.41307893
  -3.38872191  -3.3867875   -3.38446715  -3.3553423   -3.3468113
  -3.3322555   -3.3194082   -3.2502223   -3.24316662  -3.21614653
  -3.20672797  -3.1164616   -3.06518528  -2.79964746  -2.73871505
  -2.7090979   -2.67805979  -2.64166233  -2.58476163  -2.42617852
  -2.40071723  -2.13898073  -2.11290192  -2.06325843  -2.04424411
  -1.91361965  -1.78799419  -1.73565254  -1.7046524   -1.67945101]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.006760466899006019, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0464,  0.0613, -0.0692,  0.1155,  0.3894, -0.0989,  0.0026, -0.0355,
         -0.0396, -0.1622, -0.0024, -1.1244, -1.4494]], device='cuda:0'))])
end of epoch 1: val_loss 2.029991721862832, val_acc 0.855
trigger times: 1
end of epoch 2: val_loss 0.026879119226722707, val_acc 0.99
trigger times: 2
end of epoch 3: val_loss 1.2220725591043737, val_acc 0.915
trigger times: 3
end of epoch 4: val_loss 0.04917432275348972, val_acc 0.99
trigger times: 4
end of epoch 5: val_loss 0.20169661029616004, val_acc 0.965
trigger times: 5
end of epoch 6: val_loss 0.19983901825238015, val_acc 0.975
trigger times: 6
end of epoch 7: val_loss 0.11126349758258673, val_acc 0.99
trigger times: 7
end of epoch 8: val_loss 0.02119720497685993, val_acc 0.995
trigger times: 8
end of epoch 9: val_loss 0.1250315001695154, val_acc 0.98
trigger times: 9
end of epoch 10: val_loss 0.04944852259135448, val_acc 0.995
trigger times: 10
Early stopping.
0 -320.6452877521515 -54.98547503240923
1 -321.4891058206558 -49.75347184620696
2 -368.3850598335266 -46.98011874490918
3 -311.26071524620056 -44.99030608142343
4 -292.4642541408539 -43.81326882122305
5 -270.98924708366394 -42.00401746161006
6 -250.02096897363663 -41.68588229294918
7 -233.60884350538254 -40.34838365523108
8 -243.3944013118744 -39.57586365327889
9 -256.8648912906647 -38.45534493538269
10 -276.4621196985245 -38.35634328077039
11 -240.1229237318039 -37.66475323879293
12 -199.217147231102 -37.1809993033689
13 -244.86012148857117 -36.821916772458344
14 -204.21394205093384 -36.20965269874363
15 -195.62261080741882 -35.78149902167743
16 -207.3571519255638 -35.26282499693737
17 -220.3562649488449 -35.0654408505187
18 -229.83421152830124 -34.64469044638467
19 -244.76715779304504 -31.969099402548657
20 -200.81521654129028 -31.64414355845032
21 -197.36161398887634 -31.12953085092458
22 -183.80079990625381 -29.340125609942326
23 -191.21504998207092 -27.343722362182305
24 -166.23292818665504 -27.07399028854534
25 -178.1268618106842 -25.548365085275513
26 -175.3522863984108 -24.879106999799365
27 -192.23841512203217 -23.978745577896312
28 -177.11803126335144 -23.44970807952351
29 -151.3364850282669 -22.19891031871716
30 -174.6471085846424 -20.444472560731253
31 -140.35322242975235 -19.63760343800059
32 -140.9699912071228 -18.92838809611677
33 -121.72467112541199 -16.823073927842348
34 -124.57424485683441 -14.531424598833084
35 -106.72898936271667 -13.27957966807184
36 -109.59088732302189 -12.66418205637357
37 -89.00701314210892 -11.983899438286592
38 -92.2949076294899 -11.27494740992659
39 -96.41613060235977 -10.327681503524177
40 -105.6107314825058 -9.741464159424263
41 -100.69676524400711 -9.689528533421145
42 -75.56062012910843 -9.113933599858365
43 -90.63613450527191 -8.878538503946551
44 -57.734606601297855 -8.617497422248727
45 -79.81729942560196 -8.562565135830942
46 -83.02418091520667 -8.330116995310416
47 -88.1995620727539 -8.275106338530883
48 -68.53971773386002 -8.221753066471264
49 -64.60734951496124 -8.166217686314063
50 -80.9514218121767 -8.108197691178031
51 -87.13448512554169 -8.0525361308728
52 -82.78588509559631 -7.938021609437681
53 -59.97034329175949 -7.80208867511953
54 -81.31716549396515 -7.727959660645908
55 -64.28565867245197 -7.57539849177145
56 -83.15791362524033 -7.513629847611782
57 -60.7416310608387 -7.484477931873069
58 -83.52421423792839 -7.413285178359452
59 -63.178008407354355 -7.362443126623615
60 -60.320305705070496 -7.207275874555519
61 -56.709553837776184 -7.171645844959185
62 -78.05809772014618 -7.006102928366366
63 -53.03165930509567 -6.977557034812622
64 -69.77335511147976 -6.959063561385431
65 -55.218434780836105 -6.923972641968443
66 -52.403478160500526 -6.85836239896135
67 -74.00334349274635 -6.846971739801642
68 -57.323447776958346 -6.7220638398623045
69 -71.94289973378181 -6.686482268732197
70 -74.34039562940598 -6.566030258580594
71 -59.72562825679779 -6.541765681017651
72 -75.45863270759583 -6.471792857493011
73 -59.961777448654175 -6.3327104915505
74 -65.21777653694153 -6.300503794045418
75 -72.40218472480774 -6.1107134233195035
76 -52.958578020334244 -6.014747046722244
77 -58.83383825421333 -5.71443788791496
78 -54.74677041172981 -5.6196496579404664
79 -68.45856410264969 -5.615796733870542
80 -47.63337557017803 -5.475728882640741
81 -48.56090095639229 -5.450336337955995
82 -70.5780371427536 -5.34720210027791
83 -54.95801405608654 -5.201698579635878
84 -50.98784485459328 -5.125002219072155
85 -55.80672052502632 -5.039191353450661
86 -44.55721986293793 -5.011035578260587
87 -48.08235287666321 -4.8862609534974535
88 -56.34155961871147 -4.841901817924678
89 -49.15049351751804 -4.730429610144186
90 -52.32117635011673 -4.69279796051363
91 -53.819928139448166 -4.67870144532712
92 -40.820677287876606 -4.612669822135936
93 -47.083164162933826 -4.523831652896331
94 -45.58766946196556 -4.507080795614342
95 -37.26274534314871 -4.441675674817799
96 -58.56727936863899 -4.380401647675599
97 -41.140053510665894 -4.254156394916765
98 -49.8328338265419 -4.106847105598596
99 -42.146278858184814 -4.032624496447929
100 -37.31108517199755 -4.029043726175636
101 -45.04385028406978 -4.013381141636067
102 -48.44236673414707 -3.9501308795677033
103 -46.42117549479008 -3.6405538771176604
104 -39.57425528764725 -3.6365996744825937
105 -34.62435655295849 -3.5930525582292385
106 -35.06328284740448 -3.500648479903634
107 -44.01995447278023 -3.4130789257109297
108 -36.44467847049236 -3.3867874968517158
109 -40.03469881415367 -3.346811297710865
110 -38.227987200021744 -3.3194081996857228
111 -31.43220455944538 -3.2161465333163797
112 -36.12467880547047 -3.116461602948011
113 -30.28796398639679 -2.7387150511750744
114 -27.12127172201872 -2.678059789619858
115 -31.918497651815414 -2.426178523135533
116 -23.39650347083807 -2.1389807342856253
117 -24.690165616571903 -2.0442441132914695
118 -22.414456486701965 -1.7879941944360636
119 -23.76481020450592 -1.6794510066809267
train accuracy: 0.9888888888888889
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -14.30740779 -13.98079107 -13.59601285
 -13.32236484 -13.27957967 -13.27611169 -12.94828623 -12.68135973
 -12.66418206 -12.5846847  -12.30017947 -12.15190477 -11.98389944
 -11.78885214 -11.56099054 -11.27494741 -11.11801877 -10.97568411
 -10.8699891  -10.66894218 -10.3276815  -10.07945113  -9.9893235
  -9.85721598  -9.7720712   -9.75853723  -9.74146416  -9.72444275
  -9.68952853  -9.58221897  -9.39090569  -9.36478387  -9.28894947
  -9.17446256  -9.17134159  -9.1417539   -9.1139336   -8.89521187
  -8.8785385   -8.74416514  -8.68551512  -8.62690943  -8.61749742
  -8.56704868  -8.56256514  -8.53973786  -8.49091398  -8.40922415
  -8.35438131  -8.330117    -8.27677489  -8.27510634  -8.26704963
  -8.23937512  -8.22175307  -8.19850526  -8.17958458  -8.16621769
  -8.14611302  -8.13319584  -8.1168431   -8.10819769  -8.10547793
  -8.05488213  -8.05253613  -8.03779943  -8.00877278  -7.99614347
  -7.99327536  -7.93802161  -7.90063487  -7.88009553  -7.84382527
  -7.80208868  -7.73509476  -7.72997027  -7.72795966  -7.64301755
  -7.57539849  -7.56791213  -7.51801355  -7.51791934  -7.51362985
  -7.51214588  -7.48447793  -7.47105633  -7.46574771  -7.41328518
  -7.40174564  -7.36917692  -7.36244313  -7.32542828  -7.32399395
  -7.25639815  -7.21109559  -7.20727587  -7.17984693  -7.17164584
  -7.10832736  -7.06107049  -7.05559771  -7.00610293  -7.00014171
  -6.98016532  -6.97755703  -6.97360612  -6.96433162  -6.95906356
  -6.9565734   -6.92631889  -6.92397264  -6.89322056  -6.88617181
  -6.86246718  -6.8583624   -6.85054872  -6.84697174  -6.83778929
  -6.77694649  -6.72206384  -6.71997062  -6.71119111  -6.6932218
  -6.68648227  -6.64489957  -6.61460987  -6.59277381  -6.57063187
  -6.56603026  -6.55735147  -6.54201326  -6.54176568  -6.53544734
  -6.51820418  -6.47179286  -6.43394391  -6.41934922  -6.36649112
  -6.33271049  -6.30394682  -6.30065638  -6.30050379  -6.27109572
  -6.11071342  -6.07813013  -6.06942898  -6.01474705  -5.83793383
  -5.72427553  -5.71443789  -5.69109376  -5.67370728  -5.66083708
  -5.63547795  -5.61964966  -5.61863421  -5.61579673  -5.59698209
  -5.53497311  -5.52294917  -5.47572888  -5.46244731  -5.45414406
  -5.45033634  -5.38905881  -5.37711661  -5.3472021   -5.30416879
  -5.28748613  -5.26790148  -5.20169858  -5.19697957  -5.12500222
  -5.07848501  -5.07846038  -5.03919135  -5.02795798  -5.01103558
  -4.99684237  -4.99452028  -4.90498275  -4.88626095  -4.87298764
  -4.85587292  -4.84190182  -4.82757292  -4.78389056  -4.74732721
  -4.73042961  -4.69362289  -4.69279796  -4.68417408  -4.6802591
  -4.67870145  -4.63049542  -4.61266982  -4.59812978  -4.55558374
  -4.52383165  -4.50773655  -4.5070808   -4.50440842  -4.49486487
  -4.48872674  -4.44167567  -4.42388744  -4.40486459  -4.38653872
  -4.38040165  -4.36689695  -4.32438653  -4.25415639  -4.230832
  -4.1559661   -4.10684711  -4.07211922  -4.06856479  -4.0326245
  -4.03104862  -4.02904373  -4.0223706   -4.01470312  -4.01338114
  -3.97642389  -3.95013088  -3.8837253   -3.85431737  -3.83164007
  -3.79054117  -3.64055388  -3.63878996  -3.63659967  -3.63007491
  -3.62507593  -3.59305256  -3.55063774  -3.50064848  -3.4506545
  -3.42389989  -3.41307893  -3.38872191  -3.3867875   -3.38446715
  -3.3553423   -3.3468113   -3.3322555   -3.3194082   -3.2502223
  -3.24316662  -3.21614653  -3.20672797  -3.14971117  -3.1164616
  -3.06518528  -2.79964746  -2.73871505  -2.7090979   -2.67805979
  -2.64166233  -2.62784185  -2.58476163  -2.42617852  -2.40071723
  -2.13898073  -2.11290192  -2.06325843  -2.04424411  -1.91361965
  -1.89604995  -1.78799419  -1.73565254  -1.7046524   -1.67945101]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.06570308478917788, val_acc 0.975
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0934, -0.0196,  0.1040,  0.0143,  0.4043,  0.0050,  0.0079, -0.0215,
         -0.1452,  0.0666, -0.0024, -1.6203, -1.6250]], device='cuda:0'))])
end of epoch 1: val_loss 0.06863290567402856, val_acc 0.985
trigger times: 1
end of epoch 2: val_loss 0.022238755636684503, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.4804e-02,  1.1409e-01,  2.6777e-02,  3.1847e-03,  5.8435e-01,
         -1.1353e-01,  1.8958e-03, -2.2195e-02, -1.4170e-01,  3.7209e-02,
         -2.5089e-03, -3.2547e+00, -3.3766e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.007750136247102333, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.7172e-01,  1.2747e-01,  1.5265e-01, -2.3880e-01,  5.0959e-01,
          2.7426e-02, -3.2782e-02, -3.8302e-02, -2.8636e-01,  8.5177e-02,
          7.5657e-06, -3.9744e+00, -4.3831e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.028225837585984372, val_acc 0.995
trigger times: 1
end of epoch 5: val_loss 0.04931379733018549, val_acc 0.995
trigger times: 2
end of epoch 6: val_loss 2.618924233012981, val_acc 0.885
trigger times: 3
end of epoch 7: val_loss 0.12813939030409827, val_acc 0.98
trigger times: 4
end of epoch 8: val_loss 0.28996045565791856, val_acc 0.985
trigger times: 5
end of epoch 9: val_loss 0.17371275637456116, val_acc 0.97
trigger times: 6
end of epoch 10: val_loss 0.09404537821932375, val_acc 0.96
trigger times: 7
end of epoch 11: val_loss 0.043944929619025005, val_acc 0.995
trigger times: 8
end of epoch 12: val_loss 0.0726968110824437, val_acc 0.985
trigger times: 9
end of epoch 13: val_loss 0.08462507642405744, val_acc 0.99
trigger times: 10
Early stopping.
0 -345.48427963256836 -54.98547503240923
1 -325.00924921035767 -49.75347184620696
2 -285.7963545322418 -45.7351542845057
3 -270.3993890285492 -44.14602409201361
4 -248.5787924528122 -42.29180714825394
5 -267.1684089899063 -41.68588229294918
6 -244.88304889202118 -40.34838365523108
7 -251.63566410541534 -39.31972693233231
8 -243.8302105665207 -38.41270390343083
9 -252.7567402124405 -37.741528994987384
10 -232.1910139322281 -37.1809993033689
11 -245.68559056520462 -36.821916772458344
12 -233.8150903582573 -36.19207561676116
13 -244.67227268218994 -35.394503873250635
14 -217.29064847528934 -35.209705244501436
15 -225.51513767242432 -34.64469044638467
16 -240.26374530792236 -31.969099402548657
17 -204.60337680578232 -31.392382758954444
18 -223.61506688594818 -29.39157139549552
19 -183.5778512954712 -27.41102349748205
20 -192.83644765615463 -27.07399028854534
21 -173.20306074619293 -25.548365085275513
22 -173.31672099232674 -24.828695359328833
23 -169.71711707115173 -23.57262108435893
24 -165.04883992671967 -22.60679894414887
25 -178.89676374197006 -20.444472560731253
26 -142.35882505774498 -19.63760343800059
27 -156.362606883049 -17.994774057192853
28 -117.01476827263832 -14.855082803515382
29 -128.25568795204163 -14.307407786376809
30 -110.8179874420166 -13.27957966807184
31 -106.28389921784401 -12.68135972540495
32 -95.48843115568161 -12.30017947419658
33 -105.29909390211105 -11.788852141676486
34 -93.41939491033554 -11.118018771528469
35 -100.30404448509216 -10.668942180555746
36 -98.35362535715103 -9.98932350392634
37 -99.18144315481186 -9.758537230937849
38 -91.64365482330322 -9.689528533421145
39 -86.89056396484375 -9.36478386840345
40 -90.40057373046875 -9.171341593241639
41 -80.41674035787582 -8.895211870845777
42 -90.95156806707382 -8.685515121640453
43 -86.83080250024796 -8.56704868271876
44 -87.1618801355362 -8.490913981197433
45 -74.34618546068668 -8.330116995310416
46 -85.38473308086395 -8.267049625280945
47 -87.37562608718872 -8.198505263733932
48 -85.96247577667236 -8.146113022091109
49 -67.98271331191063 -8.108197691178031
50 -86.58165222406387 -8.0525361308728
51 -85.13692367076874 -7.9961434726278915
52 -68.367646753788 -7.900634873441145
53 -68.38963282108307 -7.80208867511953
54 -81.40026807785034 -7.727959660645908
55 -67.10788154602051 -7.5679121290375235
56 -84.86004841327667 -7.513629847611782
57 -70.5661871433258 -7.471056333863592
58 -65.99630081653595 -7.401745641577853
59 -80.15024095773697 -7.3254282803317174
60 -77.79791671037674 -7.211095592761851
61 -63.07536491751671 -7.171645844959185
62 -61.41826060414314 -7.055597710392153
63 -76.62940749526024 -6.980165315997038
64 -75.46878418326378 -6.964331615269467
65 -71.59410375356674 -6.926318887747202
66 -64.37337946891785 -6.886171805780251
67 -74.98723065853119 -6.850548715270908
68 -89.56671744585037 -6.776946485018116
69 -71.75370705127716 -6.711191114472605
70 -57.42594122886658 -6.644899568912027
71 -74.75543594360352 -6.570631865418147
72 -73.02798819541931 -6.542013262943102
73 -73.11660192906857 -6.51820418055673
74 -80.06592348217964 -6.419349221561011
75 -71.07926326990128 -6.303946824004978
76 -56.25331696867943 -6.271095719584616
77 -58.00503946095705 -6.069428977826416
78 -62.96644926071167 -5.724275531291021
79 -49.58831104636192 -5.673707280477651
80 -59.43592977523804 -5.6196496579404664
81 -67.15828168392181 -5.596982086500049
82 -55.67733508348465 -5.475728882640741
83 -53.993818789720535 -5.450336337955995
84 -69.02237167954445 -5.34720210027791
85 -48.60675406455994 -5.267901484025481
86 -54.86148704588413 -5.125002219072155
87 -47.46386995911598 -5.039191353450661
88 -51.09243731945753 -4.996842367348108
89 -37.3493382409215 -4.8862609534974535
90 -61.65365535020828 -4.827572916892203
91 -46.037819266319275 -4.730429610144186
92 -45.0771614164114 -4.684174080585208
93 -48.8376876488328 -4.63049541560991
94 -48.00974501669407 -4.555583735607753
95 -38.65882566571236 -4.507080795614342
96 -40.165475487709045 -4.488726742478586
97 -44.01989126205444 -4.404864589517859
98 -32.288600236177444 -4.366896951069967
99 -56.52280700951815 -4.230832004686763
100 -37.42578284442425 -4.072119220344256
101 -53.739001750946045 -4.031048624093466
102 -45.33274545520544 -4.014703118649953
103 -46.713038586080074 -3.9501308795677033
104 -41.32275569438934 -3.8316400734885523
105 -45.015965819358826 -3.6387899596012976
106 -29.624604545533657 -3.6250759257183156
107 -44.28376108407974 -3.500648479903634
108 -33.992661245167255 -3.4130789257109297
109 -40.959646955132484 -3.3844671463622564
110 -43.55163814127445 -3.3322555012187633
111 -31.123790055513382 -3.243166617301607
112 -29.57903888821602 -3.1497111726973572
113 -24.16972427070141 -2.799647459998785
114 -31.224091045558453 -2.678059789619858
115 -24.914741292595863 -2.5847616338069854
116 -27.230586923658848 -2.1389807342856253
117 -14.264092087745667 -2.0442441132914695
118 -27.801798455417156 -1.7879941944360636
119 -30.18134894222021 -1.6794510066809267
train accuracy: 0.9972222222222222
validation accuracy: 0.99
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -14.30740779 -13.98079107 -13.7294841
 -13.59601285 -13.5497249  -13.32236484 -13.27957967 -13.27611169
 -12.94828623 -12.68135973 -12.66418206 -12.5846847  -12.30017947
 -12.15190477 -11.98389944 -11.78885214 -11.56099054 -11.27494741
 -11.11801877 -10.97568411 -10.8699891  -10.66894218 -10.62169867
 -10.3276815  -10.07945113  -9.9893235   -9.85721598  -9.81687975
  -9.7720712   -9.75853723  -9.74146416  -9.72444275  -9.68952853
  -9.58221897  -9.52565215  -9.39090569  -9.36478387  -9.28894947
  -9.20564923  -9.17702082  -9.17446256  -9.17134159  -9.1417539
  -9.1139336   -8.89521187  -8.88065596  -8.8785385   -8.74416514
  -8.68551512  -8.62690943  -8.61749742  -8.56704868  -8.56256514
  -8.53973786  -8.49091398  -8.43281444  -8.40922415  -8.35438131
  -8.330117    -8.27677489  -8.27510634  -8.26704963  -8.23937512
  -8.22175307  -8.19850526  -8.19172764  -8.17958458  -8.16621769
  -8.14611302  -8.13319584  -8.1168431   -8.10819769  -8.10547793
  -8.0645313   -8.05488213  -8.05253613  -8.03779943  -8.00877278
  -7.99614347  -7.99327536  -7.93802161  -7.90063487  -7.88009553
  -7.84382527  -7.8033206   -7.80208868  -7.7958072   -7.76372701
  -7.73509476  -7.72997027  -7.72795966  -7.69914197  -7.64301755
  -7.62292288  -7.57539849  -7.56791213  -7.51801355  -7.51791934
  -7.51362985  -7.51214588  -7.48447793  -7.47105633  -7.46574771
  -7.41328518  -7.40174564  -7.38652333  -7.36917692  -7.36244313
  -7.35235199  -7.32542828  -7.32399395  -7.25639815  -7.25252466
  -7.21293053  -7.21109559  -7.20727587  -7.17984693  -7.17164584
  -7.10832736  -7.06107049  -7.05559771  -7.00610293  -7.00014171
  -6.98016532  -6.97755703  -6.97360612  -6.96433162  -6.95906356
  -6.9565734   -6.92631889  -6.92397264  -6.89322056  -6.88617181
  -6.86246718  -6.8583624   -6.85054872  -6.84697174  -6.83778929
  -6.77694649  -6.72206384  -6.71997062  -6.71119111  -6.6932218
  -6.68648227  -6.64489957  -6.61460987  -6.59277381  -6.57063187
  -6.56603026  -6.55735147  -6.54201326  -6.54176568  -6.53544734
  -6.51820418  -6.51013379  -6.47179286  -6.45691417  -6.43394391
  -6.4255869   -6.41934922  -6.36649112  -6.33310165  -6.33271049
  -6.30394682  -6.30065638  -6.30050379  -6.27109572  -6.182618
  -6.16905249  -6.16314869  -6.11071342  -6.07813013  -6.06942898
  -6.01561484  -6.01474705  -5.83793383  -5.81785746  -5.72427553
  -5.71443789  -5.69109376  -5.67370728  -5.66083708  -5.63547795
  -5.61964966  -5.61863421  -5.61579673  -5.59698209  -5.54772779
  -5.53497311  -5.52294917  -5.51259143  -5.47572888  -5.46244731
  -5.45414406  -5.45033634  -5.38905881  -5.37711661  -5.3472021
  -5.30416879  -5.28748613  -5.26790148  -5.20169858  -5.19697957
  -5.18624911  -5.12500222  -5.09695296  -5.07848501  -5.07846038
  -5.05451785  -5.03919135  -5.02795798  -5.01103558  -4.99684237
  -4.99452028  -4.985381    -4.92775592  -4.90498275  -4.88626095
  -4.87298764  -4.85587292  -4.84190182  -4.82757292  -4.78389056
  -4.74732721  -4.73042961  -4.73033883  -4.69362289  -4.69279796
  -4.68417408  -4.6802591   -4.67870145  -4.63049542  -4.61266982
  -4.60100509  -4.59812978  -4.56143634  -4.55558374  -4.54919962
  -4.52383165  -4.50773655  -4.5070808   -4.50440842  -4.49486487
  -4.48872674  -4.44391584  -4.44167567  -4.42388744  -4.4202191
  -4.40486459  -4.38653872  -4.38040165  -4.36689695  -4.32438653
  -4.298138    -4.25415639  -4.230832    -4.20845173  -4.1559661
  -4.10684711  -4.10208321  -4.07211922  -4.06856479  -4.0326245
  -4.03104862  -4.02904373  -4.0223706   -4.01470312  -4.01338114
  -3.97642389  -3.95013088  -3.90533926  -3.8837253   -3.85431737
  -3.83164007  -3.80967039  -3.79054117  -3.6858714   -3.64055388
  -3.63878996  -3.63659967  -3.63007491  -3.62507593  -3.60112417
  -3.59305256  -3.56658197  -3.55063774  -3.50997391  -3.50064848
  -3.4506545   -3.42389989  -3.41307893  -3.38872191  -3.3867875
  -3.38446715  -3.3553423   -3.3468113   -3.3322555   -3.3194082
  -3.2502223   -3.24316662  -3.21614653  -3.20672797  -3.14971117
  -3.1164616   -3.10852399  -3.0916569   -3.06518528  -2.98697144
  -2.96539153  -2.93571892  -2.79964746  -2.73871505  -2.7090979
  -2.67805979  -2.64166233  -2.62784185  -2.58476163  -2.42617852
  -2.40071723  -2.38182781  -2.17937375  -2.13898073  -2.11290192
  -2.06325843  -2.04424411  -1.91361965  -1.89604995  -1.78799419
  -1.73565254  -1.7046524   -1.67945101  -1.67232216  -1.66923331]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.08347949199623841, val_acc 0.97
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0370,  0.0286,  0.0160, -0.0822,  0.1146, -0.2279,  0.0303,  0.0199,
          0.0806,  0.0085, -0.0024, -1.5315, -1.4904]], device='cuda:0'))])
end of epoch 1: val_loss 0.10850461511090603, val_acc 0.97
trigger times: 1
end of epoch 2: val_loss 0.08331554459709746, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2097e-01,  1.1325e-01,  8.3127e-03, -4.1649e-02,  4.1187e-01,
         -1.2699e-01,  3.5454e-02,  1.3301e-02, -2.6042e-01,  7.7228e-02,
         -2.5089e-03, -3.0380e+00, -3.0784e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.06350669042092293, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.8141e-01,  1.2343e-01,  5.9472e-02,  6.4103e-02,  6.2447e-01,
         -7.2438e-02, -2.5174e-02,  3.3629e-02, -1.7632e-01,  7.3515e-02,
          7.5657e-06, -3.8133e+00, -3.8038e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.0015365745917971197, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1667e-02, -5.0172e-03, -4.0122e-02, -8.3352e-03,  4.1622e-01,
         -2.1521e-01, -1.3197e-06,  3.3137e-02, -6.0430e-02,  1.0892e-01,
         -2.2278e-05, -4.0952e+00, -4.2268e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.1881803961450287, val_acc 0.985
trigger times: 1
end of epoch 6: val_loss 0.036546560782611766, val_acc 0.995
trigger times: 2
end of epoch 7: val_loss 0.0013381330261519509, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3549e-01,  1.0291e-01, -1.1579e-02, -4.2724e-03,  2.2990e-01,
         -2.8148e-05,  2.8173e-02,  7.0146e-03,  6.0173e-02, -3.7326e-06,
          3.5488e-04, -5.2082e+00, -5.1224e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.07211860389602912, val_acc 0.99
trigger times: 1
end of epoch 9: val_loss 0.04682443583548917, val_acc 0.99
trigger times: 2
end of epoch 10: val_loss 0.06081386573928594, val_acc 0.99
trigger times: 3
end of epoch 11: val_loss 0.6082852312647069, val_acc 0.93
trigger times: 4
end of epoch 12: val_loss 0.022156615215727413, val_acc 0.995
trigger times: 5
end of epoch 13: val_loss 0.08927459020450063, val_acc 0.985
trigger times: 6
end of epoch 14: val_loss 0.002749396089131473, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.03819702032178988, val_acc 0.995
trigger times: 8
end of epoch 16: val_loss 0.04063104710157177, val_acc 0.995
trigger times: 9
end of epoch 17: val_loss 0.0010771874523478786, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.5131e-02,  1.2770e-02, -3.8200e-06, -1.8171e-02,  2.0935e-01,
         -1.2780e-02,  5.7843e-03,  3.7677e-02, -3.8372e-05,  4.7536e-05,
         -1.2006e-03, -6.3586e+00, -5.7442e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.017617683991993368, val_acc 0.995
trigger times: 1
end of epoch 19: val_loss 0.009616327747379217, val_acc 0.995
trigger times: 2
end of epoch 20: val_loss 0.2658177209735958, val_acc 0.975
trigger times: 3
end of epoch 21: val_loss 0.0729497783967923, val_acc 0.99
trigger times: 4
end of epoch 22: val_loss 0.1012750453594742, val_acc 0.99
trigger times: 5
end of epoch 23: val_loss 0.4058081115942775, val_acc 0.96
trigger times: 6
end of epoch 24: val_loss 0.0024787228200301215, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.08674989034006898, val_acc 0.99
trigger times: 8
end of epoch 26: val_loss 0.013160872906075198, val_acc 0.995
trigger times: 9
end of epoch 27: val_loss 0.0011646386876868675, val_acc 1.0
trigger times: 10
Early stopping.
0 -374.3475031852722 -54.98547503240923
1 -357.51697063446045 -49.72654640753777
2 -332.38013756275177 -45.670579884154705
3 -315.9809675216675 -43.18878399086166
4 -306.9326366186142 -41.6910044370425
5 -296.0203380584717 -40.34838365523108
6 -307.79979062080383 -39.31972693233231
7 -299.94008207321167 -38.35634328077039
8 -291.07242822647095 -37.66475323879293
9 -274.2719017267227 -37.00630588930485
10 -270.24069571495056 -36.20965269874363
11 -274.2879614830017 -35.394503873250635
12 -268.3650789260864 -35.209705244501436
13 -262.33258533477783 -33.84284985953318
14 -268.1463063955307 -31.7109134007892
15 -245.00101172924042 -31.12953085092458
16 -236.030051112175 -29.106189988903285
17 -221.60098779201508 -27.07399028854534
18 -219.4051455259323 -25.548365085275513
19 -209.99369913339615 -24.592745144504722
20 -193.63665235042572 -23.44970807952351
21 -178.01689285039902 -20.656863763892378
22 -182.85236984491348 -20.13839114930498
23 -181.59223747253418 -17.994774057192853
24 -136.89752762019634 -14.531424598833084
25 -118.23755121231079 -13.980791067329035
26 -113.37772810459137 -13.322364840633728
27 -115.88216704130173 -12.94828622921836
28 -133.1500158905983 -12.30017947419658
29 -129.6222179532051 -11.788852141676486
30 -100.83973896503448 -10.975684105592801
31 -110.55529153347015 -10.621698671550895
32 -97.8638055473566 -9.8572159761571
33 -94.89889591932297 -9.758537230937849
34 -92.11456775665283 -9.582218974836827
35 -92.84072226285934 -9.36478386840345
36 -93.8414615392685 -9.174462558852495
37 -95.74587738513947 -9.113933599858365
38 -89.48896443843842 -8.744165138630299
39 -93.37483417987823 -8.617497422248727
40 -84.28066211938858 -8.490913981197433
41 -79.35171008110046 -8.354381310031117
42 -90.39011210203171 -8.267049625280945
43 -89.06753152608871 -8.198505263733932
44 -84.23418414592743 -8.146113022091109
45 -98.02712374925613 -8.108197691178031
46 -82.36595407128334 -8.0525361308728
47 -82.3822731077671 -7.9961434726278915
48 -84.132253408432 -7.88009552712815
49 -86.19930273294449 -7.795807196607801
50 -81.81146395206451 -7.729970270096379
51 -81.98778772354126 -7.622922879913561
52 -77.1838826239109 -7.518013546198161
53 -82.39081835746765 -7.484477931873069
54 -79.92629635334015 -7.413285178359452
55 -94.62124541401863 -7.362443126623615
56 -81.67647737264633 -7.323993950203962
57 -75.1141714155674 -7.211095592761851
58 -83.93431997299194 -7.171645844959185
59 -78.39929541945457 -7.006102928366366
60 -80.85094779729843 -6.977557034812622
61 -77.00429677963257 -6.956573401660352
62 -68.91677878051996 -6.893220555868631
63 -80.83214348554611 -6.850548715270908
64 -89.84932008385658 -6.776946485018116
65 -70.71839767694473 -6.693221800935831
66 -73.60399693250656 -6.614609866811899
67 -79.02187073230743 -6.557351467475899
68 -95.82056790590286 -6.535447341844848
69 -64.54215285181999 -6.456914167352002
70 -73.56821617484093 -6.419349221561011
71 -72.97194504737854 -6.303946824004978
72 -75.55934125185013 -6.182618004272912
73 -70.69127359241247 -6.1107134233195035
74 -74.74319651722908 -6.014747046722244
75 -62.9192715883255 -5.724275531291021
76 -60.34201246500015 -5.660837083005688
77 -69.491609364748 -5.618634210290465
78 -58.9309196472168 -5.534973109746161
79 -66.72296661138535 -5.475728882640741
80 -58.35562062263489 -5.38905880861588
81 -64.5719211101532 -5.30416879324244
82 -63.95570087432861 -5.196979565892167
83 -62.24581295251846 -5.096952959931472
84 -61.37320926785469 -5.039191353450661
85 -67.94117552042007 -4.996842367348108
86 -54.08903741836548 -4.904982746844229
87 -59.73708888888359 -4.855872917390202
88 -57.48541381955147 -4.747327209165597
89 -63.66164588928223 -4.693622885576568
90 -66.0377885401249 -4.67870144532712
91 -65.73301902413368 -4.601005090750844
92 -60.56827561557293 -4.549199624582181
93 -63.4890960752964 -4.507080795614342
94 -58.703455694019794 -4.443915836036552
95 -57.969075202941895 -4.420219100911836
96 -56.48842567205429 -4.366896951069967
97 -62.28922852873802 -4.230832004686763
98 -55.55498865246773 -4.106847105598596
99 -61.140428483486176 -4.032624496447929
100 -54.736966758966446 -4.022370601756242
101 -55.079838417470455 -3.9501308795677033
102 -54.5938346683979 -3.8543173696780495
103 -55.30623114109039 -3.6858714024429995
104 -49.15852192044258 -3.6365996744825937
105 -50.715631663799286 -3.5930525582292385
106 -53.54228928685188 -3.5099739135859074
107 -54.59120982885361 -3.4130789257109297
108 -62.69120346009731 -3.3844671463622564
109 -49.37903618812561 -3.3194081996857228
110 -47.68243233859539 -3.2161465333163797
111 -50.23316967487335 -3.1085239932111026
112 -49.37956760823727 -2.9869714437286654
113 -46.19345957040787 -2.7387150511750744
114 -59.303665816783905 -2.6416623314910934
115 -43.74613180756569 -2.4007172255990907
116 -37.99746749550104 -2.1389807342856253
117 -45.081780165433884 -1.9136196540088464
118 -36.07498714327812 -1.7356525415070554
119 -40.237564742565155 -1.6692333060988482
train accuracy: 0.9955555555555555
validation accuracy: 1.0

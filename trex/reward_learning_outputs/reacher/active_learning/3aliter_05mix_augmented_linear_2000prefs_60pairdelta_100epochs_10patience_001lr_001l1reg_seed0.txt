[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:1
end of epoch 0: val_loss 0.021691418220066615, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0358,  0.1125,  0.0995, -0.0566, -0.3134, -0.0942,  0.0065,  0.0141,
         -0.0789, -0.0275, -0.0027, -2.5033]], device='cuda:1'))])
end of epoch 1: val_loss 0.0003465789319266577, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.1797e-03,  6.1967e-03,  8.4875e-02, -3.6170e-02, -4.8931e-01,
         -2.0435e-01,  4.4609e-03,  1.3994e-02,  1.3645e-04,  4.7824e-05,
         -2.6689e-03, -3.4714e+00]], device='cuda:1'))])
end of epoch 2: val_loss 4.4287008241852274e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.3053e-02,  4.0866e-02,  9.7865e-02, -2.2074e-02, -1.0857e+00,
         -1.3868e-05, -1.1510e-02,  1.6483e-06,  2.6683e-01, -2.4159e-01,
         -2.6690e-03, -5.4986e+00]], device='cuda:1'))])
end of epoch 3: val_loss 3.033776643231789e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.9433e-04, -2.1146e-05,  2.7781e-02, -1.2319e-05, -7.4212e-01,
         -9.2522e-06, -4.5162e-03,  6.2341e-04, -1.1822e-04,  2.9151e-05,
         -2.6691e-03, -4.9377e+00]], device='cuda:1'))])
end of epoch 4: val_loss 5.329095909374359e-05, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 1.4869784041217418, val_acc 0.92
trigger times: 2
end of epoch 6: val_loss 6.906680550784472e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.3866e-03, -5.4854e-03,  1.5044e-01, -7.2958e-03, -1.1309e+00,
         -7.1354e-02, -7.4993e-03, -1.6431e-03,  3.4451e-01, -2.3082e-03,
         -2.6696e-03, -6.2602e+00]], device='cuda:1'))])
end of epoch 7: val_loss 5.1265191677458686e-05, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.008127191393846083, val_acc 0.995
trigger times: 2
end of epoch 9: val_loss 2.9485369776551806e-05, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 5.757739548428731e-05, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.031035514385171048, val_acc 0.995
trigger times: 5
end of epoch 12: val_loss 7.706328630874282e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.7293e-02,  3.5996e-02,  1.6146e-01, -3.0822e-02, -7.8072e-01,
          1.4463e-05, -1.3734e-02,  3.6074e-02,  1.3490e-01, -1.5971e-01,
         -2.6702e-03, -7.9276e+00]], device='cuda:1'))])
end of epoch 13: val_loss 0.3229986228632364, val_acc 0.97
trigger times: 1
end of epoch 14: val_loss 2.7418111301358295e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.1691e-06, -7.0335e-02,  1.0458e-01,  2.6630e-02, -1.5216e+00,
          5.3904e-06, -4.6867e-03, -2.9250e-02,  2.6588e-01, -8.8681e-05,
         -2.6704e-03, -8.5845e+00]], device='cuda:1'))])
end of epoch 15: val_loss 2.2829892302667075e-06, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 8.922479811346307e-07, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.12537935452649993, val_acc 0.98
trigger times: 3
end of epoch 18: val_loss 5.017533198952151e-06, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 2.5693574226117732e-05, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 0.19979627770778607, val_acc 0.975
trigger times: 6
end of epoch 21: val_loss 1.1057012339108496e-05, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 0.0010615402308868482, val_acc 1.0
trigger times: 8
end of epoch 23: val_loss 2.1934119445177203e-07, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.11643600683110619, val_acc 0.97
trigger times: 10
Early stopping.
0 -110.35841935873032 -50.00222059884506
1 -74.20066410303116 -48.823739140882175
2 -34.79806822538376 -47.15605336419176
3 -54.05769902467728 -46.19619104961985
4 -102.95558071136475 -45.594227093057754
5 -87.25532799959183 -45.36842966452394
6 -74.79651939868927 -45.19068756322445
7 -67.20070487260818 -44.16490839583478
8 -46.877074867486954 -44.078303125872196
9 -62.234300673007965 -43.993555290419714
10 -45.608408480882645 -43.86306534422809
11 -65.6360633969307 -43.84874807044028
12 -90.96435660123825 -43.84199129025074
13 -37.37932100892067 -43.806385671938365
14 -37.045435696840286 -43.80581985978556
15 -75.96122032403946 -42.7594767358323
16 -35.72632819414139 -42.66316468983175
17 -77.20759701728821 -42.33177224591743
18 -64.82598704099655 -41.774963389485094
19 -97.869699716568 -41.410068073767725
20 -98.49289056658745 -41.17786296442943
21 -72.25695425271988 -40.723520424948155
22 -53.39989560842514 -40.527189756101116
23 -75.47487083077431 -40.49595848244517
24 -80.19221156835556 -40.429389880911344
25 -96.54919731616974 -40.05653450521898
26 -85.21894776821136 -39.59232357792555
27 -107.4440826177597 -39.54162101198148
28 -86.84038557112217 -39.195237471709476
29 -61.27842432260513 -39.172389579378766
30 -47.52413138747215 -38.51095963496708
31 -63.7880595177412 -38.447265769744824
32 -78.10901111364365 -38.392107037026264
33 -57.84249719977379 -38.00745349944469
34 -25.72498169541359 -37.46482488602393
35 -86.35193461179733 -37.10988160586883
36 -67.59099441766739 -34.27116723637227
37 -55.83981728553772 -34.14139118114101
38 -40.04162074625492 -33.263072731706835
39 -76.5187112390995 -33.13344797200536
40 -65.41935455799103 -33.07825234291984
41 -51.199352383613586 -33.0321314765637
42 -92.4862590432167 -32.44934973065406
43 -14.473561346530914 -32.4007978120153
44 -76.3596722483635 -32.40063925734975
45 -12.606833696365356 -30.734403792103194
46 -36.17429894208908 -30.57151371770873
47 -73.36685079336166 -30.131236504472803
48 -66.62879234552383 -29.99326722619033
49 -57.028883159160614 -29.66908258985071
50 -42.91369195282459 -29.297233511513635
51 -30.819791048765182 -29.288890423975797
52 -39.536481738090515 -29.145878352769948
53 -46.70772995054722 -28.49601894351319
54 -60.145657658576965 -28.492023661124072
55 -70.6744493842125 -28.315961465855167
56 -22.591560870409012 -27.121110566589827
57 -23.29443496465683 -26.064532595535336
58 -40.21128714084625 -25.520524278341334
59 -44.44321522116661 -25.27101421179229
60 -44.60881322622299 -25.066644278800943
61 -13.402305245399475 -24.925849381327673
62 -44.95495802164078 -24.188105669766596
63 -37.71567286550999 -23.48479966198816
64 -24.12349335476756 -23.153943559703283
65 -35.07473827898502 -22.954730295117237
66 0.4847452938556671 -22.74124885266394
67 -1.528711050748825 -22.739273544503753
68 -13.947808749973774 -22.264945050603636
69 -42.92254659533501 -22.15569724300287
70 -6.21109476685524 -21.055920928583344
71 -30.911880508065224 -20.543356562348553
72 -35.71001164615154 -20.33499633836848
73 -16.396360337734222 -20.18157658281111
74 -40.69498433172703 -19.58144410477429
75 -39.40399153530598 -19.377225745334304
76 -8.81931757926941 -19.243135617403095
77 -22.216381400823593 -19.060620225371707
78 -30.944091632962227 -18.964124524696246
79 -32.86215165257454 -18.448962308005108
80 -20.451743096113205 -17.740722019993825
81 -2.536936715245247 -16.85889369985028
82 -35.18132048845291 -16.3381194095591
83 -11.417561944574118 -14.535892564189266
84 -27.972464829683304 -14.443670567499144
85 -39.84102785587311 -14.200413010108107
86 -32.262090027332306 -13.936976181618805
87 -24.881577491760254 -13.862253042167257
88 8.52278333902359 -13.483098530680483
89 -43.357905596494675 -13.455892754889845
90 -13.952259674668312 -13.355868275096913
91 -3.8020555675029755 -12.278515244993585
92 -3.380812104791403 -12.227387460046547
93 -6.242182493209839 -12.020717825467683
94 -30.01125930249691 -11.910094799877324
95 -16.640926524996758 -11.400284019256157
96 -2.1182810217142105 -11.134618158086587
97 -10.452218655496836 -10.859166921158222
98 -1.8863982930779457 -9.595137958067907
99 -38.47541853785515 -9.289921608799773
100 -38.44802912324667 -8.230877068641124
101 -24.32340093702078 -7.882363241796725
102 -16.051625430583954 -7.6478984168416355
103 -23.028585594147444 -7.459623237418707
104 -12.859050080180168 -7.124357312750265
105 -49.747527956962585 -7.05379065585803
106 -6.306155100464821 -6.853091098326624
107 -0.4978940635919571 -6.6211384471641495
108 -48.312359631061554 -6.494555224953677
109 -7.897224977612495 -6.117354180737655
110 -24.013582669198513 -6.087055095509873
111 20.580523788928986 -5.43500831968483
112 -31.45464489609003 -5.105291741614599
113 -22.21268753707409 -4.628649413275992
114 -24.78541912138462 -4.471031187897325
115 24.755632400512695 -4.455504779070034
116 -8.323319114744663 -4.2805498188182405
117 -6.998050890862942 -3.7944735717969627
118 6.082026481628418 -2.9512438456190186
119 6.1143281906843185 -2.541618164765197
train accuracy: 0.9644444444444444
validation accuracy: 0.97
[-101.98391219 -101.67638251 -101.53078388  -96.71369094  -95.59773327
  -95.47358786  -95.19216142  -95.18309889  -94.71462103  -50.0022206
  -47.15605336  -45.59422709  -44.1649084   -44.07830313  -43.80581986
  -42.75947674  -42.66316469  -41.77496339  -41.41006807  -40.72352042
  -40.49595848  -40.42938988  -39.54162101  -39.19523747  -39.17238958
  -38.51095963  -38.44726577  -37.10988161  -34.27116724  -34.14139118
  -33.26307273  -33.07825234  -33.03213148  -32.40063926  -31.10853252
  -30.57151372  -30.1312365   -29.99326723  -29.82012805  -29.29723351
  -29.28889042  -28.31596147  -27.12111057  -26.0645326   -25.28720536
  -25.27101421  -25.06664428  -24.92584938  -24.0272807   -23.18246156
  -23.15394356  -22.98127039  -22.9547303   -22.73927354  -22.32173199
  -21.05592093  -21.05442346  -20.49365823  -20.33499634  -19.79010251
  -19.15722802  -19.03872957  -18.96412452  -18.55656311  -18.44896231
  -18.37804235  -18.36730366  -17.74072202  -17.56386718  -17.46670988
  -17.27635342  -16.8588937   -16.41305694  -16.35844702  -16.33811941
  -16.0465148   -15.77972325  -15.74025622  -15.07369212  -14.78341369
  -14.54407008  -13.48309853  -13.11987219  -12.30431037  -12.29246002
  -12.28649072  -12.27851524  -12.22738746  -12.2055475   -12.07111647
  -11.92534251  -11.9100948   -11.85551112  -11.51965918  -11.42773194
  -11.41266724  -11.38649861  -11.35531971  -11.13461816  -10.89780063
  -10.36686369  -10.28470291  -10.12784022   -9.28992161   -8.84414439
   -8.75042283   -8.67188314   -8.01575397   -7.86857138   -7.45962324
   -7.44753399   -6.8530911    -6.49914266   -6.4802261    -6.11735418
   -5.43500832   -5.10529174   -4.47103119   -4.28054982   -3.79447357]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:1
end of epoch 0: val_loss 0.31837267391689694, val_acc 0.975
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.2054e-01, -1.7851e-02,  3.2016e-01, -1.6260e-01, -1.0735e+00,
         -2.1388e-01,  9.9081e-02,  1.0974e-01,  1.4439e-01,  1.9661e-01,
          2.0579e-03, -8.2566e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.25216204721282337, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.6932e-01, -1.6412e-01,  6.5307e-01, -2.1384e-01, -1.2632e+00,
         -3.5219e-01,  6.8491e-02, -1.2342e-02,  1.3978e-01,  1.8314e-01,
          2.0581e-03, -8.5718e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.059457065581073006, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.6097e-01, -4.5696e-02,  3.4015e-01,  1.0123e-02, -1.5652e+00,
         -2.7268e-01,  1.9876e-02,  5.1216e-02,  1.3001e-01,  1.5760e-01,
          2.0582e-03, -8.1393e+00]], device='cuda:1'))])
end of epoch 3: val_loss 0.6408062540052029, val_acc 0.95
trigger times: 1
end of epoch 4: val_loss 0.2151537134952548, val_acc 0.98
trigger times: 2
end of epoch 5: val_loss 0.3460557940147737, val_acc 0.985
trigger times: 3
end of epoch 6: val_loss 0.13731972346043222, val_acc 0.99
trigger times: 4
end of epoch 7: val_loss 0.2681076340901745, val_acc 0.99
trigger times: 5
end of epoch 8: val_loss 0.32728448747737277, val_acc 0.98
trigger times: 6
end of epoch 9: val_loss 0.08355724871133102, val_acc 0.995
trigger times: 7
end of epoch 10: val_loss 0.11415806591372356, val_acc 0.995
trigger times: 8
end of epoch 11: val_loss 0.09315355306189214, val_acc 0.99
trigger times: 9
end of epoch 12: val_loss 0.2833383695687393, val_acc 0.99
trigger times: 10
Early stopping.
0 -207.1849091053009 -101.9839121946844
1 -206.7822505235672 -101.67638251453825
2 -206.41998708248138 -101.53078388228354
3 -203.95009130239487 -96.71369093925304
4 -202.62920409440994 -95.59773326862809
5 -196.27012610435486 -95.47358786128733
6 -203.89149886369705 -95.19216141507185
7 -200.14820271730423 -95.18309888794776
8 -204.58894765377045 -94.71462102635805
9 -62.188638001680374 -50.00222059884506
10 -108.85125783085823 -47.15605336419176
11 -48.47456330060959 -45.594227093057754
12 -73.56558793783188 -44.16490839583478
13 -66.32398575544357 -44.078303125872196
14 -40.24435621500015 -43.80581985978556
15 -29.686045169830322 -42.7594767358323
16 -58.35649859905243 -42.66316468983175
17 -48.03610122203827 -41.774963389485094
18 -58.663264364004135 -41.410068073767725
19 -87.80150628834963 -40.723520424948155
20 -93.37198558449745 -40.49595848244517
21 -63.12391936779022 -40.429389880911344
22 -152.87462329864502 -39.54162101198148
23 -100.58670109510422 -39.195237471709476
24 -83.54903864860535 -39.172389579378766
25 -62.45547603070736 -38.51095963496708
26 -110.73604142665863 -38.447265769744824
27 -62.583293318748474 -37.10988160586883
28 -69.96665835380554 -34.27116723637227
29 -11.954135581851006 -34.14139118114101
30 -45.85982595384121 -33.263072731706835
31 -63.407804906368256 -33.07825234291984
32 -46.20672482252121 -33.0321314765637
33 -38.930841743946075 -32.40063925734975
34 24.038131058216095 -31.10853252354611
35 -7.076514780521393 -30.57151371770873
36 -40.15815889835358 -30.131236504472803
37 -100.74556159973145 -29.99326722619033
38 -31.081585928797722 -29.82012804590686
39 -52.38924393057823 -29.297233511513635
40 -34.20345355570316 -29.288890423975797
41 -82.08677726984024 -28.315961465855167
42 0.07100820541381836 -27.121110566589827
43 1.047101378440857 -26.064532595535336
44 -5.681135825812817 -25.287205359194225
45 -95.65519845485687 -25.27101421179229
46 -9.365559548139572 -25.066644278800943
47 -46.6582635641098 -24.925849381327673
48 -43.39458656311035 -24.027280698638233
49 -19.052154392004013 -23.18246155787816
50 -11.26913970708847 -23.153943559703283
51 -21.21079495549202 -22.98127038972695
52 -87.18475198745728 -22.954730295117237
53 -12.329910978674889 -22.739273544503753
54 -23.58476960659027 -22.321731990352006
55 -10.542298167943954 -21.055920928583344
56 -13.474296301603317 -21.054423463876304
57 -24.01943612098694 -20.49365823342144
58 5.989013373851776 -20.33499633836848
59 -19.853223532438278 -19.790102509564896
60 -24.437217123806477 -19.157228016270295
61 -30.94655641913414 -19.03872956621494
62 -9.17682808637619 -18.964124524696246
63 -29.231578052043915 -18.556563110884383
64 -31.468921959400177 -18.448962308005108
65 18.223047226667404 -18.378042350003966
66 35.410091280937195 -18.367303662663037
67 -19.08668240904808 -17.740722019993825
68 -9.848929204046726 -17.563867180882312
69 -15.739252611994743 -17.466709880202103
70 13.673852264881134 -17.276353422345036
71 26.671794325113297 -16.85889369985028
72 8.44950994849205 -16.413056944336628
73 19.413166970014572 -16.35844701639436
74 0.030797749757766724 -16.3381194095591
75 8.562585338950157 -16.046514798212524
76 -14.50709530711174 -15.779723245332898
77 -13.274169266223907 -15.74025621877632
78 -13.199037067592144 -15.07369212436238
79 27.090217351913452 -14.783413685710588
80 -0.7961017340421677 -14.544070077357036
81 -4.451079189777374 -13.483098530680483
82 0.46161869168281555 -13.119872190385077
83 28.420620024204254 -12.30431036670119
84 20.751825273036957 -12.292460024837176
85 12.776520490646362 -12.286490721172774
86 -5.901235803961754 -12.278515244993585
87 28.191968500614166 -12.227387460046547
88 25.209546506404877 -12.205547501809747
89 27.480849236249924 -12.07111646737387
90 28.19645118713379 -11.92534250584751
91 12.604577600955963 -11.910094799877324
92 34.766336381435394 -11.855511119457228
93 38.47503471374512 -11.519659182742824
94 12.280926942825317 -11.42773193858301
95 28.497404158115387 -11.412667237833352
96 44.36951857805252 -11.38649860999212
97 2.8199165016412735 -11.355319709011528
98 -3.8919268548488617 -11.134618158086587
99 30.964999228715897 -10.89780063252017
100 37.56390160322189 -10.366863692799834
101 32.093496680259705 -10.284702914751124
102 46.48255306482315 -10.127840217970697
103 7.296274721622467 -9.289921608799773
104 48.240945130586624 -8.844144390525843
105 39.464148223400116 -8.750422834512282
106 46.58075261116028 -8.67188313734802
107 50.04478567838669 -8.015753968638247
108 45.18783946335316 -7.868571375554545
109 -22.27474617958069 -7.459623237418707
110 50.69888547062874 -7.447533991200331
111 -7.302323222160339 -6.853091098326624
112 35.53323572874069 -6.49914266328626
113 39.85908745229244 -6.480226103997047
114 -6.904976285994053 -6.117354180737655
115 5.213515102863312 -5.43500831968483
116 11.294711634516716 -5.105291741614599
117 13.543272644281387 -4.471031187897325
118 29.539008051156998 -4.2805498188182405
119 26.722785353660583 -3.7944735717969627
train accuracy: 0.99
validation accuracy: 0.99
[-101.98391219 -101.67638251  -96.71369094  -95.18309889  -94.71462103
  -64.25256941  -62.47110201  -58.12889372  -57.41850482  -57.13214112
  -56.77442484  -53.11619925  -50.82628725  -50.0022206   -48.95079059
  -47.222154    -45.59422709  -44.1649084   -44.07830313  -43.80581986
  -43.01220691  -41.61028904  -41.41006807  -41.40102394  -40.72352042
  -40.49595848  -39.84868049  -39.19523747  -38.51095963  -38.44726577
  -34.27116724  -34.14139118  -33.63287552  -33.07825234  -30.1312365
  -29.99326723  -28.273941    -26.0645326   -25.41095184  -25.28720536
  -23.76802732  -23.18246156  -23.15394356  -22.32173199  -21.23257837
  -21.20635914  -21.12773701  -21.05442346  -20.63602017  -20.49365823
  -20.34903989  -19.9813142   -19.96538812  -19.79010251  -19.78562284
  -19.72907462  -19.15722802  -19.09047279  -18.96412452  -18.76240263
  -18.67386419  -18.53621167  -18.44896231  -18.37804235  -18.36730366
  -17.98900808  -17.85248342  -17.56386718  -17.46670988  -17.27635342
  -16.41305694  -16.0465148   -15.45318948  -15.07369212  -14.78341369
  -14.54407008  -14.49159165  -14.17871458  -13.9506222   -13.51882567
  -13.48309853  -13.11987219  -12.78720873  -12.29246002  -12.28649072
  -12.27851524  -12.25147473  -12.22738746  -12.2055475   -11.99761851
  -11.92534251  -11.91539992  -11.85551112  -11.80891287  -11.41266724
  -10.89780063  -10.86154273  -10.80212992  -10.73482241  -10.36686369
   -9.74464247   -9.65777446   -9.28992161   -9.1141844    -8.87594166
   -8.84414439   -8.67092204   -8.51274545   -8.28953301   -8.23876416
   -7.76570497   -7.60733738   -7.14447663   -7.13533961   -6.91821094
   -6.56171738   -6.49914266   -6.4802261    -4.47103119   -4.28054982]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:3
end of epoch 0: val_loss 0.03889643981303024, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.3578e-01,  1.9953e-01,  3.7740e-01, -4.7482e-02, -8.9039e-01,
          2.5374e-01,  7.6581e-02, -1.7609e-01,  5.5884e-02, -6.1328e-02,
         -5.5806e-04, -8.3907e+00]], device='cuda:3'))])
end of epoch 1: val_loss 0.2799378014105537, val_acc 0.98
trigger times: 1
end of epoch 2: val_loss 0.25764966391697924, val_acc 0.98
trigger times: 2
end of epoch 3: val_loss 0.028147826292868067, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5208e-01,  3.2746e-01,  2.3385e-01,  3.4638e-02, -3.8322e-02,
          1.2114e-01,  5.1361e-02, -1.3558e-01,  1.4797e-01, -8.7021e-02,
         -5.5785e-04, -9.7217e+00]], device='cuda:3'))])
end of epoch 4: val_loss 0.23838672611280232, val_acc 0.975
trigger times: 1
end of epoch 5: val_loss 0.048008921952256714, val_acc 0.99
trigger times: 2
end of epoch 6: val_loss 0.12146793823382594, val_acc 0.98
trigger times: 3
end of epoch 7: val_loss 0.14795834149233997, val_acc 0.985
trigger times: 4
end of epoch 8: val_loss 0.034753337352679804, val_acc 0.995
trigger times: 5
end of epoch 9: val_loss 0.07447789443022543, val_acc 0.985
trigger times: 6
end of epoch 10: val_loss 0.014555268922699547, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0741e-01,  2.5033e-01,  2.4926e-01, -2.1715e-02, -4.2442e-05,
         -8.3230e-03,  1.4177e-02, -6.0067e-02,  8.7226e-02, -1.2338e-04,
         -5.5733e-04, -1.0864e+01]], device='cuda:3'))])
end of epoch 11: val_loss 0.22136287205459668, val_acc 0.98
trigger times: 1
end of epoch 12: val_loss 0.3682295780169079, val_acc 0.98
trigger times: 2
end of epoch 13: val_loss 0.15965217087970815, val_acc 0.98
trigger times: 3
end of epoch 14: val_loss 0.06429353933723121, val_acc 0.99
trigger times: 4
end of epoch 15: val_loss 1.6601940253211525, val_acc 0.955
trigger times: 5
end of epoch 16: val_loss 0.14702847587184806, val_acc 0.98
trigger times: 6
end of epoch 17: val_loss 0.5610903145264454, val_acc 0.965
trigger times: 7
end of epoch 18: val_loss 0.09169938551168759, val_acc 0.985
trigger times: 8
end of epoch 19: val_loss 0.4224112699490433, val_acc 0.98
trigger times: 9
end of epoch 20: val_loss 0.08360042101736269, val_acc 0.99
trigger times: 10
Early stopping.
0 -597.8858439922333 -101.9839121946844
1 -613.2546017169952 -101.67638251453825
2 -587.0811027288437 -96.71369093925304
3 -593.3107120990753 -95.18309888794776
4 -587.5360200405121 -94.71462102635805
5 -58.77114209532738 -64.25256941089486
6 -105.69536113739014 -62.471102011605396
7 -71.54995566606522 -58.12889372019274
8 -71.61269706487656 -57.41850482415838
9 -101.13436806201935 -57.132141123080615
10 -49.37594294548035 -56.7744248393342
11 -56.80250981450081 -53.116199249047185
12 -41.86911582946777 -50.826287247278245
13 -76.99296605587006 -50.00222059884506
14 -47.461010694503784 -48.950790585462585
15 -48.786340177059174 -47.222154002855625
16 -117.53618323802948 -45.594227093057754
17 -118.70427256822586 -44.16490839583478
18 -178.83237636089325 -44.078303125872196
19 -72.82613790035248 -43.80581985978556
20 -47.320462703704834 -43.01220691354654
21 -47.56996136903763 -41.610289044981414
22 -70.80904942750931 -41.410068073767725
23 -28.863909304142 -41.40102394461203
24 -52.63221292197704 -40.723520424948155
25 -133.25381880998611 -40.49595848244517
26 -34.94401317834854 -39.84868048746036
27 -164.28569948673248 -39.195237471709476
28 -120.18752145767212 -38.51095963496708
29 -149.44427073001862 -38.447265769744824
30 -94.54859226942062 -34.27116723637227
31 2.8852145671844482 -34.14139118114101
32 -53.32052430510521 -33.63287552131218
33 -90.63003915548325 -33.07825234291984
34 -46.38701468706131 -30.131236504472803
35 -101.38409781455994 -29.99326722619033
36 -47.601434737443924 -28.27394099600584
37 -1.4853761494159698 -26.064532595535336
38 -59.54329773783684 -25.410951841203016
39 -12.797423884272575 -25.287205359194225
40 -39.05234059691429 -23.76802732112401
41 -9.955530539155006 -23.18246155787816
42 -3.27712818980217 -23.153943559703283
43 -16.738783322274685 -22.321731990352006
44 -40.50613135099411 -21.23257836600756
45 -1.8958967477083206 -21.206359135277687
46 -47.372752249240875 -21.127737012177565
47 -2.909384809434414 -21.054423463876304
48 -48.397803366184235 -20.63602017160168
49 -18.658835984766483 -20.49365823342144
50 -23.49889174103737 -20.349039892984273
51 -29.559527173638344 -19.98131420440513
52 -31.514297604560852 -19.965388115205258
53 -17.19415646791458 -19.790102509564896
54 -35.73215812444687 -19.78562284314248
55 -41.083490788936615 -19.72907462354597
56 -51.70730406045914 -19.157228016270295
57 -18.946661844849586 -19.090472790514447
58 -22.795965284109116 -18.964124524696246
59 -20.859747990965843 -18.76240262699245
60 -17.305113315582275 -18.673864187827807
61 -21.572120487689972 -18.536211669075957
62 -62.145817548036575 -18.448962308005108
63 30.612380146980286 -18.378042350003966
64 -8.986452892422676 -18.367303662663037
65 -12.379421710968018 -17.989008079194633
66 -22.257038608193398 -17.852483417587056
67 4.7663048803806305 -17.563867180882312
68 -46.90019226074219 -17.466709880202103
69 26.518557965755463 -17.276353422345036
70 13.025257736444473 -16.413056944336628
71 13.432921670377254 -16.046514798212524
72 -44.4623259305954 -15.453189476030609
73 -0.6983402371406555 -15.07369212436238
74 24.262260302901268 -14.783413685710588
75 -30.711193710565567 -14.544070077357036
76 -19.81714227795601 -14.491591654219343
77 -41.05917966365814 -14.178714582123185
78 -15.634460762143135 -13.950622199451754
79 -16.27948760986328 -13.518825667152155
80 -39.54611121863127 -13.483098530680483
81 -33.35456961393356 -13.119872190385077
82 -0.015020899474620819 -12.78720872997726
83 16.98477192223072 -12.292460024837176
84 -22.08370952308178 -12.286490721172774
85 -41.96562302112579 -12.278515244993585
86 -18.26269607245922 -12.251474732229568
87 20.60817138850689 -12.227387460046547
88 21.000028491020203 -12.205547501809747
89 -13.105968326330185 -11.997618508316913
90 -13.887136273086071 -11.92534250584751
91 -27.117887027561665 -11.915399922657347
92 27.0258347094059 -11.855511119457228
93 3.9014506191015244 -11.808912869503724
94 34.61983776092529 -11.412667237833352
95 -12.778984069824219 -10.89780063252017
96 16.366712152957916 -10.861542728875564
97 7.850932866334915 -10.802129919934712
98 18.172106578946114 -10.734822405045552
99 17.68678092956543 -10.366863692799834
100 9.243615359067917 -9.744642467544434
101 23.253784865140915 -9.657774459628016
102 -5.4420164823532104 -9.289921608799773
103 25.393001914024353 -9.114184400440584
104 25.054525464773178 -8.875941659495073
105 34.70128232240677 -8.844144390525843
106 22.32405588030815 -8.67092204021975
107 35.87547433376312 -8.512745445266134
108 -7.256767433136702 -8.28953301070337
109 35.19717901945114 -8.238764159414929
110 36.31075823307037 -7.7657049708146975
111 33.83984702825546 -7.607337380438922
112 -3.6725939214229584 -7.144476626891946
113 31.65260362625122 -7.135339605634409
114 36.88737744092941 -6.918210937503306
115 3.1755525171756744 -6.561717383514745
116 24.934575974941254 -6.49914266328626
117 29.81775814294815 -6.480226103997047
118 18.171996414661407 -4.471031187897325
119 -4.950056284666061 -4.2805498188182405
train accuracy: 0.9877777777777778
validation accuracy: 0.99

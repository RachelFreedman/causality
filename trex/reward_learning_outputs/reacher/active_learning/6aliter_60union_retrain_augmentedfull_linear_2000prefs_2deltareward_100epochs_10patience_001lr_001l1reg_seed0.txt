[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.17338410421189832, val_acc 0.955
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.2063e-02,  6.9983e-02,  1.1024e-01, -4.7749e-02,  4.8931e-02,
          1.0211e-01, -7.8276e-03,  5.4551e-02, -9.9177e-05, -2.6311e-02,
         -3.7692e-04, -1.2449e+00, -2.0623e+00]], device='cuda:0'))])
end of epoch 1: val_loss 1.0778926035051528, val_acc 0.925
trigger times: 1
end of epoch 2: val_loss 0.014457785773898628, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2291e-02, -9.2462e-02,  2.8598e-02, -4.4316e-02,  4.2748e-01,
          8.6441e-02, -1.5810e-02, -2.8282e-02, -2.4519e-01, -1.7837e-01,
         -3.7710e-04, -2.8558e+00, -4.7500e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.03872131903718451, val_acc 0.99
trigger times: 1
end of epoch 4: val_loss 0.024573106069376606, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 0.4119820646757391, val_acc 0.96
trigger times: 3
end of epoch 6: val_loss 0.028052451130435932, val_acc 0.99
trigger times: 4
end of epoch 7: val_loss 0.007914294379803195, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.4469e-02,  3.9523e-02, -3.2590e-02, -2.0829e-01,  1.5054e-01,
         -2.1216e-02, -5.6012e-03,  6.5626e-02,  2.9166e-01,  1.3163e-03,
         -3.7754e-04, -4.2646e+00, -8.5688e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.013950043864297861, val_acc 0.995
trigger times: 1
end of epoch 9: val_loss 0.23436809426886593, val_acc 0.98
trigger times: 2
end of epoch 10: val_loss 0.0073600825674537875, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2209e-01, -2.4439e-01,  9.3909e-02, -6.3796e-02,  2.7944e-01,
          7.0487e-02, -4.3862e-02, -6.8830e-02, -7.7285e-02, -1.0730e-01,
         -3.7781e-04, -4.3432e+00, -8.7069e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.09072725781606994, val_acc 0.99
trigger times: 1
end of epoch 12: val_loss 0.008188796772891891, val_acc 0.995
trigger times: 2
end of epoch 13: val_loss 0.05594890620559426, val_acc 0.985
trigger times: 3
end of epoch 14: val_loss 0.0006990154778189606, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3008e-02, -1.0723e-01,  3.6303e-03, -7.5206e-02,  2.3402e-01,
          8.7524e-02,  6.0612e-03,  1.9494e-02,  5.9987e-05, -1.1932e-01,
         -3.7823e-04, -4.5115e+00, -9.0930e+00]], device='cuda:0'))])
end of epoch 15: val_loss 0.047462844941510235, val_acc 0.985
trigger times: 1
end of epoch 16: val_loss 2.74311875694897, val_acc 0.92
trigger times: 2
end of epoch 17: val_loss 0.03916233204314519, val_acc 0.99
trigger times: 3
end of epoch 18: val_loss 0.048718886351698173, val_acc 0.99
trigger times: 4
end of epoch 19: val_loss 0.18980070815428504, val_acc 0.975
trigger times: 5
end of epoch 20: val_loss 0.06044404918778582, val_acc 0.985
trigger times: 6
end of epoch 21: val_loss 0.14549745660594693, val_acc 0.985
trigger times: 7
end of epoch 22: val_loss 0.2542602202067144, val_acc 0.975
trigger times: 8
end of epoch 23: val_loss 0.0001494039863609231, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0195e-01, -3.0549e-01, -7.7568e-03, -1.3759e-01,  3.1822e-01,
          6.2400e-06,  1.9941e-02,  3.1880e-02, -2.3554e-05,  2.7404e-05,
         -3.7917e-04, -4.9645e+00, -9.8064e+00]], device='cuda:0'))])
end of epoch 24: val_loss 0.08740017801362228, val_acc 0.985
trigger times: 1
end of epoch 25: val_loss 0.016339501143037262, val_acc 0.995
trigger times: 2
end of epoch 26: val_loss 0.27422065064225176, val_acc 0.975
trigger times: 3
end of epoch 27: val_loss 0.0019219951323702134, val_acc 1.0
trigger times: 4
end of epoch 28: val_loss 0.010314583236461594, val_acc 0.995
trigger times: 5
end of epoch 29: val_loss 0.03071288168388886, val_acc 0.99
trigger times: 6
end of epoch 30: val_loss 0.0009444888681164798, val_acc 1.0
trigger times: 7
end of epoch 31: val_loss 0.01869662820443402, val_acc 0.995
trigger times: 8
end of epoch 32: val_loss 0.02941361771678203, val_acc 0.995
trigger times: 9
end of epoch 33: val_loss 0.016318105123937058, val_acc 0.995
trigger times: 10
Early stopping.
0 -521.7305054664612 -54.98547503240923
1 -482.8258068561554 -50.492268601198035
2 -477.04428791999817 -50.03933801517046
3 -488.7367811203003 -49.75347184620696
4 -469.7062191963196 -49.72654640753777
5 -475.6574020385742 -46.98011874490918
6 -455.47216391563416 -45.7351542845057
7 -449.54950761795044 -45.670579884154705
8 -461.0928111076355 -44.99030608142343
9 -445.39200949668884 -44.14602409201361
10 -443.85955142974854 -43.81326882122305
11 -440.94013929367065 -43.18878399086166
12 -434.85448455810547 -42.29180714825394
13 -438.3244662284851 -42.00401746161006
14 -435.6370770931244 -41.6910044370425
15 -435.4197404384613 -41.68588229294918
16 -426.75941228866577 -41.281777102712205
17 -426.31885981559753 -40.44278203413966
18 -418.63843309879303 -40.34838365523108
19 -422.0433280467987 -39.599701153458774
20 -413.28803420066833 -39.57586365327889
21 -414.91398215293884 -39.31972693233231
22 -400.96089720726013 -39.024610555047154
23 -405.35768961906433 -38.45534493538269
24 -402.3702757358551 -38.41270390343083
25 -421.87125420570374 -38.35634328077039
26 -391.50072145462036 -37.79713616772368
27 -398.44110202789307 -37.741528994987384
28 -409.7332705259323 -37.66475323879293
29 -405.5395405292511 -37.513139380385574
30 -404.4673240184784 -37.1809993033689
31 -397.06879138946533 -37.100703136010694
32 -383.4025044441223 -37.00630588930485
33 -398.1896939277649 -36.821916772458344
34 -403.28123688697815 -36.48799015296732
35 -375.2537341117859 -36.20965269874363
36 -386.3899711370468 -36.19207561676116
37 -394.73173117637634 -36.114459029559086
38 -379.1917049884796 -35.78149902167743
39 -378.8006327152252 -35.394503873250635
40 -387.97383511066437 -35.26282499693737
41 -394.06847059726715 -35.24303541418371
42 -376.3943729400635 -35.209705244501436
43 -383.3442323207855 -35.0654408505187
44 -360.76673394441605 -34.80241747531743
45 -368.0253139734268 -34.64469044638467
46 -353.0849703550339 -33.84284985953318
47 -351.278214097023 -32.70706485357069
48 -358.4107999801636 -31.969099402548657
49 -349.89888322353363 -31.7109134007892
50 -343.5890762805939 -31.64414355845032
51 -339.5310107469559 -31.392382758954444
52 -343.0926575064659 -31.223196019713853
53 -329.85631358623505 -31.12953085092458
54 -347.6529850959778 -29.39157139549552
55 -332.94502902030945 -29.340125609942326
56 -326.46063327789307 -29.106189988903285
57 -313.2224860191345 -27.41102349748205
58 -317.8389046192169 -27.343722362182305
59 -314.1630581021309 -27.196681629483837
60 -315.3670638203621 -27.07399028854534
61 -312.9733281135559 -26.7047217556024
62 -312.95253801345825 -26.244794902859052
63 -297.31567680835724 -25.548365085275513
64 -301.0248295068741 -25.45878528601009
65 -294.88924688100815 -24.879106999799365
66 -284.9861100912094 -24.828695359328833
67 -295.35185849666595 -24.592745144504722
68 -293.03069388866425 -23.978745577896312
69 -277.9287043809891 -23.57262108435893
70 -278.17708480358124 -23.44970807952351
71 -286.4465373158455 -22.745309160183492
72 -272.5876325368881 -22.60679894414887
73 -270.41260528564453 -22.19891031871716
74 -262.51808562874794 -20.656863763892378
75 -268.3295439481735 -20.444472560731253
76 -244.56397783756256 -20.19699010077007
77 -255.0760868191719 -20.13839114930498
78 -243.8890439271927 -19.63760343800059
79 -249.985018491745 -19.515598718228343
80 -238.9277554154396 -18.92838809611677
81 -247.76124513149261 -17.994774057192853
82 -226.39691030979156 -17.55742370467821
83 -225.70152258872986 -16.823073927842348
84 -201.52982360124588 -14.855082803515382
85 -196.93746203184128 -14.531424598833084
86 -208.45523446798325 -14.442420089224363
87 -192.89970183372498 -13.596012850960644
88 -167.13472145795822 -12.68135972540495
89 -177.93007431924343 -12.66418205637357
90 -178.44650480151176 -12.30017947419658
91 -178.873439848423 -12.151904772081672
92 -175.43897300958633 -11.788852141676486
93 -171.1632001399994 -10.869989101210326
94 -161.91132536530495 -10.327681503524177
95 -142.58798789978027 -9.8572159761571
96 -138.66683180630207 -8.330116995310416
97 -129.3276061564684 -8.133195842510668
98 -134.5561194717884 -8.108197691178031
99 -124.86137121915817 -7.57539849177145
100 -120.86257427930832 -7.362443126623615
101 -112.7918177396059 -7.108327355338034
102 -117.5777904689312 -6.959063561385431
103 -123.36805695295334 -6.776946485018116
104 -110.58445456624031 -6.7220638398623045
105 -122.95604509115219 -6.719970621583102
106 -129.27108046412468 -6.535447341844848
107 -122.53370606899261 -6.51820418055673
108 -115.28719282150269 -5.615796733870542
109 -101.30738192796707 -5.34720210027791
110 -100.59646934270859 -5.078485007852753
111 -101.64552146196365 -5.027957977402961
112 -93.93944814056158 -4.827572916892203
113 -94.22744697332382 -4.63049541560991
114 -90.08443534374237 -4.230832004686763
115 -88.79522944241762 -4.031048624093466
116 -78.84770534187555 -3.3844671463622564
117 -79.57862623035908 -3.3322555012187633
118 -77.19408506155014 -2.6416623314910934
119 -59.57004810124636 -1.9136196540088464
train accuracy: 0.9994444444444445
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -12.03876266 -11.78885214 -11.50342998
 -11.17227792 -11.15068141 -11.05685807 -10.8699891  -10.3276815
 -10.04606649  -9.95058412  -9.85721598  -9.80411195  -9.28298288
  -9.18321593  -8.54599172  -8.330117    -8.24054637  -8.16125845
  -8.13319584  -8.10819769  -7.79169537  -7.60143925  -7.57787028
  -7.57539849  -7.36244313  -7.28899364  -7.22330361  -7.10832736
  -7.0834432   -6.95906356  -6.77694649  -6.72206384  -6.71997062
  -6.62510243  -6.53544734  -6.53472444  -6.51820418  -6.40172218
  -6.00945721  -5.90681709  -5.85989749  -5.84707082  -5.76313916
  -5.73210893  -5.65191833  -5.61579673  -5.61157023  -5.59858366
  -5.58594783  -5.5083445   -5.44966417  -5.39976331  -5.37233753
  -5.3472021   -5.34435441  -5.31127297  -5.27742895  -5.21344847
  -5.19726821  -5.13438502  -5.08711915  -5.07848501  -5.03387903
  -5.02795798  -4.96729202  -4.86914179  -4.82757292  -4.78896372
  -4.76433577  -4.63049542  -4.570024    -4.230832    -4.03104862
  -3.95621907  -3.95582809  -3.87074694  -3.85647259  -3.84067182
  -3.57304656  -3.42657396  -3.38446715  -3.3322555   -3.05073542
  -2.99015747  -2.86030708  -2.85315444  -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.06976199915025863, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.0534e-02,  7.9130e-02, -6.2345e-02, -4.6588e-02,  2.1955e-01,
         -2.0884e-01,  4.7028e-03,  2.3959e-02, -8.0296e-02,  3.9704e-01,
         -3.7692e-04, -1.5844e+00, -2.2057e+00]], device='cuda:2'))])
end of epoch 1: val_loss 0.8306356413766031, val_acc 0.93
trigger times: 1
end of epoch 2: val_loss 0.5641522299003283, val_acc 0.955
trigger times: 2
end of epoch 3: val_loss 3.92352507624414, val_acc 0.92
trigger times: 3
end of epoch 4: val_loss 0.8824999252481323, val_acc 0.95
trigger times: 4
end of epoch 5: val_loss 0.28843508606788076, val_acc 0.96
trigger times: 5
end of epoch 6: val_loss 0.22498713127447142, val_acc 0.975
trigger times: 6
end of epoch 7: val_loss 0.25718689128092137, val_acc 0.975
trigger times: 7
end of epoch 8: val_loss 0.07933140547470402, val_acc 0.985
trigger times: 8
end of epoch 9: val_loss 0.11328339807530441, val_acc 0.98
trigger times: 9
end of epoch 10: val_loss 0.09176388942922348, val_acc 0.985
trigger times: 10
Early stopping.
0 -407.64709639549255 -54.98547503240923
1 -373.60384130477905 -50.03933801517046
2 -388.25290989875793 -49.75347184620696
3 -387.4982032775879 -46.98011874490918
4 -355.1112377643585 -45.7351542845057
5 -366.1036710739136 -44.99030608142343
6 -329.5400644540787 -44.14602409201361
7 -319.32232189178467 -43.18878399086166
8 -325.78154587745667 -42.29180714825394
9 -336.03151631355286 -41.6910044370425
10 -339.19409227371216 -41.68588229294918
11 -329.20491337776184 -40.44278203413966
12 -303.36277854442596 -40.34838365523108
13 -302.4359679222107 -39.57586365327889
14 -317.23414945602417 -39.31972693233231
15 -326.51305186748505 -38.45534493538269
16 -308.37575101852417 -38.41270390343083
17 -310.08762991428375 -37.79713616772368
18 -296.8941171169281 -37.741528994987384
19 -309.4361267089844 -37.513139380385574
20 -299.62472438812256 -37.1809993033689
21 -295.1205714941025 -37.00630588930485
22 -302.80816411972046 -36.821916772458344
23 -297.54790234565735 -36.20965269874363
24 -299.4537694454193 -36.19207561676116
25 -285.6107140779495 -35.78149902167743
26 -280.933180809021 -35.394503873250635
27 -302.38325023651123 -35.24303541418371
28 -286.7495746910572 -35.209705244501436
29 -278.53186213970184 -34.80241747531743
30 -267.38476395606995 -34.64469044638467
31 -269.0778020620346 -32.70706485357069
32 -289.1408498287201 -31.969099402548657
33 -262.65639942884445 -31.64414355845032
34 -259.7883858680725 -31.392382758954444
35 -265.56535482406616 -31.12953085092458
36 -268.1536754965782 -29.39157139549552
37 -247.5095089673996 -29.106189988903285
38 -229.65716567635536 -27.41102349748205
39 -249.77396655082703 -27.196681629483837
40 -241.62596794962883 -27.07399028854534
41 -248.18698453903198 -26.244794902859052
42 -234.1982946395874 -25.548365085275513
43 -225.1254963874817 -24.879106999799365
44 -221.5027461051941 -24.828695359328833
45 -241.64368510246277 -23.978745577896312
46 -220.48421478271484 -23.57262108435893
47 -205.32287192344666 -22.745309160183492
48 -204.39493584632874 -22.60679894414887
49 -188.46753859519958 -20.656863763892378
50 -196.2206673026085 -20.444472560731253
51 -197.9276583790779 -20.13839114930498
52 -173.78887164592743 -19.63760343800059
53 -175.02923604846 -18.92838809611677
54 -200.67959678173065 -17.994774057192853
55 -172.2519593834877 -16.823073927842348
56 -160.7587193250656 -14.855082803515382
57 -153.52700716257095 -14.442420089224363
58 -149.58882421255112 -13.596012850960644
59 -134.91159904003143 -12.66418205637357
60 -132.6465823352337 -12.30017947419658
61 -110.26510405540466 -12.038762664538694
62 -143.12273955345154 -11.788852141676486
63 -107.63877272605896 -11.172277917644152
64 -108.26676619052887 -11.150681410541043
65 -140.39143705368042 -10.869989101210326
66 -120.89857914298773 -10.327681503524177
67 -102.73342031240463 -9.950584120932124
68 -99.27205710113049 -9.8572159761571
69 -92.01472437381744 -9.282982883985197
70 -91.68169009685516 -9.183215927791332
71 -102.14334480464458 -8.330116995310416
72 -92.14874649047852 -8.240546368387013
73 -100.09737792611122 -8.133195842510668
74 -100.64346744120121 -8.108197691178031
75 -84.09046930074692 -7.601439253533187
76 -80.19155666232109 -7.5778702831550095
77 -91.7657739520073 -7.362443126623615
78 -76.67933535575867 -7.2889936408176155
79 -90.84045159816742 -7.108327355338034
80 -84.92722469568253 -7.083443196201368
81 -87.02558660507202 -6.776946485018116
82 -90.07146179676056 -6.7220638398623045
83 -75.43259353935719 -6.625102425967422
84 -100.78403416275978 -6.535447341844848
85 -105.65134325623512 -6.51820418055673
86 -78.1342505812645 -6.401722181555061
87 -69.9794608950615 -5.906817089641233
88 -64.89622309803963 -5.859897494166216
89 -68.37337240576744 -5.763139161566328
90 -70.285135358572 -5.732108934485532
91 -102.09516716003418 -5.615796733870542
92 -74.07047307491302 -5.611570232886707
93 -69.09919327497482 -5.585947833145062
94 -68.83681181073189 -5.5083445022364455
95 -60.07706071436405 -5.399763312756924
96 -82.58188909292221 -5.372337529209199
97 -74.51315703988075 -5.344354409948577
98 -60.676999278366566 -5.311272965903258
99 -62.80873557925224 -5.213448468923901
100 -72.92644792795181 -5.19726821032636
101 -60.18679690361023 -5.087119147583023
102 -77.40334293246269 -5.078485007852753
103 -71.9116591066122 -5.027957977402961
104 -68.97276851534843 -4.967292017493492
105 -66.40655478835106 -4.827572916892203
106 -59.16982114315033 -4.7889637182540605
107 -74.98201643675566 -4.63049541560991
108 -64.43558937311172 -4.570024001992525
109 -72.2175877764821 -4.031048624093466
110 -56.67232236266136 -3.956219070100057
111 -50.95754728466272 -3.8707469442824127
112 -53.53760427236557 -3.8564725920033256
113 -52.83248467743397 -3.573046562605251
114 -52.46966691315174 -3.4265739646588558
115 -61.34715348482132 -3.3322555012187633
116 -55.96543797850609 -3.0507354153231963
117 -50.723724484443665 -2.8603070847096705
118 -41.15660375356674 -2.853154438109651
119 -42.62791083753109 -1.9136196540088464
train accuracy: 0.9944444444444445
validation accuracy: 0.985
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -12.03876266 -11.78885214 -11.50342998
 -11.17227792 -11.15068141 -11.05685807 -10.8699891  -10.3276815
 -10.04606649  -9.95058412  -9.85721598  -9.81100055  -9.80411195
  -9.62645705  -9.60739508  -9.28298288  -9.18321593  -9.0831059
  -8.80529633  -8.74582306  -8.54599172  -8.48339257  -8.47608826
  -8.46182044  -8.39459794  -8.37884663  -8.330117    -8.29377495
  -8.24054637  -8.16125845  -8.15882305  -8.13625094  -8.13319584
  -8.10819769  -8.02522288  -8.01805343  -7.98923078  -7.8333171
  -7.79457133  -7.79169537  -7.60143925  -7.57787028  -7.57539849
  -7.38884702  -7.36244313  -7.28899364  -7.22330361  -7.10832736
  -7.08364335  -7.0834432   -6.95906356  -6.81739378  -6.81266907
  -6.77694649  -6.72206384  -6.71997062  -6.62510243  -6.57972984
  -6.53544734  -6.53472444  -6.51820418  -6.40172218  -6.29473787
  -6.29069253  -6.15083379  -6.11093606  -6.00945721  -5.92306092
  -5.90681709  -5.85989749  -5.84707082  -5.76313916  -5.73210893
  -5.72912203  -5.65191833  -5.61579673  -5.61157023  -5.60750431
  -5.59858366  -5.58594783  -5.5679035   -5.52392948  -5.5083445
  -5.44966417  -5.39976331  -5.37233753  -5.3472021   -5.34435441
  -5.31127297  -5.27742895  -5.26470638  -5.21344847  -5.19726821
  -5.13438502  -5.08711915  -5.08330944  -5.07848501  -5.07092202
  -5.068821    -5.0579129   -5.03387903  -5.0325851   -5.02795798
  -4.96729202  -4.95826798  -4.86914179  -4.82757292  -4.78896372
  -4.76433577  -4.69057282  -4.63049542  -4.570024    -4.50732939
  -4.4899532   -4.24438995  -4.230832    -4.03104862  -4.02895974
  -4.01741858  -3.95621907  -3.95582809  -3.87074694  -3.85647259
  -3.84067182  -3.74995966  -3.73349182  -3.70689576  -3.57304656
  -3.5374298   -3.47180307  -3.45446107  -3.42657396  -3.38446715
  -3.3322555   -3.30583509  -3.13372286  -3.05073542  -2.99015747
  -2.89513627  -2.87064024  -2.86030708  -2.85315444  -2.67572149
  -2.64166233  -2.62856817  -2.46804878  -2.1701481   -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.0675564692091304, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.8957e-02, -4.5872e-02,  1.6950e-02,  9.7261e-03,  5.9669e-01,
          3.8785e-02, -1.4952e-02, -2.1056e-02, -1.6381e-02, -5.4204e-02,
         -3.7692e-04, -1.5084e+00, -1.7424e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.21118210041081395, val_acc 0.965
trigger times: 1
end of epoch 2: val_loss 0.13700066591943802, val_acc 0.985
trigger times: 2
end of epoch 3: val_loss 0.027579906874577346, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4456e-02, -2.4288e-02,  1.6999e-02,  5.5318e-02,  9.5349e-01,
         -2.0846e-01,  6.4691e-03,  3.8008e-02,  1.5232e-01,  2.6610e-01,
         -3.7719e-04, -2.8619e+00, -4.4845e+00]], device='cuda:1'))])
end of epoch 4: val_loss 0.15099340897719443, val_acc 0.985
trigger times: 1
end of epoch 5: val_loss 0.026040613657639718, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1538e-02,  2.6558e-02,  6.7878e-03,  4.3653e-02,  1.3843e+00,
         -4.1065e-01,  2.8153e-02,  1.8293e-02, -3.2041e-02,  1.0941e-01,
         -3.7736e-04, -3.3923e+00, -5.0857e+00]], device='cuda:1'))])
end of epoch 6: val_loss 0.08198909504770235, val_acc 0.99
trigger times: 1
end of epoch 7: val_loss 2.2169369610420686, val_acc 0.875
trigger times: 2
end of epoch 8: val_loss 0.009682229430349097, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7576e-05,  5.9340e-03, -8.1045e-02,  1.1877e-02,  1.6366e+00,
         -3.6686e-01,  3.7994e-02,  1.5949e-02, -9.7752e-02,  3.6628e-02,
         -3.7761e-04, -3.5324e+00, -6.0598e+00]], device='cuda:1'))])
end of epoch 9: val_loss 0.026181403230119537, val_acc 0.99
trigger times: 1
end of epoch 10: val_loss 0.284309464123048, val_acc 0.98
trigger times: 2
end of epoch 11: val_loss 0.06358035419271267, val_acc 0.985
trigger times: 3
end of epoch 12: val_loss 0.006116935875402518, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.3633e-02, -8.7998e-02, -1.3261e-01, -1.2639e-05,  1.4907e+00,
         -3.4529e-01,  5.6657e-03, -2.0169e-02, -5.4415e-02,  3.6673e-02,
         -3.7802e-04, -3.3577e+00, -5.8530e+00]], device='cuda:1'))])
end of epoch 13: val_loss 0.026778433602042746, val_acc 0.995
trigger times: 1
end of epoch 14: val_loss 0.06794033534893501, val_acc 0.995
trigger times: 2
end of epoch 15: val_loss 0.0970971239437564, val_acc 0.985
trigger times: 3
end of epoch 16: val_loss 0.037066433092790306, val_acc 0.995
trigger times: 4
end of epoch 17: val_loss 0.031799168358388634, val_acc 0.99
trigger times: 5
end of epoch 18: val_loss 0.33178266252332717, val_acc 0.98
trigger times: 6
end of epoch 19: val_loss 0.04459111652530762, val_acc 0.985
trigger times: 7
end of epoch 20: val_loss 0.1162871223625552, val_acc 0.985
trigger times: 8
end of epoch 21: val_loss 0.1894101877945857, val_acc 0.985
trigger times: 9
end of epoch 22: val_loss 0.06471962436718454, val_acc 0.985
trigger times: 10
Early stopping.
0 -372.33555793762207 -54.98547503240923
1 -356.41736006736755 -50.03933801517046
2 -325.0284662246704 -49.72654640753777
3 -332.18534207344055 -45.7351542845057
4 -316.69674491882324 -44.99030608142343
5 -297.49502444267273 -43.81326882122305
6 -324.58224868774414 -42.29180714825394
7 -301.3632049560547 -41.6910044370425
8 -301.24084877967834 -41.281777102712205
9 -284.901700258255 -40.34838365523108
10 -291.1479744911194 -39.57586365327889
11 -299.95412135124207 -39.024610555047154
12 -277.86578834056854 -38.41270390343083
13 -282.4150587320328 -37.79713616772368
14 -286.11756503582 -37.66475323879293
15 -284.9205729961395 -37.1809993033689
16 -274.54161274433136 -37.00630588930485
17 -259.04250049591064 -36.48799015296732
18 -286.832006752491 -36.19207561676116
19 -262.8934464454651 -35.78149902167743
20 -257.92582738399506 -35.26282499693737
21 -260.01404559612274 -35.209705244501436
22 -254.80036240816116 -34.80241747531743
23 -243.31519883871078 -33.84284985953318
24 -249.0310401916504 -31.969099402548657
25 -243.92650020122528 -31.64414355845032
26 -238.5265673995018 -31.223196019713853
27 -236.1182897090912 -29.39157139549552
28 -224.61508625745773 -29.106189988903285
29 -231.44702515006065 -27.343722362182305
30 -230.38016045093536 -27.07399028854534
31 -229.7634447813034 -26.244794902859052
32 -214.3466773033142 -25.45878528601009
33 -196.65215665102005 -24.828695359328833
34 -189.57833456993103 -23.978745577896312
35 -195.78280729055405 -23.44970807952351
36 -188.20117962360382 -22.60679894414887
37 -163.0837009549141 -20.656863763892378
38 -181.59288322925568 -20.19699010077007
39 -167.2771633863449 -19.63760343800059
40 -173.55474904179573 -18.92838809611677
41 -158.68512773513794 -17.55742370467821
42 -142.46252019703388 -14.855082803515382
43 -144.78854823112488 -14.442420089224363
44 -123.92612066864967 -12.68135972540495
45 -130.52191162109375 -12.30017947419658
46 -114.75586295127869 -12.038762664538694
47 -113.99010097980499 -11.503429977290427
48 -107.579316675663 -11.150681410541043
49 -141.3109166622162 -10.869989101210326
50 -101.46674919128418 -10.046066492216982
51 -102.53419184684753 -9.8572159761571
52 -103.36514776945114 -9.804111953920238
53 -104.62957614660263 -9.607395075700598
54 -96.55025345087051 -9.183215927791332
55 -99.36055558919907 -8.805296331183552
56 -96.00985604524612 -8.545991718792262
57 -97.32179868221283 -8.47608826112123
58 -74.1179670393467 -8.39459794485309
59 -106.04150021076202 -8.330116995310416
60 -91.54840403795242 -8.161258447219758
61 -87.86961442232132 -8.136250944097627
62 -103.59862670302391 -8.108197691178031
63 -72.24406532943249 -8.018053431934826
64 -74.90210828185081 -7.833317097924241
65 -81.73242321610451 -7.791695374741341
66 -82.6469116806984 -7.5778702831550095
67 -78.63352584838867 -7.388847019510429
68 -75.37550991773605 -7.2889936408176155
69 -85.55577605962753 -7.108327355338034
70 -79.62971860170364 -7.083443196201368
71 -57.48887902498245 -6.817393778147772
72 -76.45584520697594 -6.776946485018116
73 -71.36142344772816 -6.719970621583102
74 -61.91653702408075 -6.579729840661269
75 -70.5216728746891 -6.534724440491901
76 -80.03595048189163 -6.401722181555061
77 -53.081298269331455 -6.290692525763503
78 -59.77662219107151 -6.110936063372426
79 -56.19358493387699 -5.923060924233818
80 -61.16328090429306 -5.859897494166216
81 -63.799793764948845 -5.763139161566328
82 -55.92720115184784 -5.729122026973944
83 -80.8775556832552 -5.615796733870542
84 -57.27917043864727 -5.6075043053076215
85 -66.22384113073349 -5.585947833145062
86 -79.20980644226074 -5.523929484100368
87 -56.77136346697807 -5.449664174148938
88 -62.554709270596504 -5.372337529209199
89 -69.44689202308655 -5.344354409948577
90 -64.40886297821999 -5.277428945831074
91 -57.24000635743141 -5.213448468923901
92 -55.47878059744835 -5.134385017334278
93 -58.1100689470768 -5.083309440801453
94 -52.079302564263344 -5.070922019578875
95 -52.23088385909796 -5.057912895769321
96 -52.58800882846117 -5.032585097508004
97 -65.61545321345329 -4.967292017493492
98 -56.680766850709915 -4.8691417895559495
99 -47.09631888568401 -4.7889637182540605
100 -47.670414462685585 -4.690572823646376
101 -57.79907539486885 -4.570024001992525
102 -39.76738864183426 -4.489953204375678
103 -63.08101935684681 -4.230832004686763
104 -33.68357879668474 -4.0289597368494015
105 -52.1538562476635 -3.956219070100057
106 -43.17141832411289 -3.8707469442824127
107 -47.10396670550108 -3.840671824026386
108 -51.47888192534447 -3.733491819239518
109 -47.47800427675247 -3.573046562605251
110 -34.94113843142986 -3.47180306881435
111 -47.84871956706047 -3.4265739646588558
112 -61.19462434947491 -3.3322555012187633
113 -33.80826807022095 -3.1337228649797164
114 -42.02799938619137 -2.9901574687978743
115 -27.861683443188667 -2.8706402350041693
116 -38.37882028520107 -2.853154438109651
117 -61.12015491724014 -2.6416623314910934
118 -22.491018012166023 -2.4680487780541096
119 -46.356167018413544 -1.9136196540088464
train accuracy: 0.995
validation accuracy: 0.985
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.19774201 -12.68135973
 -12.66418206 -12.36141056 -12.30017947 -12.15190477 -12.03876266
 -11.78885214 -11.7544424  -11.58125245 -11.50342998 -11.44758345
 -11.44027698 -11.263937   -11.17227792 -11.15068141 -11.05685807
 -10.8699891  -10.66652037 -10.63179642 -10.3276815  -10.04606649
  -9.95058412  -9.85721598  -9.81100055  -9.80411195  -9.62645705
  -9.60739508  -9.28298288  -9.18321593  -9.0831059   -8.80529633
  -8.74582306  -8.66757674  -8.60665033  -8.54599172  -8.50428503
  -8.48339257  -8.47608826  -8.46182044  -8.39459794  -8.37884663
  -8.330117    -8.29377495  -8.24054637  -8.16125845  -8.15882305
  -8.13625094  -8.13319584  -8.10819769  -8.02522288  -8.01805343
  -7.98923078  -7.96553165  -7.8333171   -7.79457133  -7.79169537
  -7.60143925  -7.57787028  -7.57539849  -7.54845142  -7.49301515
  -7.38884702  -7.36606029  -7.36244313  -7.28899364  -7.22330361
  -7.10832736  -7.08364335  -7.0834432   -7.06146099  -7.01267849
  -6.99470221  -6.95906356  -6.81739378  -6.81266907  -6.77694649
  -6.72206384  -6.71997062  -6.62510243  -6.57972984  -6.53544734
  -6.53504175  -6.53472444  -6.51820418  -6.40172218  -6.39502342
  -6.33692603  -6.29473787  -6.29069253  -6.27441201  -6.18716155
  -6.15083379  -6.14091783  -6.1391128   -6.11093606  -6.10666554
  -6.07997549  -6.00945721  -6.00323896  -5.93199156  -5.92306092
  -5.90681709  -5.85989749  -5.84707082  -5.76313916  -5.73210893
  -5.72912203  -5.65191833  -5.61579673  -5.61157023  -5.60750431
  -5.59858366  -5.58594783  -5.5679035   -5.52392948  -5.5083445
  -5.49168143  -5.48907321  -5.44966417  -5.39976331  -5.37233753
  -5.3472021   -5.34435441  -5.31127297  -5.30476697  -5.29347176
  -5.27742895  -5.26470638  -5.21344847  -5.19726821  -5.13438502
  -5.08711915  -5.08330944  -5.07848501  -5.07092202  -5.068821
  -5.0579129   -5.03387903  -5.0325851   -5.02795798  -4.96729202
  -4.95826798  -4.90534583  -4.86914179  -4.82757292  -4.78896372
  -4.76433577  -4.70531971  -4.69057282  -4.68989129  -4.63049542
  -4.570024    -4.56262066  -4.51792143  -4.50732939  -4.4899532
  -4.47069905  -4.47054951  -4.42209995  -4.41137624  -4.4006447
  -4.32278906  -4.24438995  -4.230832    -4.08998614  -4.03104862
  -4.02895974  -4.01741858  -3.95621907  -3.95582809  -3.94746337
  -3.92759712  -3.87074694  -3.86023677  -3.85647259  -3.84067182
  -3.79322214  -3.74995966  -3.73349182  -3.72014202  -3.70689576
  -3.57438158  -3.57304656  -3.5374298   -3.51681177  -3.47180307
  -3.45446107  -3.42657396  -3.38446715  -3.3322555   -3.30583509
  -3.15109589  -3.14641338  -3.13372286  -3.05073542  -2.99015747
  -2.89513627  -2.87064024  -2.86030708  -2.85315444  -2.81621421
  -2.67572149  -2.64858572  -2.64166233  -2.62856817  -2.52464242
  -2.46838949  -2.46804878  -2.1701481   -1.91361965  -1.8736319 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.1324918173700131, val_acc 0.965
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3738e-01, -6.4472e-02, -2.6371e-02,  7.2737e-02,  2.4218e-01,
          9.1856e-02,  4.4220e-02,  1.5165e-02,  4.7495e-02, -9.0888e-02,
         -3.7692e-04, -1.4831e+00, -1.6364e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.014794820915986478, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1049e-01,  1.3129e-02, -7.8479e-02,  6.0686e-02,  6.6700e-01,
         -2.5506e-01,  2.7363e-02,  4.0908e-02, -1.3133e-01,  2.7264e-02,
         -3.7700e-04, -2.4668e+00, -2.7056e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.3709868251656683, val_acc 0.945
trigger times: 1
end of epoch 3: val_loss 0.3513670809939917, val_acc 0.94
trigger times: 2
end of epoch 4: val_loss 0.09209729634762784, val_acc 0.975
trigger times: 3
end of epoch 5: val_loss 0.0006563447401304857, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.5456e-02,  1.9265e-01, -4.5187e-02,  1.3440e-01,  1.0783e+00,
          1.1282e-01,  2.0665e-03, -4.3475e-02, -1.2575e-01, -1.9070e-01,
         -3.7736e-04, -4.1573e+00, -4.9692e+00]], device='cuda:0'))])
end of epoch 6: val_loss 4.330403554604345e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.3365e-02,  1.2377e-01,  1.1140e-05, -7.5784e-02,  1.1250e+00,
         -1.4170e-05,  3.8157e-07, -7.1895e-03, -7.0814e-02, -4.6277e-05,
         -3.7745e-04, -4.5315e+00, -5.5989e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.021233626668621924, val_acc 0.995
trigger times: 1
end of epoch 8: val_loss 1.6748031599433943e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1777e-02,  6.4411e-04, -2.2419e-01,  1.1538e-02,  8.5174e-01,
          1.7933e-05,  2.6686e-02,  1.4504e-02,  5.5956e-02,  3.1413e-05,
         -3.7761e-04, -4.0184e+00, -6.0378e+00]], device='cuda:0'))])
end of epoch 9: val_loss 6.561704513224242e-05, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.00011864232566463073, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.024244662979178173, val_acc 0.995
trigger times: 3
end of epoch 12: val_loss 0.02470798261023745, val_acc 0.995
trigger times: 4
end of epoch 13: val_loss 0.005291602058376945, val_acc 0.995
trigger times: 5
end of epoch 14: val_loss 2.7656711412049615e-06, val_acc 1.0
trigger times: 6
end of epoch 15: val_loss 2.443785188432912e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.8130e-01, -6.5503e-03, -2.2166e-01,  1.0086e-01,  1.3712e+00,
         -2.5275e-01,  5.7717e-02, -1.6170e-02, -2.5963e-01,  3.9265e-05,
         -3.7833e-04, -5.1669e+00, -5.6238e+00]], device='cuda:0'))])
end of epoch 16: val_loss 2.443618642971046e-06, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 1.7152028399891606e-06, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 6.508640592528536e-07, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 3.871510569730674e-05, val_acc 1.0
trigger times: 4
end of epoch 20: val_loss 6.220410383939168e-06, val_acc 1.0
trigger times: 5
end of epoch 21: val_loss 0.11344004855451739, val_acc 0.99
trigger times: 6
end of epoch 22: val_loss 0.0012007562938805094, val_acc 1.0
trigger times: 7
end of epoch 23: val_loss 2.9885849345703265e-05, val_acc 1.0
trigger times: 8
end of epoch 24: val_loss 2.9802246785948227e-08, val_acc 1.0
trigger times: 9
end of epoch 25: val_loss 4.7683532713449494e-08, val_acc 1.0
trigger times: 10
Early stopping.
0 -344.46720337867737 -54.98547503240923
1 -330.48520040512085 -49.75347184620696
2 -309.2043820619583 -46.98011874490918
3 -293.25884652137756 -44.99030608142343
4 -288.1958198547363 -43.81326882122305
5 -289.7219091653824 -42.00401746161006
6 -295.19875717163086 -41.68588229294918
7 -265.49855506420135 -40.34838365523108
8 -272.69474947452545 -39.57586365327889
9 -262.2132360935211 -38.45534493538269
10 -284.5460592508316 -38.35634328077039
11 -268.9767198562622 -37.66475323879293
12 -247.82782554626465 -37.1809993033689
13 -252.5983167886734 -36.821916772458344
14 -236.75147879123688 -36.20965269874363
15 -236.8744000196457 -35.78149902167743
16 -248.50115382671356 -35.26282499693737
17 -248.74182105064392 -35.0654408505187
18 -219.9005744457245 -34.64469044638467
19 -236.83058309555054 -31.969099402548657
20 -211.4350841343403 -31.64414355845032
21 -193.3524969816208 -31.12953085092458
22 -194.15835964679718 -29.340125609942326
23 -209.49349683523178 -27.343722362182305
24 -216.70973175764084 -27.07399028854534
25 -181.2657886147499 -25.548365085275513
26 -167.66205716133118 -24.879106999799365
27 -189.72408598661423 -23.978745577896312
28 -183.8878989815712 -23.44970807952351
29 -162.9801440834999 -22.19891031871716
30 -182.20639955997467 -20.444472560731253
31 -125.7945875376463 -19.63760343800059
32 -157.49365377426147 -18.92838809611677
33 -156.1931700706482 -16.823073927842348
34 -121.28394129872322 -14.531424598833084
35 -124.27583122253418 -13.197742005137894
36 -92.74299901723862 -12.66418205637357
37 -117.01285529136658 -12.151904772081672
38 -124.89073887467384 -11.788852141676486
39 -95.28415101766586 -11.503429977290427
40 -100.75380146503448 -11.263936998680489
41 -89.63959157466888 -11.150681410541043
42 -107.83062016963959 -10.666520373376597
43 -102.49039877951145 -10.327681503524177
44 -80.13564117252827 -9.8572159761571
45 -84.8420735001564 -9.804111953920238
46 -68.8111172914505 -9.282982883985197
47 -91.92937606573105 -9.083105900182241
48 -84.35593605041504 -8.667576741643373
49 -70.27916979789734 -8.545991718792262
50 -75.44177180528641 -8.47608826112123
51 -78.81018960475922 -8.39459794485309
52 -78.11156111955643 -8.293774950229068
53 -70.07144764065742 -8.161258447219758
54 -64.67817870527506 -8.133195842510668
55 -76.27211880683899 -8.025222881600993
56 -87.28938436508179 -7.965531645697844
57 -78.70637685060501 -7.794571331111077
58 -78.51946157217026 -7.5778702831550095
59 -77.77954179048538 -7.548451418241032
60 -78.03610947728157 -7.366060286776804
61 -44.828711330890656 -7.2889936408176155
62 -71.56392395496368 -7.083643351583706
63 -65.66599664092064 -7.061460991106612
64 -89.55116629600525 -6.959063561385431
65 -64.72095721960068 -6.812669072047181
66 -82.68218052387238 -6.719970621583102
67 -54.246959052979946 -6.579729840661269
68 -64.743309289217 -6.534724440491901
69 -46.38196691125631 -6.401722181555061
70 -72.05286860466003 -6.2947378714322015
71 -63.77943933010101 -6.27441201389399
72 -56.41191330552101 -6.140917833260766
73 -56.844542652368546 -6.110936063372426
74 -34.58618849515915 -6.009457213882819
75 -58.5230675637722 -5.931991558818616
76 -34.776498571038246 -5.859897494166216
77 -38.50538391619921 -5.763139161566328
78 -38.73538413643837 -5.6519183275843705
79 -54.17598929256201 -5.611570232886707
80 -39.57565029710531 -5.585947833145062
81 -40.777853421866894 -5.5083445022364455
82 -40.67701639980078 -5.489073208908178
83 -50.804359778761864 -5.372337529209199
84 -58.12934158742428 -5.344354409948577
85 -54.33373028039932 -5.293471763990831
86 -33.7059336528182 -5.264706377717007
87 -25.889415562152863 -5.134385017334278
88 -23.182818986475468 -5.083309440801453
89 -40.97119305282831 -5.068821004516289
90 -50.54208543896675 -5.033879032133242
91 -31.328298047184944 -4.967292017493492
92 -43.898739874362946 -4.9053458349614205
93 -30.16248508542776 -4.7889637182540605
94 -27.947003662586212 -4.70531970862426
95 -64.61168414354324 -4.63049541560991
96 -45.69709451496601 -4.562620661690879
97 -40.60913619399071 -4.489953204375678
98 -20.30470260977745 -4.470549505143693
99 -27.67537246644497 -4.400644702369627
100 -30.681215912103653 -4.2443899463055965
101 -43.68938085436821 -4.031048624093466
102 -20.347350656986237 -4.017418582042493
103 -13.508733704686165 -3.94746336797888
104 -13.505718529224396 -3.8707469442824127
105 -11.127872928977013 -3.840671824026386
106 -23.28561019152403 -3.74995966281504
107 -23.429578214883804 -3.7068957604541564
108 -23.261236399412155 -3.573046562605251
109 -27.4445725902915 -3.47180306881435
110 -24.096537336707115 -3.4265739646588558
111 -11.977196678519249 -3.3058350915403607
112 -26.246471270918846 -3.146413382929384
113 -18.05751060694456 -2.9901574687978743
114 -15.466962411999702 -2.8706402350041693
115 -14.886229947209358 -2.816214205114024
116 -5.639150947332382 -2.648585721105015
117 -13.02517680823803 -2.5246424218346637
118 -6.788853086531162 -2.4680487780541096
119 -1.8462026417255402 -1.8736319030296489
train accuracy: 0.9966666666666667
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.19774201 -12.68135973
 -12.66418206 -12.36141056 -12.30017947 -12.19069082 -12.15190477
 -12.03876266 -11.78885214 -11.7544424  -11.7486032  -11.60578338
 -11.58125245 -11.50342998 -11.44758345 -11.44027698 -11.43752132
 -11.263937   -11.17227792 -11.15068141 -11.05685807 -10.9481994
 -10.8699891  -10.66652037 -10.63179642 -10.4916003  -10.3276815
 -10.04606649  -9.95058412  -9.85721598  -9.8337543   -9.81100055
  -9.81038305  -9.80411195  -9.68216981  -9.62645705  -9.60739508
  -9.28298288  -9.18321593  -9.0831059   -8.95040663  -8.80529633
  -8.74582306  -8.66757674  -8.60665033  -8.54599172  -8.50428503
  -8.49820798  -8.48339257  -8.47608826  -8.46182044  -8.39459794
  -8.37884663  -8.330117    -8.29377495  -8.24054637  -8.16125845
  -8.15882305  -8.13625094  -8.13319584  -8.10819769  -8.02522288
  -8.01805343  -7.98923078  -7.96553165  -7.8333171   -7.81228846
  -7.79457133  -7.79169537  -7.60143925  -7.57787028  -7.57539849
  -7.54845142  -7.49301515  -7.43125388  -7.41043184  -7.38884702
  -7.36606029  -7.36244313  -7.28899364  -7.22330361  -7.10832736
  -7.08949692  -7.08364335  -7.0834432   -7.06146099  -7.01267849
  -6.99470221  -6.95906356  -6.84790582  -6.81739378  -6.81266907
  -6.77694649  -6.72206384  -6.71997062  -6.66094614  -6.65220004
  -6.62510243  -6.58528979  -6.57972984  -6.53544734  -6.53504175
  -6.53472444  -6.51820418  -6.49339869  -6.40172218  -6.39502342
  -6.33692603  -6.29473787  -6.29069253  -6.28883286  -6.27441201
  -6.2490551   -6.18716155  -6.15883397  -6.15097165  -6.15083379
  -6.14091783  -6.1391128   -6.11093606  -6.10666554  -6.07997549
  -6.0296951   -6.00945721  -6.00323896  -5.99246386  -5.93199156
  -5.92306092  -5.90681709  -5.85989749  -5.84707082  -5.83739035
  -5.76313916  -5.73395517  -5.73210893  -5.72912203  -5.69770479
  -5.65191833  -5.61579673  -5.61157023  -5.60750431  -5.59858366
  -5.58594783  -5.5679035   -5.52392948  -5.5083445   -5.49168143
  -5.48907321  -5.44966417  -5.39976331  -5.39108598  -5.37233753
  -5.3545642   -5.3472021   -5.34435441  -5.31619277  -5.31136598
  -5.31127297  -5.30476697  -5.29347176  -5.27742895  -5.26470638
  -5.21344847  -5.19726821  -5.13438502  -5.08711915  -5.0846262
  -5.08330944  -5.07848501  -5.07092202  -5.068821    -5.0579129
  -5.03387903  -5.0325851   -5.02795798  -4.96729202  -4.95826798
  -4.90534583  -4.86914179  -4.82757292  -4.81229025  -4.78896372
  -4.76884416  -4.76433577  -4.70825149  -4.70531971  -4.69057282
  -4.68989129  -4.63049542  -4.61186713  -4.57574494  -4.570024
  -4.56262066  -4.52397219  -4.51792143  -4.50828701  -4.50732939
  -4.4899532   -4.47069905  -4.47054951  -4.44132681  -4.4349221
  -4.42209995  -4.41137624  -4.4006447   -4.37736032  -4.32278906
  -4.24438995  -4.230832    -4.15475892  -4.08998614  -4.0692781
  -4.03104862  -4.02895974  -4.01741858  -3.95621907  -3.95582809
  -3.94746337  -3.92759712  -3.90435854  -3.90359795  -3.87074694
  -3.86023677  -3.85647259  -3.84067182  -3.79322214  -3.75643983
  -3.74995966  -3.73349182  -3.72014202  -3.70689576  -3.66793231
  -3.66109683  -3.57438158  -3.57304656  -3.5374298   -3.51681177
  -3.47180307  -3.45446107  -3.42657396  -3.38446715  -3.3322555
  -3.30583509  -3.27058411  -3.24866599  -3.15109589  -3.14641338
  -3.13372286  -3.05073542  -3.04266895  -2.99015747  -2.93487271
  -2.89513627  -2.88798778  -2.87064024  -2.86030708  -2.85315444
  -2.81621421  -2.70303836  -2.67572149  -2.66380806  -2.64858572
  -2.64166233  -2.62856817  -2.52464242  -2.47440275  -2.46838949
  -2.46804878  -2.36648187  -2.1701481   -1.91361965  -1.8736319 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.16915083457911487, val_acc 0.97
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1188e-01,  1.0771e-01,  3.8065e-02, -3.7415e-02,  4.3526e-01,
          1.5900e-01, -5.6579e-02, -2.5649e-03, -3.3124e-01,  2.8344e-01,
         -3.7692e-04, -1.8360e+00, -1.4138e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.08437826459620937, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.9959e-01,  9.8361e-02,  2.0578e-02,  4.4854e-02,  4.5918e-01,
          4.1593e-05, -4.1744e-02, -1.8786e-02, -5.2289e-02,  2.1946e-01,
         -3.7700e-04, -2.4180e+00, -2.4094e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.26245606836181984, val_acc 0.95
trigger times: 1
end of epoch 3: val_loss 0.013148706544283932, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.9188e-02,  3.0784e-02,  1.1084e-01,  2.4624e-02,  9.5419e-01,
          2.7323e-03, -1.1098e-02, -3.3837e-03, -3.4962e-01,  1.3884e-01,
         -3.7719e-04, -3.9463e+00, -3.2600e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.003940621168269161, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.7011e-02, -5.7497e-06, -1.1059e-02,  8.6877e-06,  7.9361e-01,
         -8.6092e-02,  5.1092e-03,  4.6968e-03, -4.5635e-05,  1.1286e-01,
         -3.7728e-04, -3.8668e+00, -3.3498e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.11627381061569021, val_acc 0.98
trigger times: 1
end of epoch 6: val_loss 0.08499480386016409, val_acc 0.99
trigger times: 2
end of epoch 7: val_loss 0.027101149119245774, val_acc 0.99
trigger times: 3
end of epoch 8: val_loss 0.0018257158855527677, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7062e-01, -1.0256e-01,  4.8767e-02,  6.5118e-02,  1.2264e+00,
          8.6955e-02, -2.0164e-02, -2.9850e-02,  2.0619e-04,  1.6130e-01,
         -3.7761e-04, -5.1980e+00, -4.5402e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.00017510643642278722, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.4203e-02, -4.5322e-03,  1.5202e-01,  7.8278e-02,  1.4283e+00,
         -5.2978e-03, -6.1756e-02, -3.1360e-02, -2.3172e-01,  1.3181e-01,
         -3.7770e-04, -5.8395e+00, -4.9580e+00]], device='cuda:0'))])
end of epoch 10: val_loss 1.2613326821586668e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.3255e-02, -4.4090e-02,  1.2739e-01,  9.4339e-02,  1.2865e+00,
         -1.3768e-01, -1.4695e-02, -4.3243e-02, -4.8432e-04,  3.1805e-01,
         -3.7781e-04, -5.5241e+00, -5.4432e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.0033394396585051923, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.08230384410608436, val_acc 0.985
trigger times: 2
end of epoch 13: val_loss 0.0247235491661084, val_acc 0.995
trigger times: 3
end of epoch 14: val_loss 0.0002661462581531282, val_acc 1.0
trigger times: 4
end of epoch 15: val_loss 0.019514262742706272, val_acc 0.995
trigger times: 5
end of epoch 16: val_loss 0.0012693357464148747, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 1.079972758937231e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.9211e-03,  2.4187e-02,  1.0955e-01, -9.3739e-07,  1.1392e+00,
         -1.6930e-01, -3.8379e-02, -3.5001e-02,  1.3725e-04,  2.2348e-01,
         -3.7854e-04, -6.4660e+00, -6.1224e+00]], device='cuda:0'))])
end of epoch 18: val_loss 2.67044750583878e-05, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.2139371381567059, val_acc 0.975
trigger times: 2
end of epoch 20: val_loss 0.023246177337265018, val_acc 0.99
trigger times: 3
end of epoch 21: val_loss 0.010601556090329397, val_acc 0.995
trigger times: 4
end of epoch 22: val_loss 0.02516653355991824, val_acc 0.995
trigger times: 5
end of epoch 23: val_loss 0.1558507941506502, val_acc 0.965
trigger times: 6
end of epoch 24: val_loss 1.716677900933661e-05, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 0.015675938372697224, val_acc 0.995
trigger times: 8
end of epoch 26: val_loss 0.0004948431311307289, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 0.20241922963221473, val_acc 0.97
trigger times: 10
Early stopping.
0 -259.2600164413452 -54.98547503240923
1 -215.48334795236588 -49.75347184620696
2 -219.44892239570618 -45.7351542845057
3 -243.96139705181122 -44.14602409201361
4 -204.6730033159256 -42.29180714825394
5 -189.97111451625824 -41.68588229294918
6 -194.02177584171295 -40.34838365523108
7 -199.40427684783936 -39.31972693233231
8 -172.73117458820343 -38.41270390343083
9 -209.68644338846207 -37.741528994987384
10 -189.93163239955902 -37.1809993033689
11 -179.6647315621376 -36.821916772458344
12 -163.043430685997 -36.19207561676116
13 -169.40353780984879 -35.394503873250635
14 -146.38905718922615 -35.209705244501436
15 -151.74786031246185 -34.64469044638467
16 -162.4822404384613 -31.969099402548657
17 -132.98763078451157 -31.392382758954444
18 -141.52705073356628 -29.39157139549552
19 -139.73239383101463 -27.41102349748205
20 -139.39906537532806 -27.07399028854534
21 -109.84334734082222 -25.548365085275513
22 -111.83202242851257 -24.828695359328833
23 -138.26202017068863 -23.57262108435893
24 -142.41106569766998 -22.60679894414887
25 -130.20412296056747 -20.444472560731253
26 -102.28181383013725 -19.63760343800059
27 -125.00505220890045 -17.994774057192853
28 -73.7135700546205 -14.855082803515382
29 -63.89077630639076 -13.596012850960644
30 -78.35712242126465 -12.36141055653559
31 -94.39654248952866 -12.151904772081672
32 -74.30311954021454 -11.754442396551
33 -76.89199072122574 -11.58125245460887
34 -77.01072126626968 -11.440276982718862
35 -93.12308472394943 -11.172277917644152
36 -106.93713116645813 -10.948199401833106
37 -72.05152297019958 -10.63179642380298
38 -84.29223531484604 -10.046066492216982
39 -75.07731220126152 -9.833754303108183
40 -84.95610076189041 -9.804111953920238
41 -71.91757589578629 -9.607395075700598
42 -66.54625049233437 -9.083105900182241
43 -85.685881793499 -8.745823057304115
44 -76.58293372392654 -8.545991718792262
45 -61.21812802553177 -8.483392567552581
46 -79.38021743297577 -8.39459794485309
47 -59.24893456697464 -8.293774950229068
48 -62.41117903590202 -8.158823049498253
49 -35.914027750492096 -8.108197691178031
50 -59.31070274114609 -7.989230784618642
51 -53.2198940217495 -7.8122884587201264
52 -68.7568987607956 -7.601439253533187
53 -51.32561084628105 -7.548451418241032
54 -49.91313725709915 -7.410431838341715
55 -71.98244008421898 -7.362443126623615
56 -71.36498910188675 -7.108327355338034
57 -46.00004306435585 -7.083443196201368
58 -63.07830935716629 -6.994702212975434
59 -67.18662440776825 -6.817393778147772
60 -69.51202061772346 -6.7220638398623045
61 -44.16128233075142 -6.652200037220215
62 -65.91461658477783 -6.579729840661269
63 -40.852785393595695 -6.534724440491901
64 -61.64728170633316 -6.401722181555061
65 -50.92327469587326 -6.2947378714322015
66 -38.95961934328079 -6.27441201389399
67 -38.30156637728214 -6.158833966799478
68 -35.033847749233246 -6.140917833260766
69 -38.34931444376707 -6.106665540821106
70 -54.86901938915253 -6.009457213882819
71 -36.179319716989994 -5.931991558818616
72 -53.01947546005249 -5.859897494166216
73 -54.19811503589153 -5.763139161566328
74 -60.591070145368576 -5.729122026973944
75 -38.85703954845667 -5.615796733870542
76 -56.0828418135643 -5.598583657445803
77 -39.58987471833825 -5.523929484100368
78 -54.07794797420502 -5.489073208908178
79 -51.87028729915619 -5.391085977106272
80 -35.50166779384017 -5.34720210027791
81 -31.437076970934868 -5.3113659781216525
82 -32.571683436632156 -5.293471763990831
83 -53.30362391471863 -5.213448468923901
84 -29.316497206687927 -5.087119147583023
85 -31.677826948463917 -5.078485007852753
86 -56.78674140572548 -5.057912895769321
87 -33.9189886637032 -5.027957977402961
88 -28.996663104742765 -4.9053458349614205
89 -49.956669330596924 -4.812290252381995
90 -28.51929811388254 -4.708251494928173
91 -51.65876895189285 -4.689891294335198
92 -25.730793803930283 -4.57574494266731
93 -28.508917272090912 -4.523972189173236
94 -35.548095397651196 -4.50732939454084
95 -45.96580082178116 -4.470549505143693
96 -25.593629747629166 -4.422099954832091
97 -44.864363968372345 -4.37736032188778
98 -30.992636881768703 -4.230832004686763
99 -46.40499898791313 -4.0692781033180445
100 -34.848986744880676 -4.017418582042493
101 -39.15783876180649 -3.94746336797888
102 -43.92471835017204 -3.9035979499012985
103 -42.845291405916214 -3.8564725920033256
104 -17.941886499524117 -3.7564398279379416
105 -41.833697855472565 -3.7201420162334404
106 -38.73459753394127 -3.661096831041371
107 -40.61569365859032 -3.537429795563063
108 -39.617434203624725 -3.454461067043341
109 -17.72790515422821 -3.3322555012187633
110 -35.004428029060364 -3.2486659853444317
111 -39.954288601875305 -3.1337228649797164
112 -7.932927057147026 -2.9901574687978743
113 -32.406568586826324 -2.8879877785143586
114 -6.255247130990028 -2.853154438109651
115 -30.51961813867092 -2.675721485626715
116 -10.898646235466003 -2.6416623314910934
117 -21.203272260725498 -2.474402754634763
118 -28.899297297000885 -2.366481866360528
119 -2.4415695816278458 -1.8736319030296489
train accuracy: 0.9655555555555555
validation accuracy: 0.97
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -13.19774201 -12.68135973
 -12.67705898 -12.66418206 -12.5165585  -12.36141056 -12.30017947
 -12.19069082 -12.15190477 -12.03876266 -11.78885214 -11.7544424
 -11.7486032  -11.60578338 -11.58125245 -11.50342998 -11.44758345
 -11.44027698 -11.43752132 -11.263937   -11.17227792 -11.15068141
 -11.06688866 -11.05685807 -10.9481994  -10.8699891  -10.68188728
 -10.66652037 -10.63179642 -10.4916003  -10.3276815  -10.04606649
  -9.95058412  -9.8820048   -9.85721598  -9.8337543   -9.81100055
  -9.81038305  -9.80411195  -9.68216981  -9.62645705  -9.60739508
  -9.28298288  -9.18321593  -9.10176995  -9.0831059   -8.95040663
  -8.80529633  -8.74582306  -8.66757674  -8.60665033  -8.54599172
  -8.50428503  -8.49820798  -8.48339257  -8.47608826  -8.46182044
  -8.39459794  -8.37884663  -8.330117    -8.29377495  -8.24054637
  -8.16125845  -8.15882305  -8.13625094  -8.13319584  -8.10819769
  -8.02522288  -8.01805343  -7.98923078  -7.96553165  -7.94118588
  -7.8333171   -7.81228846  -7.79457133  -7.79169537  -7.77582069
  -7.71778558  -7.60143925  -7.59958019  -7.57787028  -7.57539849
  -7.54845142  -7.49301515  -7.43125388  -7.41043184  -7.38884702
  -7.37423508  -7.36606029  -7.36244313  -7.36054637  -7.28899364
  -7.22330361  -7.10832736  -7.08949692  -7.08364335  -7.0834432
  -7.06146099  -7.01267849  -7.00244155  -6.99470221  -6.98457114
  -6.95906356  -6.9435563   -6.92222681  -6.86307055  -6.84790582
  -6.81739378  -6.81266907  -6.77694649  -6.74942045  -6.72206384
  -6.71997062  -6.70539912  -6.67025469  -6.66094614  -6.65220004
  -6.62510243  -6.59839208  -6.58611973  -6.58528979  -6.57972984
  -6.53544734  -6.53504175  -6.53472444  -6.51820418  -6.49339869
  -6.40172218  -6.39502342  -6.36210763  -6.33692603  -6.29473787
  -6.29069253  -6.28883286  -6.27441201  -6.24960382  -6.2490551
  -6.23144657  -6.21751152  -6.2020568   -6.18772149  -6.18716155
  -6.17269007  -6.15883397  -6.15399606  -6.15097165  -6.15083379
  -6.14091783  -6.1391128   -6.11093606  -6.10666554  -6.07997549
  -6.0296951   -6.00945721  -6.00323896  -5.99246386  -5.93199156
  -5.92306092  -5.90681709  -5.85989749  -5.84707082  -5.83739035
  -5.83029774  -5.76313916  -5.73395517  -5.73210893  -5.72912203
  -5.71280901  -5.69770479  -5.65551462  -5.65191833  -5.64808835
  -5.61579673  -5.61157023  -5.60750431  -5.59858366  -5.58594783
  -5.5679035   -5.52392948  -5.5083445   -5.49168143  -5.48907321
  -5.44966417  -5.39976331  -5.39108598  -5.37233753  -5.3545642
  -5.3472021   -5.34435441  -5.31619277  -5.31136598  -5.31127297
  -5.30476697  -5.29347176  -5.27742895  -5.26470638  -5.21344847
  -5.19726821  -5.13438502  -5.08711915  -5.0846262   -5.08330944
  -5.07848501  -5.07092202  -5.068821    -5.0579129   -5.03387903
  -5.0325851   -5.02795798  -4.96729202  -4.95826798  -4.90534583
  -4.86914179  -4.84861002  -4.82757292  -4.81229025  -4.78896372
  -4.76884416  -4.76433577  -4.76218596  -4.73502237  -4.70825149
  -4.70531971  -4.69057282  -4.68989129  -4.63049542  -4.6280215
  -4.61186713  -4.59235011  -4.57574494  -4.570024    -4.56262066
  -4.52397219  -4.51792143  -4.50828701  -4.50732939  -4.50203133
  -4.50080854  -4.4899532   -4.47069905  -4.47054951  -4.45435127
  -4.44132681  -4.4349221   -4.42209995  -4.41137624  -4.40832387
  -4.4006447   -4.39266224  -4.37736032  -4.32278906  -4.24438995
  -4.230832    -4.15475892  -4.14463776  -4.08998614  -4.0692781
  -4.03361298  -4.03104862  -4.02895974  -4.01741858  -3.95621907
  -3.95582809  -3.94746337  -3.92759712  -3.90435854  -3.90359795
  -3.87074694  -3.86023677  -3.85647259  -3.84067182  -3.79322214
  -3.75643983  -3.74995966  -3.73349182  -3.72014202  -3.70689576
  -3.66793231  -3.66247854  -3.66109683  -3.57555093  -3.57438158
  -3.57304656  -3.5374298   -3.53528161  -3.51681177  -3.47180307
  -3.45446107  -3.42657396  -3.38446715  -3.3772705   -3.36618513
  -3.3322555   -3.30583509  -3.27058411  -3.24866599  -3.15109589
  -3.14641338  -3.13372286  -3.09042406  -3.05073542  -3.04266895
  -3.02616186  -2.99015747  -2.94646144  -2.93487271  -2.91146625
  -2.89513627  -2.88798778  -2.87064024  -2.86030708  -2.85315444
  -2.81621421  -2.75848691  -2.71743154  -2.70303836  -2.67572149
  -2.66380806  -2.64858572  -2.64166233  -2.62856817  -2.52464242
  -2.47440275  -2.46838949  -2.46804878  -2.44448029  -2.36648187
  -2.31513817  -2.1701481   -1.94099867  -1.91361965  -1.8736319 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.07819332333972497, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2814e-01,  9.6918e-02,  4.1059e-02, -3.0919e-02,  6.8437e-01,
          2.2765e-02, -2.9504e-02, -6.3293e-03, -9.4654e-02,  1.5052e-01,
         -3.7692e-04, -1.7603e+00, -1.5416e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.0984186414131295, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.03653895190745505, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4012e-01,  1.4284e-01, -6.2575e-02, -6.5044e-02,  1.0966e+00,
         -2.2435e-01, -2.0299e-02,  2.6150e-02, -1.3792e-01,  8.9399e-02,
         -3.7710e-04, -3.2084e+00, -3.0823e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.10975084831117485, val_acc 0.99
trigger times: 1
end of epoch 4: val_loss 4.01268785793718e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.3453e-01,  1.6101e-01, -3.4367e-04,  1.5523e-02,  1.3090e+00,
         -3.1738e-01,  1.0960e-02,  6.3877e-04,  1.7665e-02,  2.5830e-01,
         -3.7728e-04, -3.9480e+00, -4.1254e+00]], device='cuda:0'))])
end of epoch 5: val_loss 0.0001299012883506734, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.41074408158314396, val_acc 0.955
trigger times: 2
end of epoch 7: val_loss 0.13257266661516212, val_acc 0.99
trigger times: 3
end of epoch 8: val_loss 2.422946562742112e-05, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 0.1149329691928821, val_acc 0.99
trigger times: 5
end of epoch 10: val_loss 6.001866153582114e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.3142e-02,  1.5322e-01,  1.5144e-02,  5.4734e-02,  1.3179e+00,
         -1.2442e-01, -1.6323e-02, -1.5047e-02, -2.3406e-01,  7.3207e-02,
         -3.7781e-04, -6.0324e+00, -5.6218e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.1013110884601059, val_acc 0.985
trigger times: 1
end of epoch 12: val_loss 0.00013296281912285934, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1482e-01,  1.5409e-01,  4.4556e-04,  1.2022e-02,  1.3578e+00,
         -4.0351e-02, -4.3228e-03, -2.2832e-02, -6.6446e-02, -3.1773e-05,
         -3.7812e-04, -6.2152e+00, -5.9293e+00]], device='cuda:0'))])
end of epoch 14: val_loss 0.0003557944576845884, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 0.16800655008055373, val_acc 0.99
trigger times: 2
end of epoch 16: val_loss 2.9802313861182485e-09, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 1.5429351516296208e-05, val_acc 1.0
trigger times: 4
end of epoch 18: val_loss 2.026554053458085e-08, val_acc 1.0
trigger times: 5
end of epoch 19: val_loss 5.604200293802819e-06, val_acc 1.0
trigger times: 6
end of epoch 20: val_loss 3.461113426507012e-05, val_acc 1.0
trigger times: 7
end of epoch 21: val_loss 0.051691717037465425, val_acc 0.995
trigger times: 8
end of epoch 22: val_loss 0.12756561138318598, val_acc 0.985
trigger times: 9
end of epoch 23: val_loss 0.04070563997327326, val_acc 0.995
trigger times: 10
Early stopping.
0 -385.5370969772339 -54.98547503240923
1 -336.8666766881943 -49.72654640753777
2 -305.19008207321167 -45.670579884154705
3 -321.75965642929077 -43.18878399086166
4 -299.80808687210083 -41.6910044370425
5 -285.8227870464325 -40.34838365523108
6 -324.98474049568176 -39.31972693233231
7 -310.47698283195496 -38.35634328077039
8 -290.3791182041168 -37.66475323879293
9 -274.1929290294647 -37.00630588930485
10 -265.8686625957489 -36.20965269874363
11 -267.7784605026245 -35.394503873250635
12 -246.03178519010544 -35.209705244501436
13 -261.2978343963623 -33.84284985953318
14 -254.12370884418488 -31.7109134007892
15 -212.7465282678604 -31.12953085092458
16 -242.01629543304443 -29.106189988903285
17 -239.30989801883698 -27.07399028854534
18 -197.2682704925537 -25.548365085275513
19 -215.67838764190674 -24.592745144504722
20 -191.73467141389847 -23.44970807952351
21 -171.20020991563797 -20.656863763892378
22 -153.61933332681656 -20.13839114930498
23 -172.39212548732758 -17.994774057192853
24 -128.78699338436127 -14.531424598833084
25 -144.10349667072296 -13.197742005137894
26 -148.67138051986694 -12.516558496374797
27 -131.34108579158783 -12.190690822448978
28 -126.92343854904175 -11.754442396551
29 -130.0962232351303 -11.58125245460887
30 -144.69933331012726 -11.437521319922642
31 -103.33663535118103 -11.150681410541043
32 -122.52765727043152 -10.869989101210326
33 -123.82399505376816 -10.63179642380298
34 -111.36824119091034 -9.950584120932124
35 -129.9596300125122 -9.833754303108183
36 -106.76262658834457 -9.682169807823145
37 -83.67206853628159 -9.282982883985197
38 -94.43992787599564 -8.95040663459217
39 -98.85864037275314 -8.667576741643373
40 -100.89811152219772 -8.49820797786387
41 -98.89081704616547 -8.461820438939686
42 -93.55662786960602 -8.293774950229068
43 -91.94025003910065 -8.158823049498253
44 -92.09206825494766 -8.025222881600993
45 -95.41519781947136 -7.965531645697844
46 -85.21868968009949 -7.794571331111077
47 -94.58408439159393 -7.717785577412557
48 -107.85250240564346 -7.57539849177145
49 -80.2468911409378 -7.410431838341715
50 -84.91877076029778 -7.366060286776804
51 -79.94698017835617 -7.22330361482762
52 -77.37296682596207 -7.083643351583706
53 -77.91387903690338 -7.002441551744797
54 -103.64131182432175 -6.959063561385431
55 -73.53418901562691 -6.8479058222173625
56 -92.02465170621872 -6.776946485018116
57 -93.45005494356155 -6.705399124265959
58 -74.70707386732101 -6.652200037220215
59 -73.04748928546906 -6.585289787204661
60 -72.60836920142174 -6.535041751864854
61 -59.13175602257252 -6.401722181555061
62 -71.64858976006508 -6.336926030371917
63 -69.54067671298981 -6.27441201389399
64 -89.13662672042847 -6.231446567160199
65 -71.65110716223717 -6.187161554873795
66 -85.22268486022949 -6.153996058466646
67 -73.9788488149643 -6.1391128046539185
68 -71.87066221237183 -6.079975490236236
69 -64.91724994778633 -5.992463859144966
70 -46.6845745742321 -5.906817089641233
71 -62.4253778681159 -5.830297739720067
72 -54.69105768203735 -5.729122026973944
73 -57.21105387061834 -5.655514617825038
74 -60.63611352443695 -5.611570232886707
75 -44.88493610918522 -5.585947833145062
76 -54.71414767950773 -5.4916814327364225
77 -33.63670962303877 -5.399763312756924
78 -65.49468851089478 -5.34720210027791
79 -61.96344439685345 -5.3113659781216525
80 -41.46793396770954 -5.277428945831074
81 -56.779792100191116 -5.19726821032636
82 -32.81185870617628 -5.083309440801453
83 -46.20208363234997 -5.068821004516289
84 -82.34690767526627 -5.027957977402961
85 -45.11971007287502 -4.9053458349614205
86 -36.13378243148327 -4.812290252381995
87 -45.242896378040314 -4.7643357680855365
88 -38.829746805131435 -4.70531970862426
89 -75.39687666296959 -4.63049541560991
90 -40.09476648271084 -4.57574494266731
91 -47.55084075033665 -4.523972189173236
92 -70.74648469686508 -4.502031327425925
93 -30.21935050934553 -4.470699048583563
94 -22.381359465420246 -4.434922100666425
95 -65.09019136428833 -4.408323866958383
96 -28.245235458016396 -4.322789056139424
97 -41.69027654826641 -4.144637762114799
98 -40.887001261115074 -4.033612978544639
99 -33.59595130383968 -3.956219070100057
100 -26.251570761203766 -3.927597122366986
101 -26.040086194872856 -3.860236772294199
102 -22.799687281250954 -3.7932221417852072
103 -20.498016640543938 -3.7201420162334404
104 -46.69901469349861 -3.662478539868289
105 -30.902388088405132 -3.573046562605251
106 -26.914247930049896 -3.5168117701900856
107 -52.801453076303005 -3.3844671463622564
108 -33.99339202046394 -3.3322555012187633
109 -11.414401218295097 -3.1510958941048406
110 -26.098612047731876 -3.090424060902202
111 -21.78951482474804 -2.9901574687978743
112 -29.01027800142765 -2.9114662538091856
113 -24.296813741326332 -2.8603070847096705
114 -33.61828134581447 -2.758486912410167
115 -8.764217913150787 -2.6638080634370302
116 -11.143407374620438 -2.6285681737656295
117 -8.509971961379051 -2.4680487780541096
118 -24.284094482660294 -2.3151381663534036
119 -4.20949974656105 -1.8736319030296489
train accuracy: 0.9966666666666667
validation accuracy: 0.995

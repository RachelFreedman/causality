[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:3
end of epoch 0: val_loss 0.04267763541990824, val_acc 0.975
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.0205e-02, -4.1206e-02, -9.4065e-03, -3.2914e-02, -7.2130e-01,
         -1.5672e-02, -9.5210e-03, -1.9513e-03,  2.8188e-01, -5.1980e-01,
          1.7067e-03, -2.5890e+00]], device='cuda:3'))])
end of epoch 1: val_loss 0.0007303968374784375, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4161e-02,  5.5135e-07,  1.3752e-02,  3.4597e-04, -5.8542e-01,
          6.1290e-05,  3.5420e-04,  1.6770e-03, -3.5030e-05, -9.9137e-02,
          1.7069e-03, -2.9732e+00]], device='cuda:3'))])
end of epoch 2: val_loss 1.4663146384000922, val_acc 0.88
trigger times: 1
end of epoch 3: val_loss 4.9848189220007557e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.4848e-06, -5.3904e-06,  2.4237e-02, -1.0504e-02, -3.9233e-01,
          3.2693e-08, -3.2297e-03,  9.3296e-03,  1.6968e-05, -2.0504e-01,
          1.7072e-03, -4.2606e+00]], device='cuda:3'))])
end of epoch 4: val_loss 0.6243453776017316, val_acc 0.925
trigger times: 1
end of epoch 5: val_loss 1.6883819346098504e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4842e-03, -4.0392e-04,  1.8070e-02,  5.0459e-03, -7.7034e-01,
         -4.0167e-05, -4.9748e-07, -1.0505e-06,  1.8493e-05, -3.9561e-02,
          1.7076e-03, -5.5873e+00]], device='cuda:3'))])
end of epoch 6: val_loss 0.04997921994697378, val_acc 0.985
trigger times: 1
end of epoch 7: val_loss 2.0756840809212916e-05, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 2.094061184905627e-05, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 1.27668507108325e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.8139e-02, -3.7267e-02,  9.4965e-02, -1.3031e-02, -7.4798e-01,
         -1.8215e-02, -7.1322e-03,  2.0231e-02,  1.6257e-01, -6.2878e-05,
          1.7082e-03, -6.6451e+00]], device='cuda:3'))])
end of epoch 10: val_loss 9.795760840066947e-05, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 2.7060136325474105e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7959e-01, -1.4057e-01,  7.2851e-02, -3.2354e-05, -6.3044e-01,
         -8.3492e-03,  3.1910e-04, -2.1422e-03,  7.7461e-05,  1.2313e-05,
          1.7086e-03, -7.6136e+00]], device='cuda:3'))])
end of epoch 12: val_loss 4.712616011976726, val_acc 0.8
trigger times: 1
end of epoch 13: val_loss 0.00033398776573900335, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 8.69496987249363e-06, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 5.544364765448506e-05, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 6.532799267802147e-06, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 0.0260748699231085, val_acc 0.985
trigger times: 6
end of epoch 18: val_loss 0.00023518479452313556, val_acc 1.0
trigger times: 7
end of epoch 19: val_loss 5.761432248831966e-06, val_acc 1.0
trigger times: 8
end of epoch 20: val_loss 2.7975519760694566e-05, val_acc 1.0
trigger times: 9
end of epoch 21: val_loss 9.840246585213207e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -103.24306550621986 -50.00222059884506
1 -73.21678823232651 -48.823739140882175
2 -75.66602948307991 -47.15605336419176
3 -74.45797544717789 -46.19619104961985
4 -102.04917585849762 -45.594227093057754
5 -88.13706082105637 -45.36842966452394
6 -113.26958012580872 -45.19068756322445
7 -82.0720425248146 -44.16490839583478
8 -55.53613956272602 -44.078303125872196
9 -89.2738515585661 -43.993555290419714
10 -61.29067404568195 -43.86306534422809
11 -90.76153060793877 -43.84874807044028
12 -83.66692972183228 -43.84199129025074
13 -58.48459681868553 -43.806385671938365
14 -55.765030920505524 -43.80581985978556
15 -66.92742788791656 -42.7594767358323
16 -53.200587432831526 -42.66316468983175
17 -84.30961689352989 -42.33177224591743
18 -79.53078782558441 -41.774963389485094
19 -76.01623699069023 -41.410068073767725
20 -81.38040524721146 -41.17786296442943
21 -68.51564002037048 -40.723520424948155
22 -83.49472564458847 -40.527189756101116
23 -78.658946454525 -40.49595848244517
24 -85.701036632061 -40.429389880911344
25 -78.96701312065125 -40.05653450521898
26 -76.23029270768166 -39.59232357792555
27 -100.75621604919434 -39.54162101198148
28 -78.09008422493935 -39.195237471709476
29 -89.57820624113083 -39.172389579378766
30 -72.21302470564842 -38.51095963496708
31 -61.84885357320309 -38.447265769744824
32 -83.44933319091797 -38.392107037026264
33 -56.146942764520645 -38.00745349944469
34 -57.54266133904457 -37.46482488602393
35 -82.12603718042374 -37.10988160586883
36 -72.13870561122894 -34.27116723637227
37 -50.821291357278824 -34.14139118114101
38 -50.32991113513708 -33.263072731706835
39 -64.31020185351372 -33.13344797200536
40 -49.010226130485535 -33.07825234291984
41 -74.315061211586 -33.0321314765637
42 -92.27844616770744 -32.44934973065406
43 -47.62838298082352 -32.4007978120153
44 -64.0533436536789 -32.40063925734975
45 -38.83912014961243 -30.734403792103194
46 -39.62094330787659 -30.57151371770873
47 -59.149610579013824 -30.131236504472803
48 -50.06433856487274 -29.99326722619033
49 -52.32942968606949 -29.66908258985071
50 -83.86322577297688 -29.297233511513635
51 -50.840511590242386 -29.288890423975797
52 -39.91457745432854 -29.145878352769948
53 -39.97254441678524 -28.49601894351319
54 -54.74791193008423 -28.492023661124072
55 -62.88829459249973 -28.315961465855167
56 -44.951569959521294 -27.121110566589827
57 -35.242810562253 -26.064532595535336
58 -46.307939901947975 -25.520524278341334
59 -43.472893327474594 -25.27101421179229
60 -45.80853536725044 -25.066644278800943
61 -52.355364382267 -24.925849381327673
62 -51.03028151392937 -24.188105669766596
63 -41.433886498212814 -23.48479966198816
64 -24.542610801756382 -23.153943559703283
65 -37.264746978878975 -22.954730295117237
66 -41.013392984867096 -22.74124885266394
67 -42.022287314757705 -22.739273544503753
68 -29.700884521007538 -22.264945050603636
69 -49.09773504734039 -22.15569724300287
70 -38.74954038858414 -21.055920928583344
71 -30.591085739433765 -20.543356562348553
72 -37.01789936423302 -20.33499633836848
73 -36.636556036770344 -20.18157658281111
74 -37.94100325554609 -19.58144410477429
75 -37.68727542459965 -19.377225745334304
76 -36.107713490724564 -19.243135617403095
77 -31.223204359412193 -19.060620225371707
78 -47.68237718939781 -18.964124524696246
79 -41.06034304201603 -18.448962308005108
80 -45.955642104148865 -17.740722019993825
81 -26.090509057044983 -16.85889369985028
82 -29.438136398792267 -16.3381194095591
83 -30.38762902095914 -14.535892564189266
84 -31.778800785541534 -14.443670567499144
85 -26.34175643697381 -14.200413010108107
86 -35.643298119306564 -13.936976181618805
87 -30.861468940973282 -13.862253042167257
88 -36.56946128979325 -13.483098530680483
89 -29.603876426815987 -13.455892754889845
90 -28.165038041770458 -13.355868275096913
91 -30.930635385215282 -12.278515244993585
92 -20.98414658755064 -12.227387460046547
93 -25.39039920270443 -12.020717825467683
94 -35.69336259365082 -11.910094799877324
95 -27.538830429315567 -11.400284019256157
96 -27.929926231503487 -11.134618158086587
97 -25.511300697922707 -10.859166921158222
98 -23.079774633049965 -9.595137958067907
99 -27.87058413773775 -9.289921608799773
100 -29.921457402408123 -8.230877068641124
101 -32.457085490226746 -7.882363241796725
102 -35.2559550255537 -7.6478984168416355
103 -31.321979641914368 -7.459623237418707
104 -32.81473591923714 -7.124357312750265
105 -32.52124413847923 -7.05379065585803
106 -28.604531325399876 -6.853091098326624
107 -29.438547872006893 -6.6211384471641495
108 -35.304250329732895 -6.494555224953677
109 -25.54028818011284 -6.117354180737655
110 -33.48844684660435 -6.087055095509873
111 -30.708822071552277 -5.43500831968483
112 -21.137439843267202 -5.105291741614599
113 -25.90047113597393 -4.628649413275992
114 -19.0531756170094 -4.471031187897325
115 -23.802089646458626 -4.455504779070034
116 -26.064760148525238 -4.2805498188182405
117 -26.138620764017105 -3.7944735717969627
118 -17.372392550110817 -2.9512438456190186
119 -17.45249381661415 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0
[-50.0022206  -46.19619105 -45.19068756 -44.1649084  -43.99355529
 -43.84874807 -43.80581986 -42.66316469 -42.34663237 -42.29828951
 -41.77496339 -41.41006807 -41.17786296 -40.49595848 -40.05653451
 -39.59232358 -39.19523747 -39.17238958 -37.27603281 -33.03213148
 -32.44934973 -32.40079781 -31.65006914 -30.78479398 -30.73440379
 -30.57151372 -29.29723351 -29.28889042 -29.14587835 -28.81230637
 -28.49601894 -28.49202366 -28.04803787 -27.12111057 -26.15529378
 -25.06664428 -24.35366133 -24.3046485  -24.26504435 -23.83896276
 -23.75885688 -23.69203839 -23.48479966 -23.1475703  -22.74124885
 -22.61758703 -22.44305591 -22.37598735 -22.26494505 -22.25969991
 -22.18948644 -22.15569724 -22.04234284 -21.91659714 -21.83260292
 -21.75365053 -21.72006867 -21.70003809 -21.36811475 -21.25494569
 -21.25037649 -21.08331237 -21.07664654 -20.84329586 -20.75322228
 -20.54335656 -20.33499634 -20.18157658 -19.79746146 -19.76602055
 -19.37722575 -19.24313562 -18.96427493 -18.96412452 -18.44896231
 -18.26569363 -17.88388901 -17.19202756 -16.22159713 -16.1432061
 -15.96925781 -15.30362217 -14.95644557 -14.51686155 -14.20041301
 -13.93697618 -13.86225304 -13.39496134 -13.35586828 -13.21973003
 -12.94976194 -12.84162996 -12.5863684  -12.33507567 -12.27851524
 -12.23534438 -12.23397498 -12.22738746 -12.22005307 -12.02071783
 -11.9100948  -11.61190075 -11.52875734 -11.41760731 -11.40081341
 -10.85916692 -10.25438423 -10.15333736  -8.23087707  -7.88236324
  -7.45962324  -7.05379066  -6.0870551   -5.43500832  -4.62864941
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:1
end of epoch 0: val_loss 0.2400487216593187, val_acc 0.955
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.3902e-01,  2.3042e-01, -6.7561e-02,  1.2625e-01, -5.6976e-01,
          7.0588e-02, -3.7549e-02, -5.0669e-02,  1.7021e+00,  8.6499e-02,
          7.1457e-04, -7.1877e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.3699997919952792, val_acc 0.94
trigger times: 1
end of epoch 2: val_loss 0.37319372026229997, val_acc 0.935
trigger times: 2
end of epoch 3: val_loss 0.8116884830636133, val_acc 0.935
trigger times: 3
end of epoch 4: val_loss 0.204298288524276, val_acc 0.955
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1136e-01,  2.4403e-01, -4.6352e-02,  1.6466e-02, -6.2569e-02,
          1.1893e-02, -3.4064e-02, -1.3730e-02,  3.4847e+00,  4.4786e-02,
          5.0364e-04, -7.2437e+00]], device='cuda:1'))])
end of epoch 5: val_loss 0.1265751709676407, val_acc 0.97
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0985e-01,  1.4129e-01, -1.1490e-02, -4.2926e-02, -1.0347e-02,
          1.0926e-01, -5.5251e-03,  2.5863e-02,  3.6516e+00, -2.6603e-02,
          1.2201e-03, -7.1136e+00]], device='cuda:1'))])
end of epoch 6: val_loss 0.4425379903286755, val_acc 0.935
trigger times: 1
end of epoch 7: val_loss 0.16784788195246125, val_acc 0.975
trigger times: 2
end of epoch 8: val_loss 0.1426413128179743, val_acc 0.975
trigger times: 3
end of epoch 9: val_loss 0.13357317345416433, val_acc 0.965
trigger times: 4
end of epoch 10: val_loss 0.14715535254331588, val_acc 0.97
trigger times: 5
end of epoch 11: val_loss 0.3289658798593698, val_acc 0.96
trigger times: 6
end of epoch 12: val_loss 0.2823931603138084, val_acc 0.95
trigger times: 7
end of epoch 13: val_loss 0.1647501787875979, val_acc 0.975
trigger times: 8
end of epoch 14: val_loss 0.40442316935338496, val_acc 0.945
trigger times: 9
end of epoch 15: val_loss 0.6245014054126031, val_acc 0.9
trigger times: 10
Early stopping.
0 -34.36218719184399 -50.00222059884506
1 -107.82960051298141 -46.19619104961985
2 -59.64141970872879 -45.19068756322445
3 -54.582531571388245 -44.16490839583478
4 -120.53499245643616 -43.993555290419714
5 -75.90139134973288 -43.84874807044028
6 -36.451235234737396 -43.80581985978556
7 -61.14803060889244 -42.66316468983175
8 -24.61560356616974 -42.34663237067036
9 -14.04822402447462 -42.298289505457404
10 -31.296270966529846 -41.774963389485094
11 -36.36682386696339 -41.410068073767725
12 -53.83689334988594 -41.17786296442943
13 -101.72723931074142 -40.49595848244517
14 -31.256673604249954 -40.05653450521898
15 -86.44798165559769 -39.59232357792555
16 -75.46439207345247 -39.195237471709476
17 -96.31298184394836 -39.172389579378766
18 -12.949704222381115 -37.276032810632174
19 -19.287785828113556 -33.0321314765637
20 -47.58954805135727 -32.44934973065406
21 -40.92248970270157 -32.4007978120153
22 -28.00104057788849 -31.650069137050153
23 -24.95179322361946 -30.78479398275386
24 -15.2166738063097 -30.734403792103194
25 -25.375955298542976 -30.57151371770873
26 -24.847399294376373 -29.297233511513635
27 -25.995282739400864 -29.288890423975797
28 -27.752478450536728 -29.145878352769948
29 -14.109042197465897 -28.812306368123174
30 -38.200324319303036 -28.49601894351319
31 -16.61042606830597 -28.492023661124072
32 -16.417237788438797 -28.048037873082546
33 -8.423246711492538 -27.121110566589827
34 -12.958541728556156 -26.155293781020156
35 -36.28526684641838 -25.066644278800943
36 -7.5662151873111725 -24.353661332278367
37 -21.462755545973778 -24.304648501919143
38 -16.110692374408245 -24.26504435390199
39 -22.524262726306915 -23.838962755231172
40 -15.923910304903984 -23.758856882069406
41 -23.18085965514183 -23.692038391746667
42 -51.26590150594711 -23.48479966198816
43 -18.80856867134571 -23.147570298597067
44 -10.302007034420967 -22.74124885266394
45 -15.149306297302246 -22.617587028632922
46 -22.952258437871933 -22.44305590886941
47 -17.59236016869545 -22.375987345484475
48 -7.988317966461182 -22.264945050603636
49 -8.35344484448433 -22.2596999138188
50 -23.71611063182354 -22.189486442026126
51 -29.254634268581867 -22.15569724300287
52 -20.57851992547512 -22.042342839687073
53 -17.81086751818657 -21.916597135542844
54 -6.6034179255366325 -21.832602924545377
55 -23.926250845193863 -21.753650531916133
56 -8.216875292360783 -21.720068673949605
57 -26.615024238824844 -21.700038091900502
58 -29.054394394159317 -21.36811475445031
59 -25.91534325480461 -21.254945688365115
60 -15.583168089389801 -21.250376493390593
61 -20.940549537539482 -21.083312372601757
62 -15.932825982570648 -21.076646540244756
63 -26.796204417943954 -20.843295863616007
64 -21.151674911379814 -20.753222276721996
65 -15.084898561239243 -20.543356562348553
66 -15.444727689027786 -20.33499633836848
67 -18.983952723443508 -20.18157658281111
68 -12.49051322042942 -19.797461464413242
69 -31.129024296998978 -19.76602054655031
70 -11.735770292580128 -19.377225745334304
71 -11.16502171009779 -19.243135617403095
72 -21.187717586755753 -18.96427492901539
73 -5.195377513766289 -18.964124524696246
74 -21.588658392429352 -18.448962308005108
75 -6.7644476890563965 -18.26569363295007
76 -20.219460412859917 -17.883889007567344
77 -8.472047969698906 -17.192027558547206
78 -6.09707547724247 -16.22159713437397
79 -5.604422926902771 -16.14320610064953
80 -29.824007004499435 -15.969257812581287
81 -1.133063681423664 -15.30362217270701
82 -29.497252762317657 -14.956445567200944
83 -30.688908368349075 -14.516861549097893
84 -17.775109127163887 -14.200413010108107
85 -20.76293908059597 -13.936976181618805
86 -0.13021251559257507 -13.862253042167257
87 -32.18153843283653 -13.394961342216297
88 -5.097598947584629 -13.355868275096913
89 -14.939134888350964 -13.219730028414281
90 -22.71176254749298 -12.949761944117121
91 -24.522424548864365 -12.84162995606277
92 4.106110595166683 -12.5863683965907
93 -19.520647302269936 -12.335075668815614
94 -14.00991027802229 -12.278515244993585
95 -0.9531144797801971 -12.235344376448804
96 -21.358226388692856 -12.233974982405684
97 0.3495217300951481 -12.227387460046547
98 4.857908330857754 -12.2200530708718
99 -7.939597114920616 -12.020717825467683
100 -19.61690367758274 -11.910094799877324
101 0.4069722667336464 -11.61190074687837
102 -18.71095734834671 -11.528757343356707
103 4.0588118471205235 -11.417607312121094
104 -16.056601777672768 -11.40081340732895
105 3.955441877245903 -10.859166921158222
106 -1.3634087666869164 -10.254384229869821
107 -3.7997622564435005 -10.153337363143722
108 -27.20747424289584 -8.230877068641124
109 -16.271092757582664 -7.882363241796725
110 -12.490313395857811 -7.459623237418707
111 -22.43005335330963 -7.05379065585803
112 -9.556191444396973 -6.087055095509873
113 -6.830718860030174 -5.43500831968483
114 -12.644465498626232 -4.628649413275992
115 2.0730348601937294 -4.455504779070034
116 2.8337785750627518 -4.2805498188182405
117 3.902167961001396 -3.7944735717969627
118 8.029863759875298 -2.9512438456190186
119 7.218474306166172 -2.541618164765197
train accuracy: 0.9105555555555556
validation accuracy: 0.9
[-61.9569965  -50.0022206  -46.19619105 -45.19068756 -44.1649084
 -43.99355529 -43.80581986 -42.66316469 -39.59232358 -39.19523747
 -39.17238958 -37.27603281 -37.11976008 -32.40079781 -31.65006914
 -29.28889042 -28.49601894 -28.04803787 -26.30424413 -26.15529378
 -25.69114722 -25.30546537 -25.06664428 -24.35366133 -24.3046485
 -23.83896276 -23.69203839 -23.60631249 -23.58959374 -23.49146689
 -23.48479966 -23.1475703  -22.74124885 -22.26494505 -22.25969991
 -22.15569724 -21.83260292 -21.72006867 -21.36811475 -21.08331237
 -21.07664654 -20.70068218 -20.18157658 -19.8041653  -19.79746146
 -19.62989371 -19.46075207 -19.38787434 -19.37722575 -19.2966477
 -19.24313562 -18.76732015 -18.26512041 -17.67357171 -17.60197618
 -17.58132022 -17.46979313 -17.22354048 -16.82758417 -16.22159713
 -16.14295237 -15.96925781 -15.78637364 -14.95644557 -14.90359504
 -14.59788005 -14.51686155 -14.40687125 -14.3401438  -14.20041301
 -14.07927045 -14.0343984  -13.86225304 -13.66269291 -13.57851789
 -13.1732971  -12.94976194 -12.84973219 -12.84162996 -12.636257
 -12.46622309 -12.43841493 -12.27851524 -12.23534438 -12.22005307
 -12.02071783 -11.9100948  -11.61190075 -11.30386944 -10.94595062
 -10.71571847 -10.62178606 -10.18306265 -10.15333736  -9.71885203
  -9.45842557  -9.24080489  -9.23540785  -8.67729611  -8.51435605
  -8.380886    -8.23087707  -8.22964353  -7.88236324  -7.51146972
  -7.45962324  -7.43272843  -7.40772367  -7.22202271  -6.47992542
  -6.31484567  -5.9124145   -5.55451833  -5.43500832  -4.62864941
  -4.48430243  -4.28054982  -3.79447357  -2.95124385  -1.33161076]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:2
end of epoch 0: val_loss 0.3457735470043168, val_acc 0.935
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4592e-01,  3.9676e-01,  2.2536e-01,  1.7140e-01,  6.2142e-01,
          2.4247e-02, -5.1450e-02,  4.5566e-02,  3.2885e+00,  1.0597e+00,
          2.0150e-03, -6.7105e+00]], device='cuda:2'))])
end of epoch 1: val_loss 0.4669548255503934, val_acc 0.94
trigger times: 1
end of epoch 2: val_loss 0.3922238263899911, val_acc 0.93
trigger times: 2
end of epoch 3: val_loss 0.6399477926771263, val_acc 0.925
trigger times: 3
end of epoch 4: val_loss 0.5123935102070227, val_acc 0.945
trigger times: 4
end of epoch 5: val_loss 0.3413592616825514, val_acc 0.945
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6299e-01,  2.9276e-01,  1.5986e-01,  2.1497e-01,  9.9183e-01,
         -5.7518e-02, -2.6197e-02, -1.0708e-02,  3.6237e+00,  1.2928e+00,
          4.3648e-04, -6.3222e+00]], device='cuda:2'))])
end of epoch 6: val_loss 0.2945696859578349, val_acc 0.94
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0586e-01,  3.5930e-01,  8.3011e-02,  1.1364e-01,  8.9679e-01,
          2.5814e-01, -2.7145e-02,  3.8661e-02,  3.6584e+00,  1.0128e+00,
         -6.9375e-04, -6.3564e+00]], device='cuda:2'))])
end of epoch 7: val_loss 0.2526160610940178, val_acc 0.945
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.5177e-01,  3.9353e-01,  2.0637e-01,  1.8796e-01,  1.1639e+00,
          2.3338e-01, -3.8008e-02, -4.7505e-02,  3.5180e+00,  1.3329e+00,
          2.0154e-03, -6.0054e+00]], device='cuda:2'))])
end of epoch 8: val_loss 0.5143905913152613, val_acc 0.925
trigger times: 1
end of epoch 9: val_loss 0.43488188196635397, val_acc 0.94
trigger times: 2
end of epoch 10: val_loss 0.2522730094707085, val_acc 0.93
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.4328e-01,  4.1102e-01,  1.6580e-01,  1.5202e-01,  1.0919e+00,
         -1.7568e-02, -3.4631e-02, -9.6226e-02,  3.7772e+00,  1.3904e+00,
         -1.1228e-03, -5.8897e+00]], device='cuda:2'))])
end of epoch 11: val_loss 0.45801820472688193, val_acc 0.935
trigger times: 1
end of epoch 12: val_loss 0.384656833136073, val_acc 0.935
trigger times: 2
end of epoch 13: val_loss 0.34740176638079734, val_acc 0.945
trigger times: 3
end of epoch 14: val_loss 0.4131091370496718, val_acc 0.935
trigger times: 4
end of epoch 15: val_loss 0.342614602966175, val_acc 0.95
trigger times: 5
end of epoch 16: val_loss 0.37018431945561653, val_acc 0.945
trigger times: 6
end of epoch 17: val_loss 0.4580957344305391, val_acc 0.94
trigger times: 7
end of epoch 18: val_loss 0.8301351581528438, val_acc 0.93
trigger times: 8
end of epoch 19: val_loss 0.3827197002611423, val_acc 0.94
trigger times: 9
end of epoch 20: val_loss 0.40473523881976675, val_acc 0.94
trigger times: 10
Early stopping.
0 -23.77470028400421 -61.956996504006504
1 -45.3359312415123 -50.00222059884506
2 -81.48349368572235 -46.19619104961985
3 -41.7384073138237 -45.19068756322445
4 -25.067185044288635 -44.16490839583478
5 -73.64290654659271 -43.993555290419714
6 -19.332724437117577 -43.80581985978556
7 -33.424691051244736 -42.66316468983175
8 -63.042714804410934 -39.59232357792555
9 -51.172272086143494 -39.195237471709476
10 -50.638841927051544 -39.172389579378766
11 -10.602729890495539 -37.276032810632174
12 -6.738598853349686 -37.11976007784499
13 -34.73265019059181 -32.4007978120153
14 -20.9898923933506 -31.650069137050153
15 -23.059037506580353 -29.288890423975797
16 -35.56573363021016 -28.49601894351319
17 -11.171973496675491 -28.048037873082546
18 -17.17246988415718 -26.304244126879738
19 -9.926598850637674 -26.155293781020156
20 -18.06221716105938 -25.69114721815352
21 -17.08206883817911 -25.305465367924395
22 -19.02326688170433 -25.066644278800943
23 10.484192684292793 -24.353661332278367
24 -23.833194091916084 -24.304648501919143
25 -22.445754945278168 -23.838962755231172
26 -8.577576830983162 -23.692038391746667
27 -20.820819824934006 -23.606312489071794
28 -17.609952688217163 -23.58959373596641
29 -16.84912098944187 -23.49146688753982
30 -34.84317660331726 -23.48479966198816
31 -17.747295066714287 -23.147570298597067
32 -1.0681209936738014 -22.74124885266394
33 9.878814809024334 -22.264945050603636
34 -14.881615944206715 -22.2596999138188
35 -23.33029842376709 -22.15569724300287
36 -10.557921551167965 -21.832602924545377
37 -6.995223766192794 -21.720068673949605
38 -21.453532427549362 -21.36811475445031
39 -17.566183269023895 -21.083312372601757
40 -19.97859723865986 -21.076646540244756
41 -23.958710461854935 -20.700682181709826
42 -1.6094909012317657 -20.18157658281111
43 -15.211082637310028 -19.804165300484573
44 -9.404589414596558 -19.797461464413242
45 -17.209130942821503 -19.629893712291977
46 -5.240397829562426 -19.460752071720233
47 -13.967061012983322 -19.387874338320817
48 -5.774240925908089 -19.377225745334304
49 5.757477283477783 -19.296647704278197
50 2.7849077209830284 -19.243135617403095
51 -3.956781007349491 -18.767320147507185
52 -14.558001428842545 -18.265120408289683
53 -18.697235643863678 -17.673571710533185
54 -10.28385914862156 -17.601976177066014
55 11.356547772884369 -17.581320221219105
56 17.254738956689835 -17.469793129053137
57 18.720073521137238 -17.22354047528747
58 -9.784217953681946 -16.827584168681355
59 4.589158117771149 -16.22159713437397
60 22.597923159599304 -16.142952371279982
61 -22.591136395931244 -15.969257812581287
62 11.244543313980103 -15.786373640195631
63 -22.07395777106285 -14.956445567200944
64 8.756779193878174 -14.903595040281619
65 -10.808164335787296 -14.59788004579155
66 -25.50920879840851 -14.516861549097893
67 10.197773814201355 -14.406871245813003
68 1.761788323521614 -14.340143797525398
69 -10.559715412557125 -14.200413010108107
70 -6.150822527706623 -14.079270452292477
71 -8.607956349849701 -14.034398399140361
72 -5.8936251029372215 -13.862253042167257
73 0.5818198844790459 -13.662692907343303
74 1.497390277683735 -13.578517887738183
75 0.13952791690826416 -13.173297095414247
76 -12.165741439908743 -12.949761944117121
77 2.4126792177557945 -12.849732194451764
78 -13.607536163181067 -12.84162995606277
79 18.377702564001083 -12.636256998640581
80 -0.6295119971036911 -12.466223094376293
81 -1.4513831287622452 -12.438414927352152
82 -1.2029335275292397 -12.278515244993585
83 8.367154657840729 -12.235344376448804
84 10.846508890390396 -12.2200530708718
85 12.465445175766945 -12.020717825467683
86 -9.456031948328018 -11.910094799877324
87 11.310335338115692 -11.61190074687837
88 14.755549967288971 -11.303869441015525
89 19.83078694343567 -10.945950615483753
90 19.2659530043602 -10.71571847189881
91 11.339253082871437 -10.6217860568235
92 32.87264025211334 -10.183062650698592
93 13.473296135663986 -10.153337363143722
94 11.648286998271942 -9.718852025031394
95 10.560930699110031 -9.45842556960762
96 15.154030218720436 -9.240804885051093
97 10.98093892633915 -9.235407850279152
98 15.562480092048645 -8.677296111941759
99 13.357196807861328 -8.514356046657323
100 24.467438280582428 -8.38088600101569
101 -22.731386050581932 -8.230877068641124
102 31.201056480407715 -8.229643529552472
103 -6.798129610717297 -7.882363241796725
104 34.22384923696518 -7.511469718332719
105 -3.39487025141716 -7.459623237418707
106 25.138886064291 -7.432728427414028
107 26.48532521724701 -7.4077236733875225
108 13.018313467502594 -7.222022712769271
109 17.55918660759926 -6.4799254164272355
110 37.23925131559372 -6.314845667351462
111 33.871343076229095 -5.91241450373038
112 39.65209698677063 -5.5545183324994385
113 3.2153605297207832 -5.43500831968483
114 4.066945105791092 -4.628649413275992
115 31.423979878425598 -4.4843024257298705
116 0.7526571601629257 -4.2805498188182405
117 5.170049954205751 -3.7944735717969627
118 29.75538969039917 -2.9512438456190186
119 34.89078205823898 -1.331610759934135
train accuracy: 0.9622222222222222
validation accuracy: 0.94

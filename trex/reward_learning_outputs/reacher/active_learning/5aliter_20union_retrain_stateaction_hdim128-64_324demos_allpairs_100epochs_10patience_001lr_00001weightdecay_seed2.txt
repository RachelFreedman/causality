sorted_train_rewards: [-54.46505855 -51.17814647 -49.6099766  -49.13001653 -49.03866708
 -49.0331838  -48.84818037 -48.58850771 -48.19509289 -47.96991522
 -47.53054283 -47.33487662 -47.14538324 -47.10889581 -46.92993068
 -46.92815321 -46.7141986  -46.51840523 -46.50762306 -46.08033935
 -45.96910073 -45.94004954 -45.34946654 -45.23627882 -45.23541431
 -44.99494244 -44.99184926 -44.7644634  -44.62310756 -44.53780078
 -44.32699505 -44.29565561 -43.84464734 -43.7920154  -43.22211069
 -43.2041764  -43.14444939 -42.63170505 -42.49903983 -42.18990615
 -42.17868583 -42.08941791 -41.97868948 -41.96459824 -41.91819405
 -41.89252759 -41.76630373 -41.75643755 -41.50687738 -41.35020296
 -41.32515547 -41.22179983 -41.18200046 -41.03992135 -41.03294083
 -40.99991982 -40.98448758 -40.86627436 -40.84937019 -40.63970267
 -40.33775477 -40.11576908 -39.97085477 -39.92712396 -39.89750906
 -39.72931271 -39.35357961 -39.14272956 -38.91826709 -38.52213956
 -38.29413029 -38.28199013 -38.28138827 -38.04661848 -37.94300503
 -37.88709355 -37.79694607 -37.70658771 -37.51412288 -37.37133772
 -37.3251261  -37.15321081 -37.11841822 -37.10344577 -36.99477768
 -36.69656462 -36.65639269 -36.38667894 -36.33815177 -36.28177521
 -36.03378032 -36.02351695 -35.97994531 -35.89835563 -35.77878287
 -35.74239985 -35.67327882 -35.43726783 -35.41556943 -35.28979253
 -35.19353216 -35.12234482 -35.07963015 -35.06137062 -35.02402835
 -35.01091702 -34.95588623 -34.91427059 -34.88495647 -34.86472602
 -34.63851695 -34.34266183 -33.86129846 -33.77802335 -33.7629591
 -33.680383   -33.4779641  -33.26579709 -33.04852912 -32.41611773
 -32.41053817 -32.02508724 -31.90686017 -31.71028623 -31.64335581
 -31.48876757 -31.44406811 -31.37649579 -31.35230126 -31.24419956
 -31.21642916 -31.15850305 -31.06744474 -30.95117262 -30.772676
 -30.76937686 -30.63466958 -30.59673213 -30.42034344 -30.36982897
 -30.36743577 -30.27374553 -30.10812976 -30.06299271 -29.98083048
 -29.97767593 -29.94526105 -29.22737048 -29.20595883 -28.99559546
 -28.95484843 -28.59191373 -28.5494411  -28.42026397 -28.31342715
 -28.1712167  -28.13409378 -27.79654204 -27.70726756 -27.60103341
 -27.27876998 -26.73963502 -26.7058308  -26.50715502 -26.39763335
 -26.36197542 -26.19987885 -25.99907479 -25.47507675 -25.31217996
 -25.29134047 -25.11681575 -24.96039421 -24.91350015 -24.71538539
 -24.58444108 -24.25434383 -24.12244562 -23.96980649 -23.76917367
 -23.71252839 -23.48246476 -23.05835519 -22.79553869 -22.61211306
 -22.55283027 -22.53572133 -22.14406511 -21.86139195 -21.84262733
 -21.72370465 -21.7167854  -21.64185366 -21.62657756 -21.62597106
 -20.76562386 -20.68104247 -20.66287704 -20.66020817 -20.5100558
 -20.35567458 -20.10309328 -19.94369578 -19.67432429 -19.50391949
 -19.30989567 -19.03153444 -19.02121155 -18.9341265  -18.87189699
 -18.77655165 -18.71737606 -18.4634721  -18.2465562  -18.08445937
 -17.94319772 -17.85124778 -17.82515236 -17.77364067 -17.67989719
 -17.48133145 -17.10217865 -17.06769934 -16.68041488 -16.325741
 -16.25152473 -16.24688067 -16.23288157 -16.01542464 -15.4984665
 -15.44345121 -15.26574292 -15.017961   -14.79738881 -14.47834852
 -14.39794675 -14.24892226 -14.15625316 -13.8953595  -13.73405689
 -13.43873671 -13.16397094 -12.97987294 -12.6606731  -12.5410027
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.97407887 -11.96626278 -11.75626016
 -11.47155848 -11.04421156 -10.859881   -10.78272714 -10.76409188
 -10.64954177 -10.49423816 -10.01645863  -9.92399886  -9.67992171
  -9.56750101  -9.4003034   -8.4761537   -8.40276084  -8.32638751
  -8.0269637   -7.70107293  -7.68743448  -7.67259169  -7.57539849
  -7.54460172  -7.3740849   -7.36244313  -7.34546388  -7.1893336
  -7.15421432  -7.10832736  -7.08431127  -6.95906356  -6.92008425
  -6.77694649  -6.72206384  -6.71997062  -6.64795748  -6.51820418
  -6.47884361  -6.29894775  -6.05448903  -5.89467275  -5.85405865
  -5.64485143  -5.61579673  -5.39544196  -5.38326081  -5.3472021
  -5.25793019  -5.24856721  -5.07848501  -5.06486011  -5.02795798
  -4.90228293  -4.82757292  -4.63049542  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.00401798  -3.97870856
  -3.65032555  -3.3322555   -3.329867    -3.29356852  -2.88591659
  -2.83192847  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-55.21445752 -46.46343149 -45.47189047 -41.86250044 -39.17708227
 -39.07321282 -38.77785603 -37.47672391 -36.58958472 -33.17157461
 -31.00231541 -30.64081715 -30.52049104 -29.66568464 -28.11939797
 -27.77638947 -27.50815322 -21.44290047 -18.65414818 -18.36358054
 -17.64064603 -17.27921081 -16.44066556 -16.35270212 -16.20186147
 -15.29527985 -13.88057881 -13.69675016 -11.9523875  -11.4307611
  -7.5455064   -7.52029309  -3.38446715  -3.07904644  -2.67390706
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 630
num val_labels 630
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.09022135032359047, val_acc 0.9571428571428572
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.14803279186763013, val_acc 0.9555555555555556
trigger times: 1
end of epoch 2: val_loss 0.1709751369813689, val_acc 0.9587301587301588
trigger times: 2
end of epoch 3: val_loss 0.09594830715603095, val_acc 0.9571428571428572
trigger times: 3
end of epoch 4: val_loss 0.10212432161158257, val_acc 0.9666666666666667
trigger times: 4
end of epoch 5: val_loss 0.08860756617890253, val_acc 0.9634920634920635
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.16113672242735755, val_acc 0.9619047619047619
trigger times: 1
end of epoch 7: val_loss 0.08829134153743613, val_acc 0.9603174603174603
trigger times: 0
saving model weights...
end of epoch 8: val_loss 1.1683929966545445, val_acc 0.8809523809523809
trigger times: 1
end of epoch 9: val_loss 0.19499393105499838, val_acc 0.9285714285714286
trigger times: 2
end of epoch 10: val_loss 0.08794559416154879, val_acc 0.9619047619047619
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.10426884979290035, val_acc 0.9523809523809523
trigger times: 1
end of epoch 12: val_loss 0.09698300125465915, val_acc 0.9571428571428572
trigger times: 2
end of epoch 13: val_loss 0.0896632171291441, val_acc 0.9587301587301588
trigger times: 3
end of epoch 14: val_loss 0.09032708570252618, val_acc 0.9603174603174603
trigger times: 4
end of epoch 15: val_loss 0.08437648425260603, val_acc 0.9587301587301588
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.11685521744185472, val_acc 0.9476190476190476
trigger times: 1
end of epoch 17: val_loss 0.11165130134686684, val_acc 0.9476190476190476
trigger times: 2
end of epoch 18: val_loss 0.10635324339300911, val_acc 0.9571428571428572
trigger times: 3
end of epoch 19: val_loss 0.09285115442828551, val_acc 0.9492063492063492
trigger times: 4
end of epoch 20: val_loss 0.16741464972127099, val_acc 0.9444444444444444
trigger times: 5
end of epoch 21: val_loss 0.16597281286049242, val_acc 0.953968253968254
trigger times: 6
end of epoch 22: val_loss 0.10549714320282695, val_acc 0.953968253968254
trigger times: 7
end of epoch 23: val_loss 0.10576960589890742, val_acc 0.9587301587301588
trigger times: 8
end of epoch 24: val_loss 0.23082676285216838, val_acc 0.9238095238095239
trigger times: 9
end of epoch 25: val_loss 0.1280581871250309, val_acc 0.9365079365079365
trigger times: 10
Early stopping.
0 -48.00911757349968 -55.21445752462833
1 -44.12424412369728 -46.463431492780636
2 -45.699679017066956 -45.47189046691946
3 -44.92548727989197 -41.86250043832566
4 -40.958439499139786 -39.17708227320146
5 -38.89219382405281 -39.07321282259262
6 -40.05160188674927 -38.77785603121339
7 -42.66760614514351 -37.47672390906202
8 -37.97515872120857 -36.58958472489279
9 -34.60912561416626 -33.17157461008636
10 -38.07563269138336 -31.002315411370166
11 -39.68007454276085 -30.640817151541906
12 -35.91198813915253 -30.520491036162525
13 -36.810700446367264 -29.66568463563045
14 -33.69471496343613 -28.11939796920964
15 -35.47611278295517 -27.776389469559156
16 -34.03537604212761 -27.50815321826314
17 -29.65633362531662 -21.442900472353543
18 -28.28589305281639 -18.654148179968352
19 -29.752856880426407 -18.363580535271076
20 -29.925051599740982 -17.640646027280784
21 -25.524609595537186 -17.27921080502319
22 -26.658964186906815 -16.440665556315388
23 -26.486339569091797 -16.352702123112557
24 -24.8657263815403 -16.201861466781004
25 -27.377166718244553 -15.295279851466828
26 -25.410690635442734 -13.880578810015049
27 -24.058307260274887 -13.696750155340938
28 -23.491181164979935 -11.952387503909815
29 -24.36588165163994 -11.430761099930217
30 -16.884191244840622 -7.545506402407465
31 -20.83113443851471 -7.5202930852533045
32 -16.92326456308365 -3.3844671463622564
33 -17.169618904590607 -3.079046443285274
34 -16.89092242717743 -2.673907055233777
35 -16.94099909067154 -2.49009826539426
train accuracy: 0.9510568359897565
validation accuracy: 0.9365079365079365
sorted_train_rewards: [-55.21445752 -54.46505855 -51.17814647 -49.6099766  -49.13001653
 -49.03866708 -49.0331838  -48.84818037 -48.58850771 -48.19509289
 -47.53054283 -47.33487662 -47.14538324 -47.10889581 -46.92993068
 -46.92815321 -46.7141986  -46.51840523 -46.46343149 -46.08033935
 -45.96910073 -45.94004954 -45.47189047 -45.34946654 -45.23627882
 -45.23541431 -44.99494244 -44.99184926 -44.7644634  -44.62310756
 -44.53780078 -44.32699505 -44.29565561 -43.84464734 -43.7920154
 -43.22211069 -43.2041764  -42.63170505 -42.49903983 -42.18990615
 -42.17868583 -42.08941791 -41.97868948 -41.91819405 -41.89252759
 -41.86250044 -41.76630373 -41.75643755 -41.50687738 -41.35020296
 -41.32515547 -41.18200046 -41.03992135 -41.03294083 -40.99991982
 -40.98448758 -40.86627436 -40.84937019 -40.63970267 -40.33775477
 -40.11576908 -39.97085477 -39.92712396 -39.89750906 -39.72931271
 -39.35357961 -39.17708227 -39.14272956 -39.07321282 -38.77785603
 -38.52213956 -38.29413029 -38.28199013 -38.28138827 -38.04661848
 -37.94300503 -37.88709355 -37.79694607 -37.51412288 -37.47672391
 -37.37133772 -37.3251261  -37.15321081 -37.11841822 -37.10344577
 -36.99477768 -36.69656462 -36.65639269 -36.58958472 -36.38667894
 -36.33815177 -36.28177521 -36.03378032 -36.02351695 -35.97994531
 -35.89835563 -35.77878287 -35.74239985 -35.67327882 -35.43726783
 -35.41556943 -35.28979253 -35.19353216 -35.07963015 -35.06137062
 -35.02402835 -35.01091702 -34.95588623 -34.91427059 -34.88495647
 -34.86472602 -34.63851695 -34.34266183 -33.86129846 -33.77802335
 -33.7629591  -33.680383   -33.4779641  -33.26579709 -33.17157461
 -33.04852912 -32.41611773 -32.02508724 -31.90686017 -31.71028623
 -31.64335581 -31.48876757 -31.44406811 -31.37649579 -31.35230126
 -31.24419956 -31.21642916 -31.15850305 -31.06744474 -30.95117262
 -30.772676   -30.76937686 -30.64081715 -30.63466958 -30.59673213
 -30.52049104 -30.42034344 -30.36982897 -30.36743577 -30.10812976
 -30.06299271 -29.98083048 -29.97767593 -29.94526105 -29.66568464
 -29.22737048 -29.20595883 -28.99559546 -28.95484843 -28.59191373
 -28.5494411  -28.42026397 -28.1712167  -28.13409378 -28.11939797
 -27.79654204 -27.77638947 -27.60103341 -27.27876998 -26.73963502
 -26.7058308  -26.39763335 -26.36197542 -26.19987885 -25.99907479
 -25.47507675 -25.29134047 -25.11681575 -24.91350015 -24.71538539
 -24.58444108 -24.25434383 -24.12244562 -23.96980649 -23.76917367
 -23.71252839 -23.05835519 -22.61211306 -22.53572133 -22.14406511
 -21.86139195 -21.84262733 -21.72370465 -21.64185366 -21.62657756
 -21.62597106 -21.44290047 -20.76562386 -20.68104247 -20.66287704
 -20.66020817 -20.5100558  -20.35567458 -20.10309328 -19.94369578
 -19.67432429 -19.50391949 -19.30989567 -19.03153444 -19.02121155
 -18.9341265  -18.87189699 -18.77655165 -18.71737606 -18.65414818
 -18.36358054 -18.08445937 -17.94319772 -17.85124778 -17.82515236
 -17.77364067 -17.67989719 -17.64064603 -17.48133145 -17.27921081
 -17.10217865 -17.06769934 -16.68041488 -16.44066556 -16.325741
 -16.25152473 -16.24688067 -16.23288157 -16.01542464 -15.93004515
 -15.4984665  -15.44345121 -15.29527985 -15.26574292 -15.017961
 -14.60342651 -14.47834852 -14.39794675 -14.24892226 -14.15625316
 -13.8953595  -13.88057881 -13.73405689 -13.69675016 -13.43873671
 -13.16397094 -13.06151456 -12.97987294 -12.6606731  -12.5410027
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.16816754
 -12.13592928 -12.10202442 -11.97407887 -11.96626278 -11.9523875
 -11.75626016 -11.47155848 -11.4307611  -11.11452078 -11.04421156
 -10.859881   -10.85483091 -10.78272714 -10.76409188 -10.49423816
 -10.41717512 -10.34239021 -10.11660614 -10.01645863  -9.92399886
  -9.70333238  -9.67992171  -9.56750101  -9.4003034   -8.75795528
  -8.4761537   -8.32638751  -8.0269637   -7.70107293  -7.68743448
  -7.67259169  -7.57539849  -7.54460172  -7.3740849   -7.36244313
  -7.34546388  -7.1893336   -7.12445111  -7.11601941  -7.10832736
  -7.08431127  -6.95906356  -6.92008425  -6.77694649  -6.72206384
  -6.71997062  -6.66621245  -6.64795748  -6.51820418  -6.47884361
  -6.44811604  -6.38264725  -6.29894775  -6.05448903  -6.01870081
  -5.89467275  -5.85405865  -5.64485143  -5.61579673  -5.39544196
  -5.3472021   -5.25793019  -5.24856721  -5.07848501  -5.06486011
  -5.02795798  -4.90228293  -4.82757292  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.01883024  -4.00401798
  -3.97870856  -3.65032555  -3.3322555   -3.329867    -3.29356852
  -2.88591659  -2.83192847  -2.64166233  -2.49009827  -2.24005036
  -2.07879873  -1.91361965]
sorted_val_rewards: [-47.96991522 -46.50762306 -43.14444939 -41.96459824 -41.22179983
 -38.91826709 -37.70658771 -35.12234482 -32.41053817 -31.00231541
 -30.27374553 -28.31342715 -27.70726756 -27.50815322 -26.50715502
 -25.31217996 -24.96039421 -23.48246476 -22.79553869 -22.55283027
 -21.7167854  -18.4634721  -18.2465562  -16.35270212 -16.20186147
 -15.61270366 -14.79738881 -12.26611853 -10.64954177  -8.40276084
  -7.5455064   -7.52029309  -7.15421432  -5.38326081  -4.63049542
  -3.38446715  -3.07904644  -2.67390706]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 703
num val_labels 703
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 0.08150168815220933, val_acc 0.9672830725462305
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.078530633061691, val_acc 0.9658605974395448
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1502100104367808, val_acc 0.9615931721194879
trigger times: 1
end of epoch 3: val_loss 0.22603607502830173, val_acc 0.918918918918919
trigger times: 2
end of epoch 4: val_loss 0.4856913701490883, val_acc 0.9331436699857752
trigger times: 3
end of epoch 5: val_loss 0.10263676697632505, val_acc 0.9459459459459459
trigger times: 4
end of epoch 6: val_loss 0.09826559230289642, val_acc 0.9559032716927454
trigger times: 5
end of epoch 7: val_loss 0.48310374968095865, val_acc 0.9559032716927454
trigger times: 6
end of epoch 8: val_loss 0.08669940955532392, val_acc 0.9615931721194879
trigger times: 7
end of epoch 9: val_loss 0.08658277793392838, val_acc 0.9701280227596017
trigger times: 8
end of epoch 10: val_loss 0.0895831504953117, val_acc 0.9658605974395448
trigger times: 9
end of epoch 11: val_loss 0.11290290233797637, val_acc 0.9544807965860598
trigger times: 10
Early stopping.
0 -34.293215189129114 -47.96991522128461
1 -34.06236878037453 -46.50762305558868
2 -32.35724542662501 -43.14444938675049
3 -35.7851327508688 -41.96459824260299
4 -29.519909285008907 -41.221799828567185
5 -30.0156825222075 -38.91826708792281
6 -32.0850685685873 -37.706587708128204
7 -29.787963062524796 -35.12234482386259
8 -27.13385671004653 -32.41053816503779
9 -24.87886980175972 -31.002315411370166
10 -25.237494211643934 -30.27374552587196
11 -25.152875564992428 -28.313427148940267
12 -22.671937875449657 -27.70726756416977
13 -21.63582630455494 -27.50815321826314
14 -24.35219284147024 -26.50715501561981
15 -20.585165660828352 -25.312179958453797
16 -22.425058595836163 -24.960394212780514
17 -19.119952995330095 -23.48246476105765
18 -16.01011999323964 -22.795538693648083
19 -20.462190870195627 -22.552830265023072
20 -21.697354450821877 -21.716785395985877
21 -14.47742434591055 -18.463472103110583
22 -15.15518543869257 -18.24655620273112
23 -13.427335154265165 -16.352702123112557
24 -14.525495678186417 -16.201861466781004
25 -12.944783478975296 -15.612703655525749
26 -13.810832552611828 -14.797388813532848
27 -10.291845515370369 -12.266118533227734
28 -9.411214370280504 -10.649541772408808
29 -5.357967898249626 -8.402760838629018
30 -4.853153560310602 -7.545506402407465
31 -7.338545821607113 -7.5202930852533045
32 -5.720571402460337 -7.1542143150163025
33 -3.6736373491585255 -5.383260807350201
34 -4.785906482487917 -4.63049541560991
35 -4.095951296389103 -3.3844671463622564
36 -3.9918630607426167 -3.079046443285274
37 -3.5541834123432636 -2.673907055233777
train accuracy: 0.944081336238199
validation accuracy: 0.9544807965860598
sorted_train_rewards: [-55.21445752 -51.17814647 -49.6099766  -49.03866708 -49.0331838
 -48.84818037 -48.58850771 -48.19509289 -47.96991522 -47.53054283
 -47.33487662 -47.14538324 -47.10889581 -46.92993068 -46.92815321
 -46.7141986  -46.46343149 -46.08033935 -45.96910073 -45.94004954
 -45.47189047 -45.34946654 -45.23627882 -45.23541431 -44.99494244
 -44.99184926 -44.7644634  -44.62310756 -44.53780078 -44.32699505
 -43.84464734 -43.7920154  -43.22211069 -43.2041764  -43.14444939
 -42.63170505 -42.49903983 -42.18990615 -42.17868583 -42.08941791
 -41.97868948 -41.96459824 -41.86250044 -41.75643755 -41.50687738
 -41.35020296 -41.32515547 -41.22179983 -41.18200046 -41.03992135
 -41.03294083 -40.99991982 -40.98448758 -40.86627436 -40.84937019
 -40.63970267 -40.33775477 -40.11576908 -39.97085477 -39.89750906
 -39.72931271 -39.35357961 -39.17708227 -39.14272956 -39.07321282
 -38.91826709 -38.77785603 -38.52213956 -38.29413029 -38.28199013
 -38.28138827 -38.04661848 -37.94300503 -37.88709355 -37.79694607
 -37.70658771 -37.51412288 -37.47672391 -37.37133772 -37.3251261
 -37.15321081 -37.11841822 -37.10344577 -36.99477768 -36.69656462
 -36.65639269 -36.58958472 -36.38667894 -36.33815177 -36.28177521
 -36.03378032 -36.02351695 -35.89835563 -35.77878287 -35.74239985
 -35.67327882 -35.43726783 -35.41556943 -35.28979253 -35.19353216
 -35.12234482 -35.07963015 -35.06137062 -35.02402835 -35.01091702
 -34.95588623 -34.91427059 -34.88495647 -34.86472602 -34.63851695
 -34.34266183 -33.86129846 -33.77802335 -33.7629591  -33.680383
 -33.4779641  -33.26579709 -33.17157461 -33.04852912 -32.41053817
 -31.90686017 -31.71028623 -31.48876757 -31.44406811 -31.37649579
 -31.35230126 -31.24419956 -31.21642916 -31.15850305 -31.06744474
 -31.00231541 -30.95117262 -30.772676   -30.76937686 -30.64081715
 -30.63466958 -30.59673213 -30.52049104 -30.42034344 -30.36982897
 -30.36743577 -30.27374553 -30.10812976 -30.06299271 -29.98083048
 -29.94526105 -29.66568464 -29.22737048 -29.20595883 -28.99559546
 -28.95484843 -28.59191373 -28.5494411  -28.42026397 -28.31342715
 -28.1712167  -28.11939797 -27.77638947 -27.70726756 -27.60103341
 -27.50815322 -27.27876998 -26.73963502 -26.7058308  -26.50715502
 -26.39763335 -26.36197542 -25.99907479 -25.47507675 -25.31217996
 -25.11681575 -24.96039421 -24.91350015 -24.71538539 -24.58444108
 -24.25434383 -24.12244562 -23.96980649 -23.76917367 -23.71252839
 -23.05835519 -22.79553869 -22.61211306 -22.55283027 -22.53572133
 -22.14406511 -21.86139195 -21.84262733 -21.7167854  -21.64185366
 -21.62657756 -21.62597106 -21.44290047 -20.76562386 -20.68104247
 -20.66287704 -20.66020817 -20.5100558  -20.35567458 -20.10309328
 -19.94369578 -19.67432429 -19.50391949 -19.30989567 -19.03153444
 -19.02121155 -18.9341265  -18.87189699 -18.77655165 -18.71737606
 -18.4634721  -18.2465562  -18.08445937 -17.94319772 -17.85124778
 -17.82515236 -17.77364067 -17.67989719 -17.64064603 -17.48133145
 -17.27921081 -17.10217865 -17.06769934 -16.68041488 -16.44066556
 -16.325741   -16.25152473 -16.24688067 -16.23288157 -16.20186147
 -16.01542464 -15.93004515 -15.61270366 -15.4984665  -15.44345121
 -15.26574292 -15.017961   -14.79738881 -14.60342651 -14.47834852
 -14.39794675 -14.24892226 -14.15625316 -13.94339237 -13.8953595
 -13.88057881 -13.73405689 -13.43873671 -13.21636887 -13.16397094
 -13.06151456 -12.97987294 -12.6606731  -12.65396483 -12.5410027
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -12.10202442 -11.97407887 -11.96626278
 -11.9523875  -11.87662961 -11.86567913 -11.86430296 -11.75626016
 -11.47155848 -11.4307611  -11.11452078 -11.04421156 -10.89117527
 -10.859881   -10.85483091 -10.78272714 -10.76409188 -10.68580682
 -10.65949216 -10.49423816 -10.41717512 -10.34239021 -10.15973794
 -10.11660614 -10.01645863  -9.92399886  -9.76720767  -9.70333238
  -9.67992171  -9.60127809  -9.56750101  -9.53798298  -9.4003034
  -9.37967001  -8.9441418   -8.87450032  -8.75795528  -8.4761537
  -8.32638751  -8.0269637   -7.70107293  -7.68743448  -7.67259169
  -7.57539849  -7.54460172  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.12445111  -7.11601941  -7.10832736  -7.08431127
  -6.95906356  -6.92008425  -6.77694649  -6.72206384  -6.71997062
  -6.66621245  -6.64795748  -6.51820418  -6.47884361  -6.44811604
  -6.38264725  -6.29894775  -6.05448903  -6.01870081  -5.89467275
  -5.85405865  -5.64485143  -5.61579673  -5.39544196  -5.3472021
  -5.25793019  -5.24856721  -5.07848501  -5.06486011  -5.02795798
  -4.90228293  -4.82757292  -4.37983153  -4.35856953  -4.230832
  -4.0660223   -4.03104862  -4.01883024  -4.00401798  -3.97870856
  -3.65032555  -3.3322555   -3.329867    -3.29356852  -2.88591659
  -2.83192847  -2.64166233  -2.24005036  -2.07879873  -1.91361965]
sorted_val_rewards: [-54.46505855 -49.13001653 -46.51840523 -46.50762306 -44.29565561
 -41.91819405 -41.89252759 -41.76630373 -39.92712396 -35.97994531
 -32.41611773 -32.02508724 -31.64335581 -29.97767593 -28.13409378
 -27.79654204 -26.19987885 -25.29134047 -23.48246476 -21.72370465
 -18.65414818 -18.36358054 -16.35270212 -15.29527985 -13.69675016
 -12.25819562 -11.60934218 -11.52377055 -10.64954177  -9.84457087
  -8.40276084  -7.5455064   -7.52029309  -7.15421432  -5.38326081
  -4.63049542  -3.38446715  -3.07904644  -2.67390706  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 780
num val_labels 780
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:0
end of epoch 0: val_loss 0.1083593189159032, val_acc 0.9602564102564103
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.08603655234252341, val_acc 0.9666666666666667
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.10225556444813326, val_acc 0.9666666666666667
trigger times: 1
end of epoch 3: val_loss 0.09539986254241956, val_acc 0.9615384615384616
trigger times: 2
end of epoch 4: val_loss 0.10995921797998509, val_acc 0.9615384615384616
trigger times: 3
end of epoch 5: val_loss 0.12921061420187593, val_acc 0.9538461538461539
trigger times: 4
end of epoch 6: val_loss 0.8152945939437916, val_acc 0.9358974358974359
trigger times: 5
end of epoch 7: val_loss 0.12581661408669695, val_acc 0.9551282051282052
trigger times: 6
end of epoch 8: val_loss 0.06056414190638298, val_acc 0.9730769230769231
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.08114071578897925, val_acc 0.9692307692307692
trigger times: 1
end of epoch 10: val_loss 0.1272020863881162, val_acc 0.9602564102564103
trigger times: 2
end of epoch 11: val_loss 0.11154827848874173, val_acc 0.958974358974359
trigger times: 3
end of epoch 12: val_loss 0.1772041683246673, val_acc 0.9628205128205128
trigger times: 4
end of epoch 13: val_loss 0.09178737748579183, val_acc 0.9666666666666667
trigger times: 5
end of epoch 14: val_loss 0.10681758878127894, val_acc 0.9666666666666667
trigger times: 6
end of epoch 15: val_loss 0.09760328808469566, val_acc 0.9615384615384616
trigger times: 7
end of epoch 16: val_loss 0.12203172649198454, val_acc 0.9628205128205128
trigger times: 8
end of epoch 17: val_loss 0.08971894446577576, val_acc 0.9692307692307692
trigger times: 9
end of epoch 18: val_loss 0.07318208095344073, val_acc 0.9705128205128205
trigger times: 10
Early stopping.
0 -43.811190366744995 -54.46505855143417
1 -42.78724591434002 -49.13001653445393
2 -44.7515082359314 -46.51840522660448
3 -41.298318445682526 -46.50762305558868
4 -38.65579354763031 -44.29565561256425
5 -38.74848359823227 -41.918194052096716
6 -39.73374459147453 -41.89252759410746
7 -39.55877557396889 -41.76630372811994
8 -38.17149965465069 -39.927123963254076
9 -37.342746302485466 -35.97994530541655
10 -31.651231914758682 -32.416117728185334
11 -33.20819255709648 -32.025087237182525
12 -35.042468294501305 -31.64335580631245
13 -31.694734364748 -29.977675928046363
14 -31.57264420390129 -28.134093781998498
15 -30.914373964071274 -27.796542043451076
16 -28.94634409248829 -26.199878851984913
17 -29.0267481058836 -25.29134046899922
18 -28.21964679658413 -23.48246476105765
19 -28.003304168581963 -21.723704649211356
20 -23.682617157697678 -18.654148179968352
21 -25.699142947793007 -18.363580535271076
22 -23.549605801701546 -16.352702123112557
23 -22.257893070578575 -15.295279851466828
24 -20.179538667201996 -13.696750155340938
25 -21.506688803434372 -12.258195624229982
26 -19.02065210044384 -11.609342183371819
27 -19.207895904779434 -11.523770550257357
28 -17.830387741327286 -10.649541772408808
29 -18.6386576294899 -9.844570869870905
30 -14.664698094129562 -8.402760838629018
31 -13.648996159434319 -7.545506402407465
32 -16.016764521598816 -7.5202930852533045
33 -14.043882817029953 -7.1542143150163025
34 -13.1282829195261 -5.383260807350201
35 -13.328881278634071 -4.63049541560991
36 -12.142740175127983 -3.3844671463622564
37 -12.427682131528854 -3.079046443285274
38 -12.015075325965881 -2.673907055233777
39 -12.15594744682312 -2.49009826539426
train accuracy: 0.9582616672399954
validation accuracy: 0.9705128205128205
sorted_train_rewards: [-78.82803921 -76.43570385 -75.66491152 -75.10649943 -74.32686041
 -74.28113176 -74.16223804 -72.96727971 -72.88744715 -72.80827843
 -72.05846729 -71.82748874 -71.77271563 -71.48295963 -71.41555074
 -70.56929291 -69.84848134 -55.21445752 -54.46505855 -51.17814647
 -49.6099766  -49.13001653 -49.03866708 -49.0331838  -48.84818037
 -48.58850771 -48.19509289 -47.53054283 -47.33487662 -47.14538324
 -47.10889581 -46.92993068 -46.92815321 -46.7141986  -46.51840523
 -46.50762306 -46.46343149 -46.08033935 -45.96910073 -45.94004954
 -45.47189047 -45.34946654 -45.23627882 -45.23541431 -44.99494244
 -44.99184926 -44.7644634  -44.62310756 -44.53780078 -44.32699505
 -44.29565561 -43.84464734 -43.7920154  -43.22211069 -43.2041764
 -43.14444939 -42.63170505 -42.49903983 -42.18990615 -42.08941791
 -41.97868948 -41.96459824 -41.91819405 -41.89252759 -41.86250044
 -41.76630373 -41.75643755 -41.35020296 -41.32515547 -41.22179983
 -41.18200046 -41.03294083 -40.99991982 -40.98448758 -40.86627436
 -40.84937019 -40.63970267 -40.33775477 -40.11576908 -39.97085477
 -39.92712396 -39.89750906 -39.72931271 -39.17708227 -39.14272956
 -39.07321282 -38.91826709 -38.77785603 -38.52213956 -38.29413029
 -38.28199013 -38.28138827 -38.04661848 -37.94300503 -37.88709355
 -37.79694607 -37.70658771 -37.51412288 -37.47672391 -37.37133772
 -37.3251261  -37.15321081 -37.11841822 -37.10344577 -36.99477768
 -36.65639269 -36.58958472 -36.38667894 -36.33815177 -36.28177521
 -36.03378032 -36.02351695 -35.97994531 -35.89835563 -35.77878287
 -35.74239985 -35.67327882 -35.43726783 -35.41556943 -35.28979253
 -35.19353216 -35.12234482 -35.07963015 -35.06137062 -35.02402835
 -35.01091702 -34.95588623 -34.91427059 -34.88495647 -34.86472602
 -33.86129846 -33.77802335 -33.680383   -33.4779641  -33.26579709
 -33.17157461 -33.04852912 -32.41611773 -32.41053817 -32.02508724
 -31.90686017 -31.64335581 -31.48876757 -31.44406811 -31.37649579
 -31.35230126 -31.24419956 -31.21642916 -31.15850305 -31.06744474
 -30.95117262 -30.772676   -30.76937686 -30.64081715 -30.63466958
 -30.59673213 -30.52049104 -30.42034344 -30.36982897 -30.36743577
 -30.27374553 -30.10812976 -30.06299271 -29.98083048 -29.97767593
 -29.94526105 -29.66568464 -29.22737048 -28.99559546 -28.95484843
 -28.59191373 -28.5494411  -28.42026397 -28.31342715 -28.1712167
 -28.11939797 -27.77638947 -27.70726756 -27.60103341 -27.27876998
 -26.7058308  -26.50715502 -26.39763335 -26.36197542 -25.99907479
 -25.47507675 -25.31217996 -25.11681575 -24.96039421 -24.91350015
 -24.71538539 -24.58444108 -24.12244562 -23.96980649 -23.76917367
 -23.71252839 -23.48246476 -23.05835519 -22.79553869 -22.61211306
 -22.55283027 -22.53572133 -22.14406511 -21.86139195 -21.84262733
 -21.72370465 -21.7167854  -21.64185366 -21.62657756 -21.62597106
 -21.44290047 -20.76562386 -20.68104247 -20.66287704 -20.66020817
 -20.5100558  -20.35567458 -20.10309328 -19.94369578 -19.67432429
 -19.50391949 -19.30989567 -19.03153444 -19.02121155 -18.9341265
 -18.87189699 -18.77655165 -18.71737606 -18.4634721  -18.08445937
 -17.94319772 -17.85124778 -17.82515236 -17.77364067 -17.67989719
 -17.64064603 -17.48133145 -17.27921081 -17.10217865 -17.06769934
 -16.68041488 -16.44066556 -16.325741   -16.25152473 -16.24688067
 -16.23288157 -16.01542464 -15.93004515 -15.61270366 -15.4984665
 -15.44345121 -15.26574292 -15.017961   -14.79738881 -14.60342651
 -14.47834852 -14.39794675 -14.24892226 -14.15625316 -13.94339237
 -13.8953595  -13.73405689 -13.69675016 -13.43873671 -13.21636887
 -13.16397094 -13.06151456 -12.97987294 -12.6606731  -12.65396483
 -12.5410027  -12.53347897 -12.37040308 -12.29992996 -12.26611853
 -12.25819562 -12.16816754 -12.13592928 -12.10202442 -11.97407887
 -11.96626278 -11.9523875  -11.87662961 -11.86430296 -11.75626016
 -11.60934218 -11.52377055 -11.47155848 -11.4307611  -11.11452078
 -11.04421156 -10.89117527 -10.859881   -10.85483091 -10.78272714
 -10.76409188 -10.68580682 -10.65949216 -10.41717512 -10.34239021
 -10.15973794 -10.11660614 -10.01645863  -9.92399886  -9.84457087
  -9.76720767  -9.70333238  -9.67992171  -9.60127809  -9.56750101
  -9.53798298  -9.4003034   -9.37967001  -8.9441418   -8.87450032
  -8.75795528  -8.4761537   -8.40276084  -8.32638751  -8.0269637
  -7.70107293  -7.68743448  -7.67259169  -7.57539849  -7.5455064
  -7.54460172  -7.52029309  -7.3740849   -7.36244313  -7.34546388
  -7.1893336   -7.12445111  -7.11601941  -7.10832736  -6.95906356
  -6.92008425  -6.77694649  -6.72206384  -6.71997062  -6.64795748
  -6.47884361  -6.44811604  -6.38264725  -6.29894775  -6.05448903
  -6.01870081  -5.89467275  -5.85405865  -5.64485143  -5.61579673
  -5.39544196  -5.3472021   -5.25793019  -5.24856721  -5.07848501
  -5.06486011  -5.02795798  -4.90228293  -4.82757292  -4.37983153
  -4.35856953  -4.230832    -4.0660223   -4.03104862  -4.01883024
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.3322555
  -3.329867    -3.29356852  -2.88591659  -2.83192847  -2.64166233
  -2.24005036  -2.07879873  -1.91361965]
sorted_val_rewards: [-75.80347902 -74.36305169 -69.80329493 -47.96991522 -42.17868583
 -41.50687738 -41.03992135 -39.35357961 -36.69656462 -34.63851695
 -34.34266183 -33.7629591  -31.71028623 -31.00231541 -29.20595883
 -28.13409378 -27.79654204 -27.50815322 -26.73963502 -26.19987885
 -25.29134047 -24.25434383 -18.65414818 -18.36358054 -18.2465562
 -16.35270212 -16.20186147 -15.29527985 -13.88057881 -12.51335454
 -11.86567913 -10.64954177 -10.49423816  -7.15421432  -7.08431127
  -6.66621245  -6.51820418  -5.38326081  -4.63049542  -3.07904644
  -2.67390706  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 861
num val_labels 861
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.15781313347302037, val_acc 0.943089430894309
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.11501823522903777, val_acc 0.959349593495935
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.10255160553177821, val_acc 0.9616724738675958
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1129627038045858, val_acc 0.9488966318234611
trigger times: 1
end of epoch 4: val_loss 0.5466362119600057, val_acc 0.9500580720092915
trigger times: 2
end of epoch 5: val_loss 0.1139851996089583, val_acc 0.9523809523809523
trigger times: 3
end of epoch 6: val_loss 0.10695762639204572, val_acc 0.9570267131242741
trigger times: 4
end of epoch 7: val_loss 0.14363599880221287, val_acc 0.9512195121951219
trigger times: 5
end of epoch 8: val_loss 0.23651892033394586, val_acc 0.9036004645760743
trigger times: 6
end of epoch 9: val_loss 0.14370065484695055, val_acc 0.9396051103368177
trigger times: 7
end of epoch 10: val_loss 0.31137611109525676, val_acc 0.9488966318234611
trigger times: 8
end of epoch 11: val_loss 0.7864879580009649, val_acc 0.9547038327526133
trigger times: 9
end of epoch 12: val_loss 0.4324822724639591, val_acc 0.9256678281068524
trigger times: 10
Early stopping.
0 -204.1950398683548 -75.8034790240744
1 -204.45107197761536 -74.36305168713733
2 -202.75107300281525 -69.80329492905709
3 -59.3393519371748 -47.96991522128461
4 -52.32781857997179 -42.17868583233954
5 -44.5268280133605 -41.50687737708331
6 -42.71345900837332 -41.03992135193175
7 -54.49519535899162 -39.353579610085184
8 -57.77415493130684 -36.69656461923256
9 -42.142221324145794 -34.63851695272603
10 -42.55695951450616 -34.34266182533649
11 -43.038660790771246 -33.762959097993274
12 -32.57118752133101 -31.710286229852567
13 -49.2564312890172 -31.002315411370166
14 -29.137172173708677 -29.20595883063468
15 -47.222779244184494 -28.134093781998498
16 -40.67886431887746 -27.796542043451076
17 -41.34464827924967 -27.50815321826314
18 -41.122110202908516 -26.739635023445388
19 -38.90737423300743 -26.199878851984913
20 -41.06354358792305 -25.29134046899922
21 -40.364698842167854 -24.254343826223852
22 -32.459704145789146 -18.654148179968352
23 -36.07769836857915 -18.363580535271076
24 -33.39527396671474 -18.24655620273112
25 -30.779091102071106 -16.352702123112557
26 -23.840957408305258 -16.201861466781004
27 -31.714245818555355 -15.295279851466828
28 -28.735492695122957 -13.880578810015049
29 -28.007530950009823 -12.513354538352278
30 -28.221237123012543 -11.8656791260697
31 -24.949030116200447 -10.649541772408808
32 -26.346334233880043 -10.494238159943446
33 -20.78712324053049 -7.1542143150163025
34 -20.559103786945343 -7.08431127494343
35 -19.282369077205658 -6.666212449853871
36 -20.726184248924255 -6.51820418055673
37 -18.35031934827566 -5.383260807350201
38 -14.650131989270449 -4.63049541560991
39 -14.74848422408104 -3.079046443285274
40 -13.724989727139473 -2.673907055233777
41 -13.742213010787964 -2.49009826539426
train accuracy: 0.9096433895195505
validation accuracy: 0.9256678281068524
sorted_train_rewards: [-78.82803921 -76.43570385 -75.80347902 -75.66491152 -75.10649943
 -74.36305169 -74.32686041 -74.28113176 -74.16223804 -72.96727971
 -72.88744715 -72.80827843 -72.05846729 -71.82748874 -71.48295963
 -71.41555074 -70.56929291 -69.84848134 -69.80329493 -55.21445752
 -51.17814647 -49.6099766  -49.03866708 -49.0331838  -48.84818037
 -48.58850771 -48.19509289 -47.96991522 -47.33487662 -47.14538324
 -47.10889581 -46.92815321 -46.7141986  -46.51840523 -46.50762306
 -46.46343149 -46.08033935 -45.96910073 -45.94004954 -45.34946654
 -45.23627882 -45.23541431 -44.99494244 -44.99184926 -44.7644634
 -44.62310756 -44.53780078 -44.32699505 -44.29565561 -43.84464734
 -43.7920154  -43.22211069 -43.2041764  -43.14444939 -42.63170505
 -42.49903983 -42.18990615 -42.17868583 -42.08941791 -41.97868948
 -41.96459824 -41.91819405 -41.89252759 -41.86250044 -41.76630373
 -41.75643755 -41.50687738 -41.35020296 -41.32515547 -41.22179983
 -41.18200046 -41.03992135 -40.99991982 -40.98448758 -40.86627436
 -40.84937019 -40.33775477 -39.97085477 -39.92712396 -39.89750906
 -39.72931271 -39.35357961 -39.14272956 -39.07321282 -38.91826709
 -38.77785603 -38.52213956 -38.29413029 -38.28199013 -38.28138827
 -37.94300503 -37.88709355 -37.79694607 -37.70658771 -37.51412288
 -37.47672391 -37.37133772 -37.3251261  -37.15321081 -37.11841822
 -37.10344577 -36.99477768 -36.69656462 -36.65639269 -36.58958472
 -36.38667894 -36.33815177 -36.28177521 -36.03378032 -36.02351695
 -35.97994531 -35.89835563 -35.77878287 -35.67327882 -35.43726783
 -35.41556943 -35.28979253 -35.19353216 -35.12234482 -35.07963015
 -35.02402835 -35.01091702 -34.95588623 -34.91427059 -34.88495647
 -34.86472602 -34.63851695 -34.34266183 -33.86129846 -33.77802335
 -33.7629591  -33.680383   -33.4779641  -33.26579709 -33.17157461
 -33.04852912 -32.41611773 -32.41053817 -32.02508724 -31.90686017
 -31.71028623 -31.64335581 -31.48876757 -31.44406811 -31.37649579
 -31.24419956 -31.21642916 -31.15850305 -31.06744474 -31.00231541
 -30.772676   -30.76937686 -30.64081715 -30.63466958 -30.59673213
 -30.52049104 -30.36982897 -30.36743577 -30.27374553 -30.10812976
 -30.06299271 -29.98083048 -29.97767593 -29.94526105 -29.66568464
 -29.22737048 -29.20595883 -28.59191373 -28.42026397 -28.31342715
 -28.1712167  -28.13409378 -28.11939797 -27.70726756 -27.60103341
 -27.27876998 -26.7058308  -26.50715502 -26.39763335 -26.36197542
 -25.99907479 -25.47507675 -25.31217996 -25.29134047 -25.11681575
 -24.96039421 -24.91350015 -24.71538539 -24.58444108 -24.25434383
 -24.12244562 -23.96980649 -23.76917367 -23.71252839 -23.48246476
 -23.05835519 -22.79553869 -22.61211306 -22.53572133 -22.14406511
 -21.86139195 -21.84262733 -21.72370465 -21.7167854  -21.64185366
 -21.62657756 -21.62597106 -21.44290047 -20.76562386 -20.68104247
 -20.66287704 -20.66020817 -20.5100558  -20.35567458 -20.10309328
 -19.94369578 -19.67432429 -19.50391949 -19.30989567 -19.03153444
 -19.02121155 -18.9341265  -18.87189699 -18.77655165 -18.71737606
 -18.65414818 -18.36358054 -18.2465562  -18.08445937 -17.94319772
 -17.85124778 -17.82515236 -17.77364067 -17.67989719 -17.64064603
 -17.48133145 -17.27921081 -17.10217865 -17.06769934 -16.68041488
 -16.44066556 -16.325741   -16.25152473 -16.24688067 -16.23288157
 -16.01542464 -15.93004515 -15.61270366 -15.4984665  -15.44345121
 -15.26574292 -15.017961   -14.60342651 -14.58840707 -14.47834852
 -14.39794675 -14.33383654 -14.24892226 -14.15625316 -13.94339237
 -13.92795472 -13.8953595  -13.81645072 -13.73405689 -13.69675016
 -13.43873671 -13.36563421 -13.21636887 -13.16397094 -13.06151456
 -12.97987294 -12.6606731  -12.65396483 -12.5410027  -12.53347897
 -12.51335454 -12.37040308 -12.29992996 -12.26611853 -12.25819562
 -12.16816754 -12.13592928 -12.10202442 -11.97407887 -11.96626278
 -11.9523875  -11.87662961 -11.86430296 -11.75626016 -11.60934218
 -11.53232877 -11.52377055 -11.11452078 -11.04421156 -10.89117527
 -10.859881   -10.85483091 -10.78272714 -10.7761712  -10.76409188
 -10.68580682 -10.65949216 -10.64954177 -10.41717512 -10.39554938
 -10.34239021 -10.15973794 -10.11660614 -10.01645863  -9.92399886
  -9.84457087  -9.81238151  -9.76720767  -9.70333238  -9.67992171
  -9.60127809  -9.56750101  -9.53798298  -9.5310375   -9.4003034
  -9.37967001  -9.35146048  -9.2655951   -9.21517276  -8.9441418
  -8.87450032  -8.75795528  -8.4761537   -8.40276084  -8.32638751
  -8.0269637   -7.70107293  -7.68743448  -7.67259169  -7.57539849
  -7.5455064   -7.54460172  -7.52029309  -7.3740849   -7.36244313
  -7.34546388  -7.1893336   -7.15421432  -7.12445111  -7.11601941
  -7.10832736  -7.08431127  -7.00144823  -6.95906356  -6.92008425
  -6.77694649  -6.72206384  -6.71997062  -6.64795748  -6.47884361
  -6.44811604  -6.38264725  -6.29894775  -6.05448903  -6.01870081
  -5.89467275  -5.85405865  -5.79967332  -5.64485143  -5.61579673
  -5.39544196  -5.3472021   -5.25793019  -5.24856721  -5.07848501
  -5.06486011  -5.02795798  -4.90228293  -4.82757292  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.01883024  -4.00401798  -3.97870856  -3.65032555  -3.38446715
  -3.3322555   -3.329867    -3.29356852  -3.07904644  -2.88591659
  -2.83192847  -2.64166233  -2.49009827  -2.24005036  -2.07879873
  -1.91361965]
sorted_val_rewards: [-71.77271563 -54.46505855 -49.13001653 -47.53054283 -46.92993068
 -45.47189047 -41.03294083 -40.63970267 -40.11576908 -39.17708227
 -38.04661848 -35.74239985 -35.06137062 -31.35230126 -30.95117262
 -30.42034344 -28.99559546 -28.95484843 -28.5494411  -27.79654204
 -27.77638947 -27.50815322 -26.73963502 -26.19987885 -22.55283027
 -18.4634721  -16.35270212 -16.20186147 -15.29527985 -15.16678243
 -14.79738881 -13.88057881 -11.86567913 -11.80017871 -11.47155848
 -11.4307611  -10.69977063 -10.49423816  -9.7064803   -6.66621245
  -6.51820418  -5.63752003  -5.38326081  -2.67390706]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 946
num val_labels 946
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.18866548861181578, val_acc 0.9471458773784355
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.7712515099859423, val_acc 0.9006342494714588
trigger times: 1
end of epoch 2: val_loss 0.18925683125825474, val_acc 0.9492600422832981
trigger times: 2
end of epoch 3: val_loss 0.5954993369427983, val_acc 0.9069767441860465
trigger times: 3
end of epoch 4: val_loss 0.5086472684625234, val_acc 0.9418604651162791
trigger times: 4
end of epoch 5: val_loss 23.400834616193805, val_acc 0.7875264270613108
trigger times: 5
end of epoch 6: val_loss 0.37190022490613717, val_acc 0.9482029598308668
trigger times: 6
end of epoch 7: val_loss 0.1373390311812788, val_acc 0.9429175475687104
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.15728315646035554, val_acc 0.9545454545454546
trigger times: 1
end of epoch 9: val_loss 0.1668387570729564, val_acc 0.9439746300211417
trigger times: 2
end of epoch 10: val_loss 0.3030590035011918, val_acc 0.9513742071881607
trigger times: 3
end of epoch 11: val_loss 2.4371697620351402, val_acc 0.9080338266384778
trigger times: 4
end of epoch 12: val_loss 0.32408808753232093, val_acc 0.9376321353065539
trigger times: 5
end of epoch 13: val_loss 0.42201108706499274, val_acc 0.9238900634249472
trigger times: 6
end of epoch 14: val_loss 0.7666320811429191, val_acc 0.9090909090909091
trigger times: 7
end of epoch 15: val_loss 0.1335805010968161, val_acc 0.9503171247357294
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.15384120503569396, val_acc 0.9556025369978859
trigger times: 1
end of epoch 17: val_loss 0.23721048535532133, val_acc 0.9355179704016914
trigger times: 2
end of epoch 18: val_loss 0.41803456743162304, val_acc 0.9408033826638478
trigger times: 3
end of epoch 19: val_loss 0.3161447380306643, val_acc 0.9492600422832981
trigger times: 4
end of epoch 20: val_loss 0.11409190711843432, val_acc 0.9608879492600423
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.1399887140233565, val_acc 0.9503171247357294
trigger times: 1
end of epoch 22: val_loss 0.23835346586646125, val_acc 0.9418604651162791
trigger times: 2
end of epoch 23: val_loss 6.3329689654787735, val_acc 0.9038054968287527
trigger times: 3
end of epoch 24: val_loss 0.2897946737337806, val_acc 0.9207188160676533
trigger times: 4
end of epoch 25: val_loss 2.0557999121354507, val_acc 0.8446088794926004
trigger times: 5
end of epoch 26: val_loss 0.9622508596350386, val_acc 0.8858350951374208
trigger times: 6
end of epoch 27: val_loss 0.1404620182364058, val_acc 0.9503171247357294
trigger times: 7
end of epoch 28: val_loss 1.1586937323014344, val_acc 0.927061310782241
trigger times: 8
end of epoch 29: val_loss 1.4548851601462331, val_acc 0.8942917547568711
trigger times: 9
end of epoch 30: val_loss 0.12202912264514912, val_acc 0.9566596194503171
trigger times: 10
Early stopping.
0 -222.80331873893738 -71.77271563258427
1 -61.00787973403931 -54.46505855143417
2 -58.99759644269943 -49.13001653445393
3 -48.068508476018906 -47.53054283339663
4 -52.359682992100716 -46.929930683395
5 -52.105215817689896 -45.47189046691946
6 -53.587139800190926 -41.03294083119757
7 -48.16860218346119 -40.63970267477056
8 -48.3531543686986 -40.11576908191633
9 -47.97152505815029 -39.17708227320146
10 -51.81669768691063 -38.04661848486053
11 -47.07579220831394 -35.74239985466346
12 -44.05350470542908 -35.06137062419995
13 -37.81729330122471 -31.352301261142387
14 -45.42819495499134 -30.9511726207529
15 -41.854902379214764 -30.42034343549779
16 -37.25479386001825 -28.995595459355016
17 -41.65538163483143 -28.954848426086794
18 -39.51280476152897 -28.549441103161325
19 -38.400723837316036 -27.796542043451076
20 -37.4953573718667 -27.776389469559156
21 -37.654760867357254 -27.50815321826314
22 -36.21780666708946 -26.739635023445388
23 -34.19813158363104 -26.199878851984913
24 -36.33354793488979 -22.552830265023072
25 -27.63143364340067 -18.463472103110583
26 -24.75597732514143 -16.352702123112557
27 -25.215812101960182 -16.201861466781004
28 -24.518007218837738 -15.295279851466828
29 -26.62127736210823 -15.166782431856365
30 -23.87586299329996 -14.797388813532848
31 -22.458841376006603 -13.880578810015049
32 -19.2499555721879 -11.8656791260697
33 -22.75063693523407 -11.800178707474927
34 -18.64293022453785 -11.47155847649776
35 -21.041680477559566 -11.430761099930217
36 -20.773062705993652 -10.699770632698094
37 -19.060374408960342 -10.494238159943446
38 -19.708279564976692 -9.706480298706222
39 -13.554683044552803 -6.666212449853871
40 -11.017481029033661 -6.51820418055673
41 -13.333356440067291 -5.637520027835424
42 -10.320709370076656 -5.383260807350201
43 -8.730701334774494 -2.673907055233777
train accuracy: 0.964338951955051
validation accuracy: 0.9566596194503171

[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 0.00014142747549215074, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6738e-05,  7.2994e-06,  4.8564e-02, -1.4043e-02,  7.1490e-07,
         -1.0105e-05, -5.2565e-04,  4.2830e-04,  5.1838e-05, -1.7621e-05,
         -1.4320e-03, -3.0832e-01, -4.0922e-01]], device='cuda:3'))])
end of epoch 1: val_loss 0.05764577349837932, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.8424e-02,  1.1424e-01,  4.0264e-01, -2.3493e-01, -2.0633e-05,
         -5.5251e-05,  7.1901e-03,  2.7380e-02,  5.7915e-05,  7.1902e-05,
         -1.4320e-03, -1.4740e+00, -2.1452e+00]], device='cuda:3'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.2277e-05,  6.1546e-06,  2.6768e-01, -1.2555e-01, -4.2775e-05,
         -1.4546e-04, -1.3273e-06,  2.5334e-03, -4.0568e-05,  1.5724e-04,
         -1.4320e-03, -5.3046e-01, -1.9255e+00]], device='cuda:3'))])
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7382e-04,  3.1369e-05,  6.5485e-06, -6.2472e-05, -1.5387e-04,
         -3.6299e-04,  1.5399e-06, -6.7298e-06, -9.9962e-05,  3.6741e-04,
         -1.4320e-03, -2.2884e-04, -1.3848e+00]], device='cuda:3'))])
end of epoch 5: val_loss 0.41440693900786213, val_acc 0.945
trigger times: 1
end of epoch 6: val_loss 4.628369096870699e-06, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 2.5629965350049134e-08, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 4.231928215858716e-08, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.4164e-01,  2.4629e-01,  4.2925e-01, -1.7559e-01,  4.2303e-01,
         -2.9643e-01, -1.2354e-02,  9.7122e-03, -1.3814e-01,  2.5713e-01,
         -1.4320e-03, -1.7484e+00, -2.4995e+00]], device='cuda:3'))])
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0240e-01,  1.2221e-01,  3.2924e-01, -1.2102e-01,  1.9920e-02,
          4.3358e-05, -7.5627e-03,  1.3972e-06,  1.4098e-05,  1.1120e-04,
         -1.4320e-03, -1.0971e+00, -2.3573e+00]], device='cuda:3'))])
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0080e-06, -1.2454e-05,  9.6822e-02, -8.5847e-06,  1.2894e-04,
          1.3681e-05,  4.1881e-07,  3.6899e-06,  2.0273e-04,  2.3171e-04,
         -1.4321e-03, -9.9375e-05, -2.0006e+00]], device='cuda:3'))])
end of epoch 12: val_loss 2.9802319900795737e-09, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 4.529932539298898e-08, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 3.5762784023063432e-09, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 3.6446875310502945e-06, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.1749e-06,  7.7063e-07, -5.8110e-06,  8.2262e-02, -2.3034e-06,
         -6.7665e-05, -5.0656e-04,  3.5377e-02,  9.2045e-05, -2.9817e-05,
         -1.4323e-03, -6.1287e-01, -1.6691e+00]], device='cuda:3'))])
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1204e-05, -1.1931e-05, -1.3243e-05,  6.5263e-06, -2.7189e-04,
         -1.5064e-04, -8.5674e-08,  1.1870e-05,  2.3867e-04, -6.4377e-05,
         -1.4323e-03, -2.3342e-04, -1.2790e+00]], device='cuda:3'))])
end of epoch 19: val_loss 0.10287585107422345, val_acc 0.97
trigger times: 1
end of epoch 20: val_loss 6.55650637781946e-09, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 1.5497205154701987e-08, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 1.3230710464995354e-06, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 1.7881391656260348e-09, val_acc 1.0
trigger times: 6
end of epoch 25: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 7
end of epoch 26: val_loss 2.1828411150792704e-05, val_acc 1.0
trigger times: 8
end of epoch 27: val_loss 1.4143233420327305e-05, val_acc 1.0
trigger times: 9
end of epoch 28: val_loss 6.55650637781946e-09, val_acc 1.0
trigger times: 10
Early stopping.
0 -119.77256375551224 -54.98547503240923
1 -131.32177141308784 -50.492268601198035
2 -117.14314290881157 -50.03933801517046
3 -107.99895465373993 -49.75347184620696
4 -110.60265189409256 -49.72654640753777
5 -98.22557318210602 -46.98011874490918
6 -106.16180121898651 -45.7351542845057
7 -95.4098424911499 -45.670579884154705
8 -97.74888065457344 -44.99030608142343
9 -115.93766340613365 -44.14602409201361
10 -95.5541627407074 -43.81326882122305
11 -100.48614525794983 -43.18878399086166
12 -122.832240909338 -42.29180714825394
13 -98.88812094926834 -42.00401746161006
14 -116.04266232252121 -41.6910044370425
15 -108.88489824533463 -41.68588229294918
16 -83.97170582413673 -41.281777102712205
17 -101.36500307917595 -40.44278203413966
18 -103.40985715389252 -40.34838365523108
19 -90.13592365384102 -39.599701153458774
20 -96.77347019314766 -39.57586365327889
21 -112.4412614107132 -39.31972693233231
22 -86.30250938236713 -39.024610555047154
23 -94.97707641124725 -38.45534493538269
24 -101.88902169466019 -38.41270390343083
25 -96.56895461678505 -38.35634328077039
26 -106.16350100934505 -37.79713616772368
27 -92.58828574419022 -37.741528994987384
28 -97.56502360105515 -37.66475323879293
29 -76.95914424955845 -37.513139380385574
30 -106.11355888843536 -37.1809993033689
31 -92.8584948182106 -37.100703136010694
32 -95.94877108931541 -37.00630588930485
33 -81.91095742583275 -36.821916772458344
34 -85.9568050801754 -36.48799015296732
35 -90.58500337600708 -36.20965269874363
36 -83.21714687347412 -36.19207561676116
37 -87.09308677911758 -36.114459029559086
38 -95.95787662267685 -35.78149902167743
39 -73.5121963173151 -35.394503873250635
40 -105.92481797933578 -35.26282499693737
41 -78.60014513134956 -35.24303541418371
42 -84.47184334695339 -35.209705244501436
43 -85.34750974178314 -35.0654408505187
44 -71.49782860279083 -34.80241747531743
45 -74.27813777327538 -34.64469044638467
46 -94.04275593161583 -33.84284985953318
47 -83.84212735295296 -32.70706485357069
48 -71.1323656141758 -31.969099402548657
49 -74.74168375134468 -31.7109134007892
50 -73.74043881893158 -31.64414355845032
51 -67.07260437309742 -31.392382758954444
52 -65.38292180001736 -31.223196019713853
53 -69.02971406280994 -31.12953085092458
54 -90.6688384860754 -29.39157139549552
55 -70.73135498166084 -29.340125609942326
56 -75.43061038851738 -29.106189988903285
57 -79.23253901302814 -27.41102349748205
58 -67.34480920433998 -27.343722362182305
59 -62.22973372042179 -27.196681629483837
60 -85.34177406132221 -27.07399028854534
61 -77.60010850429535 -26.7047217556024
62 -67.03963074088097 -26.244794902859052
63 -58.89303930103779 -25.548365085275513
64 -65.39895725250244 -25.45878528601009
65 -55.785564720630646 -24.879106999799365
66 -59.39189748466015 -24.828695359328833
67 -77.14149449765682 -24.592745144504722
68 -72.14531891047955 -23.978745577896312
69 -60.62631814181805 -23.57262108435893
70 -58.52700390666723 -23.44970807952351
71 -66.88757261633873 -22.745309160183492
72 -76.64386954903603 -22.60679894414887
73 -51.553618371486664 -22.19891031871716
74 -67.81026378273964 -20.656863763892378
75 -56.0483338907361 -20.444472560731253
76 -40.15790270268917 -20.19699010077007
77 -46.01901312172413 -20.13839114930498
78 -42.2513672709465 -19.63760343800059
79 -70.4370831400156 -19.515598718228343
80 -48.87860606610775 -18.92838809611677
81 -46.75960086286068 -17.994774057192853
82 -58.58940176665783 -17.55742370467821
83 -51.089001193642616 -16.823073927842348
84 -50.7752127610147 -14.855082803515382
85 -37.448603704571724 -14.531424598833084
86 -45.37238631397486 -14.442420089224363
87 -40.81978330016136 -13.596012850960644
88 -32.980783343315125 -12.68135972540495
89 -25.969921126961708 -12.66418205637357
90 -30.714898854494095 -12.30017947419658
91 -36.209686517715454 -12.151904772081672
92 -52.57517006993294 -11.788852141676486
93 -30.496614456176758 -10.869989101210326
94 -28.489150501787663 -10.327681503524177
95 -21.10767286270857 -9.8572159761571
96 -23.875196143984795 -8.330116995310416
97 -11.331160232424736 -8.133195842510668
98 -13.065243035554886 -8.108197691178031
99 -24.634339578449726 -7.57539849177145
100 -21.578223258256912 -7.362443126623615
101 -24.037220790982246 -7.108327355338034
102 -21.644602231681347 -6.959063561385431
103 -29.433417983353138 -6.776946485018116
104 -20.53398610651493 -6.7220638398623045
105 -32.61560283601284 -6.719970621583102
106 -16.819742500782013 -6.535447341844848
107 -33.66684952378273 -6.51820418055673
108 -30.100096680223942 -5.615796733870542
109 -13.819592267274857 -5.34720210027791
110 -16.76712181419134 -5.078485007852753
111 -14.976158000528812 -5.027957977402961
112 -9.34026712179184 -4.827572916892203
113 -13.437901744619012 -4.63049541560991
114 -8.695687592029572 -4.230832004686763
115 -5.830489903688431 -4.031048624093466
116 -6.87691655755043 -3.3844671463622564
117 -1.0265100747346878 -3.3322555012187633
118 -1.8077614530920982 -2.6416623314910934
119 6.020898252725601 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -49.75347185 -49.72654641 -46.98011874
 -45.67057988 -44.14602409 -43.81326882 -41.68588229 -40.34838366
 -39.31972693 -39.02461056 -38.4127039  -38.35634328 -37.79713617
 -37.51313938 -37.00630589 -36.82191677 -35.78149902 -35.262825
 -34.80241748 -34.64469045 -33.84284986 -31.9690994  -31.7109134
 -31.39238276 -31.12953085 -29.34012561 -27.4110235  -26.70472176
 -24.879107   -24.59274514 -23.57262108 -20.65686376 -20.44447256
 -20.1969901  -20.13839115 -19.63760344 -18.47613272 -17.84704807
 -17.70010796 -17.5574237  -17.27264837 -17.26964167 -16.94523023
 -16.49492982 -15.35248307 -15.20660005 -14.8550828  -14.77335287
 -14.72806836 -14.5314246  -14.47909134 -14.44242009 -14.2565193
 -14.24605567 -13.59601285 -13.35611538 -13.25382224 -12.91732158
 -12.86251458 -12.84568081 -12.71765016 -12.6905914  -12.68135973
 -12.66418206 -12.56087773 -12.52944011 -12.45732713 -12.45309085
 -12.27640887 -12.2546258  -12.15190477 -12.0734103  -11.88665645
 -11.6434476  -11.58287387 -11.39706868 -11.22962363 -11.09986923
 -10.75001894 -10.69700463 -10.64979922 -10.44266125 -10.23104389
 -10.0753169   -9.80600141  -9.7847044   -9.6131024   -9.35370278
  -9.33004788  -9.31790133  -9.22960414  -9.19147386  -8.96879584
  -8.9396056   -8.91581609  -8.90074077  -8.90069653  -8.73791992
  -8.68248772  -8.49606581  -8.41569884  -8.40114546  -8.23672992
  -8.13319584  -7.57539849  -7.36244313  -7.10832736  -6.95906356
  -6.94464949  -6.77694649  -6.72206384  -6.53544734  -5.61579673
  -5.07848501  -5.02795798  -4.230832    -3.3322555   -2.64166233]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.0004660924631433616, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.4431e-02, -3.9199e-02,  6.5643e-03,  2.6093e-02, -1.3216e-04,
          7.0247e-02, -2.1728e-02, -5.4599e-03,  1.2472e-04, -2.3378e-01,
         -4.8354e-04, -2.9634e-01, -1.0801e+00]], device='cuda:1'))])
end of epoch 1: val_loss 4.6906845199146117e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0078e-01,  3.3787e-01,  1.8715e-01, -4.7064e-02,  1.7378e-05,
          3.6771e-01, -5.6674e-02, -5.2674e-02,  7.5813e-02, -7.5006e-01,
          8.0948e-04, -1.2611e+00, -1.6944e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.05901068329811089, val_acc 0.985
trigger times: 1
end of epoch 3: val_loss 4.7146420779853314e-07, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 2.3995283696010007e-06, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2528e-05,  5.1788e-02, -1.5448e-01, -5.4373e-06, -2.3385e-05,
          6.0212e-01,  2.5532e-02,  6.2666e-02,  2.4286e-05, -9.4324e-01,
          1.1226e-05, -1.8330e+00, -2.7909e+00]], device='cuda:1'))])
end of epoch 6: val_loss 2.0038186413628977e-06, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.8108e-02, -2.8398e-02,  4.6859e-02,  1.1353e-02, -6.7962e-05,
          6.8290e-02, -2.1147e-02,  5.7744e-03,  3.0353e-01, -6.5026e-01,
          7.2326e-04, -1.5732e+00, -2.6723e+00]], device='cuda:1'))])
end of epoch 8: val_loss 1.654003383653403, val_acc 0.9
trigger times: 1
end of epoch 9: val_loss 7.834558026331706e-06, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 1.6335361069224065e-05, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 4.750384858596135e-06, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 8.936639015708181e-05, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 5.960462772236497e-09, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 6.19887574160316e-08, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.07713960961188021, val_acc 0.98
trigger times: 8
end of epoch 16: val_loss 6.556509184463266e-09, val_acc 1.0
trigger times: 9
end of epoch 17: val_loss 0.00012058623172261917, val_acc 1.0
trigger times: 10
Early stopping.
0 -96.59396839141846 -50.492268601198035
1 -93.76880544424057 -50.03933801517046
2 -94.14215517044067 -49.75347184620696
3 -92.10445883870125 -49.72654640753777
4 -92.85238867998123 -46.98011874490918
5 -78.96949906647205 -45.670579884154705
6 -77.91882294416428 -44.14602409201361
7 -82.22347694635391 -43.81326882122305
8 -85.34546330571175 -41.68588229294918
9 -74.11133348941803 -40.34838365523108
10 -83.53588742017746 -39.31972693233231
11 -76.94683289527893 -39.024610555047154
12 -82.64887547492981 -38.41270390343083
13 -79.20706737041473 -38.35634328077039
14 -77.71807613968849 -37.79713616772368
15 -68.86771810054779 -37.513139380385574
16 -77.23655465245247 -37.00630588930485
17 -70.22655177116394 -36.821916772458344
18 -75.96228921413422 -35.78149902167743
19 -70.95023491978645 -35.26282499693737
20 -70.62040083110332 -34.80241747531743
21 -68.06899985671043 -34.64469044638467
22 -72.63685500621796 -33.84284985953318
23 -67.1150529384613 -31.969099402548657
24 -70.73823362588882 -31.7109134007892
25 -65.64213610440493 -31.392382758954444
26 -59.01059429347515 -31.12953085092458
27 -57.81506606936455 -29.340125609942326
28 -60.13722898066044 -27.41102349748205
29 -59.59510365128517 -26.7047217556024
30 -49.549599438905716 -24.879106999799365
31 -59.696782648563385 -24.592745144504722
32 -48.53670045733452 -23.57262108435893
33 -43.85357886552811 -20.656863763892378
34 -48.46348915994167 -20.444472560731253
35 -42.797285959124565 -20.19699010077007
36 -49.72964692115784 -20.13839114930498
37 -42.891270875930786 -19.63760343800059
38 -33.60250794887543 -18.476132715319725
39 -32.42906165122986 -17.84704807413179
40 -33.309224009513855 -17.700107960196565
41 -46.8431720584631 -17.55742370467821
42 -30.65599086880684 -17.272648368669138
43 -32.509974122047424 -17.269641672565246
44 -32.676704823970795 -16.945230233936098
45 -29.347984075546265 -16.49492982219607
46 -29.18548294901848 -15.352483070393303
47 -29.63337779045105 -15.206600054934952
48 -43.198445841670036 -14.855082803515382
49 -27.797487258911133 -14.773352865013601
50 -28.603604406118393 -14.728068360403778
51 -38.61315356940031 -14.531424598833084
52 -29.34162402153015 -14.479091335668526
53 -36.02333655953407 -14.442420089224363
54 -28.759341150522232 -14.256519300904145
55 -29.02336785197258 -14.246055672917981
56 -40.336562007665634 -13.596012850960644
57 -27.48593059182167 -13.35611538306493
58 -27.161513805389404 -13.253822242841537
59 -26.45864687860012 -12.917321581688867
60 -27.324582189321518 -12.86251457548991
61 -27.06320610642433 -12.84568080741886
62 -27.99818917363882 -12.71765016283811
63 -27.14820235967636 -12.690591399552646
64 -27.141987996175885 -12.68135972540495
65 -34.241320606321096 -12.66418205637357
66 -27.665484338998795 -12.560877731741472
67 -26.767136335372925 -12.5294401050962
68 -25.22525265812874 -12.45732712735986
69 -26.024891778826714 -12.453090850027106
70 -26.36609399318695 -12.276408870437042
71 -27.004908323287964 -12.254625803818083
72 -31.277134954929352 -12.151904772081672
73 -26.792110860347748 -12.073410300448248
74 -26.534760788083076 -11.886656451515504
75 -26.006811290979385 -11.643447601728788
76 -24.037456840276718 -11.582873868230912
77 -26.687018528580666 -11.397068675172687
78 -24.75697374343872 -11.22962362513982
79 -25.89228782057762 -11.099869227839568
80 -24.18137091398239 -10.750018944661386
81 -24.76249197125435 -10.69700462668569
82 -23.97258684039116 -10.649799217302224
83 -24.91626328229904 -10.442661253237253
84 -24.89051127433777 -10.23104388786872
85 -24.731331899762154 -10.075316903648199
86 -22.70379939675331 -9.806001406172223
87 -23.456825897097588 -9.784704404750451
88 -23.277662605047226 -9.613102397987658
89 -21.51428197324276 -9.353702777509321
90 -23.60814943909645 -9.33004787766574
91 -22.484840497374535 -9.31790132590727
92 -20.91404066234827 -9.229604143911953
93 -22.614731416106224 -9.19147386225842
94 -23.506793469190598 -8.968795841626976
95 -20.75373975932598 -8.939605595422433
96 -22.538103476166725 -8.91581608531327
97 -21.603635281324387 -8.900740765348912
98 -22.769465044140816 -8.900696530885693
99 -22.34346543252468 -8.737919924162648
100 -22.32026332616806 -8.682487717368405
101 -22.866398841142654 -8.496065812426837
102 -20.518217638134956 -8.41569884078823
103 -21.974135398864746 -8.401145461327584
104 -21.267439275979996 -8.236729918616229
105 -25.76961151137948 -8.133195842510668
106 -21.991779796779156 -7.57539849177145
107 -22.00896890461445 -7.362443126623615
108 -17.321184700354934 -7.108327355338034
109 -25.59915928542614 -6.959063561385431
110 -18.423480704426765 -6.944649489864398
111 -20.694464594125748 -6.776946485018116
112 -19.0231651365757 -6.7220638398623045
113 -27.141101591289043 -6.535447341844848
114 -26.904268570244312 -5.615796733870542
115 -22.93173687160015 -5.078485007852753
116 -22.71987284719944 -5.027957977402961
117 -16.944195054471493 -4.230832004686763
118 -16.048706989735365 -3.3322555012187633
119 -16.91843941062689 -2.6416623314910934
train accuracy: 1.0
validation accuracy: 1.0
[-50.4922686  -50.03933802 -46.98011874 -45.67057988 -44.14602409
 -38.4127039  -38.35634328 -36.82191677 -35.262825   -34.64469045
 -33.84284986 -31.9690994  -27.4110235  -26.70472176 -24.879107
 -24.59274514 -20.65686376 -20.13839115 -18.47613272 -17.5574237
 -17.27264837 -16.49492982 -15.35248307 -15.20660005 -14.8550828
 -14.77335287 -14.72806836 -14.5314246  -14.47909134 -14.44242009
 -14.2565193  -13.52966712 -13.29183258 -13.25382224 -12.99909297
 -12.91732158 -12.71765016 -12.68135973 -12.67638946 -12.66418206
 -12.56087773 -12.45932141 -12.45732713 -12.15190477 -12.0734103
 -11.58287387 -11.47939853 -11.39706868 -11.28771953 -11.23536942
 -11.22962363 -11.19748198 -11.09986923 -10.88849793 -10.82921066
 -10.75496234 -10.74576902 -10.44266125 -10.23104389 -10.0753169
  -9.7847044   -9.61909916  -9.53921498  -9.44945883  -9.26140449
  -9.24356359  -9.05364645  -8.9396056   -8.91581609  -8.90074077
  -8.68248772  -8.51026036  -8.49606581  -8.43112821  -8.42665082
  -8.42167857  -8.40114546  -8.2607869   -8.20613311  -8.13319584
  -8.03503139  -7.78531663  -7.57539849  -7.24749029  -7.21498356
  -7.21230699  -7.15489884  -7.0834787   -7.03289538  -6.94464949
  -6.93139782  -6.86990384  -6.77694649  -6.76159169  -6.72206384
  -6.51438479  -6.26422779  -6.11876735  -5.8371616   -5.80815414
  -5.65240433  -5.24623053  -5.21405067  -5.13289642  -5.02795798
  -5.01139413  -4.79884393  -4.76031121  -4.72717494  -4.24437325
  -4.23748142  -4.21127191  -3.83985223  -3.83781248  -3.74011888
  -3.72992108  -3.10914049  -3.0240266   -2.36709321  -1.99548813]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.014843560535024664, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.8550e-01,  2.7215e-01, -8.2105e-02, -2.1491e-01,  4.1192e-01,
          4.3906e-01,  4.3757e-02,  8.1666e-02,  6.8179e-02, -3.0239e-01,
         -2.3905e-04, -1.6567e+00, -1.5134e+00]], device='cuda:1'))])
end of epoch 1: val_loss 4.076839554656431e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.7674e-02,  2.8684e-01, -7.4882e-02, -4.2858e-03,  1.4488e-05,
          1.5774e-01, -1.9966e-06,  5.9101e-02,  5.5422e-05, -1.2560e-01,
          5.8780e-04, -1.4095e+00, -1.4476e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.0037500059906970763, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.0006235062646781487, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 7.011770046858601e-06, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.005957934649123118, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 4.3151727875496706e-05, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.00015486072206865486, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 1.0651103283620955e-06, val_acc 1.0
trigger times: 7
end of epoch 9: val_loss 6.162738842895976e-05, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 1.5497183767365642e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.4292e-01,  7.2508e-01, -1.2682e-01, -5.1733e-02,  2.9143e-01,
          3.1856e-01,  1.3191e-02,  8.8706e-02, -1.2458e-01, -6.7332e-01,
          6.5715e-04, -2.1092e+00, -2.5018e+00]], device='cuda:1'))])
end of epoch 11: val_loss 6.513380938770297e-05, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 6.174870234122665e-05, val_acc 1.0
trigger times: 2
end of epoch 13: val_loss 5.538322697873355e-06, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.024773258459231826, val_acc 0.99
trigger times: 4
end of epoch 15: val_loss 2.025658505235839e-05, val_acc 1.0
trigger times: 5
end of epoch 16: val_loss 3.3155332787302425e-05, val_acc 1.0
trigger times: 6
end of epoch 17: val_loss 0.07100506961416428, val_acc 0.985
trigger times: 7
end of epoch 18: val_loss 0.3658749554640865, val_acc 0.96
trigger times: 8
end of epoch 19: val_loss 2.9802313861182485e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.9167e-01,  6.9513e-01, -5.9235e-02,  1.5507e-01,  1.9085e-01,
          4.6048e-01,  7.5750e-02,  4.3008e-02, -2.9739e-01, -5.7624e-01,
          5.8873e-05, -2.6378e+00, -2.8209e+00]], device='cuda:1'))])
end of epoch 20: val_loss 1.5675898310973934e-07, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 5.465711776864168e-07, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 0.3649556924887824, val_acc 0.975
trigger times: 3
end of epoch 23: val_loss 1.822481738109616e-06, val_acc 1.0
trigger times: 4
end of epoch 24: val_loss 0.0010632328211147525, val_acc 1.0
trigger times: 5
end of epoch 25: val_loss 2.7326116627115482e-05, val_acc 1.0
trigger times: 6
end of epoch 26: val_loss 0.00035886981107260854, val_acc 1.0
trigger times: 7
end of epoch 27: val_loss 2.1013025242311303e-06, val_acc 1.0
trigger times: 8
end of epoch 28: val_loss 3.980215274747678e-05, val_acc 1.0
trigger times: 9
end of epoch 29: val_loss 0.001517988303664417, val_acc 1.0
trigger times: 10
Early stopping.
0 -100.59759867191315 -50.492268601198035
1 -92.76838406920433 -50.03933801517046
2 -139.0730201601982 -46.98011874490918
3 -89.8782684803009 -45.670579884154705
4 -64.9614335000515 -44.14602409201361
5 -93.79551821947098 -38.41270390343083
6 -92.20810198783875 -38.35634328077039
7 -61.15663421154022 -36.821916772458344
8 -88.85164940357208 -35.26282499693737
9 -45.58316087722778 -34.64469044638467
10 -72.25487840175629 -33.84284985953318
11 -73.83918151259422 -31.969099402548657
12 -58.46119813621044 -27.41102349748205
13 -53.94315931200981 -26.7047217556024
14 -35.3780879676342 -24.879106999799365
15 -68.39988270401955 -24.592745144504722
16 -57.33297322690487 -20.656863763892378
17 -22.234142139554024 -20.13839114930498
18 -40.3030871078372 -18.476132715319725
19 -74.11444566398859 -17.55742370467821
20 -37.045149862766266 -17.272648368669138
21 -29.322727754712105 -16.49492982219607
22 -29.213378250598907 -15.352483070393303
23 -33.82034622132778 -15.206600054934952
24 -64.29121571779251 -14.855082803515382
25 -28.732929967343807 -14.773352865013601
26 -31.86282016336918 -14.728068360403778
27 -20.768974348902702 -14.531424598833084
28 -35.88379415869713 -14.479091335668526
29 -44.21328108012676 -14.442420089224363
30 -27.314743921160698 -14.256519300904145
31 -37.58222933858633 -13.529667123029132
32 -38.328395649790764 -13.29183257725024
33 -27.711798697710037 -13.253822242841537
34 -40.694935040548444 -12.999092968462566
35 -19.74646345898509 -12.917321581688867
36 -22.664685115218163 -12.71765016283811
37 -40.2876403182745 -12.68135972540495
38 -38.67349674552679 -12.676389463043602
39 -7.393261596560478 -12.66418205637357
40 -33.32986271381378 -12.560877731741472
41 -42.57783853262663 -12.45932141127743
42 -23.378639549016953 -12.45732712735986
43 -25.13259881734848 -12.151904772081672
44 -29.49717615917325 -12.073410300448248
45 -17.020913816988468 -11.582873868230912
46 -35.08867494761944 -11.479398534537331
47 -31.16935685276985 -11.397068675172687
48 -37.7625884860754 -11.28771952721508
49 -39.85147151350975 -11.235369418265275
50 -22.262874592095613 -11.22962362513982
51 -37.89754620194435 -11.197481975615037
52 -23.38707587122917 -11.099869227839568
53 -39.733145862817764 -10.888497934357732
54 14.175100475549698 -10.829210660670224
55 -40.28917956352234 -10.754962341338413
56 19.844601802527905 -10.745769022183167
57 -23.378289833664894 -10.442661253237253
58 -15.952675625681877 -10.23104388786872
59 -20.80819897353649 -10.075316903648199
60 -17.493377938866615 -9.784704404750451
61 21.50312615185976 -9.619099158896539
62 -2.7263195291161537 -9.539214979973648
63 7.911362566053867 -9.449458829240886
64 -32.7743169516325 -9.261404487844624
65 4.491234607994556 -9.24356358708577
66 -37.62035658955574 -9.053646448851621
67 1.6451921686530113 -8.939605595422433
68 -11.713183432817459 -8.91581608531327
69 -10.839617995545268 -8.900740765348912
70 -7.6635530441999435 -8.682487717368405
71 -0.9587684795260429 -8.51026036188598
72 -13.059110075235367 -8.496065812426837
73 18.299183823168278 -8.431128208967747
74 16.927695274353027 -8.426650819908778
75 10.382100157439709 -8.421678566028149
76 -10.83959124982357 -8.401145461327584
77 -0.4429006353020668 -8.260786902912665
78 10.176293574273586 -8.206133110275566
79 -0.30669573694467545 -8.133195842510668
80 -7.154727891087532 -8.035031385803068
81 3.4974438548088074 -7.785316625079481
82 -17.320430666208267 -7.57539849177145
83 9.553133681416512 -7.2474902917010295
84 10.153460435569286 -7.214983559806713
85 10.174106076359749 -7.21230699268799
86 20.455982990562916 -7.154898836174495
87 25.260860972106457 -7.083478704695206
88 10.696393467485905 -7.032895379946882
89 0.9063939638435841 -6.944649489864398
90 2.615119829773903 -6.931397818536349
91 2.4488689824938774 -6.869903843776713
92 -19.680085375905037 -6.776946485018116
93 9.340534299612045 -6.761591689460641
94 -16.258756577968597 -6.7220638398623045
95 10.713323190808296 -6.5143847926754495
96 20.71555308997631 -6.264227793877518
97 24.10808502137661 -6.118767350664909
98 21.024680614471436 -5.837161601487908
99 13.819126650691032 -5.808154141755308
100 10.4754668623209 -5.652404328871853
101 12.28360290825367 -5.24623053012754
102 14.186902731657028 -5.2140506690820665
103 11.369649194180965 -5.132896421563707
104 -16.942987114191055 -5.027957977402961
105 30.775023385882378 -5.01139413497221
106 21.442995980381966 -4.798843926042663
107 15.422313451766968 -4.760311205741645
108 25.028895184397697 -4.727174943788789
109 19.741270378232002 -4.244373254657557
110 19.71290096640587 -4.2374814192439825
111 21.980427458882332 -4.211271910517213
112 31.37828190624714 -3.8398522344775814
113 21.597382597625256 -3.8378124824710977
114 19.229818016290665 -3.7401188774802345
115 10.61159136891365 -3.7299210750081735
116 29.24775056540966 -3.1091404924546686
117 35.23634822666645 -3.0240266041122754
118 26.39175733923912 -2.3670932068274717
119 32.70102849602699 -1.9954881286263761
train accuracy: 0.9988888888888889
validation accuracy: 1.0
[-50.4922686  -50.03933802 -45.67057988 -44.14602409 -38.4127039
 -31.9690994  -26.70472176 -25.35090221 -25.13182956 -24.879107
 -24.77059952 -24.22401635 -23.66116276 -22.61707333 -22.49196978
 -22.39962705 -22.07392471 -21.2950064  -21.25631337 -21.22640421
 -20.95053973 -20.9318415  -20.89581141 -20.80912832 -20.72775643
 -20.63694914 -20.35395075 -20.13839115 -19.91539867 -19.78795026
 -19.46544136 -19.39514141 -19.32574042 -19.22727138 -19.10871805
 -18.47613272 -18.14616242 -17.5574237  -17.5113341  -17.24680972
 -17.11877166 -16.8879246  -16.49492982 -14.77335287 -14.72806836
 -14.5314246  -14.47909134 -14.44242009 -13.29183258 -13.25382224
 -12.99909297 -12.71765016 -12.68135973 -12.67638946 -12.15190477
 -11.58287387 -11.39706868 -11.09986923 -10.82921066 -10.74576902
 -10.44266125 -10.23104389  -9.79433623  -9.53921498  -9.44945883
  -9.31655682  -9.26140449  -9.1341823   -9.05364645  -8.91581609
  -8.8588328   -8.55097297  -8.51026036  -8.42665082  -8.40114546
  -8.20781611  -8.20632376  -8.20613311  -8.13319584  -7.78531663
  -7.54959848  -7.29996128  -7.18779636  -7.15489884  -7.09717538
  -7.0834787   -6.93139782  -6.86990384  -6.69440942  -6.59578428
  -6.57218733  -6.43367982  -6.14376011  -6.11876735  -5.8371616
  -5.75379231  -5.65240433  -5.50315869  -5.45189113  -5.45174257
  -5.35841304  -5.24751817  -5.21405067  -5.17809597  -5.02795798
  -4.76031121  -4.72717494  -4.43120342  -4.4153911   -4.3892415
  -4.24437325  -4.23748142  -4.21127191  -3.83781248  -3.74011888
  -3.27863595  -3.10914049  -2.49159669  -1.99548813  -1.96812075]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5999e-01,  8.8307e-02, -1.3692e-01, -1.9547e-01,  3.7383e-01,
          4.8763e-01, -2.9052e-02,  6.5272e-02, -2.8382e-03, -3.5893e-01,
          1.9401e-06, -1.2081e+00, -1.5298e+00]], device='cuda:3'))])
end of epoch 1: val_loss 1.8583033024910379e-06, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 1.0728722369179877e-07, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.5700e-01,  5.2103e-01, -9.2389e-02, -3.1072e-01,  7.1294e-01,
          4.3629e-01, -8.6772e-02,  1.3459e-01, -2.5103e-01, -5.0515e-01,
          2.3240e-06, -1.8664e+00, -2.3594e+00]], device='cuda:3'))])
end of epoch 4: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 1.0624012175937463, val_acc 0.965
trigger times: 2
end of epoch 6: val_loss 1.3749131312579267e-06, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 0.0007432262094698672, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.2838e-01,  5.1104e-01, -9.6714e-02, -1.0824e-01,  2.6520e-01,
          2.4409e-01, -4.1252e-02, -2.6058e-02,  8.8529e-05, -1.9875e-01,
          2.9854e-06, -1.1465e+00, -2.3491e+00]], device='cuda:3'))])
end of epoch 9: val_loss 0.00010985387823438941, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.9891e-01,  4.3226e-03, -1.0187e-01, -2.8883e-01,  6.6057e-01,
          6.8839e-01, -1.2010e-01,  7.9045e-02, -2.4607e-01, -5.3360e-01,
          3.3402e-06, -1.5659e+00, -2.8008e+00]], device='cuda:3'))])
end of epoch 12: val_loss 0.0031115489075165216, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 2.9443882340274283e-07, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 2.2114445027909822e-06, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 1.6686732835680118e-06, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 1.4756196660208332e-05, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 1.0227134048044206e-05, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 0.0045732957124658925, val_acc 0.995
trigger times: 7
end of epoch 19: val_loss 3.099431978625944e-08, val_acc 1.0
trigger times: 8
end of epoch 20: val_loss 7.301175288887407e-07, val_acc 1.0
trigger times: 9
end of epoch 21: val_loss 1.3709048971577432e-08, val_acc 1.0
trigger times: 10
Early stopping.
0 -214.05608367919922 -50.492268601198035
1 -207.01428270339966 -50.03933801517046
2 -126.33153438568115 -45.670579884154705
3 -238.79163444042206 -44.14602409201361
4 -149.248539686203 -38.41270390343083
5 -63.563686430454254 -31.969099402548657
6 -130.47197172045708 -26.7047217556024
7 -339.69488310813904 -25.350902208440505
8 -331.7968499660492 -25.13182956266665
9 -79.04914990067482 -24.879106999799365
10 -338.8252305984497 -24.770599519856063
11 -327.92302656173706 -24.224016348984055
12 -320.9905352592468 -23.661162760317627
13 -321.06279134750366 -22.617073327896666
14 -302.9963183403015 -22.49196977682103
15 -285.8484697341919 -22.39962704733981
16 -299.02994775772095 -22.07392470750057
17 -301.76454305648804 -21.295006395997163
18 -301.92682003974915 -21.25631337227572
19 -293.09396529197693 -21.226404209140988
20 -297.5152897834778 -20.950539728529453
21 -288.15543723106384 -20.931841499851824
22 -278.6963129043579 -20.89581140780471
23 -287.7068500518799 -20.80912832181743
24 -262.534245967865 -20.727756429336704
25 -285.86757040023804 -20.63694913573254
26 -295.6811909675598 -20.353950747516674
27 -38.66146546602249 -20.13839114930498
28 -286.7147915363312 -19.91539867243913
29 -279.50568532943726 -19.787950261286458
30 -259.8906090259552 -19.465441359246128
31 -274.55779695510864 -19.39514140618821
32 -279.34392046928406 -19.32574042159682
33 -247.8765549659729 -19.227271380776806
34 -274.84741473197937 -19.1087180520384
35 -73.91082808375359 -18.476132715319725
36 -262.115624666214 -18.146162421657106
37 -102.75829267501831 -17.55742370467821
38 -245.17315196990967 -17.511334101347448
39 -224.54448318481445 -17.246809723734373
40 -249.5957736968994 -17.118771657998455
41 -183.61346608400345 -16.887924603035206
42 -60.40496081113815 -16.49492982219607
43 -50.61658617854118 -14.773352865013601
44 -58.73100474476814 -14.728068360403778
45 -29.786495879292488 -14.531424598833084
46 -64.17773765325546 -14.479091335668526
47 -50.89910188317299 -14.442420089224363
48 -5.891639336943626 -13.29183257725024
49 -53.73873193562031 -13.253822242841537
50 3.360823929309845 -12.999092968462566
51 -69.76796561479568 -12.71765016283811
52 -68.92151106894016 -12.68135972540495
53 -3.6469513177871704 -12.676389463043602
54 -71.03725206851959 -12.151904772081672
55 -55.136722058057785 -11.582873868230912
56 -67.02456504106522 -11.397068675172687
57 -57.954066306352615 -11.099869227839568
58 -17.723474100232124 -10.829210660670224
59 -14.500645771622658 -10.745769022183167
60 -53.685801953077316 -10.442661253237253
61 -51.30992618203163 -10.23104388786872
62 7.704354360699654 -9.794336232384618
63 -4.262283802032471 -9.539214979973648
64 -1.2086576893925667 -9.449458829240886
65 -3.5547299534082413 -9.316556816066724
66 1.7731655836105347 -9.261404487844624
67 10.870044931769371 -9.134182295078087
68 2.4867440685629845 -9.053646448851621
69 -59.14438906311989 -8.91581608531327
70 -13.331096276640892 -8.858832795908487
71 -11.95816458761692 -8.550972968227613
72 -2.9681658297777176 -8.51026036188598
73 -6.927206017076969 -8.426650819908778
74 -55.49793839454651 -8.401145461327584
75 12.85607099533081 -8.20781611161077
76 2.4230591505765915 -8.206323759642153
77 -2.581080451607704 -8.206133110275566
78 -10.966424614191055 -8.133195842510668
79 -1.9792637899518013 -7.785316625079481
80 15.562914982438087 -7.54959847792525
81 16.584113225340843 -7.299961277522449
82 13.920674592256546 -7.187796364507886
83 -1.956252783536911 -7.154898836174495
84 2.091981329023838 -7.097175375551228
85 3.356123775243759 -7.083478704695206
86 -5.972660519182682 -6.931397818536349
87 -0.47139880806207657 -6.869903843776713
88 12.451295427978039 -6.694409422426024
89 13.127317100763321 -6.595784282291193
90 17.48141784965992 -6.572187334653257
91 18.435568004846573 -6.433679822070972
92 15.672193735837936 -6.143760106964395
93 1.569483608007431 -6.118767350664909
94 14.3601014316082 -5.837161601487908
95 17.601990967988968 -5.753792306667663
96 6.8616605550050735 -5.652404328871853
97 19.606140688061714 -5.503158691576936
98 21.08139905333519 -5.451891134830988
99 21.498233884572983 -5.451742569527069
100 4.777039781212807 -5.358413037367681
101 20.720318019390106 -5.24751817255558
102 12.28465811908245 -5.2140506690820665
103 23.13473677635193 -5.178095967720668
104 -42.92107492685318 -5.027957977402961
105 11.270611822605133 -4.760311205741645
106 13.778090476989746 -4.727174943788789
107 13.010541439056396 -4.431203417875307
108 21.645879328250885 -4.415391098183372
109 16.70455041527748 -4.389241503465346
110 13.94294847548008 -4.244373254657557
111 17.252123445272446 -4.2374814192439825
112 16.16292142868042 -4.211271910517213
113 13.590918689966202 -3.8378124824710977
114 16.050355792045593 -3.7401188774802345
115 25.419275730848312 -3.2786359542823234
116 14.72202843427658 -3.1091404924546686
117 25.80622524023056 -2.4915966948269075
118 25.31867742538452 -1.9954881286263761
119 30.54382935166359 -1.9681207484404744
train accuracy: 0.9994444444444445
validation accuracy: 1.0
[-50.4922686  -38.4127039  -31.9690994  -25.35090221 -24.879107
 -23.66116276 -22.61707333 -22.39962705 -20.9318415  -20.89581141
 -20.35395075 -20.13839115 -19.91539867 -19.39514141 -19.34274112
 -19.22727138 -19.10871805 -18.75843307 -18.47613272 -18.40889954
 -18.27358232 -17.90868001 -17.5574237  -17.5113341  -17.45772452
 -17.40987579 -17.40237128 -16.79999467 -16.49492982 -16.46084616
 -15.97072117 -15.46849349 -15.38765232 -15.15362236 -14.77335287
 -14.68911824 -14.54645788 -14.44242009 -14.32245704 -14.29358462
 -14.02855896 -13.96141593 -13.67943382 -13.49095627 -13.40880918
 -13.25382224 -13.13369405 -12.99909297 -12.72788302 -12.68135973
 -12.67638946 -12.54374282 -12.39952248 -12.17391336 -12.01527717
 -11.82373885 -11.72382345 -11.58287387 -10.83252736 -10.82921066
 -10.69638705 -10.44266125  -9.79433623  -9.44945883  -9.35786068
  -9.31655682  -9.18445304  -9.1341823   -9.00661149  -8.8588328
  -8.51026036  -8.47312507  -8.40114546  -8.20781611  -8.20691014
  -8.19239925  -8.13319584  -7.62447409  -7.47144063  -7.23387618
  -7.18779636  -7.11465578  -7.09717538  -7.05838164  -7.05496377
  -7.02279879  -7.0102418   -6.9601919   -6.93139782  -6.86990384
  -6.69440942  -6.57218733  -6.43367982  -6.31362049  -6.00130935
  -5.84815775  -5.8371616   -5.83174813  -5.50315869  -5.45189113
  -5.45174257  -5.35841304  -5.32997155  -5.24751817  -5.17809597
  -5.13460895  -5.08879519  -4.98256425  -4.87179369  -4.76031121
  -4.72717494  -4.57728317  -4.3892415   -4.24437325  -4.21127191
  -3.99381447  -3.74011888  -3.10914049  -2.2787824   -1.96812075]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 5.32762856643032e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.2186,  0.2835, -0.1773, -0.0833,  0.0831, -0.0682, -0.0446,  0.0350,
         -0.0024,  0.0738, -0.0027, -1.5699, -1.0604]], device='cuda:0'))])
end of epoch 1: val_loss 0.000771405942445682, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 6.442395338552842e-05, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 6.041078501350938e-05, val_acc 1.0
trigger times: 3
end of epoch 4: val_loss 3.886486360649144e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.7521e-01,  5.7133e-03, -1.6099e-01, -2.7886e-01,  9.5195e-01,
         -3.6480e-02, -1.0158e-01,  1.0904e-01, -2.6211e-01,  1.9209e-01,
          4.8893e-04, -1.4390e+00, -1.1754e+00]], device='cuda:0'))])
end of epoch 5: val_loss 8.099734527888813e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1514e-01,  1.0581e-04, -4.2286e-06, -3.6494e-01,  5.6203e-01,
          6.9575e-02, -6.5377e-02,  1.6434e-01,  7.1278e-05, -4.4487e-05,
         -6.9147e-04, -1.6450e+00, -1.7602e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.00010613685850433541, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 1.0624442268145628e-05, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 7.61144707368544e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.2659e-05, -2.5109e-05,  3.6876e-02, -1.8189e-01,  2.4533e-01,
          1.5488e-05, -2.8133e-02,  6.5702e-02,  4.4576e-06,  7.3559e-05,
          1.4935e-03, -1.3270e+00, -1.8174e+00]], device='cuda:0'))])
end of epoch 9: val_loss 7.813700904080179e-07, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 5.061110826964921e-06, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.000561430630860933, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 7.990150895089699e-06, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 5.357262436135102e-05, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 0.002562284668251635, val_acc 1.0
trigger times: 6
end of epoch 15: val_loss 7.247905794191211e-05, val_acc 1.0
trigger times: 7
end of epoch 16: val_loss 9.031001103164015e-05, val_acc 1.0
trigger times: 8
end of epoch 17: val_loss 0.008215698064164983, val_acc 0.995
trigger times: 9
end of epoch 18: val_loss 8.444580893439024e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -71.82841783761978 -50.492268601198035
1 -66.73055976629257 -38.41270390343083
2 -64.78767308592796 -31.969099402548657
3 -31.618370912969112 -25.350902208440505
4 -39.989184841513634 -24.879106999799365
5 -34.132519491016865 -23.661162760317627
6 -31.41118758916855 -22.617073327896666
7 -32.563292652368546 -22.39962704733981
8 -28.751424752175808 -20.931841499851824
9 -29.815255783498287 -20.89581140780471
10 -27.290532186627388 -20.353950747516674
11 -33.41855841130018 -20.13839114930498
12 -26.918802671134472 -19.91539867243913
13 -27.680052548646927 -19.39514140618821
14 -23.25107631087303 -19.342741122672273
15 -29.096382699906826 -19.227271380776806
16 -26.60479871928692 -19.1087180520384
17 -22.94307392835617 -18.758433074586584
18 -25.616448253393173 -18.476132715319725
19 -21.262803822755814 -18.408899542335767
20 -24.12406188249588 -18.27358231919303
21 -20.920030266046524 -17.908680006646982
22 -35.43831969052553 -17.55742370467821
23 -25.67909975349903 -17.511334101347448
24 -20.316619843244553 -17.45772452446851
25 -19.944764390587807 -17.409875792172233
26 -22.598003163933754 -17.402371282531398
27 -19.53228561580181 -16.799994666335202
28 -22.725934624671936 -16.49492982219607
29 -19.838255614042282 -16.46084615657533
30 -20.944627597928047 -15.97072116659416
31 -19.642796203494072 -15.46849348575115
32 -19.240599527955055 -15.387652321423325
33 -19.372152537107468 -15.153622359725011
34 -18.99030166864395 -14.773352865013601
35 -17.979886069893837 -14.689118242709847
36 -18.52734950184822 -14.546457875995491
37 -40.165590703487396 -14.442420089224363
38 -16.550900906324387 -14.32245704426185
39 -15.330433011054993 -14.293584624803232
40 -16.185926392674446 -14.028558957875923
41 -15.630992241203785 -13.961415929531919
42 -16.015088342130184 -13.679433821359366
43 -14.961062721908092 -13.490956265660559
44 -14.422177195549011 -13.408809184568147
45 -17.81253193318844 -13.253822242841537
46 -13.616870120167732 -13.133694046337084
47 -27.763341069221497 -12.999092968462566
48 -14.034389160573483 -12.727883023753359
49 -29.256330773234367 -12.68135972540495
50 -27.28953291475773 -12.676389463043602
51 -15.700382344424725 -12.543742819812733
52 -15.021873049438 -12.399522475481872
53 -14.302284449338913 -12.17391336371053
54 -13.918814845383167 -12.015277165656068
55 -13.654384359717369 -11.823738854838806
56 -15.232106627896428 -11.723823452199369
57 -16.4409808665514 -11.582873868230912
58 -11.138131596148014 -10.832527357494007
59 -17.674651131033897 -10.829210660670224
60 -11.81558272242546 -10.696387049851678
61 -14.410926461219788 -10.442661253237253
62 -14.188077747821808 -9.794336232384618
63 -11.675173614174128 -9.449458829240886
64 -9.505415391176939 -9.357860677862147
65 -14.799870043992996 -9.316556816066724
66 -10.647094340994954 -9.18445303723539
67 -12.257891476154327 -9.134182295078087
68 -9.934540521353483 -9.006611493267414
69 -16.64632822573185 -8.858832795908487
70 -12.0761743709445 -8.51026036188598
71 -8.96835071220994 -8.473125071444349
72 -14.50797963514924 -8.401145461327584
73 -11.05854082852602 -8.20781611161077
74 -11.622248008847237 -8.206910135410356
75 -10.430437054485083 -8.192399245551151
76 -11.674852900207043 -8.133195842510668
77 -7.667185060679913 -7.624474088428272
78 -6.765415463596582 -7.471440626236785
79 -7.881691757589579 -7.233876175268499
80 -10.849171094596386 -7.187796364507886
81 -8.397769335657358 -7.114655779735902
82 -11.713971003890038 -7.097175375551228
83 -6.52721556648612 -7.058381644615669
84 -6.173469176515937 -7.054963770662531
85 -6.934202540665865 -7.022798792436748
86 -9.563909448683262 -7.0102418007094185
87 -8.710776418447495 -6.960191900671775
88 -15.078032329678535 -6.931397818536349
89 -11.127489138394594 -6.869903843776713
90 -10.573216989636421 -6.694409422426024
91 -8.921309657394886 -6.572187334653257
92 -7.425617203116417 -6.433679822070972
93 -6.694755984470248 -6.313620488611284
94 -7.14505810290575 -6.001309351275611
95 -5.396661076694727 -5.84815775358568
96 -5.39229142665863 -5.837161601487908
97 -6.51252156496048 -5.831748127633224
98 -7.902796693146229 -5.503158691576936
99 -8.567883532494307 -5.451891134830988
100 -6.919039640575647 -5.451742569527069
101 -10.002188481390476 -5.358413037367681
102 -4.750075697898865 -5.329971554560731
103 -7.165629090741277 -5.24751817255558
104 -5.385898506268859 -5.178095967720668
105 -4.663391647860408 -5.1346089543625
106 -5.385359134525061 -5.0887951887927585
107 -4.580343447625637 -4.982564251611441
108 -2.7781434394419193 -4.87179368871319
109 -3.6892895847558975 -4.760311205741645
110 -9.216659404337406 -4.727174943788789
111 -3.684467114508152 -4.577283167314312
112 -6.437219988554716 -4.389241503465346
113 -10.046368695795536 -4.244373254657557
114 -2.3892231956124306 -4.211271910517213
115 -3.2952516339719296 -3.9938144749988913
116 -1.8400797098875046 -3.7401188774802345
117 -6.861654898151755 -3.1091404924546686
118 -1.4808779619634151 -2.2787824002917763
119 -1.9882420264184475 -1.9681207484404744
train accuracy: 1.0
validation accuracy: 1.0
[-38.4127039  -25.35090221 -22.39962705 -20.89581141 -20.35395075
 -20.13839115 -19.39514141 -19.22727138 -18.75843307 -18.47613272
 -18.40889954 -18.27358232 -17.90868001 -17.5574237  -17.40237128
 -16.49492982 -15.02794537 -14.68911824 -14.49949006 -14.32245704
 -14.29358462 -14.02855896 -13.76328454 -13.67943382 -13.49095627
 -13.40880918 -13.25382224 -12.7877739  -12.68135973 -12.17391336
 -12.01527717 -11.7702958  -11.72382345 -11.48912124 -11.34578549
 -11.28928524 -11.21533751 -11.21343557 -11.17803327 -10.82921066
 -10.64468636 -10.60198827 -10.25197044 -10.2243589   -9.87553376
  -9.85480422  -9.82048819  -9.79433623  -9.78847496  -9.78779426
  -9.78314436  -9.60892896  -9.58367133  -9.53620341  -9.44945883
  -9.20447344  -9.13342885  -9.00661149  -8.83935705  -8.64312257
  -8.47312507  -8.40114546  -8.24087046  -8.20691014  -8.13319584
  -7.91396411  -7.65509408  -7.56724038  -7.36267992  -7.23942945
  -7.23387618  -7.18779636  -7.11465578  -7.05838164  -7.05428029
  -7.05204072  -7.02279879  -6.86990384  -6.57218733  -6.48600243
  -6.47707947  -6.43367982  -6.39138621  -6.3875015   -6.30251189
  -6.13015194  -5.84815775  -5.8371616   -5.83174813  -5.66982307
  -5.64982021  -5.53410709  -5.50315869  -5.45174257  -5.32997155
  -5.29149321  -5.24751817  -5.08879519  -5.051785    -5.04395847
  -5.02495214  -4.98256425  -4.96332094  -4.87179369  -4.77372323
  -4.75382908  -4.72717494  -4.62157817  -4.60830123  -4.57728317
  -4.56579695  -4.50667512  -4.3892415   -4.24437325  -4.21127191
  -3.9947703   -3.24444064  -2.91654369  -2.2787824   -1.82978328]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.19763219513403768, val_acc 0.965
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.9484e-02,  3.5533e-01,  1.9227e-01, -2.9367e-01,  3.0769e-01,
         -6.5563e-02, -1.8025e-01, -5.3853e-02, -1.9387e-01,  1.4405e-01,
          1.0903e-03, -1.6120e+00, -7.8834e-01]], device='cuda:2'))])
end of epoch 1: val_loss 0.010181943520769003, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.9811e-02,  1.9310e-01,  1.4747e-01, -8.8142e-02,  4.1391e-02,
         -6.4204e-05, -1.5577e-01,  6.0382e-02,  4.5695e-04,  7.4528e-04,
          5.9376e-04, -1.2890e+00, -4.9376e-01]], device='cuda:2'))])
end of epoch 2: val_loss 0.00016222542192195988, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.5539e-05,  3.0557e-01,  6.4680e-02, -1.1713e-01,  2.3803e-03,
          6.4580e-05, -9.6334e-02,  2.3621e-02,  4.6625e-05,  7.3198e-02,
         -2.4884e-03, -1.5538e+00, -7.9600e-01]], device='cuda:2'))])
end of epoch 3: val_loss 0.0007786657753276671, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 2.920698316742687e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5936e-05,  3.0861e-01, -1.4266e-05, -1.2879e-01,  2.0006e-01,
         -3.3996e-02, -9.6705e-02,  3.6226e-02,  8.9808e-05,  1.9448e-01,
          1.9610e-03, -1.7933e+00, -9.9841e-01]], device='cuda:2'))])
end of epoch 5: val_loss 0.00029467134560938745, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.013136079142574565, val_acc 0.995
trigger times: 2
end of epoch 7: val_loss 0.00816097873468074, val_acc 0.995
trigger times: 3
end of epoch 8: val_loss 2.6205113738555497e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0604e-01,  3.5687e-01,  2.1165e-01, -1.7981e-01,  2.5883e-01,
         -1.8031e-06, -2.2134e-01,  1.3224e-01, -1.7591e-02,  2.0367e-01,
         -7.0813e-04, -2.1444e+00, -1.2864e+00]], device='cuda:2'))])
end of epoch 9: val_loss 0.00016103914216905935, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 6.690991308481386e-05, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 0.0005197170476241908, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 1.7106381143605632e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5075e-02,  3.6957e-01,  1.5790e-01, -1.8931e-01,  4.2054e-01,
         -1.8559e-01, -1.1115e-01,  3.6738e-04, -2.3784e-01,  4.2583e-01,
          5.1444e-04, -2.1926e+00, -1.1992e+00]], device='cuda:2'))])
end of epoch 13: val_loss 9.17577187947316e-05, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 4.523922662968971e-07, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 0.00043500187850927576, val_acc 1.0
trigger times: 3
end of epoch 16: val_loss 0.0003161323674239114, val_acc 1.0
trigger times: 4
end of epoch 17: val_loss 0.00248683480780052, val_acc 1.0
trigger times: 5
end of epoch 18: val_loss 0.040566436320820076, val_acc 0.985
trigger times: 6
end of epoch 19: val_loss 0.00041402980442587276, val_acc 1.0
trigger times: 7
end of epoch 20: val_loss 0.0003547422304483305, val_acc 1.0
trigger times: 8
end of epoch 21: val_loss 0.006958541233879778, val_acc 0.995
trigger times: 9
end of epoch 22: val_loss 6.104323347031482e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -72.73411332070827 -38.41270390343083
1 -319.14936661720276 -25.350902208440505
2 -265.614639878273 -22.39962704733981
3 -266.4472874403 -20.89581140780471
4 -280.25011467933655 -20.353950747516674
5 -8.426549345254898 -20.13839114930498
6 -259.4324948787689 -19.39514140618821
7 -235.20694661140442 -19.227271380776806
8 -24.299736708402634 -18.758433074586584
9 -43.69549182057381 -18.476132715319725
10 -21.501813277602196 -18.408899542335767
11 -23.16667575761676 -18.27358231919303
12 -20.796708837151527 -17.908680006646982
13 -46.61013346910477 -17.55742370467821
14 -23.28631414845586 -17.402371282531398
15 -33.494587779045105 -16.49492982219607
16 -15.904755204916 -15.027945370532887
17 -13.425865095108747 -14.689118242709847
18 -15.62895230948925 -14.499490061574292
19 -14.796611189842224 -14.32245704426185
20 -12.796196673065424 -14.293584624803232
21 -9.608385190367699 -14.028558957875923
22 -10.972694624215364 -13.76328453715686
23 -13.801427762955427 -13.679433821359366
24 -9.942110650241375 -13.490956265660559
25 -9.10033880546689 -13.408809184568147
26 -33.73359724879265 -13.253822242841537
27 -10.575646445155144 -12.787773900450743
28 -14.627982795238495 -12.68135972540495
29 -9.667866948992014 -12.17391336371053
30 -11.053005676716566 -12.015277165656068
31 -14.284486316144466 -11.770295795047181
32 -13.47296529263258 -11.723823452199369
33 -13.742398468777537 -11.48912124268623
34 -17.686409143730998 -11.345785489951194
35 -1.152903713285923 -11.289285238495548
36 -13.129303593188524 -11.215337508507362
37 -12.823223978281021 -11.213435573115255
38 -3.946238648146391 -11.178033270668683
39 -13.270819716155529 -10.829210660670224
40 -15.378900211304426 -10.644686356401598
41 -15.287246122956276 -10.6019882673841
42 -12.239934276789427 -10.251970442760564
43 -11.903438645415008 -10.22435890245436
44 -10.525108218193054 -9.875533757238733
45 -15.94790256023407 -9.854804218184787
46 -7.550498202443123 -9.820488190086602
47 -3.9231321960687637 -9.794336232384618
48 -12.18917040526867 -9.7884749581491
49 -12.030700132250786 -9.787794258744656
50 -12.907698780298233 -9.78314435631086
51 -7.8068582862615585 -9.608928963456068
52 -10.1030418202281 -9.583671330346798
53 -7.640112429857254 -9.536203405859297
54 -13.71404630690813 -9.449458829240886
55 -14.066579842939973 -9.204473442143614
56 -12.058729022741318 -9.13342885003115
57 -3.7375084087252617 -9.006611493267414
58 -0.19153642654418945 -8.839357053510009
59 -12.94925582781434 -8.643122566746982
60 -0.6460361983627081 -8.473125071444349
61 -38.30780130624771 -8.401145461327584
62 -6.115298941731453 -8.24087045763068
63 -6.563506796956062 -8.206910135410356
64 7.830312751233578 -8.133195842510668
65 2.27605502307415 -7.9139641094046205
66 -10.295580878853798 -7.655094079301198
67 -2.602242289111018 -7.567240376251699
68 -1.3144431877881289 -7.362679919537333
69 2.177547812461853 -7.239429448903694
70 6.420906897634268 -7.233876175268499
71 0.6255565546452999 -7.187796364507886
72 1.8532859794795513 -7.114655779735902
73 4.382516343146563 -7.058381644615669
74 -6.129098862409592 -7.054280293513242
75 5.002106450498104 -7.052040724969831
76 1.3072481025010347 -7.022798792436748
77 -7.15499447286129 -6.869903843776713
78 4.193853624165058 -6.572187334653257
79 -5.177592094987631 -6.4860024287056826
80 4.603146094828844 -6.477079472828306
81 6.745112977921963 -6.433679822070972
82 7.53864199668169 -6.391386208696807
83 -2.34248199313879 -6.3875015003913616
84 7.962481513619423 -6.302511886546671
85 -0.6373671032488346 -6.13015194443597
86 6.2233697436749935 -5.84815775358568
87 2.978533498942852 -5.837161601487908
88 1.5471315905451775 -5.831748127633224
89 6.4513858035206795 -5.669823073149505
90 -2.2900682911276817 -5.649820212978581
91 -2.9022879730910063 -5.5341070899159375
92 5.201065070927143 -5.503158691576936
93 7.612496258690953 -5.451742569527069
94 7.9812497310340405 -5.329971554560731
95 5.254459910094738 -5.291493214198513
96 6.127713292837143 -5.24751817255558
97 8.202774934470654 -5.0887951887927585
98 9.353501990437508 -5.051785003331792
99 10.726153373718262 -5.043958469972898
100 2.547863382846117 -5.024952144435201
101 6.911454036831856 -4.982564251611441
102 13.338612206280231 -4.963320939711657
103 7.715036764740944 -4.87179368871319
104 3.157780919224024 -4.773723228462891
105 12.584765505045652 -4.753829082494487
106 6.960600599646568 -4.727174943788789
107 0.7898719124495983 -4.621578174919492
108 8.003971353173256 -4.608301234275807
109 8.115030001848936 -4.577283167314312
110 6.401351828128099 -4.565796954288057
111 9.24386390298605 -4.506675119159872
112 12.958784513175488 -4.389241503465346
113 6.918222852051258 -4.244373254657557
114 7.0693037286400795 -4.211271910517213
115 7.855314962565899 -3.994770298137972
116 14.616413965821266 -3.2444406423884398
117 17.400796808302402 -2.9165436931393454
118 15.595910884439945 -2.2787824002917763
119 17.362842068076134 -1.8297832760197443
train accuracy: 1.0
validation accuracy: 1.0
[-76.54658558 -75.24051864 -75.02405605 -74.73689122 -74.31688105
 -73.87998289 -73.76794491 -73.63146338 -73.61592963 -73.52153234
 -73.51445129 -73.50871325 -73.40665393 -73.16941023 -72.91786027
 -72.84891808 -72.79553255 -72.72941992 -72.69459367 -72.65029596
 -72.55586222 -72.47377415 -72.12510636 -72.01728896 -71.78059471
 -71.64135604 -71.48395509 -71.31460267 -71.1662043  -71.00694387
 -70.96747476 -70.96710401 -70.86988012 -70.80913028 -70.54823285
 -70.4903145  -70.45548942 -70.39400966 -70.35617596 -70.35503835
 -70.27076721 -70.16508414 -70.08681519 -70.01472322 -69.93406483
 -69.79716684 -69.72202435 -69.72061496 -69.40889544 -69.35260951
 -69.08192219 -68.86978463 -68.83967582 -68.81143815 -68.6894233
 -68.56166774 -68.39807159 -68.08824648 -67.6915966  -66.94308598
 -38.4127039  -25.35090221 -20.13839115 -19.39514141 -19.22727138
 -18.75843307 -18.47613272 -17.90868001 -17.40237128 -15.02794537
 -12.7877739  -12.68135973 -12.17391336 -12.01527717 -11.7702958
 -11.34578549 -11.21343557 -11.17803327 -10.82921066 -10.64468636
 -10.2243589   -9.82048819  -9.78847496  -9.13342885  -9.00661149
  -8.83935705  -8.40114546  -8.24087046  -8.20691014  -7.65509408
  -7.56724038  -7.23942945  -7.23387618  -7.18779636  -7.11465578
  -7.05838164  -6.86990384  -6.48600243  -6.47707947  -6.43367982
  -6.39138621  -6.13015194  -5.84815775  -5.8371616   -5.50315869
  -5.45174257  -5.32997155  -5.29149321  -5.051785    -4.98256425
  -4.77372323  -4.75382908  -4.62157817  -4.60830123  -4.57728317
  -4.56579695  -4.50667512  -4.24437325  -3.9947703   -1.82978328]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2725e-01,  2.8418e-01, -1.1248e-06, -2.6183e-06,  1.6309e-01,
          6.8444e-05,  7.3283e-02,  5.8932e-02,  4.9876e-05,  1.0439e-01,
         -8.9479e-06, -1.9470e+00, -1.3363e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.1997e-05, -8.1780e-05,  2.1300e-05, -9.7939e-07, -1.6958e-04,
          1.8073e-04,  7.1187e-02, -1.3630e-05,  1.6690e-04,  3.1048e-06,
         -7.6547e-04, -7.3633e-01, -1.1880e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0197e-04, -1.9412e-04,  1.1124e-04,  1.5520e-05,  1.5195e-04,
          4.3867e-04,  6.5874e-02, -3.3228e-05,  4.2433e-04, -4.7018e-04,
          5.2274e-04, -1.6209e-04, -8.1226e-01]], device='cuda:1'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.4790e-04, -4.6132e-04,  3.1666e-04, -1.7802e-04,  1.0728e-03,
          9.5102e-04,  5.2741e-02, -8.1100e-05,  8.4931e-04, -9.6148e-04,
         -1.0909e-04, -1.5301e-04, -1.7584e-04]], device='cuda:1'))])
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0006, -0.0010,  0.0008, -0.0003, -0.0016,  0.0015,  0.0204, -0.0002,
          0.0012, -0.0017, -0.0009, -0.0002, -0.0005]], device='cuda:1'))])
end of epoch 5: val_loss 3.1185100004904598e-06, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 2.4700132934185604e-06, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 2.815123178834256e-06, val_acc 1.0
trigger times: 3
end of epoch 8: val_loss 3.0016851860636963e-06, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 1.7881392366803084e-08, val_acc 1.0
trigger times: 5
end of epoch 10: val_loss 2.8413492401568876e-06, val_acc 1.0
trigger times: 6
end of epoch 11: val_loss 3.749131455776933e-07, val_acc 1.0
trigger times: 7
end of epoch 12: val_loss 5.447253203101354e-06, val_acc 1.0
trigger times: 8
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1669e-03, -1.4541e-04, -4.0009e-04,  1.3496e-03, -1.1069e-03,
         -1.2834e-03,  2.0759e-01, -5.2070e-05, -1.4259e-04,  2.8240e-03,
         -1.1518e-03, -8.3190e-04,  5.4439e-03]], device='cuda:1'))])
end of epoch 14: val_loss 3.858796955000798e-06, val_acc 1.0
trigger times: 1
end of epoch 15: val_loss 3.7580655646252125e-06, val_acc 1.0
trigger times: 2
end of epoch 16: val_loss 4.265894904165179e-06, val_acc 1.0
trigger times: 3
end of epoch 17: val_loss 2.3102731626067907e-06, val_acc 1.0
trigger times: 4
end of epoch 18: val_loss 1.065730459401948e-06, val_acc 1.0
trigger times: 5
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.6123e-04, -2.7694e-04,  7.2270e-05, -4.7665e-04,  1.6759e-05,
         -9.8614e-04,  2.5804e-02,  7.5590e-04, -1.7135e-03,  1.7612e-03,
          5.2409e-04,  3.5461e-04, -3.3478e-03]], device='cuda:1'))])
end of epoch 20: val_loss 2.1028495359587395e-06, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 9.185071303363657e-07, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 4.412521780068346e-06, val_acc 1.0
trigger times: 3
end of epoch 23: val_loss 2.8890327433828132e-06, val_acc 1.0
trigger times: 4
end of epoch 24: val_loss 1.9007902483281214e-06, val_acc 1.0
trigger times: 5
end of epoch 25: val_loss 2.575513271381169e-06, val_acc 1.0
trigger times: 6
end of epoch 26: val_loss 2.5445189311312788e-06, val_acc 1.0
trigger times: 7
end of epoch 27: val_loss 2.062318512230377e-06, val_acc 1.0
trigger times: 8
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.5088e-03, -3.7137e-05, -1.3609e-03, -7.3042e-04, -4.3717e-04,
         -9.4267e-04,  6.7237e-02,  1.0104e-03, -7.7635e-04,  1.2087e-03,
          1.9720e-03,  7.7446e-05,  1.4955e-03]], device='cuda:1'))])
end of epoch 29: val_loss 5.972383571872797e-07, val_acc 1.0
trigger times: 1
end of epoch 30: val_loss 2.2464964354185214e-06, val_acc 1.0
trigger times: 2
end of epoch 31: val_loss 4.6121965647216715e-06, val_acc 1.0
trigger times: 3
end of epoch 32: val_loss 4.190205633847199e-07, val_acc 1.0
trigger times: 4
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.5594e-04,  7.3858e-04,  1.6079e-04, -2.9365e-03,  1.9236e-03,
          5.7129e-04,  2.1893e-01, -1.2144e-03,  1.1546e-03,  7.0805e-04,
          6.0646e-04, -1.5224e-03,  2.4072e-04]], device='cuda:1'))])
end of epoch 34: val_loss 2.4676291161540576e-06, val_acc 1.0
trigger times: 1
end of epoch 35: val_loss 1.9341687783480667e-06, val_acc 1.0
trigger times: 2
end of epoch 36: val_loss 4.056683599173993e-06, val_acc 1.0
trigger times: 3
end of epoch 37: val_loss 5.536658603659816e-06, val_acc 1.0
trigger times: 4
end of epoch 38: val_loss 4.389872103729431e-06, val_acc 1.0
trigger times: 5
end of epoch 39: val_loss 3.137583298098434e-06, val_acc 1.0
trigger times: 6
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0011,  0.0025,  0.0018,  0.0006,  0.0012,  0.0030,  0.0307, -0.0008,
         -0.0013,  0.0004, -0.0003,  0.0008, -0.0006]], device='cuda:1'))])
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0014, -0.0006, -0.0004,  0.0017, -0.0019,  0.0002,  0.1727,  0.0008,
         -0.0008,  0.0016,  0.0011, -0.0014,  0.0004]], device='cuda:1'))])
end of epoch 42: val_loss 3.1793065215879324e-06, val_acc 1.0
trigger times: 1
end of epoch 43: val_loss 3.3950745523725343e-06, val_acc 1.0
trigger times: 2
end of epoch 44: val_loss 3.2335463697563682e-06, val_acc 1.0
trigger times: 3
end of epoch 45: val_loss 1.6617760491044465e-06, val_acc 1.0
trigger times: 4
end of epoch 46: val_loss 3.285402416963734e-06, val_acc 1.0
trigger times: 5
end of epoch 47: val_loss 8.881050994204998e-06, val_acc 1.0
trigger times: 6
end of epoch 48: val_loss 2.2875394665788918e-05, val_acc 1.0
trigger times: 7
end of epoch 49: val_loss 2.082583981746211e-06, val_acc 1.0
trigger times: 8
end of epoch 50: val_loss 1.971719588595988e-06, val_acc 1.0
trigger times: 9
end of epoch 51: val_loss 2.616640363157785e-06, val_acc 1.0
trigger times: 10
Early stopping.
0 -12.802068457007408 -76.54658558443121
1 -12.836070720106363 -75.24051863723625
2 -12.937353828456253 -75.02405604955553
3 -12.925715333316475 -74.7368912177618
4 -12.94684130884707 -74.31688105415357
5 -12.884202959015965 -73.87998288781841
6 -12.57517149252817 -73.7679449087589
7 -12.817558098118752 -73.63146338446498
8 -13.009788633091375 -73.61592962697809
9 -12.980917019071057 -73.52153234174574
10 -12.907149421749637 -73.51445128612797
11 -12.9732206503395 -73.50871325317863
12 -13.00490252673626 -73.40665392878776
13 -12.88703328371048 -73.16941023061624
14 -12.848926327889785 -72.91786026527573
15 -13.034580866107717 -72.84891808330663
16 -12.904562745708972 -72.79553254951952
17 -13.021586341550574 -72.72941991519761
18 -13.01013124641031 -72.69459366542144
19 -12.949722438817844 -72.65029595697325
20 -12.832655614009127 -72.55586222239299
21 -12.934519425267354 -72.4737741514389
22 -12.84002535091713 -72.12510635508214
23 -12.908572304993868 -72.0172889588047
24 -12.860086201224476 -71.78059470596456
25 -12.907634010072798 -71.64135603590506
26 -12.938863866496831 -71.48395509096926
27 -12.763123894808814 -71.31460267118413
28 -13.049661061028019 -71.16620430419536
29 -13.036168734543025 -71.00694386525308
30 -12.973191599361598 -70.96747475946309
31 -12.68734347820282 -70.96710401119967
32 -12.926280161365867 -70.86988012143992
33 -12.831552813062444 -70.80913027738585
34 -12.494610339868814 -70.54823284918166
35 -12.823832149850205 -70.49031449785608
36 -12.91715851193294 -70.4554894249313
37 -12.897938465699553 -70.39400966080062
38 -12.845972246490419 -70.35617596377216
39 -12.961818750249222 -70.35503835287626
40 -12.857010799460113 -70.27076721126542
41 -12.775966946501285 -70.16508413911119
42 -12.607190039707348 -70.08681519456212
43 -12.857857742113993 -70.01472321960378
44 -12.860539419110864 -69.93406483378659
45 -12.62354332767427 -69.79716683999581
46 -12.915116059128195 -69.72202434681647
47 -12.815987159963697 -69.7206149587515
48 -12.908272924600169 -69.40889543909748
49 -12.791467923903838 -69.35260951273085
50 -12.696310901199467 -69.08192219194142
51 -12.791531590279192 -68.86978463326788
52 -12.876118366373703 -68.83967581694809
53 -12.953754300251603 -68.81143814804408
54 -12.904665176058188 -68.68942330075967
55 -12.750221292255446 -68.56166774046974
56 -12.649119451176375 -68.39807159486296
57 -12.75615249434486 -68.08824648383452
58 -12.61177392094396 -67.69159659913008
59 -12.832496225950308 -66.94308597532141
60 0.7791913016699255 -38.41270390343083
61 6.175443941261619 -25.350902208440505
62 0.08922367141349241 -20.13839114930498
63 4.988347913371399 -19.39514140618821
64 4.4049332870636135 -19.227271380776806
65 -0.04551113353227265 -18.758433074586584
66 0.6715752959717065 -18.476132715319725
67 -0.043335790920536965 -17.908680006646982
68 -0.06935656569839921 -17.402371282531398
69 0.22068685381964315 -15.027945370532887
70 0.20511864955187775 -12.787773900450743
71 -0.045968342063133605 -12.68135972540495
72 -0.03898084771935828 -12.17391336371053
73 -0.011894896560988855 -12.015277165656068
74 -0.24444623599993065 -11.770295795047181
75 -0.05833239729690831 -11.345785489951194
76 -0.23934656975325197 -11.213435573115255
77 0.17663141715456732 -11.178033270668683
78 -0.23183782800333574 -10.829210660670224
79 -0.05496130930259824 -10.644686356401598
80 -0.15813259355491027 -10.22435890245436
81 0.22428662038146285 -9.820488190086602
82 -0.031058593798661605 -9.7884749581491
83 -0.04849516786634922 -9.13342885003115
84 0.004818325658561662 -9.006611493267414
85 0.1826219029026106 -8.839357053510009
86 0.7004500039620325 -8.401145461327584
87 0.019861429653246887 -8.24087045763068
88 -0.044792369022616185 -8.206910135410356
89 -0.07922156213317066 -7.655094079301198
90 0.009134369451203384 -7.567240376251699
91 0.1775361194740981 -7.239429448903694
92 -0.04651806759648025 -7.233876175268499
93 -0.1898939497259562 -7.187796364507886
94 -0.06445867243746761 -7.114655779735902
95 -0.0281559336654027 -7.058381644615669
96 -0.04148940445156768 -6.869903843776713
97 -0.08514964074129239 -6.4860024287056826
98 0.1512244675977854 -6.477079472828306
99 -0.08773406928230543 -6.433679822070972
100 0.03893347262783209 -6.391386208696807
101 -0.009295047202613205 -6.13015194443597
102 -0.029297019791556522 -5.84815775358568
103 -0.021061797509901226 -5.837161601487908
104 -0.14888387373503065 -5.503158691576936
105 -0.10378700202272739 -5.451742569527069
106 -0.022797582525527105 -5.329971554560731
107 0.015455712884431705 -5.291493214198513
108 0.027463186874228995 -5.051785003331792
109 -0.04255247055698419 -4.982564251611441
110 -0.03180142963537946 -4.773723228462891
111 0.0727072108129505 -4.753829082494487
112 -0.011199690350622404 -4.621578174919492
113 -0.021537043605349027 -4.608301234275807
114 -0.0438284047995694 -4.577283167314312
115 0.00827448716154322 -4.565796954288057
116 0.04283707361901179 -4.506675119159872
117 -0.3967538307188079 -4.244373254657557
118 -0.003011812805198133 -3.994770298137972
119 0.024857467025867663 -1.8297832760197443
train accuracy: 1.0
validation accuracy: 1.0
[-75.02405605 -74.31688105 -73.76794491 -73.63146338 -73.61592963
 -73.40665393 -73.16941023 -72.72941992 -72.01728896 -71.78059471
 -71.48395509 -71.31460267 -70.96710401 -70.86988012 -70.54823285
 -70.4903145  -70.45548942 -70.35503835 -70.27076721 -70.16508414
 -70.08681519 -69.93406483 -69.79716684 -69.72061496 -69.40889544
 -69.35260951 -69.08192219 -68.83967582 -68.56166774 -68.08824648
 -51.21681261 -48.00113046 -47.43051951 -45.7877315  -44.64664369
 -43.87318547 -43.75448511 -42.67949784 -42.50590987 -41.09522049
 -41.05663065 -40.35101281 -39.78443957 -39.32798087 -39.27469419
 -39.11619622 -38.4127039  -36.77542143 -36.13749973 -33.67371359
 -33.62485448 -33.39712766 -32.61652911 -32.50326111 -32.40723407
 -32.38972323 -32.38237745 -32.18396285 -32.06393253 -31.93249114
 -31.8525097  -31.59351457 -31.46989806 -31.33639403 -30.96987325
 -30.78695328 -30.74712555 -30.56612102 -30.54358908 -30.21692241
 -30.116096   -29.75946893 -29.47670082 -29.23239689 -29.01259613
 -28.98266253 -28.9697559  -28.61658187 -27.65489143 -27.58885401
 -27.22259843 -27.20573624 -27.09938064 -26.6316055  -26.60576819
 -26.35331248 -26.35124563 -26.23352113 -25.43684895 -25.35090221
 -24.96606074 -23.18656255 -19.22727138 -18.47613272 -17.90868001
 -15.02794537 -12.7877739  -12.17391336 -11.7702958  -10.82921066
  -9.78847496  -9.13342885  -8.40114546  -8.24087046  -8.20691014
  -7.18779636  -7.11465578  -7.05838164  -6.86990384  -6.48600243
  -6.39138621  -5.45174257  -5.32997155  -5.29149321  -5.051785
  -4.98256425  -4.77372323  -4.75382908  -4.56579695  -3.9947703 ]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Found existing model weights! Loading state dict...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.00013053991990997105, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.7290e-02, -2.4056e-02, -7.4514e-02, -6.3565e-02, -7.3865e-05,
         -1.3155e-04,  5.8089e-02,  9.1015e-02,  8.4009e-05, -1.0191e-04,
          3.2881e-04,  1.2717e-04, -3.7688e-01]], device='cuda:1'))])
end of epoch 1: val_loss 0.00024171733786545246, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0009567197404772898, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 2.0682384274550714e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5483e-01, -2.3258e-05, -3.3187e-01, -1.7667e-05,  8.1437e-05,
          1.2083e-04,  1.6728e-01,  8.6349e-02, -3.3616e-05, -3.7982e-05,
          5.6235e-04,  2.4812e-04, -9.2185e-01]], device='cuda:1'))])
end of epoch 4: val_loss 0.0008965186974841899, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 8.642600718644644e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4076e-01, -2.3388e-05, -5.1855e-01, -9.7269e-06, -9.0342e-05,
         -3.2091e-05,  2.7828e-01,  1.2927e-01, -8.6568e-05,  9.3217e-05,
         -1.6336e-03, -8.6162e-05, -9.9650e-01]], device='cuda:1'))])
end of epoch 6: val_loss 0.011179132785619003, val_acc 0.995
trigger times: 1
end of epoch 7: val_loss 4.1723234289747776e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4369e-05, -5.8600e-05, -3.8685e-01,  3.3463e-05, -3.6243e-04,
         -5.4873e-05,  1.9609e-01,  1.4913e-01, -4.1197e-06,  7.5901e-05,
          1.6372e-03, -2.5030e-04, -1.1411e+00]], device='cuda:1'))])
end of epoch 8: val_loss 2.7722231048343815e-06, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 2.267009828287314e-05, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 7.271713911904953e-08, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 2.229165681910672e-07, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 4.249631092179129e-07, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 6.384303519226365e-05, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 3.826484781299655e-07, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 1.907345659191151e-08, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.0002597385804619989, val_acc 1.0
trigger times: 9
end of epoch 17: val_loss 4.76837069385283e-09, val_acc 1.0
trigger times: 10
Early stopping.
0 -227.25052678585052 -75.02405604955553
1 -232.08029437065125 -74.31688105415357
2 -230.3758099079132 -73.7679449087589
3 -230.59764635562897 -73.63146338446498
4 -230.66263830661774 -73.61592962697809
5 -229.91789376735687 -73.40665392878776
6 -227.70147383213043 -73.16941023061624
7 -230.07293355464935 -72.72941991519761
8 -228.93147611618042 -72.0172889588047
9 -228.69534718990326 -71.78059470596456
10 -229.84937822818756 -71.48395509096926
11 -226.4824024438858 -71.31460267118413
12 -229.3646565079689 -70.96710401119967
13 -231.40972423553467 -70.86988012143992
14 -223.98335099220276 -70.54823284918166
15 -230.51721251010895 -70.49031449785608
16 -230.67431938648224 -70.4554894249313
17 -229.70860624313354 -70.35503835287626
18 -228.2446563243866 -70.27076721126542
19 -227.36770153045654 -70.16508413911119
20 -226.92107486724854 -70.08681519456212
21 -228.63595259189606 -69.93406483378659
22 -226.54488337039948 -69.79716683999581
23 -229.60295343399048 -69.7206149587515
24 -227.87046885490417 -69.40889543909748
25 -226.97794830799103 -69.35260951273085
26 -224.95081317424774 -69.08192219194142
27 -229.13556027412415 -68.83967581694809
28 -226.47682213783264 -68.56166774046974
29 -228.98444241285324 -68.08824648383452
30 -39.65630479156971 -51.216812606979644
31 -44.94709528982639 -48.001130464853105
32 -39.469144985079765 -47.430519511287706
33 -36.79408620297909 -45.787731498434574
34 -40.59733335673809 -44.64664368933579
35 -39.245085313916206 -43.87318546928472
36 -42.02862749993801 -43.754485114651686
37 -37.2833093740046 -42.67949783840139
38 -39.08376247435808 -42.505909865893905
39 -37.085539072752 -41.09522049477896
40 -40.03606855124235 -41.05663064618119
41 -40.31096792221069 -40.351012814388476
42 -34.05291886627674 -39.78443957419508
43 -34.79725035279989 -39.32798087039672
44 -31.852932691574097 -39.27469419310849
45 -39.656708098948 -39.1161962152276
46 -52.154152162373066 -38.41270390343083
47 -32.13933026790619 -36.77542143261405
48 -37.97685843706131 -36.137499732021986
49 -31.661885768175125 -33.6737135884927
50 -34.8738477230072 -33.6248544790351
51 -34.3998049646616 -33.39712765961779
52 -37.47804383933544 -32.61652911254669
53 -35.04770593345165 -32.50326110937716
54 -31.71700030565262 -32.40723407302404
55 -31.08433449268341 -32.38972322794931
56 -34.33085058629513 -32.3823774515091
57 -29.96435457468033 -32.18396285030293
58 -30.198975674808025 -32.063932526550275
59 -36.06467838585377 -31.93249113558785
60 -28.579724297858775 -31.85250970119326
61 -30.94349440932274 -31.593514569386272
62 -33.31295996904373 -31.469898057699048
63 -34.12674632668495 -31.336394031804996
64 -30.710977666079998 -30.969873254720266
65 -31.268525935709476 -30.786953282985454
66 -33.35216833651066 -30.74712555215774
67 -32.522144325077534 -30.5661210235516
68 -34.28429402783513 -30.54358907615602
69 -32.81962467730045 -30.216922409377972
70 -36.34747952595353 -30.116095996088358
71 -31.421822667121887 -29.75946893375072
72 -35.40042096376419 -29.476700817283042
73 -32.09918147325516 -29.23239689274766
74 -31.714637331664562 -29.012596133824392
75 -33.32803729176521 -28.982662532165175
76 -27.869664531201124 -28.96975589846033
77 -33.55303045362234 -28.61658187092169
78 -33.311124823987484 -27.654891430873104
79 -29.78407956659794 -27.588854012122916
80 -32.787278823554516 -27.222598428885075
81 -32.693352587521076 -27.205736236664066
82 -28.770793192088604 -27.099380636773844
83 -32.12916775047779 -26.63160549882054
84 -30.02409501373768 -26.60576819092319
85 -28.182153716683388 -26.353312476126284
86 -28.43823590129614 -26.351245631197006
87 -32.547472789883614 -26.233521128254093
88 -30.774006009101868 -25.436848948986267
89 58.532747745513916 -25.350902208440505
90 -30.140743628144264 -24.966060738610995
91 -30.29834635555744 -23.186562547840953
92 36.50816637277603 -19.227271380776806
93 -9.207471385598183 -18.476132715319725
94 -11.524130888283253 -17.908680006646982
95 -10.919718598946929 -15.027945370532887
96 -11.123391987755895 -12.787773900450743
97 -11.735169500112534 -12.17391336371053
98 -18.608199033886194 -11.770295795047181
99 12.410503838211298 -10.829210660670224
100 -18.273160718381405 -9.7884749581491
101 -20.7672027181834 -9.13342885003115
102 -8.33241181075573 -8.401145461327584
103 -18.419523611664772 -8.24087045763068
104 -15.983870297670364 -8.206910135410356
105 -3.9497131034731865 -7.187796364507886
106 -10.04862192645669 -7.114655779735902
107 -9.515525337308645 -7.058381644615669
108 -16.3538189381361 -6.869903843776713
109 -17.123637046664953 -6.4860024287056826
110 -11.390828793868423 -6.391386208696807
111 -4.558725547045469 -5.451742569527069
112 -10.548052184283733 -5.329971554560731
113 -15.06579343508929 -5.291493214198513
114 -13.139344226568937 -5.051785003331792
115 -9.987881522625685 -4.982564251611441
116 -13.890213504433632 -4.773723228462891
117 -7.896148918196559 -4.753829082494487
118 -13.667262952774763 -4.565796954288057
119 -13.329641928896308 -3.994770298137972
train accuracy: 1.0
validation accuracy: 1.0

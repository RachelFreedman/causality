[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.7139277  -21.70254077
 -19.72288085 -19.26334551 -18.56975936 -18.11146126 -17.71205165
 -17.6663674  -17.58971226 -16.59087236 -16.36275632 -15.99253949
 -15.33818265 -14.68824709 -14.05563167 -13.91164962 -13.42778136
 -12.61940694 -12.22562907 -11.73504365 -10.78101609 -10.72946334
  -9.78594151  -9.77591752  -9.56682446  -9.19326137  -7.57539849
  -7.36244313  -7.10832736  -7.07294326  -6.95906356  -6.77694649
  -6.72206384  -6.71997062  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.009012846271325543, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 8.738292236328125, val_acc 0.99
trigger times: 1
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.5220220947265625, val_acc 0.995
trigger times: 1
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 5.517294921875, val_acc 0.995
trigger times: 1
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 2.7141796875, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 8.7505078125, val_acc 0.995
trigger times: 1
end of epoch 16: val_loss 0.532587890625, val_acc 0.995
trigger times: 2
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 10.922578125, val_acc 0.995
trigger times: 1
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 1.0919140625, val_acc 0.995
trigger times: 1
end of epoch 21: val_loss 1.9513671875, val_acc 0.995
trigger times: 2
end of epoch 22: val_loss 3.238974609375, val_acc 0.995
trigger times: 3
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 43.5550830078125, val_acc 0.975
trigger times: 1
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 2.8345703125, val_acc 0.995
trigger times: 1
end of epoch 35: val_loss 9.16916015625, val_acc 0.995
trigger times: 2
end of epoch 36: val_loss 2.476201171875, val_acc 0.995
trigger times: 3
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 4.63341796875, val_acc 0.995
trigger times: 1
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 4.201279296875, val_acc 0.995
trigger times: 1
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 2.1015380859375, val_acc 0.995
trigger times: 1
end of epoch 50: val_loss 0.476748046875, val_acc 0.995
trigger times: 2
end of epoch 51: val_loss 1.5012353515625, val_acc 0.995
trigger times: 3
end of epoch 52: val_loss 2.33892578125, val_acc 0.995
trigger times: 4
end of epoch 53: val_loss 16.446181640625, val_acc 0.99
trigger times: 5
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.504912109375, val_acc 0.995
trigger times: 1
end of epoch 59: val_loss 3.610791015625, val_acc 0.995
trigger times: 2
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 10.4978515625, val_acc 0.99
trigger times: 1
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 6.1784326171875, val_acc 0.99
trigger times: 1
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 1.891611328125, val_acc 0.995
trigger times: 1
end of epoch 71: val_loss 0.313720703125, val_acc 0.995
trigger times: 2
end of epoch 72: val_loss 0.6993701171875, val_acc 0.995
trigger times: 3
end of epoch 73: val_loss 3.6871826171875, val_acc 0.995
trigger times: 4
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 2.769599609375, val_acc 0.995
trigger times: 1
end of epoch 83: val_loss 0.028503074645996093, val_acc 0.995
trigger times: 2
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 1.1303369140625, val_acc 0.99
trigger times: 1
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 1.05031982421875, val_acc 0.995
trigger times: 1
end of epoch 93: val_loss 1.63288818359375, val_acc 0.995
trigger times: 2
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 2.378623046875, val_acc 0.995
trigger times: 1
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -16889.658289825544 -51.11086407377863
1 -20835.525703430176 -49.11584688036212
2 -30573.925384521484 -48.97455066650668
3 -47646.07099914551 -48.87228569913625
4 -25040.490814208984 -48.723750706751915
5 -20424.59794664383 -47.242520801549375
6 -29404.213317871094 -44.969216444406115
7 -48673.06979370117 -44.85812386141036
8 -24784.73006439209 -44.465298807522004
9 -34025.9959564209 -44.438901626703945
10 -23279.973182678223 -44.1492199315879
11 -16217.48006439209 -43.750352447059115
12 -21287.803329467773 -43.58405711344297
13 -36447.009857177734 -43.39691284921414
14 -17115.532432556152 -43.22343612991455
15 -17607.78606414795 -42.92113389854083
16 -30694.5069732666 -42.65748859055198
17 -16695.130783081055 -42.60231490040316
18 -19177.70848083496 -42.3286380570152
19 -24321.566581726074 -42.30445768311566
20 -33036.335220336914 -41.12283699063089
21 -16785.231872558594 -40.758154070747366
22 -21141.385360717773 -40.70441036508885
23 -21246.93154144287 -40.037587376429016
24 -24174.063452871516 -39.90846007278091
25 -26395.591358184814 -39.57032589487153
26 -20836.135871887207 -39.31833047231749
27 -24728.311820983887 -39.27893976130787
28 -28355.233848571777 -39.11210375046212
29 -21206.139602661133 -39.07922820344388
30 -51326.36102294922 -38.73424350253481
31 -19475.42214202881 -38.46237505180561
32 -16180.579614639282 -37.86129653169397
33 -21185.77961730957 -37.308065525311996
34 -18217.438358306885 -36.41283474392047
35 -20107.633796691895 -36.307958906842664
36 -19167.14224243164 -36.12354062747346
37 -20418.847007751465 -36.012213433621724
38 -19052.904699146748 -35.76166081977253
39 -16081.224143981934 -35.558040948837615
40 -22102.07341003418 -34.83335159619024
41 -33943.32484436035 -34.24472332102743
42 -14698.295667111874 -33.78094921339959
43 -13835.035163879395 -32.949116337767926
44 -17344.58736515045 -32.774518645207735
45 -17066.925582885742 -32.11238423004613
46 -17224.183685302734 -31.534270745249735
47 -32485.414520263672 -30.667802091332018
48 -11989.370040893555 -30.402902212388465
49 -14326.561740875244 -30.233660870125824
50 -16715.895146101713 -29.87212515537898
51 -13008.473625183105 -29.40604134962864
52 -16361.926803588867 -29.13525869956722
53 -17978.033763885498 -28.84050982317397
54 -13875.940208435059 -28.578629026453186
55 -18995.83709716797 -28.438710074595054
56 -14197.492218017578 -28.433748774939595
57 -17374.608947753906 -28.16728517571421
58 -19037.55446624756 -27.920602937037046
59 -13591.630937676877 -27.739114797604373
60 -13907.317016601562 -27.29994968172231
61 -12819.876823425293 -26.752294794786035
62 -13318.84203338623 -25.939034905087084
63 -12798.624500274658 -25.929972112830644
64 -16249.941034317017 -25.901848378876174
65 -13595.616065979004 -24.374430568602165
66 -10573.428771018982 -24.180793753972072
67 -15588.606185913086 -24.11517186961488
68 -16513.779174804688 -23.991963674348654
69 -14905.55704498291 -23.291023207711152
70 -17821.142044067383 -22.595308335760205
71 -13755.232234954834 -22.402293560723127
72 -14527.988006591797 -22.256737260461836
73 -12609.350257851183 -21.713927702452338
74 -12862.17590713501 -21.702540774014164
75 -12996.150547981262 -19.722880852611173
76 -12689.890548706055 -19.26334550647668
77 -14752.826950073242 -18.569759364071594
78 -11153.934543251991 -18.111461264885516
79 -15601.094352722168 -17.712051650475324
80 -10966.639736175537 -17.66636739843018
81 -12641.550202596933 -17.589712263068826
82 -11032.098223686218 -16.590872364965673
83 -14190.44624710083 -16.362756319978686
84 -12549.02638527751 -15.992539490672064
85 -11970.9408493042 -15.338182650137867
86 -14199.083473768085 -14.688247089412398
87 -11716.580002173781 -14.055631672914624
88 -12086.977630615234 -13.911649622206243
89 -11562.146827697754 -13.427781363912512
90 -13854.99121491611 -12.619406941033082
91 -9491.228195667267 -12.225629065695466
92 -10417.258261680603 -11.735043647109059
93 -10303.462074279785 -10.781016093040188
94 -10821.410318411887 -10.729463335147118
95 -11251.470016479492 -9.785941512083282
96 -9049.398837089539 -9.775917521665287
97 -10439.051348686218 -9.56682446099046
98 -9144.59878540039 -9.193261368948118
99 -9106.05042719096 -7.57539849177145
100 -8640.938744544983 -7.362443126623615
101 -9984.672599994577 -7.108327355338034
102 -8828.974471092224 -7.072943263247867
103 -9129.70492349565 -6.959063561385431
104 -8534.342461787164 -6.776946485018116
105 -8689.940502300858 -6.7220638398623045
106 -8285.986062720418 -6.719970621583102
107 -10114.237897256913 -6.51820418055673
108 -10066.605112461373 -5.615796733870542
109 -11260.236587524414 -5.34720210027791
110 -8209.758232065069 -5.078485007852753
111 -8993.660081639886 -5.027957977402961
112 -8943.619491577148 -4.827572916892203
113 -7901.57517447602 -4.63049541560991
114 -8526.276077270508 -4.230832004686763
115 -10102.051567077637 -4.031048624093466
116 -8647.140985488892 -3.3844671463622564
117 -9667.359603881836 -3.3322555012187633
118 -7644.6614990234375 -2.6416623314910934
119 -8457.812057495117 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.7139277  -21.70254077
 -20.89318256 -20.62400298 -20.62322    -20.57241518 -20.31337217
 -20.21986302 -19.72288085 -19.6992776  -19.26334551 -19.1961232
 -19.14314028 -18.94678246 -18.91300692 -18.66966043 -18.56975936
 -18.45454304 -18.30178757 -18.24353216 -18.23095197 -18.12536478
 -18.11146126 -17.85620013 -17.74941668 -17.74698145 -17.72206351
 -17.71205165 -17.6663674  -17.58971226 -17.30476607 -17.26533128
 -17.00651319 -16.94113674 -16.70065155 -16.59087236 -16.53256642
 -16.48462918 -16.44434518 -16.36275632 -16.07250529 -15.99253949
 -15.75942547 -15.72173399 -15.68434988 -15.52502395 -15.45861498
 -15.33818265 -15.08104874 -15.04609384 -15.01348234 -14.96004084
 -14.92815424 -14.89341026 -14.8163426  -14.77006262 -14.7592488
 -14.68824709 -14.66499573 -14.48659431 -14.36108582 -14.1806428
 -14.12885066 -14.11772772 -14.05563167 -13.91164962 -13.90134631
 -13.75434677 -13.71997987 -13.71962022 -13.68443411 -13.48107249
 -13.47852574 -13.46753251 -13.4472251  -13.42791675 -13.42778136
 -13.16095841 -13.10880878 -13.0688612  -12.98159065 -12.80762312
 -12.61940694 -12.22562907 -11.96171005 -11.87399371 -11.73504365
 -11.69189859 -11.6611216  -11.56872012 -11.52351266 -11.31090681
 -11.18319497 -11.16772629 -11.08542161 -11.05153808 -11.02526816
 -10.93017881 -10.78101609 -10.77982944 -10.72946334 -10.52701581
 -10.27079184 -10.21866477 -10.17867681 -10.12534842  -9.96354758
  -9.96001316  -9.94097327  -9.92729193  -9.81586801  -9.79432765
  -9.78594151  -9.77591752  -9.63102321  -9.56682446  -9.49641157
  -9.46838438  -9.45419413  -9.37686032  -9.28663883  -9.24398838
  -9.22054043  -9.19326137  -8.98175649  -8.93222379  -8.67936707
  -8.08091889  -7.91234732  -7.60415018  -7.57539849  -7.55147958
  -7.36244313  -7.10832736  -7.07294326  -6.97871853  -6.95906356
  -6.92344022  -6.89859496  -6.77694649  -6.72461552  -6.72206384
  -6.71997062  -6.68043877  -6.57339439  -6.51820418  -6.06194077
  -5.96779051  -5.61579673  -5.54032152  -5.3472021   -5.29019147
  -5.19475761  -5.07848501  -5.02795798  -4.89289296  -4.82757292
  -4.82746883  -4.63049542  -4.57698155  -4.230832    -4.19086158
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.02091804750962183, val_acc 0.985
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 1.1831585693359374, val_acc 0.99
trigger times: 1
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 2.057125244140625, val_acc 0.99
trigger times: 1
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.979739990234375, val_acc 0.995
trigger times: 1
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 1.15304931640625, val_acc 0.995
trigger times: 1
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 3.4381675580563025, val_acc 0.985
trigger times: 1
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.5017236328125, val_acc 0.995
trigger times: 1
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 1
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.9656855499728771, val_acc 0.99
trigger times: 1
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.255198974609375, val_acc 0.995
trigger times: 1
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.25201204400509597, val_acc 0.995
trigger times: 1
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.47328857421875, val_acc 0.99
trigger times: 1
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 27.283736851215362, val_acc 0.925
trigger times: 1
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 2.647289424240589, val_acc 0.99
trigger times: 1
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 9.007167871892452, val_acc 0.97
trigger times: 1
end of epoch 74: val_loss 0.405916748046875, val_acc 0.995
trigger times: 2
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.03884122610092163, val_acc 0.995
trigger times: 1
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0447383006423388, val_acc 0.99
trigger times: 1
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 7.271713911904953e-08, val_acc 1.0
trigger times: 1
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -6721.123489379883 -51.11086407377863
1 -9656.737901687622 -48.97455066650668
2 -7809.230930328369 -48.723750706751915
3 -9300.161128997803 -44.969216444406115
4 -7889.1061000823975 -44.465298807522004
5 -8486.727865219116 -44.1492199315879
6 -6607.510271072388 -43.58405711344297
7 -5562.581561088562 -43.22343612991455
8 -9923.511138916016 -42.65748859055198
9 -5330.756064414978 -42.3286380570152
10 -11984.855131149292 -41.12283699063089
11 -7284.815637588501 -40.70441036508885
12 -9556.15179464221 -39.90846007278091
13 -7554.45334815979 -39.31833047231749
14 -9162.78886604309 -39.11210375046212
15 -20334.017169952393 -38.73424350253481
16 -4876.830922484398 -37.86129653169397
17 -6048.664169788361 -36.41283474392047
18 -5702.840816497803 -36.12354062747346
19 -6464.0254554748535 -35.76166081977253
20 -7030.7042236328125 -34.83335159619024
21 -6292.277697563171 -33.78094921339959
22 -5407.0990171432495 -32.774518645207735
23 -5380.231161117554 -31.534270745249735
24 -5067.771068572998 -30.402902212388465
25 -5494.9806760549545 -29.87212515537898
26 -5288.909601211548 -29.13525869956722
27 -4509.53221321106 -28.578629026453186
28 -5881.077324390411 -28.433748774939595
29 -5743.115089416504 -27.920602937037046
30 -3987.4775733947754 -27.29994968172231
31 -4650.984025001526 -25.939034905087084
32 -6550.970854282379 -25.901848378876174
33 -4492.156426906586 -24.180793753972072
34 -5184.038475036621 -23.991963674348654
35 -5564.126640319824 -22.595308335760205
36 -4839.686017990112 -22.256737260461836
37 -3382.199686050415 -21.702540774014164
38 -3606.5706100463867 -20.62400297794099
39 -3823.1516036987305 -20.5724151831623
40 -3935.6488494873047 -20.21986301936262
41 -3578.190689086914 -19.69927760369218
42 -3490.4203720092773 -19.196123201283406
43 -3551.0567111968994 -18.946782460741005
44 -3476.5953330993652 -18.66966043016843
45 -3414.0782318115234 -18.45454303986138
46 -3450.8707695007324 -18.243532156030067
47 -3380.1098709106445 -18.125364782802624
48 -3455.189292907715 -17.85620012604348
49 -3535.0267753601074 -17.746981447181167
50 -5146.166631698608 -17.712051650475324
51 -3478.6441453599837 -17.589712263068826
52 -3414.048873901367 -17.265331280949543
53 -3404.144145965576 -16.941136744673663
54 -3137.475907564163 -16.590872364965673
55 -3290.1195220947266 -16.484629175602237
56 -4802.001556396484 -16.362756319978686
57 -5605.412811160088 -15.992539490672064
58 -3357.6150856018066 -15.721733988613988
59 -3837.5221061706543 -15.525023951517204
60 -3406.2429695129395 -15.08104874308503
61 -3336.9475173950195 -15.01348233511118
62 -3599.559787750244 -14.928154240128466
63 -3380.830738067627 -14.81634260266616
64 -3453.0046882629395 -14.75924880328638
65 -3281.2633781433105 -14.664995734066178
66 -3382.4485473632812 -14.361085821153926
67 -3445.755889892578 -14.128850664826835
68 -3435.8810881972313 -14.055631672914624
69 -3246.2678146362305 -13.901346305146138
70 -3097.5975914001465 -13.719979872761556
71 -3379.4472885131836 -13.684434110863704
72 -3014.4354496002197 -13.478525740547907
73 -3421.605827331543 -13.447225102258235
74 -3263.287609100342 -13.427781363912512
75 -3090.4942378997803 -13.108808775702943
76 -3306.6118927001953 -12.981590646661113
77 -4039.222427368164 -12.619406941033082
78 -3238.4889640808105 -11.96171005089843
79 -3280.968810684979 -11.735043647109059
80 -3398.990463256836 -11.66112160483124
81 -3238.902280807495 -11.523512659238419
82 -3322.255096435547 -11.183194969624676
83 -3166.5380897521973 -11.0854216084259
84 -3228.508430480957 -11.025268164048521
85 -3124.376253604889 -10.781016093040188
86 -3644.0553099645767 -10.729463335147118
87 -3250.4930000305176 -10.270791843270539
88 -3318.7078819274902 -10.178676807445836
89 -3150.2820777893066 -9.96354758353631
90 -2964.9197959899902 -9.940973270323955
91 -3159.4429969787598 -9.815868010044042
92 -2779.844093322754 -9.785941512083282
93 -3104.326030731201 -9.631023212345063
94 -3176.5111656188965 -9.496411571497996
95 -3088.1162605285645 -9.454194130244643
96 -3295.306526184082 -9.286638832116193
97 -3209.3402137756348 -9.22054042914427
98 -3362.7135829925537 -8.981756488456098
99 -3055.212490081787 -8.679367065921117
100 -3164.7522354125977 -7.912347323340799
101 -2946.3000044147484 -7.57539849177145
102 -2835.6738751698285 -7.362443126623615
103 -2250.697832107544 -7.072943263247867
104 -2706.6156325638294 -6.959063561385431
105 -3012.5905952453613 -6.8985949613137185
106 -3082.385471343994 -6.724615516472137
107 -3382.4201318515697 -6.719970621583102
108 -2979.416009902954 -6.5733943908649755
109 -3189.9172382354736 -6.061940768427526
110 -2703.2844040840864 -5.615796733870542
111 -2750.618863105774 -5.34720210027791
112 -3102.0057792663574 -5.194757605593898
113 -2604.690597572946 -5.027957977402961
114 -2412.351776123047 -4.827572916892203
115 -2052.8373866190523 -4.63049541560991
116 -2369.920120239258 -4.230832004686763
117 -2393.241430282593 -4.031048624093466
118 -2138.7043991088867 -3.3322555012187633
119 -1781.6545314788818 -1.9136196540088464
train accuracy: 0.9977777777777778
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -24.37443057 -24.18079375 -24.11517187 -23.99196367 -23.29102321
 -22.59530834 -22.40229356 -22.25673726 -21.7139277  -21.70254077
 -21.3854157  -21.22769954 -21.22484726 -21.16103238 -21.06465695
 -20.89318256 -20.62400298 -20.62322    -20.57241518 -20.39554951
 -20.31337217 -20.21986302 -20.20423826 -20.08314676 -19.72288085
 -19.6992776  -19.60435553 -19.3963696  -19.31266306 -19.26443554
 -19.26334551 -19.19651054 -19.1961232  -19.14314028 -19.11745812
 -18.94678246 -18.91614563 -18.91300692 -18.76651715 -18.71555168
 -18.66966043 -18.56975936 -18.45454304 -18.30680476 -18.30178757
 -18.24353216 -18.23095197 -18.212601   -18.18632012 -18.12536478
 -18.11146126 -17.85620013 -17.74941668 -17.74698145 -17.72206351
 -17.71205165 -17.70340907 -17.6663674  -17.64422174 -17.58971226
 -17.5822701  -17.4350675  -17.41781108 -17.33478655 -17.30476607
 -17.26533128 -17.15795246 -17.00651319 -16.94139122 -16.94113674
 -16.90579141 -16.74398113 -16.70065155 -16.59087236 -16.53256642
 -16.48462918 -16.44434518 -16.42785183 -16.36275632 -16.34253724
 -16.31004305 -16.09736232 -16.07250529 -16.06457088 -16.00107617
 -15.99253949 -15.92367836 -15.91511327 -15.88598634 -15.81531539
 -15.75942547 -15.75169388 -15.72173399 -15.71977336 -15.68434988
 -15.66526845 -15.56444026 -15.52502395 -15.45861498 -15.37980369
 -15.33818265 -15.08104874 -15.04609384 -15.01348234 -14.99675024
 -14.9727838  -14.96004084 -14.92815424 -14.90902317 -14.89341026
 -14.8882129  -14.84254054 -14.8163426  -14.77006262 -14.7592488
 -14.71627777 -14.7014109  -14.68824709 -14.66499573 -14.64015392
 -14.63363887 -14.48659431 -14.42264808 -14.36108582 -14.22280276
 -14.1806428  -14.12885066 -14.11772772 -14.11538276 -14.05563167
 -13.91164962 -13.90134631 -13.75434677 -13.72366989 -13.71997987
 -13.71962022 -13.68443411 -13.48107249 -13.47852574 -13.46753251
 -13.4472251  -13.42791675 -13.42778136 -13.32613511 -13.26560738
 -13.16095841 -13.10880878 -13.0688612  -12.98159065 -12.96132693
 -12.80762312 -12.75742911 -12.72080478 -12.68986683 -12.64631452
 -12.61940694 -12.22562907 -12.20760896 -12.19602138 -12.07165519
 -11.96171005 -11.94768728 -11.87399371 -11.79229512 -11.73504365
 -11.69189859 -11.6611216  -11.57101906 -11.56872012 -11.54244403
 -11.52351266 -11.46390912 -11.31090681 -11.27222323 -11.27075503
 -11.18319497 -11.16772629 -11.1179572  -11.08542161 -11.0727365
 -11.05153808 -11.02526816 -10.98392269 -10.93569763 -10.93017881
 -10.78101609 -10.77982944 -10.72946334 -10.70670002 -10.70297356
 -10.68722361 -10.68641121 -10.64146107 -10.55139694 -10.52701581
 -10.44809971 -10.37296385 -10.27079184 -10.2635315  -10.25866026
 -10.21866477 -10.17867681 -10.12534842 -10.05857065 -10.01271589
  -9.96354758  -9.96001316  -9.94097327  -9.92729193  -9.81586801
  -9.79432765  -9.78594151  -9.77591752  -9.72984625  -9.63102321
  -9.60386214  -9.56682446  -9.49641157  -9.46838438  -9.45419413
  -9.37686032  -9.33083524  -9.28663883  -9.24398838  -9.22054043
  -9.20352712  -9.19326137  -9.15906172  -9.15427352  -8.98175649
  -8.93802219  -8.93222379  -8.67936707  -8.57826377  -8.57202317
  -8.15082638  -8.12131594  -8.08091889  -8.02322976  -7.9350814
  -7.91234732  -7.85762588  -7.64506755  -7.60415018  -7.58015691
  -7.57539849  -7.55147958  -7.47309267  -7.36755183  -7.36244313
  -7.27484897  -7.10832736  -7.07294326  -6.97871853  -6.95906356
  -6.92344022  -6.89859496  -6.77694649  -6.72461552  -6.72206384
  -6.71997062  -6.68043877  -6.63402967  -6.61055787  -6.58156549
  -6.57339439  -6.55769488  -6.51820418  -6.09941013  -6.06194077
  -5.96779051  -5.84706623  -5.61579673  -5.58714519  -5.54032152
  -5.3472021   -5.29019147  -5.19475761  -5.07848501  -5.02795798
  -4.96538494  -4.89289296  -4.82757292  -4.82746883  -4.81600172
  -4.63049542  -4.57698155  -4.31003276  -4.230832    -4.19086158
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:3
end of epoch 0: val_loss 0.6621655729971826, val_acc 0.985
trigger times: 0
saving model weights...
end of epoch 1: val_loss 6.574041675776243e-05, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 1.9123473612125964e-06, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.060634183883666995, val_acc 0.995
trigger times: 1
end of epoch 4: val_loss 3.2134693908691405, val_acc 0.97
trigger times: 2
end of epoch 5: val_loss 0.006117168664932251, val_acc 0.995
trigger times: 3
end of epoch 6: val_loss 0.028027379512786867, val_acc 0.995
trigger times: 4
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 1.1428081941604615, val_acc 0.965
trigger times: 1
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 6.258449499085827e-08, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 0.00026377723784762, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 0.5068515014648437, val_acc 0.99
trigger times: 3
end of epoch 16: val_loss 0.3028631591796875, val_acc 0.995
trigger times: 4
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 2.0261472673155367e-06, val_acc 1.0
trigger times: 1
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.692413330078125, val_acc 0.99
trigger times: 1
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.7680368041992187, val_acc 0.99
trigger times: 1
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.2649273681640625, val_acc 0.995
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0002933102659881115, val_acc 1.0
trigger times: 1
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 1
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.44484130859375, val_acc 0.995
trigger times: 1
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.378746337890625, val_acc 0.995
trigger times: 1
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 2.9206189537944736e-08, val_acc 1.0
trigger times: 1
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.9089624419284519, val_acc 0.995
trigger times: 1
end of epoch 89: val_loss 0.05855839252471924, val_acc 0.995
trigger times: 2
end of epoch 90: val_loss 0.18190429866313906, val_acc 0.995
trigger times: 3
end of epoch 91: val_loss 0.8727886962890625, val_acc 0.985
trigger times: 4
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.13362025323556737, val_acc 0.995
trigger times: 1
end of epoch 95: val_loss 15.715276308079483, val_acc 0.935
trigger times: 2
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.27315185546875, val_acc 0.995
trigger times: 1
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -4369.665034532547 -51.11086407377863
1 -7206.481371879578 -48.87228569913625
2 -4280.4028396606445 -44.969216444406115
3 -8537.108898162842 -44.438901626703945
4 -3017.500912427902 -43.58405711344297
5 -2990.421688348055 -42.92113389854083
6 -2930.5289295203984 -42.3286380570152
7 -2463.1944389902055 -40.758154070747366
8 -6547.1613001823425 -39.90846007278091
9 -6936.95349711366 -39.27893976130787
10 -16382.17820072174 -38.73424350253481
11 -2919.699882570654 -37.308065525311996
12 -2693.6464624404907 -36.12354062747346
13 -2531.8417684957385 -35.558040948837615
14 -5423.646950721741 -33.78094921339959
15 -3729.604215860367 -32.11238423004613
16 -4116.812443852425 -30.402902212388465
17 -3825.219229698181 -29.40604134962864
18 -2192.9397020339966 -28.578629026453186
19 -2451.427710533142 -28.16728517571421
20 -1897.9714722633362 -27.29994968172231
21 -3224.088142208755 -25.929972112830644
22 -3511.0291382074356 -24.180793753972072
23 -2216.6941614151 -23.291023207711152
24 -2549.819000950083 -22.256737260461836
25 -2258.463447570801 -21.38541570377559
26 -2267.466636657715 -21.161032377739385
27 -2386.8554458618164 -20.62400297794099
28 -2276.732021331787 -20.395549507226466
29 -2209.0508728027344 -20.204238264788124
30 -2197.228958129883 -19.60435553232499
31 -2145.3543968200684 -19.264435543276974
32 -2214.119680404663 -19.196123201283406
33 -2161.411340713501 -18.946782460741005
34 -2044.467269897461 -18.76651715200028
35 -2394.660924911499 -18.569759364071594
36 -2437.265205383301 -18.301787569114474
37 -2074.7821044921875 -18.212601003547334
38 -1434.9159615039825 -18.111461264885516
39 -2152.982955932617 -17.746981447181167
40 -2056.0517044067383 -17.703409074507174
41 -1566.7084628641605 -17.589712263068826
42 -1852.3144435882568 -17.417811082309413
43 -2070.8761024475098 -17.265331280949543
44 -1959.1682586669922 -16.941391222911346
45 -1877.0055332183838 -16.74398112804468
46 -1976.4931182861328 -16.53256642227629
47 -1793.968204498291 -16.427851826311485
48 -1770.731559753418 -16.310043054430455
49 -1835.8661880493164 -16.064570880381925
50 -1808.7077941894531 -15.92367836179304
51 -1668.5590438842773 -15.815315392272375
52 -1923.6138858795166 -15.721733988613988
53 -1832.4281463623047 -15.665268446613585
54 -1601.1428565979004 -15.45861497874565
55 -1872.4950695037842 -15.08104874308503
56 -1391.1866283416748 -14.996750243815644
57 -2124.112693786621 -14.928154240128466
58 -1656.0389404296875 -14.88821290221279
59 -1708.6943626403809 -14.770062615308099
60 -1785.125867843628 -14.701410896461134
61 -1757.3816452026367 -14.640153922460026
62 -1798.0567989349365 -14.422648078052765
63 -1578.908519744873 -14.18064279874261
64 -1505.7031345367432 -14.115382758890235
65 -1750.945966720581 -13.901346305146138
66 -1388.5191164016724 -13.719979872761556
67 -1539.4806699752808 -13.48107249246075
68 -1814.0238056182861 -13.447225102258235
69 -1277.5932884216309 -13.326135107923053
70 -1356.8814716339111 -13.108808775702943
71 -1337.0766353607178 -12.961326934690538
72 -1304.549560546875 -12.720804778769793
73 -2101.868944644928 -12.619406941033082
74 -1570.242431640625 -12.196021382429088
75 -1473.4274520874023 -11.947687283951629
76 -1713.6346125602722 -11.735043647109059
77 -1414.9811897277832 -11.571019057062678
78 -1523.0206575393677 -11.523512659238419
79 -1315.840871810913 -11.272223233121878
80 -1611.152042388916 -11.167726287628158
81 -1284.503999710083 -11.072736498972526
82 -1175.4715995788574 -10.98392268861583
83 -1648.0577262453735 -10.781016093040188
84 -1407.4212703704834 -10.706700024057415
85 -1169.2563209533691 -10.686411207512771
86 -1416.259973526001 -10.52701581083423
87 -1529.4013690948486 -10.270791843270539
88 -1365.7878732681274 -10.2186647707606
89 -1074.4420623779297 -10.058570649114962
90 -1138.5988159179688 -9.940973270323955
91 -1267.5945310592651 -9.79432764708296
92 -1330.508207321167 -9.729846251979577
93 -1160.628365635872 -9.56682446099046
94 -1253.7148551940918 -9.454194130244643
95 -1335.881115913391 -9.286638832116193
96 -1315.348964691162 -9.203527117964319
97 -1035.8324193954468 -9.154273524488499
98 -1329.9821138381958 -8.932223793553828
99 -1122.6766061782837 -8.572023169241138
100 -1234.580636024475 -8.080918894411614
101 -1249.0408916473389 -7.912347323340799
102 -983.8474768400192 -7.604150183138894
103 -1007.0674781799316 -7.551479584092
104 -1486.8665046691895 -7.362443126623615
105 -1023.1452938318253 -7.072943263247867
106 -936.4059677124023 -6.923440221160999
107 -1165.0012254714966 -6.724615516472137
108 -1064.6648087501526 -6.680438765523987
109 -894.8939208984375 -6.581565485968836
110 -1149.1563766598701 -6.51820418055673
111 -1007.0813999176025 -5.967790505306058
112 -850.9199504852295 -5.587145185329393
113 -929.0869646072388 -5.290191465256995
114 -1121.6132391095161 -5.027957977402961
115 -1193.4230930805206 -4.827572916892203
116 -840.4624073691666 -4.63049541560991
117 -1073.8756403923035 -4.230832004686763
118 -915.8883020877838 -3.3844671463622564
119 -650.2374732494354 -1.9136196540088464
train accuracy: 0.9994444444444445
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -25.10113857 -24.82492217 -24.37443057 -24.18079375 -24.11517187
 -24.10704836 -23.99196367 -23.85559618 -23.84671267 -23.80110721
 -23.29102321 -22.63175702 -22.59530834 -22.40229356 -22.36019567
 -22.25673726 -22.17481765 -21.92543953 -21.7139277  -21.70254077
 -21.66033175 -21.3854157  -21.27381292 -21.22769954 -21.22484726
 -21.16103238 -21.11248187 -21.10191176 -21.06465695 -20.89318256
 -20.8247226  -20.75809    -20.73395188 -20.62400298 -20.62322
 -20.57241518 -20.5478087  -20.40287539 -20.39554951 -20.31337217
 -20.21986302 -20.20423826 -20.19083382 -20.14839891 -20.11304839
 -20.09608855 -20.08314676 -20.02804356 -19.98788604 -19.86030353
 -19.72288085 -19.6992776  -19.68777137 -19.60435553 -19.3963696
 -19.37672619 -19.31266306 -19.27597062 -19.26443554 -19.26334551
 -19.19651054 -19.1961232  -19.14314028 -19.11745812 -19.09200352
 -18.94678246 -18.91614563 -18.91300692 -18.76651715 -18.74109978
 -18.71555168 -18.66966043 -18.66727006 -18.56975936 -18.45454304
 -18.39024175 -18.30680476 -18.30178757 -18.24353216 -18.23095197
 -18.212601   -18.18632012 -18.12536478 -18.11146126 -17.85620013
 -17.848403   -17.74941668 -17.74698145 -17.72206351 -17.71205165
 -17.70340907 -17.6989796  -17.6663674  -17.64422174 -17.58971226
 -17.5822701  -17.55577245 -17.53447809 -17.4350675  -17.41781108
 -17.39510553 -17.33478655 -17.30476607 -17.26533128 -17.16110906
 -17.15795246 -17.12948493 -17.11153206 -17.08867737 -17.00651319
 -16.9981261  -16.94139122 -16.94113674 -16.90579141 -16.89331433
 -16.74398113 -16.70390724 -16.70065155 -16.59382175 -16.59087236
 -16.53256642 -16.48462918 -16.44434518 -16.44255462 -16.42785183
 -16.37872874 -16.36275632 -16.34253724 -16.31004305 -16.14358893
 -16.13982043 -16.09736232 -16.07250529 -16.06457088 -16.00107617
 -15.99253949 -15.92367836 -15.91511327 -15.88598634 -15.861028
 -15.81531539 -15.75942547 -15.75169388 -15.72173399 -15.71977336
 -15.68434988 -15.66526845 -15.56444026 -15.52502395 -15.48447734
 -15.45861498 -15.37980369 -15.33818265 -15.12488432 -15.08104874
 -15.04609384 -15.0264971  -15.02200728 -15.01348234 -14.99675024
 -14.9727838  -14.96004084 -14.92815424 -14.90902317 -14.89341026
 -14.89031968 -14.8882129  -14.84254054 -14.8163426  -14.79128925
 -14.77006262 -14.7592488  -14.71627777 -14.7014109  -14.68824709
 -14.67114322 -14.66499573 -14.64015392 -14.63363887 -14.48659431
 -14.44457482 -14.43423615 -14.42264808 -14.37407174 -14.36108582
 -14.22280276 -14.1806428  -14.17635601 -14.12885066 -14.11772772
 -14.11538276 -14.08828621 -14.05563167 -13.91164962 -13.90134631
 -13.85095004 -13.83216719 -13.75434677 -13.72366989 -13.71997987
 -13.71962022 -13.68443411 -13.600824   -13.48107249 -13.47852574
 -13.46753251 -13.4472251  -13.42791675 -13.42778136 -13.32613511
 -13.30140057 -13.29791877 -13.26560738 -13.16095841 -13.11719923
 -13.10880878 -13.1007203  -13.07204945 -13.0688612  -12.98159065
 -12.9775137  -12.96132693 -12.84396769 -12.80762312 -12.75742911
 -12.72080478 -12.68986683 -12.64631452 -12.61940694 -12.5552417
 -12.54006106 -12.48937457 -12.42450605 -12.33014663 -12.22562907
 -12.20760896 -12.20735524 -12.19602138 -12.07165519 -12.0209785
 -11.96171005 -11.94768728 -11.8745299  -11.87399371 -11.85011005
 -11.79229512 -11.73504365 -11.73134636 -11.69189859 -11.6611216
 -11.62165486 -11.57101906 -11.56872012 -11.55017776 -11.54244403
 -11.52351266 -11.50665747 -11.46390912 -11.31090681 -11.27222323
 -11.27075503 -11.18319497 -11.16772629 -11.1179572  -11.08542161
 -11.0727365  -11.05153808 -11.02526816 -10.98392269 -10.93569763
 -10.93017881 -10.91617999 -10.78101609 -10.77982944 -10.72946334
 -10.70670002 -10.70297356 -10.68722361 -10.68641121 -10.64146107
 -10.55139694 -10.52701581 -10.44809971 -10.37296385 -10.35148305
 -10.27079184 -10.2635315  -10.25866026 -10.21866477 -10.17867681
 -10.14859659 -10.12534842 -10.09210295 -10.05857065 -10.03129926
 -10.01271589  -9.96354758  -9.96001316  -9.94097327  -9.92729193
  -9.86666785  -9.81586801  -9.79432765  -9.78594151  -9.77591752
  -9.72984625  -9.72366646  -9.69392233  -9.63102321  -9.60548081
  -9.60386214  -9.56682446  -9.49641157  -9.46838438  -9.45419413
  -9.37686032  -9.33083524  -9.3207827   -9.28663883  -9.24398838
  -9.22054043  -9.20352712  -9.19326137  -9.17387216  -9.15906172
  -9.15427352  -8.98175649  -8.93802219  -8.93222379  -8.87776937
  -8.84221182  -8.67936707  -8.6433267   -8.61784195  -8.57826377
  -8.57202317  -8.49793216  -8.15082638  -8.13935738  -8.12131594
  -8.11968795  -8.08091889  -8.02322976  -7.98414857  -7.9350814
  -7.91234732  -7.85762588  -7.65089898  -7.64506755  -7.60415018
  -7.58015691  -7.57539849  -7.55147958  -7.47396276  -7.47309267
  -7.40892209  -7.36755183  -7.36244313  -7.27484897  -7.10832736
  -7.07294326  -6.97871853  -6.95906356  -6.92344022  -6.89859496
  -6.77694649  -6.72461552  -6.72206384  -6.71997062  -6.68043877
  -6.67664909  -6.63402967  -6.61055787  -6.58156549  -6.57339439
  -6.55769488  -6.51820418  -6.09941013  -6.06194077  -5.96779051
  -5.90982008  -5.87180915  -5.84706623  -5.61579673  -5.58714519
  -5.58363881  -5.54032152  -5.3472021   -5.29019147  -5.24280701
  -5.19475761  -5.07848501  -5.02795798  -4.96756184  -4.96538494
  -4.89289296  -4.82757292  -4.82746883  -4.81600172  -4.63049542
  -4.57698155  -4.35851589  -4.33657128  -4.32177271  -4.31003276
  -4.230832    -4.19086158  -4.07194316  -4.03104862  -3.89906197
  -3.38446715  -3.3322555   -2.74096693  -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 0.011607037429298472, val_acc 0.99
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.046563379760646055, val_acc 0.995
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.08018112182617188, val_acc 0.995
trigger times: 1
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.006362178921699524, val_acc 0.995
trigger times: 1
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 1.9005508482456173, val_acc 0.995
trigger times: 1
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 1.4032261669635773, val_acc 0.99
trigger times: 1
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.03813598155975342, val_acc 0.995
trigger times: 1
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 44.77024841308594, val_acc 0.935
trigger times: 1
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 2.41619873046875, val_acc 0.99
trigger times: 1
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.014227672815322875, val_acc 0.995
trigger times: 1
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 88.94354187011719, val_acc 0.845
trigger times: 1
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -2191.856409549713 -51.11086407377863
1 -3051.9929975271225 -48.723750706751915
2 -3575.768190383911 -44.465298807522004
3 -2180.24010570487 -43.58405711344297
4 -3986.1545325815678 -42.65748859055198
5 -5141.188562273979 -41.12283699063089
6 -3437.185688495636 -39.90846007278091
7 -4014.7039095684886 -39.11210375046212
8 -1676.2979030013084 -37.86129653169397
9 -2026.708560988307 -36.12354062747346
10 -2960.266785621643 -34.83335159619024
11 -1955.941272675991 -32.774518645207735
12 -2190.5076672434807 -30.402902212388465
13 -1938.7894356343895 -29.13525869956722
14 -2349.8343431949615 -28.433748774939595
15 -1372.7177281277254 -27.29994968172231
16 -3030.4885835647583 -25.901848378876174
17 -2183.1212413311005 -24.180793753972072
18 -2899.4004516601562 -23.85559618089848
19 -2379.2080841064453 -22.631757024769293
20 -2379.5623741149902 -22.174817650932404
21 -2616.735076904297 -21.660331750182493
22 -1772.4644794464111 -21.22484726055165
23 -2056.9847259521484 -21.064656954738073
24 -2652.243896484375 -20.733951882908187
25 -2565.757598876953 -20.54780869726842
26 -1992.0099639892578 -20.21986301936262
27 -2420.684368133545 -20.113048387307664
28 -2525.3574409484863 -19.98788603525593
29 -2210.6885089874268 -19.687771366554934
30 -1840.470407485962 -19.312663057991184
31 -1692.088207244873 -19.196510544823035
32 -2251.8707389831543 -19.09200352372713
33 -1862.8366622924805 -18.76651715200028
34 -2190.58943939209 -18.66727006043425
35 -1782.8101902008057 -18.30680475928502
36 -1645.94136428833 -18.212601003547334
37 -1924.2350006103516 -17.85620012604348
38 -1615.4255142211914 -17.72206351371588
39 -2634.9343519210815 -17.66636739843018
40 -1905.0851421356201 -17.555772446769108
41 -1943.4081192016602 -17.39510553027295
42 -1975.9409275054932 -17.161109058978283
43 -2249.9176483154297 -17.088677374626354
44 -1700.7621269226074 -16.941136744673663
45 -2240.0909538269043 -16.70390724388583
46 -1640.8781833648682 -16.53256642227629
47 -1637.6507091522217 -16.427851826311485
48 -1525.7344226837158 -16.310043054430455
49 -1531.8325805664062 -16.072505286449037
50 -1505.0055656433105 -15.92367836179304
51 -1501.2244873046875 -15.815315392272375
52 -1538.3159465789795 -15.719773357335425
53 -1452.9873313903809 -15.525023951517204
54 -1064.6265232055448 -15.338182650137867
55 -2051.6638679504395 -15.02649709506045
56 -1273.1906719207764 -14.972783803822656
57 -1651.700647354126 -14.893410256497354
58 -1316.6637153625488 -14.81634260266616
59 -1238.237377166748 -14.716277772348088
60 -1376.8132095336914 -14.640153922460026
61 -1900.7302913665771 -14.434236147302084
62 -1381.6447734832764 -14.222802763648183
63 -1570.432077407837 -14.117727717817298
64 -949.9167997334152 -13.911649622206243
65 -1120.4205417633057 -13.754346771796104
66 -1077.3447370529175 -13.684434110863704
67 -1594.3136520385742 -13.467532512722281
68 -1185.4641723632812 -13.326135107923053
69 -1014.5939998626709 -13.16095840602929
70 -1564.5173988342285 -13.072049454954659
71 -1104.442684173584 -12.961326934690538
72 -843.8522939682007 -12.720804778769793
73 -1691.8633975982666 -12.55524170283173
74 -1462.2209243774414 -12.330146633939474
75 -1173.3196811676025 -12.196021382429088
76 -1136.7803440093994 -11.947687283951629
77 -1118.1060905456543 -11.792295122174025
78 -1254.9217624664307 -11.66112160483124
79 -1350.9460582733154 -11.550177764791247
80 -1059.4498558044434 -11.463909121126992
81 -1162.7945251464844 -11.183194969624676
82 -1061.5413646697998 -11.072736498972526
83 -1101.096284866333 -10.9356976290776
84 -1131.7716236114502 -10.779829439324157
85 -974.3130445480347 -10.687223612450175
86 -1268.3748149871826 -10.52701581083423
87 -1288.4383010864258 -10.270791843270539
88 -1207.2949028015137 -10.178676807445836
89 -913.4901247024536 -10.058570649114962
90 -1020.9381504058838 -9.960013161769803
91 -1227.9653930664062 -9.815868010044042
92 -1036.022008895874 -9.729846251979577
93 -1369.5819282531738 -9.60548080889025
94 -1219.138235092163 -9.468384378833608
95 -1356.9846420288086 -9.320782702176825
96 -985.1347999572754 -9.203527117964319
97 -826.443811416626 -9.154273524488499
98 -1090.8462867736816 -8.877769371842234
99 -1108.293664932251 -8.617841951052753
100 -1266.719835281372 -8.139357375976516
101 -730.9481658935547 -8.023229762081991
102 -727.592887878418 -7.857625875723534
103 -663.8204913139343 -7.580156905333177
104 -846.4426050186157 -7.473092668672553
105 -679.5166101455688 -7.274848973575834
106 -871.3844557749107 -6.959063561385431
107 -825.3948831558228 -6.724615516472137
108 -874.2030649185181 -6.676649092462118
109 -948.1259517669678 -6.5733943908649755
110 -702.1733226776123 -6.061940768427526
111 -651.2191476821899 -5.847066232035632
112 -881.0979499816895 -5.540321523427668
113 -800.9388179779053 -5.194757605593898
114 -482.77044105529785 -4.965384937497195
115 -535.222327709198 -4.816001724642262
116 -701.8012418746948 -4.3365712844889215
117 -711.6155223846436 -4.190861580324868
118 -436.3023681230843 -3.3844671463622564
119 -370.9258735179901 -1.9136196540088464
train accuracy: 0.9988888888888889
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -25.10113857 -24.82492217 -24.37443057 -24.18079375 -24.11517187
 -24.10704836 -23.99196367 -23.85559618 -23.84671267 -23.80110721
 -23.29102321 -22.63175702 -22.59530834 -22.40229356 -22.36019567
 -22.25673726 -22.17481765 -21.92543953 -21.92338003 -21.7139277
 -21.70254077 -21.67964655 -21.66033175 -21.3854157  -21.27381292
 -21.22769954 -21.22484726 -21.16103238 -21.12349614 -21.11248187
 -21.10191176 -21.06465695 -20.89318256 -20.88488686 -20.8247226
 -20.75809    -20.73395188 -20.67068012 -20.64532394 -20.62400298
 -20.62322    -20.58254596 -20.57241518 -20.55496752 -20.5478087
 -20.40287539 -20.39554951 -20.31337217 -20.21986302 -20.20423826
 -20.19083382 -20.14839891 -20.11304839 -20.09608855 -20.08314676
 -20.02804356 -19.98788604 -19.8859189  -19.86722444 -19.86030353
 -19.72288085 -19.6992776  -19.68777137 -19.60435553 -19.3963696
 -19.37672619 -19.31266306 -19.27597062 -19.26443554 -19.26334551
 -19.19651054 -19.1961232  -19.14314028 -19.11745812 -19.09200352
 -19.01832565 -18.94678246 -18.9263024  -18.91614563 -18.91300692
 -18.76651715 -18.74109978 -18.7299535  -18.71805502 -18.71555168
 -18.66966043 -18.66727006 -18.65799111 -18.56975936 -18.4755907
 -18.45471139 -18.45454304 -18.39024175 -18.30680476 -18.30178757
 -18.24353216 -18.23095197 -18.22811774 -18.212601   -18.1897973
 -18.18632012 -18.18409235 -18.12536478 -18.11146126 -18.051432
 -18.03391817 -17.9700688  -17.8575758  -17.85620013 -17.848403
 -17.84259535 -17.8029997  -17.74941668 -17.74698145 -17.72206351
 -17.71205165 -17.70340907 -17.6989796  -17.6663674  -17.64422174
 -17.58971226 -17.5822701  -17.55577245 -17.53447809 -17.4350675
 -17.41781108 -17.39510553 -17.35237361 -17.33478655 -17.30476607
 -17.26934492 -17.26663669 -17.26533128 -17.25215295 -17.1724259
 -17.16110906 -17.15795246 -17.12948493 -17.11153206 -17.10110308
 -17.08867737 -17.04663989 -17.04157538 -17.00651319 -16.9981261
 -16.94169195 -16.94139122 -16.94113674 -16.90579141 -16.89331433
 -16.78521689 -16.77644926 -16.74398113 -16.72058543 -16.71675936
 -16.70390724 -16.70065155 -16.59382175 -16.59087236 -16.56755081
 -16.53256642 -16.48462918 -16.44434518 -16.44255462 -16.42813352
 -16.42785183 -16.3964729  -16.37872874 -16.36275632 -16.34961457
 -16.34253724 -16.31004305 -16.23632585 -16.14358893 -16.13982043
 -16.09736232 -16.07250529 -16.06457088 -16.00177869 -16.00107617
 -15.99253949 -15.96333658 -15.92367836 -15.91511327 -15.89917914
 -15.88598634 -15.861028   -15.8324782  -15.81531539 -15.75942547
 -15.75169388 -15.72173399 -15.71977336 -15.68434988 -15.66526845
 -15.65250325 -15.64830979 -15.56444026 -15.52502395 -15.48447734
 -15.45861498 -15.37980369 -15.33818265 -15.32465257 -15.22888183
 -15.12488432 -15.08104874 -15.05876921 -15.04609384 -15.0264971
 -15.02200728 -15.01348234 -14.99675024 -14.9727838  -14.96004084
 -14.94930354 -14.92815424 -14.90902317 -14.89341026 -14.89031968
 -14.8882129  -14.84254054 -14.83244409 -14.8163426  -14.79128925
 -14.77006262 -14.7592488  -14.71627777 -14.7014109  -14.68824709
 -14.68015756 -14.67114322 -14.66499573 -14.64015392 -14.63363887
 -14.51659781 -14.48659431 -14.44457482 -14.43423615 -14.43169211
 -14.42264808 -14.3800265  -14.37407174 -14.36108582 -14.30198221
 -14.28854749 -14.24536192 -14.22280276 -14.1806428  -14.17635601
 -14.12885066 -14.11772772 -14.11538276 -14.08828621 -14.05563167
 -14.01205664 -13.91164962 -13.90134631 -13.85095004 -13.84361177
 -13.83216719 -13.7898653  -13.75434677 -13.72366989 -13.72216526
 -13.71997987 -13.71962022 -13.68443411 -13.600824   -13.54888348
 -13.51667391 -13.51096812 -13.48107249 -13.47852574 -13.46753251
 -13.4472251  -13.43031184 -13.42791675 -13.42778136 -13.40298654
 -13.37088364 -13.33848066 -13.32613511 -13.30140057 -13.29791877
 -13.26560738 -13.16095841 -13.11719923 -13.11160496 -13.10880878
 -13.1007203  -13.097551   -13.09697845 -13.07204945 -13.0688612
 -12.98159065 -12.9775137  -12.96132693 -12.84396769 -12.82321874
 -12.80762312 -12.78620264 -12.75742911 -12.74497734 -12.73293767
 -12.72080478 -12.7065766  -12.68986683 -12.64631452 -12.61940694
 -12.58016476 -12.5552417  -12.54006106 -12.48937457 -12.47596382
 -12.42450605 -12.33014663 -12.22562907 -12.20760896 -12.20735524
 -12.19602138 -12.17081371 -12.15632234 -12.13214179 -12.07165519
 -12.0398356  -12.0209785  -11.96659509 -11.96171005 -11.94768728
 -11.94652452 -11.8745299  -11.87399371 -11.87329056 -11.85011005
 -11.8018867  -11.79229512 -11.75914272 -11.73504365 -11.73134636
 -11.69189859 -11.6611216  -11.62542328 -11.62165486 -11.61918196
 -11.57101906 -11.56872012 -11.55017776 -11.54244403 -11.52428944
 -11.52351266 -11.50665747 -11.46390912 -11.33159127 -11.31090681
 -11.27222323 -11.27075503 -11.18319497 -11.16772629 -11.1179572
 -11.08542161 -11.0727365  -11.05527058 -11.05153808 -11.05038619
 -11.02526816 -10.98392269 -10.93569763 -10.93017881 -10.91617999
 -10.8582275  -10.82418728 -10.80074918 -10.78101609 -10.77982944
 -10.7573462  -10.72946334 -10.72875815 -10.72640051 -10.70670002
 -10.70297356 -10.68722361 -10.68641121 -10.64146107 -10.55139694
 -10.52701581 -10.47626793 -10.44809971 -10.37852653 -10.37296385
 -10.35148305 -10.27140567 -10.27079184 -10.2635315  -10.25866026
 -10.25256226 -10.21866477 -10.17867681 -10.17470226 -10.15843895
 -10.14859659 -10.12534842 -10.09210295 -10.05857065 -10.03129926
 -10.01271589  -9.96354758  -9.96001316  -9.94097327  -9.92729193
  -9.87678778  -9.86666785  -9.81586801  -9.79432765  -9.78594151
  -9.77591752  -9.72984625  -9.72366646  -9.69392233  -9.67427358
  -9.6740942   -9.63102321  -9.60548081  -9.60386214  -9.56682446
  -9.51288276  -9.49641157  -9.46838438  -9.45419413  -9.37686032
  -9.33083524  -9.3207827   -9.28663883  -9.25355068  -9.24398838
  -9.22054043  -9.20352712  -9.19326137  -9.19299786  -9.18064189
  -9.17387216  -9.15906172  -9.15427352  -9.06890179  -9.02169371
  -8.98175649  -8.93802219  -8.93222379  -8.87776937  -8.8730801
  -8.84221182  -8.67936707  -8.6433267   -8.61784195  -8.57826377
  -8.57202317  -8.49793216  -8.15082638  -8.13935738  -8.12131594
  -8.11968795  -8.08091889  -8.02322976  -7.98414857  -7.9350814
  -7.91234732  -7.85762588  -7.65089898  -7.64506755  -7.60415018
  -7.58015691  -7.57539849  -7.55147958  -7.47396276  -7.47309267
  -7.40892209  -7.36755183  -7.36244313  -7.27484897  -7.10832736
  -7.07294326  -6.97871853  -6.95906356  -6.92344022  -6.89859496
  -6.77694649  -6.72461552  -6.72206384  -6.71997062  -6.68043877
  -6.67664909  -6.63402967  -6.61055787  -6.58156549  -6.57339439
  -6.55769488  -6.51820418  -6.09941013  -6.06194077  -5.96779051
  -5.90982008  -5.87180915  -5.84706623  -5.61579673  -5.58714519
  -5.58363881  -5.54032152  -5.3472021   -5.29019147  -5.24280701
  -5.19475761  -5.07848501  -5.02795798  -4.96756184  -4.96538494
  -4.89289296  -4.82757292  -4.82746883  -4.81600172  -4.63049542
  -4.57698155  -4.35851589  -4.33657128  -4.32177271  -4.31003276
  -4.230832    -4.19086158  -4.07194316  -4.03104862  -3.89906197
  -3.38446715  -3.3322555   -2.74096693  -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:2
end of epoch 0: val_loss 1.814666517926611e-05, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 2.3066559379003593e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0019239994883537292, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.036085242032929725, val_acc 0.995
trigger times: 1
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 13: val_loss 1.44704442858696, val_acc 0.965
trigger times: 1
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.16160942554473878, val_acc 0.99
trigger times: 1
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 2.196264771099665, val_acc 0.96
trigger times: 1
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 25: val_loss 72.49123737104237, val_acc 0.865
trigger times: 1
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 42: val_loss 0.6649541014213537, val_acc 0.99
trigger times: 2
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 44: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.124427490234375, val_acc 0.995
trigger times: 1
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 2.8173035188112396, val_acc 0.98
trigger times: 1
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 0.6558013916015625, val_acc 0.995
trigger times: 1
end of epoch 59: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 60: val_loss 11.429868623018265, val_acc 0.95
trigger times: 1
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 63: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 64: val_loss 0.03694156885147095, val_acc 0.995
trigger times: 1
end of epoch 65: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 3.5154380020685494e-06, val_acc 1.0
trigger times: 1
end of epoch 78: val_loss 3.512521729469299, val_acc 0.965
trigger times: 2
end of epoch 79: val_loss 0.2904876708984375, val_acc 0.995
trigger times: 3
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 84: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 2.214423828125, val_acc 0.985
trigger times: 1
end of epoch 93: val_loss 2.97517457511276e-06, val_acc 1.0
trigger times: 2
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -3793.904037952423 -51.11086407377863
1 -5033.4069900512695 -47.242520801549375
2 -5127.774859428406 -44.1492199315879
3 -3251.0164833068848 -42.92113389854083
4 -8769.846844673157 -41.12283699063089
5 -5087.037434101105 -39.57032589487153
6 -12041.597646713257 -38.73424350253481
7 -3706.2510480880737 -36.307958906842664
8 -5281.216941833496 -34.83335159619024
9 -4063.9438242912292 -32.11238423004613
10 -3721.6074557304382 -29.87212515537898
11 -3817.6662702560425 -28.438710074595054
12 -2945.619050860405 -27.29994968172231
13 -4409.916091918945 -25.10113857425143
14 -4301.860336303711 -24.107048359972406
15 -3588.019702911377 -22.631757024769293
16 -3606.5751991271973 -22.174817650932404
17 -3446.1245727539062 -21.67964655316219
18 -3565.6149711608887 -21.22484726055165
19 -3660.9630088806152 -21.064656954738073
20 -4205.957336425781 -20.733951882908187
21 -3419.1341972351074 -20.582545961288048
22 -3622.1293601989746 -20.395549507226466
23 -4007.342929840088 -20.148398914808258
24 -3985.430709838867 -19.98788603525593
25 -3555.099437713623 -19.69927760369218
26 -3429.8307876586914 -19.312663057991184
27 -3414.0249099731445 -19.196123201283406
28 -3192.124439239502 -18.946782460741005
29 -3467.7774085998535 -18.7410997800197
30 -3288.073398590088 -18.66727006043425
31 -3200.273307800293 -18.45454303986138
32 -3376.8079986572266 -18.23095197097204
33 -2909.726963043213 -18.184092350554153
34 -2857.2069931030273 -17.970068795035335
35 -3036.997901916504 -17.80299969700979
36 -3246.3091888427734 -17.703409074507174
37 -3198.454414367676 -17.582270099264672
38 -3011.472812652588 -17.39510553027295
39 -2670.8023529052734 -17.266636694688245
40 -2819.4733085632324 -17.15795246429781
41 -2458.704815864563 -17.04663989169744
42 -3145.807571411133 -16.941391222911346
43 -2878.3560180664062 -16.776449261961407
44 -3019.4273567199707 -16.7006515531535
45 -2775.324996948242 -16.444345178778395
46 -2578.2302474975586 -16.378728743405894
47 -2010.1196117401123 -16.236325848903917
48 -2874.6163902282715 -16.064570880381925
49 -2919.233741760254 -15.92367836179304
50 -2448.341121673584 -15.832478197695652
51 -2560.3648719787598 -15.719773357335425
52 -2906.163528442383 -15.564440264837392
53 -2268.146686553955 -15.338182650137867
54 -2394.1418266296387 -15.058769214335191
55 -2232.296770095825 -14.996750243815644
56 -2574.4603729248047 -14.909023166597342
57 -2670.042263031006 -14.832444086090318
58 -2651.649761199951 -14.716277772348088
59 -2795.2195587158203 -14.664995734066178
60 -2939.9273948669434 -14.444574818788611
61 -2833.8759574890137 -14.374071740949763
62 -2359.593141555786 -14.222802763648183
63 -2365.328493118286 -14.115382758890235
64 -2586.9413452148438 -13.901346305146138
65 -2136.4922943115234 -13.754346771796104
66 -2028.000244140625 -13.684434110863704
67 -2181.045024871826 -13.48107249246075
68 -2620.9852714538574 -13.42791675139919
69 -2100.5531215667725 -13.326135107923053
70 -2495.1802406311035 -13.117199233121056
71 -2036.1976699829102 -13.096978454597288
72 -2073.113115310669 -12.961326934690538
73 -1959.6237182617188 -12.75742911197363
74 -2181.8132190704346 -12.689866825748549
75 -2766.002151489258 -12.48937456932423
76 -2359.4049911499023 -12.207608964237403
77 -2269.61635017395 -12.132141794891956
78 -2406.1995582580566 -11.96171005089843
79 -1700.5236511230469 -11.8732905611463
80 -2317.511514544487 -11.735043647109059
81 -2431.997926712036 -11.621654861816493
82 -1898.6976528167725 -11.542444033129573
83 -2019.8931217193604 -11.331591268227358
84 -2283.0070991516113 -11.167726287628158
85 -2109.415096282959 -11.051538084630891
86 -1860.9475002288818 -10.930178810541822
87 -2333.603848695755 -10.781016093040188
88 -1702.9205527305603 -10.726400508481433
89 -1767.439043045044 -10.641461066144862
90 -1463.1186627149582 -10.378526529704885
91 -2077.440465927124 -10.263531499368003
92 -2132.0302562713623 -10.174702255734323
93 -1665.8012771606445 -10.058570649114962
94 -1837.7523822784424 -9.940973270323955
95 -1883.3846836090088 -9.79432764708296
96 -2173.3908500671387 -9.69392233242313
97 -1962.6207523345947 -9.603862140658258
98 -1909.2649898529053 -9.454194130244643
99 -1468.5231164693832 -9.253550679992866
100 -1619.3551704883575 -9.19299786019511
101 -1724.0761351585388 -9.068901790019446
102 -1990.8459396362305 -8.877769371842234
103 -1843.9060077667236 -8.617841951052753
104 -2049.973934173584 -8.139357375976516
105 -1664.6198348999023 -7.935081396595392
106 -1511.862678527832 -7.604150183138894
107 -1664.7700901031494 -7.473092668672553
108 -2215.1348223935347 -7.108327355338034
109 -1521.422040939331 -6.8985949613137185
110 -1508.111915588379 -6.680438765523987
111 -1426.8650913238525 -6.5733943908649755
112 -1282.5870294570923 -5.967790505306058
113 -1353.1731185913086 -5.587145185329393
114 -1210.2694149017334 -5.242807011218389
115 -1320.3029975891113 -4.965384937497195
116 -1377.1223233938217 -4.63049541560991
117 -1170.7538714408875 -4.310032760188374
118 -1135.7021732330322 -3.899061970229749
119 -1181.3956761360168 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-51.11086407 -49.11584688 -48.97455067 -48.8722857  -48.72375071
 -47.2425208  -44.96921644 -44.85812386 -44.46529881 -44.43890163
 -44.14921993 -43.75035245 -43.58405711 -43.39691285 -43.22343613
 -42.9211339  -42.65748859 -42.6023149  -42.32863806 -42.30445768
 -41.12283699 -40.75815407 -40.70441037 -40.03758738 -39.90846007
 -39.57032589 -39.31833047 -39.27893976 -39.11210375 -39.0792282
 -38.7342435  -38.46237505 -37.86129653 -37.30806553 -36.41283474
 -36.30795891 -36.12354063 -36.01221343 -35.76166082 -35.55804095
 -34.8333516  -34.24472332 -33.78094921 -32.94911634 -32.77451865
 -32.11238423 -31.53427075 -30.66780209 -30.40290221 -30.23366087
 -29.87212516 -29.40604135 -29.1352587  -28.84050982 -28.57862903
 -28.43871007 -28.43374877 -28.16728518 -27.92060294 -27.7391148
 -27.29994968 -26.75229479 -25.93903491 -25.92997211 -25.90184838
 -25.10113857 -24.82492217 -24.37443057 -24.18079375 -24.11517187
 -24.10704836 -23.99196367 -23.85559618 -23.84671267 -23.80110721
 -23.29102321 -22.63175702 -22.59530834 -22.40229356 -22.36019567
 -22.25673726 -22.17481765 -21.92543953 -21.92338003 -21.7139277
 -21.70254077 -21.67964655 -21.66033175 -21.3854157  -21.27381292
 -21.22769954 -21.22484726 -21.16103238 -21.12349614 -21.11248187
 -21.10191176 -21.06465695 -20.89318256 -20.88488686 -20.8247226
 -20.75809    -20.73395188 -20.67068012 -20.64532394 -20.62400298
 -20.62322    -20.58254596 -20.57241518 -20.55496752 -20.5478087
 -20.40287539 -20.39554951 -20.31337217 -20.22404521 -20.21986302
 -20.20423826 -20.19083382 -20.14839891 -20.11304839 -20.11197442
 -20.09608855 -20.08314676 -20.02804356 -19.98788604 -19.8859189
 -19.86722444 -19.86030353 -19.72288085 -19.70977269 -19.6992776
 -19.68777137 -19.60435553 -19.3963696  -19.37672619 -19.31266306
 -19.27597062 -19.26443554 -19.26334551 -19.19651054 -19.1961232
 -19.18014748 -19.14314028 -19.11745812 -19.09200352 -19.06034516
 -19.01832565 -18.94678246 -18.9263024  -18.91614563 -18.91300692
 -18.87654565 -18.76651715 -18.74109978 -18.7299535  -18.71805502
 -18.71555168 -18.66966043 -18.66727006 -18.65799111 -18.62607501
 -18.58222767 -18.56975936 -18.560984   -18.4755907  -18.45471139
 -18.45454304 -18.39024175 -18.35721475 -18.30680476 -18.30178757
 -18.24353216 -18.23095197 -18.22811774 -18.212601   -18.1897973
 -18.18632012 -18.18409235 -18.12536478 -18.11146126 -18.051432
 -18.05104791 -18.03391817 -18.01731731 -17.9700688  -17.90456057
 -17.8575758  -17.85620013 -17.848403   -17.84259535 -17.8029997
 -17.74941668 -17.74698145 -17.72206351 -17.71205165 -17.70340907
 -17.6989796  -17.6663674  -17.65885147 -17.64422174 -17.58971226
 -17.5822701  -17.55615528 -17.55577245 -17.53447809 -17.50816276
 -17.4350675  -17.41781108 -17.39510553 -17.35237361 -17.33478655
 -17.32717165 -17.32345192 -17.30476607 -17.26934492 -17.26663669
 -17.26533128 -17.25215295 -17.24093566 -17.1724259  -17.16110906
 -17.15795246 -17.12948493 -17.11153206 -17.10110308 -17.08867737
 -17.04663989 -17.04157538 -17.00651319 -16.9981261  -16.94169195
 -16.94139122 -16.94113674 -16.90579141 -16.90387439 -16.89331433
 -16.78521689 -16.77644926 -16.74398113 -16.72058543 -16.71675936
 -16.70390724 -16.70065155 -16.66271622 -16.65168432 -16.63155615
 -16.59382175 -16.59087236 -16.5720766  -16.56755081 -16.53256642
 -16.48462918 -16.44434518 -16.44255462 -16.42813352 -16.42785183
 -16.42267399 -16.3964729  -16.37872874 -16.36275632 -16.34961457
 -16.34253724 -16.31004305 -16.23632585 -16.14358893 -16.13982043
 -16.09736232 -16.07250529 -16.06457088 -16.04011649 -16.00177869
 -16.00107617 -15.99253949 -15.96333658 -15.92367836 -15.91511327
 -15.89917914 -15.88598634 -15.861028   -15.8324782  -15.81531539
 -15.81319275 -15.76660816 -15.75942547 -15.75169388 -15.72173399
 -15.71977336 -15.70552098 -15.6950765  -15.68434988 -15.66526845
 -15.65250325 -15.64830979 -15.56444026 -15.52502395 -15.48447734
 -15.45861498 -15.37980369 -15.33818265 -15.32465257 -15.2814595
 -15.22888183 -15.22821342 -15.12488432 -15.08104874 -15.05876921
 -15.04609384 -15.0264971  -15.02200728 -15.01348234 -14.99675024
 -14.9727838  -14.96004084 -14.94930354 -14.92815424 -14.92561878
 -14.90902317 -14.89341026 -14.89031968 -14.8882129  -14.84254054
 -14.83244409 -14.8163426  -14.79128925 -14.77006262 -14.7592488
 -14.71627777 -14.7014109  -14.68824709 -14.68015756 -14.67114322
 -14.66499573 -14.64015392 -14.63363887 -14.569345   -14.51659781
 -14.48659431 -14.44457482 -14.43423615 -14.43169211 -14.42264808
 -14.3800265  -14.37407174 -14.36108582 -14.30198221 -14.28854749
 -14.26398157 -14.24536192 -14.22280276 -14.21506676 -14.1806428
 -14.17635601 -14.12885066 -14.11772772 -14.11538276 -14.08828621
 -14.05563167 -14.01205664 -13.94458214 -13.91164962 -13.90134631
 -13.893996   -13.85095004 -13.84361177 -13.83216719 -13.7898653
 -13.75434677 -13.72366989 -13.72216526 -13.71997987 -13.71962022
 -13.69680804 -13.68443411 -13.600824   -13.54888348 -13.51667391
 -13.51096812 -13.48107249 -13.47852574 -13.46753251 -13.4472251
 -13.43031184 -13.42791675 -13.42778136 -13.40298654 -13.37088364
 -13.33848066 -13.33131492 -13.32613511 -13.30140057 -13.29791877
 -13.26560738 -13.16095841 -13.11719923 -13.11160496 -13.10880878
 -13.1007203  -13.097551   -13.09697845 -13.07204945 -13.0688612
 -13.05367568 -13.0196604  -12.98159065 -12.9775137  -12.96132693
 -12.9486081  -12.87746729 -12.87208614 -12.84396769 -12.82321874
 -12.80762312 -12.80031276 -12.78620264 -12.75742911 -12.74497734
 -12.73293767 -12.72080478 -12.7065766  -12.68986683 -12.67564348
 -12.64631452 -12.61940694 -12.61401154 -12.58016476 -12.57189976
 -12.5552417  -12.54006106 -12.52166974 -12.49248277 -12.48937457
 -12.47596382 -12.42450605 -12.33014663 -12.30472594 -12.22562907
 -12.20760896 -12.20735524 -12.19602138 -12.17081371 -12.15632234
 -12.13214179 -12.07165519 -12.0398356  -12.0209785  -11.99581994
 -11.96659509 -11.96171005 -11.94768728 -11.94652452 -11.91402171
 -11.88110874 -11.8809712  -11.8745299  -11.87399371 -11.87329056
 -11.85011005 -11.8018867  -11.79229512 -11.75914272 -11.73504365
 -11.73134636 -11.69189859 -11.6611216  -11.62542328 -11.62165486
 -11.61918196 -11.57101906 -11.56872012 -11.55017776 -11.54244403
 -11.52428944 -11.52351266 -11.51820127 -11.50665747 -11.46390912
 -11.33159127 -11.31090681 -11.27222323 -11.27075503 -11.22649873
 -11.18319497 -11.16772629 -11.1179572  -11.11257916 -11.08542161
 -11.08430025 -11.08396514 -11.0727365  -11.05527058 -11.05153808
 -11.05038619 -11.02526816 -10.99671254 -10.98392269 -10.93569763
 -10.93017881 -10.91617999 -10.8582275  -10.82418728 -10.80622708
 -10.80074918 -10.78101609 -10.77982944 -10.7573462  -10.72946334
 -10.72875815 -10.72640051 -10.70670002 -10.70297356 -10.69479701
 -10.68722361 -10.68641121 -10.64146107 -10.57977264 -10.55139694
 -10.52701581 -10.50827243 -10.47626793 -10.44809971 -10.37852653
 -10.37296385 -10.35148305 -10.34028654 -10.27140567 -10.27079184
 -10.2635315  -10.25866026 -10.25256226 -10.21866477 -10.19324819
 -10.18320042 -10.17867681 -10.17470226 -10.15843895 -10.14859659
 -10.12534842 -10.12359887 -10.09210295 -10.05857065 -10.03129926
 -10.01271589  -9.96354758  -9.96001316  -9.95141585  -9.94097327
  -9.92729193  -9.87678778  -9.86666785  -9.86613106  -9.81586801
  -9.80870547  -9.79432765  -9.78953673  -9.78594151  -9.77591752
  -9.72984625  -9.72366646  -9.69392233  -9.68764774  -9.67427358
  -9.6740942   -9.63102321  -9.60548081  -9.60386214  -9.56682446
  -9.56613376  -9.55479674  -9.51288276  -9.49641157  -9.49152879
  -9.4729357   -9.46838438  -9.45419413  -9.40633445  -9.37686032
  -9.33083524  -9.3207827   -9.28663883  -9.25355068  -9.24398838
  -9.22358849  -9.22054043  -9.20352712  -9.19326137  -9.19299786
  -9.18064189  -9.17387216  -9.15906172  -9.15427352  -9.06890179
  -9.02169371  -8.98175649  -8.93820548  -8.93802219  -8.93222379
  -8.92700445  -8.89017247  -8.87776937  -8.8730801   -8.84675317
  -8.84221182  -8.77735022  -8.67936707  -8.6433267   -8.62574643
  -8.61784195  -8.58827136  -8.57826377  -8.57202317  -8.52653872
  -8.50934906  -8.49793216  -8.48368746  -8.23633009  -8.22573036
  -8.17273478  -8.15082638  -8.13935738  -8.12131594  -8.11968795
  -8.11376578  -8.08091889  -8.07712894  -8.02322976  -7.98414857
  -7.9350814   -7.91234732  -7.88812279  -7.85762588  -7.83755094
  -7.65089898  -7.64506755  -7.60415018  -7.58015691  -7.57539849
  -7.55147958  -7.47396276  -7.47309267  -7.46669429  -7.43478612
  -7.40892209  -7.36755183  -7.36244313  -7.27484897  -7.10832736
  -7.07294326  -6.98775871  -6.97871853  -6.95906356  -6.92344022
  -6.89859496  -6.82591809  -6.77694649  -6.74684377  -6.74604151
  -6.72461552  -6.72206384  -6.71997062  -6.68232818  -6.68043877
  -6.67664909  -6.65690142  -6.63402967  -6.61055787  -6.58156549
  -6.57339439  -6.55769488  -6.54236633  -6.51820418  -6.25643125
  -6.22767566  -6.09941013  -6.06194077  -5.96779051  -5.90982008
  -5.87180915  -5.84706623  -5.84072079  -5.78829552  -5.77971906
  -5.75992771  -5.70280385  -5.61579673  -5.59676059  -5.58714519
  -5.58363881  -5.54032152  -5.40818143  -5.3472021   -5.29019147
  -5.24280701  -5.19475761  -5.13647073  -5.07848501  -5.02795798
  -5.02548863  -4.96756184  -4.96538494  -4.89289296  -4.87497962
  -4.82757292  -4.82746883  -4.81600172  -4.63049542  -4.57698155
  -4.40689082  -4.35851589  -4.33657128  -4.32177271  -4.31003276
  -4.230832    -4.19086158  -4.07194316  -4.03104862  -3.89906197
  -3.38446715  -3.3322555   -2.74096693  -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10112
Number of trainable paramters: 10112
device: cuda:1
end of epoch 0: val_loss 1.3387485699846677, val_acc 0.91
trigger times: 0
saving model weights...
end of epoch 1: val_loss 4.768369308294495e-09, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0005150145664811135, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.2103813812043518, val_acc 0.99
trigger times: 2
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.8607061767578125, val_acc 0.99
trigger times: 1
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.1515843963623047, val_acc 0.995
trigger times: 1
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.31385154724121095, val_acc 0.99
trigger times: 1
end of epoch 13: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 15: val_loss 1.891908665248193, val_acc 0.975
trigger times: 1
end of epoch 16: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.03158489108085519, val_acc 0.995
trigger times: 1
end of epoch 25: val_loss 41.457947127222816, val_acc 0.86
trigger times: 2
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 30: val_loss 2.933202555179596, val_acc 0.975
trigger times: 1
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 34: val_loss 91.98910354614257, val_acc 0.815
trigger times: 1
end of epoch 35: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 36: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 1
end of epoch 37: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 41: val_loss 3.218640358682023e-08, val_acc 1.0
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 43: val_loss 0.00016160746513200764, val_acc 1.0
trigger times: 1
end of epoch 44: val_loss 1.7221389069296107, val_acc 0.955
trigger times: 2
end of epoch 45: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 47: val_loss 5.214475095236537e-06, val_acc 1.0
trigger times: 1
end of epoch 48: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 51: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 52: val_loss 0.030645948648421246, val_acc 0.99
trigger times: 1
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 55: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 58: val_loss 2.41759437084198, val_acc 0.975
trigger times: 1
end of epoch 59: val_loss 0.0213451886177063, val_acc 0.995
trigger times: 2
end of epoch 60: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 62: val_loss 0.02726344347000122, val_acc 0.995
trigger times: 1
end of epoch 63: val_loss 0.3905769348144531, val_acc 0.995
trigger times: 2
end of epoch 64: val_loss 3.2508216547966002, val_acc 0.96
trigger times: 3
end of epoch 65: val_loss 0.00010356021113693713, val_acc 1.0
trigger times: 4
end of epoch 66: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 69: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 70: val_loss 13.603280415534973, val_acc 0.94
trigger times: 1
end of epoch 71: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 72: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 73: val_loss 0.38642578125, val_acc 0.995
trigger times: 1
end of epoch 74: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 77: val_loss 0.5775363159179687, val_acc 0.995
trigger times: 1
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 81: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 82: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 83: val_loss 1.073392333984375, val_acc 0.99
trigger times: 1
end of epoch 84: val_loss 7.271713911904953e-08, val_acc 1.0
trigger times: 2
end of epoch 85: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 88: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 89: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 90: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 91: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 93: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 95: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 97: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Finished training.
0 -960.3120631547645 -51.11086407377863
1 -1572.4733789265156 -44.969216444406115
2 -1311.211770126596 -43.58405711344297
3 -1308.2740026074462 -42.3286380570152
4 -1120.7864231844433 -39.90846007278091
5 -3299.0752928219736 -38.73424350253481
6 -1551.24707198143 -36.12354062747346
7 -966.796167563647 -33.78094921339959
8 -781.6253151725978 -30.402902212388465
9 -637.1871959118871 -28.578629026453186
10 -822.405051083304 -27.29994968172231
11 -836.5224084854126 -24.824922167879542
12 -815.1711149215698 -23.846712666233493
13 -825.638261795044 -22.360195666637367
14 -749.2890218119137 -21.702540774014164
15 -1089.2483234405518 -21.22484726055165
16 -975.2963924407959 -20.89318256129012
17 -743.0317039489746 -20.645323938416784
18 -763.1857995986938 -20.54780869726842
19 -1072.6974067687988 -20.204238264788124
20 -1056.0230712890625 -20.083146764896348
21 -727.4092524070293 -19.722880852611173
22 -688.5542125701904 -19.376726187999814
23 -860.6719980239868 -19.196123201283406
24 -709.9353971481323 -19.018325651099442
25 -879.7391891479492 -18.76651715200028
26 -665.030345916748 -18.66727006043425
27 -673.9067335128784 -18.47559070494264
28 -865.4000673294067 -18.301787569114474
29 -696.6213893890381 -18.186320118437038
30 -750.1004486083984 -18.03391817269105
31 -676.2476463317871 -17.84840300474541
32 -538.557416054653 -17.712051650475324
33 -1068.557321548462 -17.589712263068826
34 -930.3836498260498 -17.435067502049318
35 -645.7706880569458 -17.323451919424222
36 -505.14639806747437 -17.172425900238256
37 -668.8119564056396 -17.088677374626354
38 -870.2919130325317 -16.941391222911346
39 -664.5664520263672 -16.776449261961407
40 -650.6530327796936 -16.662716219346578
41 -545.2640075683594 -16.567550814032472
42 -732.8101091384888 -16.427851826311485
43 -851.3084526062012 -16.3425372356379
44 -712.1753988265991 -16.072505286449037
45 -842.0867614746094 -15.963336575462456
46 -484.76341342926025 -15.832478197695652
47 -702.487795829773 -15.721733988613988
48 -556.814549446106 -15.652503246999716
49 -804.5447177886963 -15.379803687381434
50 -636.8512334823608 -15.124884320715882
51 -650.2293729782104 -15.01348233511118
52 -572.8275055885315 -14.92561878348465
53 -555.6116108894348 -14.832444086090318
54 -759.4602565765381 -14.701410896461134
55 -693.7508630752563 -14.633638870634321
56 -863.7144064903259 -14.431692106581384
57 -497.58079719543457 -14.288547491278662
58 -562.8129925727844 -14.176356011184094
59 -441.62385177612305 -14.012056641802321
60 -533.3226623535156 -13.83216719043474
61 -519.0876388549805 -13.71962021746306
62 -757.8874742984772 -13.510968121888645
63 -594.9859728813171 -13.42791675139919
64 -455.76744508743286 -13.326135107923053
65 -309.69769394397736 -13.11160496296268
66 -516.7650418281555 -13.068861200338022
67 -344.6642303466797 -12.94860810213474
68 -546.2773895263672 -12.8003127565509
69 -780.9276843070984 -12.70657659764471
70 -722.1574923992157 -12.580164756567658
71 -487.36007165908813 -12.48937456932423
72 -584.6958503723145 -12.207608964237403
73 -558.6258158683777 -12.071655190087315
74 -580.0471305847168 -11.947687283951629
75 -483.77966022491455 -11.873993709656798
76 -346.1859605355421 -11.735043647109059
77 -605.0192754268646 -11.61918195864648
78 -458.47421884536743 -11.523512659238419
79 -556.4519896507263 -11.272223233121878
80 -411.5610963702202 -11.112579160942746
81 -416.00385665893555 -11.051538084630891
82 -338.53433775901794 -10.930178810541822
83 -392.4666510950774 -10.781016093040188
84 -503.83335971832275 -10.702973557897709
85 -480.85636806488037 -10.55139693923155
86 -496.2806282043457 -10.372963850816022
87 -502.32943964004517 -10.258660258154073
88 -308.29075396060944 -10.174702255734323
89 -302.3282334804535 -10.058570649114962
90 -293.9321141242981 -9.940973270323955
91 -297.41334652900696 -9.808705467646197
92 -300.91165590286255 -9.723666457314014
93 -359.97720289230347 -9.60548080889025
94 -366.0535306930542 -9.496411571497996
95 -264.34416937828064 -9.376860321099457
96 -287.1800258755684 -9.22358849356847
97 -292.10418176651 -9.173872158601808
98 -281.69397131353617 -8.9382054789389
99 -313.49305632710457 -8.873080098446843
100 -186.14895663037896 -8.625746434132509
101 -264.75106489658356 -8.509349059239506
102 -322.52838945388794 -8.150826375179872
103 -255.99637557566166 -8.0771289416021
104 -355.45566296577454 -7.857625875723534
105 -393.8337046895176 -7.57539849177145
106 -220.5892722606659 -7.408922094898468
107 -127.02915461314842 -6.9877587117887545
108 -71.10019542090595 -6.74684377334232
109 -178.80699995160103 -6.680438765523987
110 -162.11002838145941 -6.5733943908649755
111 -156.62021154165268 -6.0994101310349205
112 -127.70677877753042 -5.840720792248756
113 -144.93657674081624 -5.596760589951263
114 -79.69218053948134 -5.290191465256995
115 -89.07082407921553 -5.025488634288654
116 -85.46472936682403 -4.827468826279823
117 -93.4526139497757 -4.3365712844889215
118 -438.9500659322366 -4.031048624093466
119 -171.13367381563876 -1.9136196540088464
train accuracy: 0.9988888888888889
validation accuracy: 1.0

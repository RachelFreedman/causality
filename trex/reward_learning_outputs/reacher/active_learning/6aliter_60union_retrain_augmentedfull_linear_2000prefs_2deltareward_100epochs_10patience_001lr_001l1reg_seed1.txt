[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.032727123908030224, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.3220e-02, -9.0720e-02,  5.7811e-02, -3.4872e-02,  4.1760e-02,
         -4.0261e-02, -2.2755e-04,  7.1705e-04, -1.5105e-01,  9.8886e-02,
         -1.4320e-03, -1.3618e+00, -2.1502e+00]], device='cuda:1'))])
end of epoch 1: val_loss 0.17904297172125322, val_acc 0.96
trigger times: 1
end of epoch 2: val_loss 0.0038863149204708238, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.5059e-02, -3.2161e-02, -1.1276e-01,  4.3662e-03,  1.3647e-01,
          7.4372e-02, -4.9008e-03, -8.3243e-04, -4.5461e-02, -1.2019e-01,
         -1.4320e-03, -2.2088e+00, -4.1665e+00]], device='cuda:1'))])
end of epoch 3: val_loss 0.4516456470595567, val_acc 0.955
trigger times: 1
end of epoch 4: val_loss 0.00021930640166516467, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.0745e-06, -4.4281e-02,  6.3362e-02, -1.6925e-02,  8.5904e-02,
         -2.7341e-05, -7.5419e-03, -1.1955e-02, -2.6210e-01,  3.8035e-03,
         -1.4320e-03, -2.9397e+00, -6.1982e+00]], device='cuda:1'))])
end of epoch 5: val_loss 0.0007377377262501383, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.4185371620004284, val_acc 0.965
trigger times: 2
end of epoch 7: val_loss 0.028055171846895065, val_acc 0.99
trigger times: 3
end of epoch 8: val_loss 0.17226156352164065, val_acc 0.97
trigger times: 4
end of epoch 9: val_loss 0.11042230385658652, val_acc 0.985
trigger times: 5
end of epoch 10: val_loss 2.4375371936535827e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7091e-02, -5.1175e-02, -3.2550e-02, -1.8457e-01,  2.4930e-01,
          1.6922e-01, -2.9479e-02,  8.0760e-02, -1.5421e-01, -5.4380e-01,
         -1.4320e-03, -4.9963e+00, -9.0399e+00]], device='cuda:1'))])
end of epoch 11: val_loss 0.0032246775441930353, val_acc 1.0
trigger times: 1
end of epoch 12: val_loss 0.2412379516299915, val_acc 0.98
trigger times: 2
end of epoch 13: val_loss 0.0026992236790635716, val_acc 1.0
trigger times: 3
end of epoch 14: val_loss 0.19981366306526963, val_acc 0.985
trigger times: 4
end of epoch 15: val_loss 0.0882829359418001, val_acc 0.985
trigger times: 5
end of epoch 16: val_loss 0.032042255996488474, val_acc 0.99
trigger times: 6
end of epoch 17: val_loss 0.014042962566004355, val_acc 0.995
trigger times: 7
end of epoch 18: val_loss 0.016461672186728185, val_acc 0.99
trigger times: 8
end of epoch 19: val_loss 0.14909284214199858, val_acc 0.985
trigger times: 9
end of epoch 20: val_loss 3.796404565374445e-05, val_acc 1.0
trigger times: 10
Early stopping.
0 -526.6344475746155 -54.98547503240923
1 -495.0041534900665 -50.492268601198035
2 -485.56030917167664 -50.03933801517046
3 -480.5951633453369 -49.75347184620696
4 -476.4560034275055 -49.72654640753777
5 -450.213002204895 -46.98011874490918
6 -473.2923312187195 -45.7351542845057
7 -459.22508895397186 -45.670579884154705
8 -457.6153426170349 -44.99030608142343
9 -471.59953451156616 -44.14602409201361
10 -441.47581481933594 -43.81326882122305
11 -451.71320366859436 -43.18878399086166
12 -457.40027809143066 -42.29180714825394
13 -442.75286412239075 -42.00401746161006
14 -450.1895520687103 -41.6910044370425
15 -434.49008417129517 -41.68588229294918
16 -426.5460307598114 -41.281777102712205
17 -432.36124897003174 -40.44278203413966
18 -431.18101465702057 -40.34838365523108
19 -411.99896335601807 -39.599701153458774
20 -416.17399406433105 -39.57586365327889
21 -426.7991645336151 -39.31972693233231
22 -407.50340127944946 -39.024610555047154
23 -404.20364820957184 -38.45534493538269
24 -398.27021074295044 -38.41270390343083
25 -415.6063494682312 -38.35634328077039
26 -402.8961341381073 -37.79713616772368
27 -407.0802631378174 -37.741528994987384
28 -409.7376300096512 -37.66475323879293
29 -400.14261186122894 -37.513139380385574
30 -428.5871567726135 -37.1809993033689
31 -386.5373750925064 -37.100703136010694
32 -395.29839181900024 -37.00630588930485
33 -404.7031627893448 -36.821916772458344
34 -388.6876721382141 -36.48799015296732
35 -383.83852458000183 -36.20965269874363
36 -391.4970969259739 -36.19207561676116
37 -402.58853340148926 -36.114459029559086
38 -391.05352425575256 -35.78149902167743
39 -370.63596737384796 -35.394503873250635
40 -401.4017870426178 -35.26282499693737
41 -391.70108103752136 -35.24303541418371
42 -385.53429514169693 -35.209705244501436
43 -394.5475492477417 -35.0654408505187
44 -365.2510882616043 -34.80241747531743
45 -371.54418671131134 -34.64469044638467
46 -362.2306514978409 -33.84284985953318
47 -357.24656677246094 -32.70706485357069
48 -350.7003700733185 -31.969099402548657
49 -345.75880432128906 -31.7109134007892
50 -355.14080286026 -31.64414355845032
51 -346.2427486181259 -31.392382758954444
52 -349.49405550956726 -31.223196019713853
53 -340.96684634685516 -31.12953085092458
54 -339.729336977005 -29.39157139549552
55 -349.38048100471497 -29.340125609942326
56 -329.08843636512756 -29.106189988903285
57 -322.38211476802826 -27.41102349748205
58 -315.66623961925507 -27.343722362182305
59 -314.6630690097809 -27.196681629483837
60 -320.84689569473267 -27.07399028854534
61 -315.27046823501587 -26.7047217556024
62 -310.9728009700775 -26.244794902859052
63 -299.7932469844818 -25.548365085275513
64 -297.1623947620392 -25.45878528601009
65 -308.7473953962326 -24.879106999799365
66 -289.34814661741257 -24.828695359328833
67 -301.07697319984436 -24.592745144504722
68 -290.51691591739655 -23.978745577896312
69 -288.0441896915436 -23.57262108435893
70 -276.72999209165573 -23.44970807952351
71 -287.13964116573334 -22.745309160183492
72 -285.7425025701523 -22.60679894414887
73 -278.615270614624 -22.19891031871716
74 -267.54886052012444 -20.656863763892378
75 -264.4756991863251 -20.444472560731253
76 -252.14127624034882 -20.19699010077007
77 -260.66534888744354 -20.13839114930498
78 -256.56386893987656 -19.63760343800059
79 -256.70623940229416 -19.515598718228343
80 -235.4581128358841 -18.92838809611677
81 -256.25706642866135 -17.994774057192853
82 -223.43410527706146 -17.55742370467821
83 -228.82185435295105 -16.823073927842348
84 -196.9216924905777 -14.855082803515382
85 -199.04987689852715 -14.531424598833084
86 -202.5920478105545 -14.442420089224363
87 -190.44278353452682 -13.596012850960644
88 -170.04192823171616 -12.68135972540495
89 -182.90688237547874 -12.66418205637357
90 -176.88293155282736 -12.30017947419658
91 -183.92180556058884 -12.151904772081672
92 -175.01281017065048 -11.788852141676486
93 -177.7759546637535 -10.869989101210326
94 -158.67176635563374 -10.327681503524177
95 -145.77646505832672 -9.8572159761571
96 -134.76346122473478 -8.330116995310416
97 -131.93629671633244 -8.133195842510668
98 -136.49434623122215 -8.108197691178031
99 -125.82721781730652 -7.57539849177145
100 -119.99948564171791 -7.362443126623615
101 -115.92170985043049 -7.108327355338034
102 -111.10211297869682 -6.959063561385431
103 -120.76011335849762 -6.776946485018116
104 -111.89542110264301 -6.7220638398623045
105 -122.41627949476242 -6.719970621583102
106 -129.48275449872017 -6.535447341844848
107 -116.50649318099022 -6.51820418055673
108 -109.59694369137287 -5.615796733870542
109 -101.6192262172699 -5.34720210027791
110 -94.23934972286224 -5.078485007852753
111 -96.76275583356619 -5.027957977402961
112 -95.23598848283291 -4.827572916892203
113 -89.15298511087894 -4.63049541560991
114 -93.49477818608284 -4.230832004686763
115 -90.92378092929721 -4.031048624093466
116 -77.31037566065788 -3.3844671463622564
117 -82.91835111379623 -3.3322555012187633
118 -77.15857565402985 -2.6416623314910934
119 -64.21933696046472 -1.9136196540088464
train accuracy: 0.9983333333333333
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
 -10.15899891 -10.09806044 -10.09122004 -10.08536919 -10.0754445
  -9.85924034  -9.85721598  -9.69621171  -9.62670303  -9.45392504
  -9.29898321  -9.2215626   -9.22147332  -8.70191472  -8.46452337
  -8.39841254  -8.330117    -8.30241719  -8.2011814   -8.13319584
  -8.10819769  -7.93829225  -7.91468595  -7.80457905  -7.78565603
  -7.71006648  -7.61778087  -7.57539849  -7.36244313  -7.30583892
  -7.10832736  -7.09853626  -7.02300895  -6.95906356  -6.77694649
  -6.72206384  -6.71997062  -6.53544734  -6.51820418  -6.45418135
  -6.37463142  -6.22808808  -6.14145732  -6.01874197  -5.94878346
  -5.92392747  -5.90783816  -5.89682875  -5.89157976  -5.88784483
  -5.61758964  -5.61579673  -5.49423459  -5.41198086  -5.3472021
  -5.29867478  -5.28116024  -5.24864356  -5.12016279  -5.11072935
  -5.07848501  -5.02795798  -4.95100524  -4.82757292  -4.81762061
  -4.74776382  -4.73862009  -4.65212029  -4.63049542  -4.54683159
  -4.50492499  -4.41325899  -4.230832    -4.03104862  -3.78651875
  -3.70896826  -3.60946381  -3.38446715  -3.36771077  -3.3322555
  -2.64166233  -2.50527906  -2.17451294  -2.04347985  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 1.2524952864368817, val_acc 0.9
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.1429e-03,  5.0506e-02, -1.3895e-01, -3.4966e-02,  7.9873e-02,
         -1.7117e-01, -4.6933e-02, -1.2466e-02, -1.2043e-02,  7.8844e-02,
         -1.4320e-03, -1.6307e+00, -1.7964e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.005951936104194359, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.9701e-02,  7.6311e-03,  7.3154e-02,  5.1609e-02,  1.1973e-01,
          2.0949e-01, -5.3810e-04, -2.3339e-02,  3.8970e-02,  4.1376e-03,
         -1.4320e-03, -2.2689e+00, -2.8168e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.12810436129778335, val_acc 0.98
trigger times: 1
end of epoch 3: val_loss 0.0914671850795169, val_acc 0.98
trigger times: 2
end of epoch 4: val_loss 0.5559344702409421, val_acc 0.985
trigger times: 3
end of epoch 5: val_loss 0.06348914629120926, val_acc 0.995
trigger times: 4
end of epoch 6: val_loss 0.2301747675002842, val_acc 0.975
trigger times: 5
end of epoch 7: val_loss 0.15512640165781363, val_acc 0.985
trigger times: 6
end of epoch 8: val_loss 0.04950160264279216, val_acc 0.99
trigger times: 7
end of epoch 9: val_loss 0.09774817704002398, val_acc 0.99
trigger times: 8
end of epoch 10: val_loss 1.2212287368187844, val_acc 0.945
trigger times: 9
end of epoch 11: val_loss 0.18724425937233719, val_acc 0.995
trigger times: 10
Early stopping.
0 -400.6397297382355 -54.98547503240923
1 -368.15938675403595 -50.03933801517046
2 -360.26052618026733 -49.75347184620696
3 -319.6399337053299 -46.98011874490918
4 -349.7349548339844 -45.7351542845057
5 -333.17625999450684 -44.99030608142343
6 -356.6093159914017 -44.14602409201361
7 -334.9153838157654 -43.18878399086166
8 -336.0677146911621 -42.29180714825394
9 -321.0586462020874 -41.6910044370425
10 -309.86856043338776 -41.68588229294918
11 -312.8134641647339 -40.44278203413966
12 -308.27001774311066 -40.34838365523108
13 -307.347495675087 -39.57586365327889
14 -324.588937997818 -39.31972693233231
15 -302.8792494535446 -38.45534493538269
16 -285.38447070121765 -38.41270390343083
17 -297.3633596301079 -37.79713616772368
18 -312.23388719558716 -37.741528994987384
19 -301.1281228065491 -37.513139380385574
20 -305.8220932483673 -37.1809993033689
21 -289.8904150724411 -37.00630588930485
22 -294.2839810848236 -36.821916772458344
23 -281.8860971927643 -36.20965269874363
24 -291.7074389010668 -36.19207561676116
25 -290.6473135948181 -35.78149902167743
26 -276.02708822488785 -35.394503873250635
27 -291.3930814266205 -35.24303541418371
28 -276.2643161416054 -35.209705244501436
29 -267.5815915465355 -34.80241747531743
30 -278.0775319337845 -34.64469044638467
31 -262.10197138786316 -32.70706485357069
32 -258.2596917152405 -31.969099402548657
33 -252.39837968349457 -31.64414355845032
34 -256.0046818256378 -31.392382758954444
35 -249.00801718235016 -31.12953085092458
36 -253.21177124977112 -29.39157139549552
37 -259.8078761100769 -29.106189988903285
38 -241.1081183552742 -27.41102349748205
39 -225.0147124528885 -27.196681629483837
40 -240.5081126689911 -27.07399028854534
41 -225.15722835063934 -26.244794902859052
42 -214.7191457748413 -25.548365085275513
43 -226.3917925953865 -24.879106999799365
44 -218.7669439315796 -24.828695359328833
45 -217.93327862024307 -23.978745577896312
46 -203.02919936180115 -23.57262108435893
47 -215.7978020310402 -22.745309160183492
48 -208.9057627916336 -22.60679894414887
49 -196.69862812757492 -20.656863763892378
50 -202.65086591243744 -20.444472560731253
51 -193.16004395484924 -20.13839114930498
52 -192.18280678987503 -19.63760343800059
53 -165.13363936543465 -18.92838809611677
54 -184.82618083059788 -17.994774057192853
55 -162.68877786397934 -16.823073927842348
56 -135.38364177942276 -14.855082803515382
57 -149.67180912196636 -14.442420089224363
58 -130.41326259076595 -13.596012850960644
59 -137.81110802292824 -12.66418205637357
60 -122.7370471060276 -12.30017947419658
61 -126.22989033162594 -11.788852141676486
62 -124.70271627604961 -10.869989101210326
63 -101.87478546798229 -10.158998910215612
64 -107.47210478782654 -10.098060438715764
65 -105.16444337368011 -10.085369192426949
66 -108.15192151069641 -10.075444499138433
67 -120.85993975400925 -9.8572159761571
68 -100.36180168390274 -9.69621171312532
69 -105.64688163995743 -9.453925036020259
70 -104.48014414310455 -9.298983214491972
71 -105.87509006261826 -9.221473317824131
72 -91.28468382358551 -8.701914716659829
73 -98.15366017818451 -8.398412539068314
74 -95.44539292901754 -8.330116995310416
75 -86.46513369679451 -8.20118140380591
76 -96.34061621129513 -8.133195842510668
77 -96.04437065124512 -7.9382922518156445
78 -88.5994843840599 -7.914685950275049
79 -95.18015390634537 -7.785656034071619
80 -96.28131020069122 -7.710066477803409
81 -90.59158654510975 -7.57539849177145
82 -86.92456670105457 -7.362443126623615
83 -82.02161540091038 -7.108327355338034
84 -71.78597854822874 -7.098536262783918
85 -83.21596388146281 -6.959063561385431
86 -94.75424319505692 -6.776946485018116
87 -95.14070451259613 -6.719970621583102
88 -89.55635018646717 -6.535447341844848
89 -82.9754074215889 -6.4541813467689275
90 -72.78714490681887 -6.374631418435545
91 -80.70802026987076 -6.1414573196020275
92 -84.33660930395126 -6.018741969640372
93 -83.48793989419937 -5.9239274671546465
94 -76.366250410676 -5.907838164629078
95 -83.11982601881027 -5.891579763393588
96 -65.934115819633 -5.887844834258455
97 -72.65914073586464 -5.615796733870542
98 -76.05234998464584 -5.49423458895592
99 -87.66056868433952 -5.34720210027791
100 -72.9575988650322 -5.298674780840363
101 -72.39384996891022 -5.24864356134819
102 -73.52568876743317 -5.120162788231898
103 -66.40484902262688 -5.078485007852753
104 -67.49656793475151 -5.027957977402961
105 -85.40056651830673 -4.827572916892203
106 -59.5251397639513 -4.817620613428298
107 -72.97763442993164 -4.738620092082911
108 -64.55639445781708 -4.652120287214291
109 -70.71908622980118 -4.546831588548999
110 -69.10514202713966 -4.504924990699003
111 -81.42667919397354 -4.230832004686763
112 -75.82587417960167 -4.031048624093466
113 -60.61333808302879 -3.7089682609536023
114 -59.05060026049614 -3.609463813019705
115 -52.511017836630344 -3.3677107699982223
116 -67.65983164310455 -3.3322555012187633
117 -51.41441422700882 -2.505279064888427
118 -42.47772516310215 -2.17451293781979
119 -49.104896292090416 -1.9136196540088464
train accuracy: 0.9938888888888889
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
 -10.15899891 -10.09806044 -10.09122004 -10.08536919 -10.0754445
  -9.85924034  -9.85721598  -9.69621171  -9.62670303  -9.49330529
  -9.45392504  -9.34667359  -9.29898321  -9.29862109  -9.2215626
  -9.22147332  -8.96833453  -8.95515821  -8.79172826  -8.7478072
  -8.70191472  -8.64042466  -8.63545724  -8.54432894  -8.5100898
  -8.46452337  -8.46238493  -8.40372613  -8.39841254  -8.34147914
  -8.330117    -8.30241719  -8.25956742  -8.2011814   -8.13319584
  -8.10819769  -7.93829225  -7.91468595  -7.82990305  -7.80457905
  -7.78565603  -7.72021534  -7.71006648  -7.69474877  -7.62143153
  -7.61778087  -7.615106    -7.57539849  -7.4341397   -7.36244313
  -7.30583892  -7.1935226   -7.13921091  -7.1088614   -7.10832736
  -7.09853626  -7.02300895  -7.0014075   -6.98193464  -6.95906356
  -6.77694649  -6.7715631   -6.72206384  -6.71997062  -6.60275149
  -6.53544734  -6.51820418  -6.45418135  -6.39995201  -6.37463142
  -6.22808808  -6.14145732  -6.12843974  -6.05947946  -6.01874197
  -5.94878346  -5.93201135  -5.92392747  -5.91922331  -5.90783816
  -5.89682875  -5.89157976  -5.88784483  -5.86443439  -5.76929682
  -5.71239179  -5.68972364  -5.61758964  -5.61579673  -5.57332661
  -5.56711692  -5.49423459  -5.41198086  -5.41194616  -5.36811827
  -5.3472021   -5.29867478  -5.28116024  -5.24864356  -5.12016279
  -5.11295056  -5.11072935  -5.10066213  -5.08529262  -5.07848501
  -5.02795798  -4.95100524  -4.82757292  -4.81762061  -4.74776382
  -4.73862009  -4.72174767  -4.65212029  -4.63049542  -4.54683159
  -4.50492499  -4.41325899  -4.30853626  -4.230832    -4.19142186
  -4.13567949  -4.12037064  -4.05641497  -4.03104862  -4.00570358
  -3.94620586  -3.80312129  -3.78651875  -3.70896826  -3.60946381
  -3.57996762  -3.50384224  -3.38446715  -3.36771077  -3.3322555
  -3.19590017  -2.9465621   -2.70523006  -2.64166233  -2.50527906
  -2.44347199  -2.2111427   -2.17451294  -2.04347985  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.15149317204845594, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.5423e-02, -2.9859e-02,  3.3024e-02,  5.3159e-02,  2.7499e-01,
          3.3136e-02, -3.6934e-02, -7.0727e-03,  6.1209e-04,  6.7009e-03,
         -1.4320e-03, -1.6320e+00, -1.4138e+00]], device='cuda:2'))])
end of epoch 1: val_loss 0.00989461844736823, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.3515e-02,  1.8535e-02, -5.6906e-03,  1.6223e-02,  1.0031e-01,
         -7.5928e-03,  5.2165e-03, -1.5348e-02,  1.0166e-01,  1.0027e-01,
         -1.4320e-03, -2.5725e+00, -2.0761e+00]], device='cuda:2'))])
end of epoch 2: val_loss 0.10194026474459314, val_acc 0.98
trigger times: 1
end of epoch 3: val_loss 0.15019180248117064, val_acc 0.99
trigger times: 2
end of epoch 4: val_loss 0.055589413769291766, val_acc 0.99
trigger times: 3
end of epoch 5: val_loss 0.01427725649773393, val_acc 0.995
trigger times: 4
end of epoch 6: val_loss 0.06602967680786605, val_acc 0.995
trigger times: 5
end of epoch 7: val_loss 0.005201643705291019, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.1949e-03,  6.3721e-02,  7.4250e-06,  6.4101e-02,  5.7203e-01,
         -8.4326e-05, -2.1830e-02, -2.3411e-02,  6.5400e-03,  1.9619e-01,
         -1.4320e-03, -5.1595e+00, -5.7617e+00]], device='cuda:2'))])
end of epoch 8: val_loss 0.384572160040545, val_acc 0.985
trigger times: 1
end of epoch 9: val_loss 0.02417936874153874, val_acc 0.99
trigger times: 2
end of epoch 10: val_loss 0.0643312449621326, val_acc 0.995
trigger times: 3
end of epoch 11: val_loss 0.0025539290343787966, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2764e-01, -7.2096e-03, -9.3802e-02,  1.0375e-01,  5.7166e-01,
          9.3386e-02,  2.5939e-02, -3.5366e-02,  4.5566e-02,  4.7420e-04,
         -1.4321e-03, -6.6689e+00, -6.4722e+00]], device='cuda:2'))])
end of epoch 12: val_loss 0.03269815289741242, val_acc 0.995
trigger times: 1
end of epoch 13: val_loss 0.03193747000980178, val_acc 0.99
trigger times: 2
end of epoch 14: val_loss 0.3180272979370749, val_acc 0.975
trigger times: 3
end of epoch 15: val_loss 7.846557922832176e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1737e-02,  4.3383e-02, -8.0607e-02,  6.6510e-02,  1.9953e-01,
         -4.8043e-04, -1.0275e-02, -2.1339e-02,  2.1406e-01, -3.6238e-05,
         -1.4322e-03, -7.0649e+00, -6.6166e+00]], device='cuda:2'))])
end of epoch 16: val_loss 0.009401400416746739, val_acc 0.995
trigger times: 1
end of epoch 17: val_loss 0.017511457960150096, val_acc 0.995
trigger times: 2
end of epoch 18: val_loss 0.09394154984968481, val_acc 0.995
trigger times: 3
end of epoch 19: val_loss 5.396595911687285e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.1084e-02,  1.7641e-01, -3.9273e-02,  7.1865e-02,  6.5705e-01,
         -9.1292e-05, -1.3705e-02, -2.4098e-02,  2.0716e-05,  1.2972e-02,
         -1.4324e-03, -7.5161e+00, -7.0263e+00]], device='cuda:2'))])
end of epoch 20: val_loss 0.01194886167358046, val_acc 0.995
trigger times: 1
end of epoch 21: val_loss 0.11610028489838096, val_acc 0.99
trigger times: 2
end of epoch 22: val_loss 0.009476977811247025, val_acc 0.995
trigger times: 3
end of epoch 23: val_loss 0.05786693358525248, val_acc 0.995
trigger times: 4
end of epoch 24: val_loss 0.000700219974223657, val_acc 1.0
trigger times: 5
end of epoch 25: val_loss 0.018874183826050838, val_acc 0.995
trigger times: 6
end of epoch 26: val_loss 0.07824718385626564, val_acc 0.995
trigger times: 7
end of epoch 27: val_loss 1.3588730766400658e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.9490e-02,  3.4106e-02, -1.3663e-02,  1.2600e-01,  4.6268e-01,
          3.2716e-02,  1.6389e-02, -3.0142e-02, -2.4573e-05,  3.1479e-05,
         -1.4326e-03, -6.7230e+00, -6.7721e+00]], device='cuda:2'))])
end of epoch 28: val_loss 0.04001763386985996, val_acc 0.995
trigger times: 1
end of epoch 29: val_loss 0.010566491649536118, val_acc 0.995
trigger times: 2
end of epoch 30: val_loss 0.0004317088695732352, val_acc 1.0
trigger times: 3
end of epoch 31: val_loss 0.0006370688195945817, val_acc 1.0
trigger times: 4
end of epoch 32: val_loss 0.05883700925736829, val_acc 0.98
trigger times: 5
end of epoch 33: val_loss 0.03570745119834459, val_acc 0.995
trigger times: 6
end of epoch 34: val_loss 0.08710754754556088, val_acc 0.985
trigger times: 7
end of epoch 35: val_loss 0.0016821124539135824, val_acc 1.0
trigger times: 8
end of epoch 36: val_loss 0.3676904239797642, val_acc 0.985
trigger times: 9
end of epoch 37: val_loss 0.038884441649934015, val_acc 0.99
trigger times: 10
Early stopping.
0 -387.6272723674774 -54.98547503240923
1 -345.02954161167145 -50.03933801517046
2 -330.68784832954407 -49.72654640753777
3 -333.1206395626068 -45.7351542845057
4 -330.5309090614319 -44.99030608142343
5 -321.7878794670105 -43.81326882122305
6 -306.27499556541443 -42.29180714825394
7 -292.1699969768524 -41.6910044370425
8 -302.01359605789185 -41.281777102712205
9 -301.9538371562958 -40.34838365523108
10 -298.6583925485611 -39.57586365327889
11 -297.24975967407227 -39.024610555047154
12 -280.85593152046204 -38.41270390343083
13 -273.69360983371735 -37.79713616772368
14 -278.0839282274246 -37.66475323879293
15 -286.49720561504364 -37.1809993033689
16 -267.6306849718094 -37.00630588930485
17 -279.27579975128174 -36.48799015296732
18 -276.4784621447325 -36.19207561676116
19 -269.41992020606995 -35.78149902167743
20 -271.1643269062042 -35.26282499693737
21 -257.7658656835556 -35.209705244501436
22 -249.25600636005402 -34.80241747531743
23 -248.89875495433807 -33.84284985953318
24 -259.7123076915741 -31.969099402548657
25 -254.65908980369568 -31.64414355845032
26 -236.5610937476158 -31.223196019713853
27 -239.19593036174774 -29.39157139549552
28 -251.1004614830017 -29.106189988903285
29 -222.14645111560822 -27.343722362182305
30 -220.40211683511734 -27.07399028854534
31 -220.54096096754074 -26.244794902859052
32 -221.41202569007874 -25.45878528601009
33 -202.80704498291016 -24.828695359328833
34 -201.4353489279747 -23.978745577896312
35 -203.6919596195221 -23.44970807952351
36 -199.3488655090332 -22.60679894414887
37 -188.5722091794014 -20.656863763892378
38 -181.89030504226685 -20.19699010077007
39 -175.92250448465347 -19.63760343800059
40 -166.53798645734787 -18.92838809611677
41 -155.90177434682846 -17.55742370467821
42 -135.07890126109123 -14.855082803515382
43 -155.3855884373188 -14.442420089224363
44 -132.42487001419067 -12.68135972540495
45 -124.06420940160751 -12.30017947419658
46 -117.38945658504963 -11.788852141676486
47 -111.8203135728836 -10.327681503524177
48 -109.52433413267136 -10.098060438715764
49 -110.81409800052643 -10.085369192426949
50 -115.27912020683289 -9.85924033853484
51 -110.66668558120728 -9.69621171312532
52 -89.42002576589584 -9.493305292557864
53 -99.61929386854172 -9.346673590181872
54 -89.17999237775803 -9.29862108632299
55 -107.79965430498123 -9.221473317824131
56 -99.25877872109413 -8.955158207015923
57 -93.84346276521683 -8.747807200878507
58 -93.6943806707859 -8.640424662099235
59 -83.85619586706161 -8.544328936152931
60 -81.88622944056988 -8.46238492549372
61 -96.42163491249084 -8.398412539068314
62 -98.87571310997009 -8.330116995310416
63 -90.28291779756546 -8.259567419743458
64 -90.30082987248898 -8.133195842510668
65 -91.14857065677643 -7.9382922518156445
66 -88.56573694944382 -7.829903046652038
67 -96.24095737934113 -7.785656034071619
68 -90.45775377750397 -7.710066477803409
69 -87.45492759346962 -7.621431526537154
70 -81.76280391216278 -7.61510600044434
71 -86.26559263467789 -7.434139704522874
72 -93.48125344514847 -7.3058389229384595
73 -86.72353327274323 -7.139210905253967
74 -93.53370440006256 -7.108327355338034
75 -92.59519267082214 -7.0230089530911926
76 -82.6734344959259 -6.981934635536004
77 -93.2713495194912 -6.776946485018116
78 -88.10506425052881 -6.7220638398623045
79 -80.95511281490326 -6.602751491917299
80 -82.16630965471268 -6.51820418055673
81 -69.01046204566956 -6.399952011001724
82 -86.47624716162682 -6.228088084684211
83 -76.12585606426 -6.1284397418701495
84 -72.51281976699829 -6.018741969640372
85 -66.12294226884842 -5.932011354319275
86 -74.81566807627678 -5.919223314878099
87 -71.27967339754105 -5.896828748664083
88 -75.73460650444031 -5.887844834258455
89 -71.55251914262772 -5.769296816183801
90 -64.70711377263069 -5.6897236363439845
91 -75.85408313572407 -5.615796733870542
92 -64.51598496735096 -5.567116915937833
93 -77.27914637327194 -5.411980858205704
94 -58.68449169397354 -5.368118269333586
95 -74.99031567573547 -5.298674780840363
96 -77.06284272670746 -5.24864356134819
97 -59.2691904604435 -5.112950561826566
98 -64.51120591163635 -5.100662134279382
99 -72.31421434879303 -5.078485007852753
100 -74.42113262414932 -4.951005235457668
101 -64.63799983263016 -4.817620613428298
102 -66.47305274009705 -4.738620092082911
103 -68.17777571082115 -4.652120287214291
104 -60.15331092476845 -4.546831588548999
105 -61.45984023809433 -4.413258988412748
106 -64.35005781054497 -4.230832004686763
107 -49.347015380859375 -4.135679485500373
108 -52.923930794000626 -4.056414971804236
109 -55.0773968398571 -4.005703575543896
110 -51.36728789657354 -3.803121287088676
111 -52.58383771777153 -3.7089682609536023
112 -43.03161834180355 -3.579967623150271
113 -53.85589836537838 -3.3844671463622564
114 -57.05025999248028 -3.3322555012187633
115 -41.22577774524689 -2.946562102718787
116 -52.870874896645546 -2.6416623314910934
117 -38.41915404051542 -2.4434719864360446
118 -36.6332139223814 -2.17451293781979
119 -40.96717368066311 -1.9136196540088464
train accuracy: 0.9966666666666667
validation accuracy: 0.99
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
 -10.15899891 -10.09806044 -10.09122004 -10.08536919 -10.0754445
  -9.85924034  -9.85721598  -9.69621171  -9.62670303  -9.49330529
  -9.45392504  -9.34667359  -9.29898321  -9.29862109  -9.2215626
  -9.22147332  -8.96833453  -8.95515821  -8.79172826  -8.76601986
  -8.7478072   -8.70191472  -8.64042466  -8.63545724  -8.54432894
  -8.53899737  -8.5100898   -8.46452337  -8.46238493  -8.40372613
  -8.39841254  -8.34147914  -8.330117    -8.31543989  -8.30241719
  -8.25956742  -8.2011814   -8.18554927  -8.13319584  -8.10819769
  -7.93829225  -7.91468595  -7.85403284  -7.82990305  -7.80457905
  -7.79944242  -7.78565603  -7.72021534  -7.71006648  -7.70081189
  -7.69474877  -7.6693101   -7.62143153  -7.61778087  -7.615106
  -7.58290383  -7.57539849  -7.4341397   -7.36914872  -7.36244313
  -7.30704176  -7.30583892  -7.1935226   -7.13921091  -7.1088614
  -7.10832736  -7.09853626  -7.02300895  -7.0014075   -6.98193464
  -6.95906356  -6.89750815  -6.88744101  -6.77694649  -6.7715631
  -6.75311803  -6.72206384  -6.71997062  -6.67110049  -6.6407036
  -6.60275149  -6.59603729  -6.53544734  -6.51820418  -6.47267245
  -6.45418135  -6.39995201  -6.37463142  -6.26675617  -6.23508769
  -6.22808808  -6.20794976  -6.19933028  -6.17801577  -6.15611194
  -6.14145732  -6.12843974  -6.05947946  -6.01874197  -5.95591024
  -5.94878346  -5.93201135  -5.92392747  -5.91922331  -5.90783816
  -5.89682875  -5.89157976  -5.88784483  -5.86443439  -5.82596346
  -5.76929682  -5.7314621   -5.71239179  -5.70248839  -5.68972364
  -5.62366158  -5.61758964  -5.61579673  -5.59127538  -5.57332661
  -5.56711692  -5.5653042   -5.50255223  -5.49423459  -5.42210384
  -5.41198086  -5.41194616  -5.36811827  -5.35804664  -5.35005336
  -5.3472021   -5.29867478  -5.28116024  -5.24864356  -5.12016279
  -5.11295056  -5.11072935  -5.10066213  -5.09705939  -5.08529262
  -5.07848501  -5.07561368  -5.02795798  -4.98449127  -4.98375864
  -4.95100524  -4.82757292  -4.81762061  -4.77972945  -4.77138844
  -4.74776382  -4.73862009  -4.72174767  -4.65212029  -4.63049542
  -4.62063889  -4.59416515  -4.56308144  -4.54683159  -4.50492499
  -4.46871391  -4.41325899  -4.31859925  -4.30853626  -4.230832
  -4.20257165  -4.19142186  -4.17876032  -4.13567949  -4.12037064
  -4.05641497  -4.03104862  -4.00570358  -3.94620586  -3.89458328
  -3.81149942  -3.80312129  -3.78651875  -3.74779378  -3.70896826
  -3.64711211  -3.60946381  -3.59652899  -3.57996762  -3.55805518
  -3.55130493  -3.50384224  -3.38446715  -3.36771077  -3.3576332
  -3.3322555   -3.21502887  -3.19590017  -2.9465621   -2.92216004
  -2.70523006  -2.64166233  -2.50527906  -2.44347199  -2.2111427
  -2.17451294  -2.04347985  -1.99687296  -1.91361965  -1.91219107]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.11837585688256397, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 7.2844e-02,  4.6720e-03, -3.0048e-02,  1.0667e-03,  3.1335e-01,
          1.4014e-01, -2.4085e-02,  1.3651e-02, -1.5029e-01,  4.3453e-02,
         -1.4320e-03, -1.7001e+00, -1.4881e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.31518518172149446, val_acc 0.96
trigger times: 1
end of epoch 2: val_loss 0.0024489947146494728, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.1214e-02,  2.0810e-02,  1.5646e-02,  2.7828e-02,  7.0426e-01,
         -2.7885e-02, -7.8954e-03, -7.7166e-04, -1.7023e-01,  1.2224e-01,
         -1.4320e-03, -3.0227e+00, -3.0459e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.5819499768223665, val_acc 0.935
trigger times: 1
end of epoch 4: val_loss 0.016238452501079016, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 0.0838072455838747, val_acc 0.98
trigger times: 3
end of epoch 6: val_loss 0.02115327478307176, val_acc 0.985
trigger times: 4
end of epoch 7: val_loss 0.003533403079827586, val_acc 1.0
trigger times: 5
end of epoch 8: val_loss 0.12744666270175933, val_acc 0.975
trigger times: 6
end of epoch 9: val_loss 0.0015673667533511093, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2626e-02, -3.6282e-02,  2.7675e-02,  1.4391e-02,  5.2979e-01,
          3.1602e-02, -5.9558e-03, -2.1138e-02,  3.1198e-05,  7.1981e-02,
         -1.4320e-03, -4.8430e+00, -4.4070e+00]], device='cuda:0'))])
end of epoch 10: val_loss 0.00899074192606772, val_acc 0.99
trigger times: 1
end of epoch 11: val_loss 0.007413098543523091, val_acc 0.99
trigger times: 2
end of epoch 12: val_loss 0.006967564651056861, val_acc 1.0
trigger times: 3
end of epoch 13: val_loss 0.015909974340776428, val_acc 0.99
trigger times: 4
end of epoch 14: val_loss 0.018257877289438867, val_acc 0.99
trigger times: 5
end of epoch 15: val_loss 1.4712493304081453, val_acc 0.9
trigger times: 6
end of epoch 16: val_loss 1.5315276918315568, val_acc 0.905
trigger times: 7
end of epoch 17: val_loss 0.03266436093410505, val_acc 0.99
trigger times: 8
end of epoch 18: val_loss 0.6716588446157331, val_acc 0.96
trigger times: 9
end of epoch 19: val_loss 0.047028746437248935, val_acc 0.985
trigger times: 10
Early stopping.
0 -315.6751034259796 -54.98547503240923
1 -285.8278588652611 -49.75347184620696
2 -263.82454723119736 -46.98011874490918
3 -268.5410852432251 -44.99030608142343
4 -266.81600737571716 -43.81326882122305
5 -266.00242280960083 -42.00401746161006
6 -256.4587353467941 -41.68588229294918
7 -255.72486281394958 -40.34838365523108
8 -246.27322912216187 -39.57586365327889
9 -238.50936889648438 -38.45534493538269
10 -249.122074842453 -38.35634328077039
11 -236.6786539554596 -37.66475323879293
12 -247.82622480392456 -37.1809993033689
13 -238.73295378684998 -36.821916772458344
14 -215.74952244758606 -36.20965269874363
15 -221.713805437088 -35.78149902167743
16 -235.73011434078217 -35.26282499693737
17 -237.2445112466812 -35.0654408505187
18 -209.60532981157303 -34.64469044638467
19 -210.92419409751892 -31.969099402548657
20 -213.80205714702606 -31.64414355845032
21 -197.20565056800842 -31.12953085092458
22 -198.12622249126434 -29.340125609942326
23 -182.75909399986267 -27.343722362182305
24 -187.6596838235855 -27.07399028854534
25 -171.15716361999512 -25.548365085275513
26 -184.02159464359283 -24.879106999799365
27 -171.33439214527607 -23.978745577896312
28 -162.62277382612228 -23.44970807952351
29 -156.0239795446396 -22.19891031871716
30 -158.80886602401733 -20.444472560731253
31 -137.51521584391594 -19.63760343800059
32 -134.4451408982277 -18.92838809611677
33 -137.40689343214035 -16.823073927842348
34 -113.24399787187576 -14.531424598833084
35 -112.92312961816788 -12.68135972540495
36 -96.45319619774818 -12.30017947419658
37 -109.27486824989319 -10.869989101210326
38 -83.48532038927078 -10.158998910215612
39 -78.33331882953644 -10.085369192426949
40 -78.76507160067558 -9.8572159761571
41 -74.35921388864517 -9.626703027773607
42 -80.96242809295654 -9.346673590181872
43 -73.85667270421982 -9.29862108632299
44 -81.1966362297535 -8.968334533897297
45 -78.86915308237076 -8.791728255336933
46 -85.49776062369347 -8.701914716659829
47 -68.8840724825859 -8.635457240259818
48 -68.74502021074295 -8.510089797441157
49 -68.19508284330368 -8.46238492549372
50 -68.1450086236 -8.341479143233226
51 -75.85549980401993 -8.315439889368845
52 -78.76125049591064 -8.20118140380591
53 -67.92017886787653 -8.133195842510668
54 -67.8290855884552 -7.914685950275049
55 -69.48270839452744 -7.829903046652038
56 -79.47727727890015 -7.785656034071619
57 -64.51234710216522 -7.710066477803409
58 -70.78454393148422 -7.669310102089584
59 -77.10764056444168 -7.617780867238701
60 -79.22671811282635 -7.57539849177145
61 -69.48639744520187 -7.369148719756636
62 -75.90356895327568 -7.3058389229384595
63 -65.54814052581787 -7.139210905253967
64 -61.046919733285904 -7.098536262783918
65 -63.37607479095459 -7.0014074989268815
66 -61.86903190612793 -6.897508147133652
67 -75.50275537371635 -6.776946485018116
68 -71.1513009518385 -6.7220638398623045
69 -62.144933953881264 -6.671100489257628
70 -63.742046386003494 -6.596037293903165
71 -72.14238095283508 -6.51820418055673
72 -51.50744077563286 -6.399952011001724
73 -60.818945705890656 -6.266756167836701
74 -60.70017620921135 -6.2079497578588105
75 -55.22584477066994 -6.178015770496259
76 -57.99891656823456 -6.1284397418701495
77 -51.67340315878391 -6.018741969640372
78 -56.08124479651451 -5.932011354319275
79 -51.31815016269684 -5.919223314878099
80 -51.758283123373985 -5.891579763393588
81 -55.68658732622862 -5.82596346085507
82 -51.72778385877609 -5.731462104966822
83 -46.42359545826912 -5.6897236363439845
84 -45.246379762887955 -5.617589639827789
85 -46.64439518749714 -5.57332661417246
86 -49.13363456726074 -5.565304201887495
87 -51.342898830771446 -5.422103844472523
88 -49.9323910176754 -5.411946158511884
89 -52.504592813551426 -5.350053362725614
90 -61.071615159511566 -5.298674780840363
91 -61.556668281555176 -5.120162788231898
92 -59.09445145726204 -5.1107293513462375
93 -45.72146789729595 -5.085292622705928
94 -50.6496000289917 -5.075613680927074
95 -48.53534206748009 -4.983758644561877
96 -51.13530991971493 -4.827572916892203
97 -43.30316821485758 -4.771388444240095
98 -49.249930173158646 -4.738620092082911
99 -52.21649296954274 -4.63049541560991
100 -47.5705921202898 -4.594165148719001
101 -51.503815323114395 -4.504924990699003
102 -47.05885595083237 -4.413258988412748
103 -45.9555623754859 -4.230832004686763
104 -39.62761174142361 -4.191421860155279
105 -38.09780240058899 -4.120370641246954
106 -47.976720195263624 -4.031048624093466
107 -39.070417530834675 -3.89458328248424
108 -37.90443840622902 -3.803121287088676
109 -39.67691731452942 -3.7089682609536023
110 -40.855576649308205 -3.609463813019705
111 -24.647944532334805 -3.5580551829563145
112 -37.08671776205301 -3.5038422380998253
113 -33.44710732251406 -3.3576332022767414
114 -27.9985131919384 -3.215028868371669
115 -33.44737111777067 -2.9221600364839664
116 -38.33768177032471 -2.6416623314910934
117 -20.770056754350662 -2.2111426960790292
118 -21.192635916173458 -2.0434798503235894
119 -23.445813037455082 -1.9121910677393041
train accuracy: 0.9988888888888889
validation accuracy: 0.985
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
 -10.15899891 -10.09806044 -10.09122004 -10.08536919 -10.0754445
  -9.85924034  -9.85721598  -9.69621171  -9.62670303  -9.49330529
  -9.45392504  -9.34667359  -9.29898321  -9.29862109  -9.2215626
  -9.22147332  -9.14864331  -8.96833453  -8.95515821  -8.79172826
  -8.76601986  -8.7478072   -8.70191472  -8.64042466  -8.63545724
  -8.54432894  -8.53899737  -8.5100898   -8.49523978  -8.46452337
  -8.46238493  -8.40372613  -8.39841254  -8.34147914  -8.330117
  -8.31543989  -8.30241719  -8.25956742  -8.2011814   -8.18554927
  -8.13319584  -8.10819769  -8.03998478  -7.97885058  -7.93829225
  -7.91468595  -7.85403284  -7.82990305  -7.80457905  -7.79944242
  -7.78565603  -7.76629741  -7.72021534  -7.71006648  -7.70081189
  -7.69474877  -7.6693101   -7.65786517  -7.62143153  -7.61778087
  -7.615106    -7.58290383  -7.57539849  -7.55238168  -7.4341397
  -7.39690029  -7.36914872  -7.36244313  -7.34400592  -7.30704176
  -7.30583892  -7.27060683  -7.1935226   -7.13921091  -7.12901111
  -7.1088614   -7.10832736  -7.09853626  -7.02300895  -7.0014075
  -6.98193464  -6.95906356  -6.89750815  -6.88744101  -6.83942598
  -6.77694649  -6.7715631   -6.75746151  -6.75311803  -6.72857353
  -6.72206384  -6.71997062  -6.67110049  -6.6407036   -6.60275149
  -6.59603729  -6.57444638  -6.56409725  -6.55391978  -6.53544734
  -6.5321565   -6.51820418  -6.47684499  -6.47267245  -6.45418135
  -6.39995201  -6.37463142  -6.3092634   -6.29611133  -6.28402383
  -6.26675617  -6.23508769  -6.23201256  -6.22808808  -6.20794976
  -6.20775779  -6.19933028  -6.17801577  -6.15738684  -6.15611194
  -6.14518869  -6.14145732  -6.12843974  -6.05947946  -6.01874197
  -5.99986928  -5.96523445  -5.95591024  -5.94878346  -5.93201135
  -5.92392747  -5.91922331  -5.90783816  -5.89682875  -5.89157976
  -5.88784483  -5.86443439  -5.82596346  -5.82363497  -5.76929682
  -5.7314621   -5.71239179  -5.70455686  -5.70248839  -5.68972364
  -5.65812922  -5.62366158  -5.61758964  -5.61579673  -5.59127538
  -5.58687303  -5.57332661  -5.56711692  -5.5653042   -5.50436425
  -5.50255223  -5.49423459  -5.42210384  -5.41198086  -5.41194616
  -5.36811827  -5.35804664  -5.35005336  -5.3472021   -5.33854996
  -5.32351373  -5.29867478  -5.28116024  -5.24864356  -5.18290674
  -5.17448189  -5.12016279  -5.11295056  -5.11072935  -5.10813871
  -5.10066213  -5.09705939  -5.08529262  -5.07848501  -5.07561368
  -5.02795798  -4.98449127  -4.98375864  -4.95100524  -4.82757292
  -4.81762061  -4.79575679  -4.77972945  -4.77138844  -4.74776382
  -4.73862009  -4.72174767  -4.65212029  -4.63049542  -4.62063889
  -4.59416515  -4.56928302  -4.56308144  -4.54683159  -4.54219599
  -4.50492499  -4.46877891  -4.46871391  -4.4559329   -4.41325899
  -4.38873448  -4.31859925  -4.30853626  -4.230832    -4.20257165
  -4.19142186  -4.17876032  -4.13567949  -4.12037064  -4.05742201
  -4.05641497  -4.0502646   -4.03104862  -4.02232746  -4.00570358
  -3.94620586  -3.90922314  -3.89458328  -3.81149942  -3.80312129
  -3.78651875  -3.78305712  -3.74779378  -3.70896826  -3.67557906
  -3.64993559  -3.64711211  -3.60946381  -3.59652899  -3.57996762
  -3.55805518  -3.55130493  -3.50384224  -3.4343312   -3.38446715
  -3.36771077  -3.3576332   -3.3322555   -3.24811009  -3.22637915
  -3.21502887  -3.19590017  -3.11268798  -2.9465621   -2.92216004
  -2.83278821  -2.80316874  -2.70523006  -2.64166233  -2.50527906
  -2.44347199  -2.42077762  -2.2111427   -2.17451294  -2.04347985
  -1.99687296  -1.98010897  -1.91361965  -1.91219107  -1.70402195]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.16292699897069582, val_acc 0.945
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4584e-02,  1.1820e-01, -6.2480e-02, -8.0678e-03,  3.6814e-01,
         -8.9429e-02, -1.3544e-02, -3.3069e-02, -2.0873e-01,  8.5245e-02,
         -1.4320e-03, -1.7264e+00, -1.5251e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.4799028513441496, val_acc 0.935
trigger times: 1
end of epoch 2: val_loss 0.06205852036906102, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.1494e-03,  4.0496e-03, -8.4307e-03,  1.5634e-01,  7.0516e-01,
          4.3172e-02,  1.8382e-02, -2.0157e-02,  2.2185e-02,  6.0557e-02,
         -1.4320e-03, -2.4453e+00, -3.0177e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.5727547102919693, val_acc 0.95
trigger times: 1
end of epoch 4: val_loss 7.89666783180465e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.9515e-02, -1.0190e-01,  7.6308e-02,  3.9163e-02,  1.1549e+00,
          1.0446e-04, -3.7113e-02, -4.2696e-02,  4.4764e-05,  3.0229e-01,
         -1.4320e-03, -4.0291e+00, -4.4438e+00]], device='cuda:0'))])
end of epoch 5: val_loss 1.2059665414803078e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6160e-01, -2.0899e-02, -6.0876e-02, -6.3190e-03,  9.9864e-01,
          2.8316e-02,  1.6178e-02, -1.1844e-02, -1.6978e-05,  1.3569e-02,
         -1.4320e-03, -3.9264e+00, -4.6788e+00]], device='cuda:0'))])
end of epoch 6: val_loss 2.5405326859058162e-05, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 0.050549868279768685, val_acc 0.985
trigger times: 2
end of epoch 8: val_loss 0.41940166776010124, val_acc 0.97
trigger times: 3
end of epoch 9: val_loss 0.08033432976724071, val_acc 0.99
trigger times: 4
end of epoch 10: val_loss 0.00017736977620657512, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 0.40282106506403914, val_acc 0.965
trigger times: 6
end of epoch 12: val_loss 0.0387094747958264, val_acc 0.99
trigger times: 7
end of epoch 13: val_loss 0.02974030854964411, val_acc 0.995
trigger times: 8
end of epoch 14: val_loss 5.495274051270371e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4610e-01,  7.5339e-03, -1.3259e-02,  5.5385e-02,  1.0664e+00,
          9.5242e-02,  4.5853e-04, -4.3563e-02,  7.6555e-06,  2.6664e-02,
         -1.4322e-03, -4.8762e+00, -6.3335e+00]], device='cuda:0'))])
end of epoch 15: val_loss 0.0008493977990466917, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 0.0073616238797148984, val_acc 0.995
trigger times: 2
end of epoch 17: val_loss 6.848166280448709e-07, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 8.344606836629964e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.3174e-01, -1.3143e-01,  5.9703e-02,  1.1806e-01,  1.1809e+00,
          2.3539e-01, -2.2246e-02, -3.4255e-02,  2.4078e-01,  1.9590e-01,
         -1.4323e-03, -5.7081e+00, -6.5728e+00]], device='cuda:0'))])
end of epoch 19: val_loss 0.0017755394349739718, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 1.0907611905253134e-07, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 0.051141785216859394, val_acc 0.995
trigger times: 3
end of epoch 22: val_loss 3.0041664444979688e-06, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 0.00018160394282343617, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 7.188133553981402e-07, val_acc 1.0
trigger times: 6
end of epoch 25: val_loss 0.00013377657737446925, val_acc 1.0
trigger times: 7
end of epoch 26: val_loss 1.3025720791878284, val_acc 0.94
trigger times: 8
end of epoch 27: val_loss 0.00029583395288725, val_acc 1.0
trigger times: 9
end of epoch 28: val_loss 1.9311665489851747e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -340.43451857566833 -54.98547503240923
1 -310.7443583011627 -49.75347184620696
2 -290.0469057559967 -45.7351542845057
3 -308.64719581604004 -44.14602409201361
4 -286.9186375141144 -42.29180714825394
5 -276.876633644104 -41.68588229294918
6 -269.353950381279 -40.34838365523108
7 -279.17076563835144 -39.31972693233231
8 -255.7812168598175 -38.41270390343083
9 -279.39891052246094 -37.741528994987384
10 -257.1768811941147 -37.1809993033689
11 -258.2285016775131 -36.821916772458344
12 -248.89218825101852 -36.19207561676116
13 -254.58760559558868 -35.394503873250635
14 -230.3836643397808 -35.209705244501436
15 -235.70566308498383 -34.64469044638467
16 -227.78075504302979 -31.969099402548657
17 -213.2985777258873 -31.392382758954444
18 -216.20244532823563 -29.39157139549552
19 -205.40857341885567 -27.41102349748205
20 -203.9357146024704 -27.07399028854534
21 -179.79981911182404 -25.548365085275513
22 -173.17653858661652 -24.828695359328833
23 -183.8731308579445 -23.57262108435893
24 -177.16441869735718 -22.60679894414887
25 -186.63755077123642 -20.444472560731253
26 -156.47728991508484 -19.63760343800059
27 -163.6404681801796 -17.994774057192853
28 -121.74292649328709 -14.855082803515382
29 -117.14664939045906 -13.596012850960644
30 -122.46875181794167 -12.151904772081672
31 -102.490114569664 -10.327681503524177
32 -102.43099522590637 -10.09122003580262
33 -88.66389614343643 -9.85924033853484
34 -100.1736192703247 -9.626703027773607
35 -92.92409765720367 -9.346673590181872
36 -97.41497427225113 -9.221562595458241
37 -88.07487404346466 -8.968334533897297
38 -88.06677827239037 -8.76601986064386
39 -81.68823829293251 -8.640424662099235
40 -92.40269973874092 -8.538997371746566
41 -85.85378557443619 -8.464523371731232
42 -93.74849903583527 -8.398412539068314
43 -91.17299556732178 -8.315439889368845
44 -86.84746158123016 -8.20118140380591
45 -83.47881565988064 -8.108197691178031
46 -89.3485558629036 -7.9382922518156445
47 -74.77263569831848 -7.829903046652038
48 -71.19207212328911 -7.785656034071619
49 -81.34150344133377 -7.710066477803409
50 -83.8658709526062 -7.669310102089584
51 -70.74484564363956 -7.617780867238701
52 -98.4364441037178 -7.57539849177145
53 -74.02182495594025 -7.3969002913305575
54 -81.41393727064133 -7.344005916861745
55 -73.29291442036629 -7.270606833061273
56 -64.38927549123764 -7.129011108002192
57 -66.0889035910368 -7.098536262783918
58 -73.9003078341484 -6.981934635536004
59 -73.16739413142204 -6.887441006100602
60 -65.87816217541695 -6.771563102051013
61 -80.77059549093246 -6.72857353493394
62 -78.46932646632195 -6.671100489257628
63 -66.3884205520153 -6.596037293903165
64 -58.93424824625254 -6.5539197829400315
65 -70.88625159859657 -6.51820418055673
66 -70.7714833021164 -6.4541813467689275
67 -77.65043750405312 -6.309263396805544
68 -76.43904680013657 -6.266756167836701
69 -64.52381241321564 -6.228088084684211
70 -64.60094733536243 -6.199330283858771
71 -69.23882542550564 -6.156111941215931
72 -70.37307085096836 -6.1284397418701495
73 -67.63972848653793 -5.999869281895605
74 -65.89769439026713 -5.948783464825211
75 -74.26617163419724 -5.919223314878099
76 -72.43805587291718 -5.891579763393588
77 -75.96558871865273 -5.82596346085507
78 -41.93758441507816 -5.731462104966822
79 -61.17428730428219 -5.702488391892685
80 -67.03260427713394 -5.62366158123635
81 -43.23667228221893 -5.591275382598729
82 -47.899239495396614 -5.567116915937833
83 -42.87765419483185 -5.502552225229215
84 -52.38987448811531 -5.411980858205704
85 -53.09133103489876 -5.358046637173484
86 -62.932537496089935 -5.338549956561249
87 -59.076946675777435 -5.281160240600011
88 -51.34352961182594 -5.174481893459073
89 -55.77460527420044 -5.1107293513462375
90 -41.54032610356808 -5.085292622705928
91 -71.80118560791016 -5.027957977402961
92 -53.98671191185713 -4.951005235457668
93 -41.147627755999565 -4.795756788659454
94 -51.83448788523674 -4.747763824701839
95 -49.07314310222864 -4.652120287214291
96 -44.78816209733486 -4.594165148719001
97 -54.02731296420097 -4.546831588548999
98 -50.81903751194477 -4.468778910671018
99 -46.843610510230064 -4.413258988412748
100 -43.278493866324425 -4.308536257242343
101 -41.0791899561882 -4.191421860155279
102 -42.396082162857056 -4.120370641246954
103 -51.2312234044075 -4.050264599066057
104 -49.62490265071392 -4.005703575543896
105 -40.91463245078921 -3.89458328248424
106 -40.62264882028103 -3.7865187507817453
107 -40.361089780926704 -3.7089682609536023
108 -43.911596208810806 -3.647112105523818
109 -31.633761167526245 -3.579967623150271
110 -39.18247117847204 -3.5038422380998253
111 -38.13339487463236 -3.3677107699982223
112 -35.10629544407129 -3.2481100852574314
113 -46.469550877809525 -3.1959001711145913
114 -37.44047889113426 -2.9221600364839664
115 -35.528981164097786 -2.705230055237873
116 -31.35480123758316 -2.4434719864360446
117 -26.76229440048337 -2.17451293781979
118 -31.65034712292254 -1.9801089659332811
119 -27.87691642343998 -1.704021949797342
train accuracy: 0.9983333333333333
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.66212941 -13.59601285 -12.68135973
 -12.66418206 -12.30017947 -12.19557377 -12.15190477 -11.78885214
 -11.68411247 -10.96374037 -10.9613868  -10.8699891  -10.72612739
 -10.44520851 -10.3276815  -10.15899891 -10.09806044 -10.09122004
 -10.08536919 -10.0754445  -10.01766797  -9.85924034  -9.85721598
  -9.74913286  -9.69621171  -9.62670303  -9.49330529  -9.45392504
  -9.34667359  -9.29898321  -9.29862109  -9.2215626   -9.22147332
  -9.14864331  -8.96833453  -8.95515821  -8.81919187  -8.79172826
  -8.76601986  -8.7478072   -8.70191472  -8.64042466  -8.63545724
  -8.62211506  -8.54432894  -8.53899737  -8.5100898   -8.49523978
  -8.46452337  -8.46238493  -8.40372613  -8.39841254  -8.34147914
  -8.330117    -8.31543989  -8.31417347  -8.30241719  -8.25956742
  -8.2011814   -8.19351164  -8.18554927  -8.15566622  -8.13319584
  -8.10819769  -8.03998478  -7.98298876  -7.97885058  -7.93829225
  -7.92077432  -7.91468595  -7.87361193  -7.85403284  -7.82990305
  -7.80457905  -7.79944242  -7.78565603  -7.76800057  -7.76629741
  -7.72021534  -7.71006648  -7.70081189  -7.69474877  -7.6693101
  -7.65786517  -7.62143153  -7.61778087  -7.615106    -7.58290383
  -7.57539849  -7.55238168  -7.48447732  -7.4341397   -7.39690029
  -7.36914872  -7.36257912  -7.36244313  -7.34400592  -7.30704176
  -7.30583892  -7.27060683  -7.1935226   -7.18362256  -7.13921091
  -7.12901111  -7.11063309  -7.1088614   -7.10832736  -7.09853626
  -7.02300895  -7.0014075   -6.98193464  -6.95906356  -6.94197695
  -6.89750815  -6.88744101  -6.83942598  -6.80309216  -6.77694649
  -6.7715631   -6.75746151  -6.75311803  -6.72857353  -6.72206384
  -6.71997062  -6.67110049  -6.6407036   -6.60494025  -6.60275149
  -6.59603729  -6.57444638  -6.56409725  -6.55391978  -6.53544734
  -6.5321565   -6.51820418  -6.47684499  -6.47267245  -6.45418135
  -6.4216756   -6.39995201  -6.37463142  -6.3092634   -6.29611133
  -6.29387626  -6.28402383  -6.26675617  -6.24435373  -6.23508769
  -6.23201256  -6.22808808  -6.20794976  -6.20775779  -6.19933028
  -6.17801577  -6.15738684  -6.15611194  -6.14518869  -6.14145732
  -6.12843974  -6.05947946  -6.01874197  -5.99986928  -5.96523445
  -5.95591024  -5.94878346  -5.93201135  -5.92392747  -5.91922331
  -5.90783816  -5.89682875  -5.89157976  -5.88784483  -5.86505633
  -5.86443439  -5.82596346  -5.82363497  -5.81546979  -5.76929682
  -5.7314621   -5.71239179  -5.70455686  -5.70248839  -5.68972364
  -5.68067186  -5.65812922  -5.62366158  -5.61758964  -5.61579673
  -5.59668033  -5.59127538  -5.58687303  -5.57332661  -5.56711692
  -5.5653042   -5.50436425  -5.50255223  -5.4969105   -5.49423459
  -5.45749576  -5.45457711  -5.42210384  -5.41198086  -5.41194616
  -5.36811827  -5.35804664  -5.35005336  -5.34897993  -5.3472021
  -5.34035857  -5.33854996  -5.32351373  -5.29867478  -5.28116024
  -5.24864356  -5.18290674  -5.17448189  -5.12016279  -5.11295056
  -5.11072935  -5.10813871  -5.10066213  -5.09705939  -5.08529262
  -5.07848501  -5.07561368  -5.05123026  -5.03971452  -5.02795798
  -4.98449127  -4.98375864  -4.95100524  -4.88525428  -4.84355863
  -4.82757292  -4.81762061  -4.79575679  -4.77972945  -4.77138844
  -4.75116294  -4.74776382  -4.73862009  -4.72174767  -4.65212029
  -4.63049542  -4.62063889  -4.59416515  -4.56928302  -4.56308144
  -4.54683159  -4.54219599  -4.53501353  -4.50492499  -4.46877891
  -4.46871391  -4.4559329   -4.45557334  -4.41325899  -4.38990224
  -4.38873448  -4.37569781  -4.31859925  -4.30853626  -4.230832
  -4.20257165  -4.19142186  -4.17876032  -4.13567949  -4.12037064
  -4.06953845  -4.05742201  -4.05641497  -4.0502646   -4.03104862
  -4.02361879  -4.02232746  -4.00657891  -4.00570358  -3.94620586
  -3.91689791  -3.90922314  -3.89458328  -3.8726588   -3.83834886
  -3.81149942  -3.80312129  -3.78651875  -3.78305712  -3.74779378
  -3.70896826  -3.68750253  -3.67557906  -3.64993559  -3.64711211
  -3.60946381  -3.59652899  -3.57996762  -3.55805518  -3.55130493
  -3.50384224  -3.48579942  -3.48512377  -3.48105323  -3.4343312
  -3.38446715  -3.36771077  -3.3576332   -3.3322555   -3.24811009
  -3.22637915  -3.21502887  -3.19590017  -3.14989251  -3.11268798
  -2.9465621   -2.92216004  -2.83278821  -2.80316874  -2.80184248
  -2.70523006  -2.64166233  -2.50527906  -2.44347199  -2.42077762
  -2.26953107  -2.2111427   -2.17451294  -2.04347985  -1.99687296
  -1.98010897  -1.91361965  -1.91219107  -1.70402195  -1.68245221]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.018499961702047152, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.8970e-03,  8.2085e-02, -2.7511e-02, -2.5110e-02,  2.4657e-01,
          1.7608e-01,  1.3536e-02,  9.7804e-04, -2.2292e-01,  9.8561e-02,
         -1.4320e-03, -1.4776e+00, -1.1501e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.06509477748444283, val_acc 0.985
trigger times: 1
end of epoch 2: val_loss 0.05784764269360867, val_acc 0.985
trigger times: 2
end of epoch 3: val_loss 0.08075292212211224, val_acc 0.99
trigger times: 3
end of epoch 4: val_loss 0.0024630910824626896, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.4803e-07,  5.0364e-02,  2.4456e-02, -1.4750e-02,  7.3372e-01,
         -2.4007e-05, -5.2185e-03, -1.6454e-03, -7.2395e-05, -1.0073e-04,
         -1.4320e-03, -3.6362e+00, -3.7588e+00]], device='cuda:0'))])
end of epoch 5: val_loss 2.9800033015412452e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.5646e-02,  8.4500e-02,  4.7100e-02,  8.9528e-03,  1.0708e+00,
          1.2356e-01, -4.4241e-02,  1.7377e-02, -5.6015e-01,  8.4063e-02,
         -1.4320e-03, -4.4464e+00, -4.5179e+00]], device='cuda:0'))])
end of epoch 6: val_loss 0.19862395255516063, val_acc 0.985
trigger times: 1
end of epoch 7: val_loss 0.06216218020323602, val_acc 0.99
trigger times: 2
end of epoch 8: val_loss 0.047957927577038505, val_acc 0.99
trigger times: 3
end of epoch 9: val_loss 0.00013436023751136616, val_acc 1.0
trigger times: 4
end of epoch 10: val_loss 0.00013138455772093493, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 0.19282152407518666, val_acc 0.99
trigger times: 6
end of epoch 12: val_loss 0.0007334929350955832, val_acc 1.0
trigger times: 7
end of epoch 13: val_loss 0.8932707795805777, val_acc 0.925
trigger times: 8
end of epoch 14: val_loss 0.10245406983906317, val_acc 0.99
trigger times: 9
end of epoch 15: val_loss 0.008581363748381747, val_acc 1.0
trigger times: 10
Early stopping.
0 -307.9142715930939 -54.98547503240923
1 -275.87955057621 -49.72654640753777
2 -255.56992864608765 -45.670579884154705
3 -276.25479006767273 -43.18878399086166
4 -267.56411600112915 -41.6910044370425
5 -242.11586999893188 -40.34838365523108
6 -293.8645739555359 -39.31972693233231
7 -229.41785800457 -38.35634328077039
8 -229.59568881988525 -37.66475323879293
9 -234.15785765647888 -37.00630588930485
10 -223.46125173568726 -36.20965269874363
11 -202.98271095752716 -35.394503873250635
12 -209.21586605906487 -35.209705244501436
13 -230.2198406457901 -33.84284985953318
14 -199.56621897220612 -31.7109134007892
15 -177.28790390491486 -31.12953085092458
16 -201.34208607673645 -29.106189988903285
17 -209.43299865722656 -27.07399028854534
18 -160.66965168714523 -25.548365085275513
19 -188.7753847837448 -24.592745144504722
20 -167.98787200450897 -23.44970807952351
21 -147.21153432130814 -20.656863763892378
22 -133.90660724043846 -20.13839114930498
23 -131.91685035824776 -17.994774057192853
24 -117.41685113310814 -14.531424598833084
25 -102.80867359042168 -13.596012850960644
26 -103.67656910419464 -12.19557376520152
27 -91.6207959651947 -11.68411247487254
28 -90.5799001455307 -10.72612738641354
29 -70.03170585632324 -10.158998910215612
30 -82.79859018325806 -10.075444499138433
31 -84.31212550401688 -9.8572159761571
32 -75.20500230789185 -9.493305292557864
33 -85.56873118877411 -9.298983214491972
34 -91.86926114559174 -9.148643307299071
35 -73.24104529619217 -8.819191869263417
36 -64.4677422940731 -8.701914716659829
37 -71.83288770914078 -8.622115061384498
38 -85.41745513677597 -8.495239778971564
39 -69.83297997713089 -8.403726130034142
40 -68.85183426737785 -8.315439889368845
41 -66.19468002021313 -8.259567419743458
42 -58.90719002485275 -8.155666217614
43 -68.73343928158283 -8.039984784723863
44 -76.32217621803284 -7.920774323585604
45 -62.24382407963276 -7.854032835169835
46 -66.38402152061462 -7.785656034071619
47 -64.88725110888481 -7.720215340218916
48 -84.3096461892128 -7.669310102089584
49 -72.21393924951553 -7.61510600044434
50 -79.79675793647766 -7.552381677625814
51 -62.23102676868439 -7.369148719756636
52 -81.27202141284943 -7.344005916861745
53 -60.9267196059227 -7.193522599704719
54 -77.41209405660629 -7.129011108002192
55 -42.73329282552004 -7.098536262783918
56 -51.35039933770895 -6.981934635536004
57 -50.82283926010132 -6.887441006100602
58 -72.44788998365402 -6.776946485018116
59 -53.111203759908676 -6.72857353493394
60 -53.76270721107721 -6.671100489257628
61 -76.9330226778984 -6.596037293903165
62 -41.77274064719677 -6.5539197829400315
63 -70.77400410175323 -6.476844989612643
64 -44.86097139120102 -6.421675597356853
65 -73.51991760730743 -6.296111328364889
66 -74.92818307876587 -6.266756167836701
67 -54.623384803533554 -6.228088084684211
68 -74.30068898200989 -6.199330283858771
69 -40.5620801076293 -6.14518869474255
70 -59.38068372011185 -6.059479462951173
71 -38.41602338850498 -5.95591023523657
72 -44.08621750026941 -5.919223314878099
73 -64.16474264860153 -5.891579763393588
74 -68.6235037446022 -5.82596346085507
75 -36.32717753946781 -5.769296816183801
76 -33.15621091425419 -5.702488391892685
77 -38.562408328056335 -5.658129216552653
78 -56.48297104239464 -5.5966803266951555
79 -50.121491849422455 -5.57332661417246
80 -55.51751363277435 -5.502552225229215
81 -49.92490974068642 -5.457495762754132
82 -39.853369280695915 -5.411946158511884
83 -63.68837529420853 -5.350053362725614
84 -33.930142775177956 -5.338549956561249
85 -50.52406617999077 -5.281160240600011
86 -45.52282524108887 -5.120162788231898
87 -50.85674053430557 -5.1081387106203335
88 -55.72388454526663 -5.078485007852753
89 -34.83166886866093 -5.039714524709599
90 -43.87981244176626 -4.951005235457668
91 -58.1722591817379 -4.827572916892203
92 -52.3497334420681 -4.771388444240095
93 -40.910712003707886 -4.738620092082911
94 -26.851995065808296 -4.620638886868729
95 -51.50135236978531 -4.563081443170549
96 -34.34929804503918 -4.504924990699003
97 -40.478157453238964 -4.455573339212736
98 -41.76111572980881 -4.388734478221863
99 -51.80048197507858 -4.230832004686763
100 -37.36339974403381 -4.178760323691622
101 -32.472826436161995 -4.0574220119529505
102 -47.70844127610326 -4.031048624093466
103 -33.508612148463726 -4.005703575543896
104 -38.858102805912495 -3.909223142968567
105 -35.535857670009136 -3.811499424651851
106 -35.684043899178505 -3.7830571188263526
107 -35.32198476046324 -3.675579062401723
108 -22.058553531765938 -3.609463813019705
109 -35.53207701444626 -3.551304934789056
110 -33.856251664459705 -3.485123772051692
111 -15.924594566226006 -3.3677107699982223
112 -28.586373209953308 -3.2481100852574314
113 -19.82857419550419 -3.149892510476507
114 -14.299902603030205 -2.9221600364839664
115 -20.192092671990395 -2.705230055237873
116 -16.085278809070587 -2.4434719864360446
117 -4.92391300201416 -2.17451293781979
118 -5.942910268902779 -1.9801089659332811
119 -7.006828591227531 -1.682452207877752
train accuracy: 0.9766666666666667
validation accuracy: 1.0

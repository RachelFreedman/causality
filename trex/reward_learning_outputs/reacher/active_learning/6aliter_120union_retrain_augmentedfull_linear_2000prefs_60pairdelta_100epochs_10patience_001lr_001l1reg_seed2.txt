[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 1.485391225770627e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3933e-08,  2.3951e-02,  7.3328e-02, -2.0048e-06, -5.6122e-06,
         -1.6519e-04, -7.3494e-04, -2.8699e-06,  5.4908e-05,  6.7828e-06,
         -2.3655e-03, -2.5001e-01, -5.3214e-01]], device='cuda:2'))])
end of epoch 1: val_loss 0.0004558915230305161, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.0744e-02,  1.6581e-01,  3.6617e-02, -4.5260e-02, -1.6779e-05,
         -1.5639e-02,  2.2383e-07,  7.3669e-07, -1.5481e-04, -2.9418e-05,
         -2.5089e-03, -2.8572e-01, -1.4107e+00]], device='cuda:2'))])
end of epoch 3: val_loss 6.5565099660602754e-09, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.08008709829393496, val_acc 0.99
trigger times: 2
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.7386e-02,  2.7315e-02,  1.2264e-01, -6.6314e-06,  6.6077e-02,
          5.3750e-07,  1.3739e-02, -2.7585e-06,  1.5770e-04,  1.4990e-04,
          1.5752e-03, -7.3283e-01, -1.6812e+00]], device='cuda:2'))])
end of epoch 6: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.7553e-06, -2.3462e-06, -1.3315e-05, -4.2806e-05, -1.7315e-04,
          1.5520e-04, -2.1644e-06, -6.1703e-06,  3.7667e-04,  8.5894e-05,
         -1.2028e-03,  2.0378e-06, -1.3624e+00]], device='cuda:2'))])
end of epoch 7: val_loss 1.7453582216298004e-05, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 1.013277938000101e-08, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 2.4038126406722425e-06, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.3850e-06,  4.9292e-02,  1.1863e-01, -2.8288e-01,  1.0494e-04,
         -1.6852e-01,  5.4318e-07,  4.3023e-02,  1.6175e-04, -3.0278e-05,
         -2.3633e-03, -6.9090e-01, -1.4263e+00]], device='cuda:2'))])
end of epoch 12: val_loss 3.516669121239602e-08, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 3.5349004811344935, val_acc 0.71
trigger times: 2
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.4687e-03,  4.2008e-01,  1.6907e-01, -2.6917e-01, -9.4448e-05,
          3.1385e-05, -2.2777e-02,  2.6094e-02, -1.1739e-05, -1.0203e-01,
          9.7824e-06, -1.1105e+00, -2.0897e+00]], device='cuda:2'))])
end of epoch 15: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 3.5762784023063432e-09, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3569e-01, -9.2607e-02,  5.5456e-02, -1.6754e-01,  2.9089e-01,
         -1.3447e-01, -9.6825e-03,  5.3921e-02, -1.3183e-01, -6.7023e-02,
         -1.2006e-03, -1.6074e+00, -2.0703e+00]], device='cuda:2'))])
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.5538e-02, -2.1852e-02,  1.6805e-05, -8.3721e-02, -9.7925e-06,
          2.1210e-05, -2.2831e-02,  3.2126e-02,  9.6743e-05, -1.2258e-04,
          3.5707e-04, -1.1204e+00, -1.9494e+00]], device='cuda:2'))])
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.8965e-05,  8.2424e-06,  4.3980e-05, -2.4612e-05, -3.3590e-05,
          2.4860e-05, -7.2484e-03,  3.6770e-06,  4.1276e-05, -3.0398e-04,
         -3.1727e-04, -2.2512e-05, -1.6262e+00]], device='cuda:2'))])
end of epoch 20: val_loss 3.910050129363185e-07, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.7064e-05, -3.0025e-05,  2.9024e-01, -1.1860e-01,  3.3087e-05,
         -3.4935e-05, -2.1098e-02,  2.7051e-02, -5.7964e-05,  1.2177e-05,
         -2.3611e-03, -6.7159e-01, -1.9731e+00]], device='cuda:2'))])
end of epoch 23: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1107e-04, -6.5305e-05,  3.7627e-02, -1.7763e-05,  7.1343e-05,
         -1.0382e-04, -1.5648e-03,  1.5013e-05, -1.2026e-04,  4.0384e-05,
          3.8004e-03,  3.9267e-05, -1.5347e+00]], device='cuda:2'))])
end of epoch 24: val_loss 0.00016028102797950084, val_acc 1.0
trigger times: 1
end of epoch 25: val_loss 1.681893365457654e-05, val_acc 1.0
trigger times: 2
end of epoch 26: val_loss 8.904142304899665e-07, val_acc 1.0
trigger times: 3
end of epoch 27: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 4
end of epoch 28: val_loss 2.5331607047007765e-06, val_acc 1.0
trigger times: 5
end of epoch 29: val_loss 1.9713553319533616e-06, val_acc 1.0
trigger times: 6
end of epoch 30: val_loss 7.212155818336896e-08, val_acc 1.0
trigger times: 7
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.9362e-02,  2.1889e-01,  1.8524e-01, -7.5711e-02,  2.5531e-01,
         -3.9593e-01,  1.5483e-02,  2.0517e-02, -7.8650e-07,  1.1115e-01,
         -5.1686e-04, -1.4584e+00, -2.2448e+00]], device='cuda:2'))])
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6040e-05,  1.2156e-01,  7.7089e-02,  1.0424e-05,  2.8691e-05,
         -2.2569e-02,  8.4556e-03,  4.9838e-04, -3.3858e-05, -2.0011e-05,
          8.1852e-04, -6.7063e-01, -2.0809e+00]], device='cuda:2'))])
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3431e-06, -2.0303e-05, -1.8599e-05,  2.4496e-05,  1.5024e-04,
          5.6757e-05,  1.3534e-06,  2.6578e-06,  4.8496e-05, -9.9642e-05,
         -2.3589e-03,  5.9167e-04, -1.6778e+00]], device='cuda:2'))])
end of epoch 34: val_loss 3.396191794600156e-06, val_acc 1.0
trigger times: 1
end of epoch 35: val_loss 7.676500473507985e-07, val_acc 1.0
trigger times: 2
end of epoch 36: val_loss 6.556509610788907e-09, val_acc 1.0
trigger times: 3
end of epoch 37: val_loss 6.276570529507808e-05, val_acc 1.0
trigger times: 4
end of epoch 38: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.8069e-02,  1.5591e-01,  1.5172e-01,  1.0960e-02,  1.4638e-01,
         -7.3539e-02, -9.3518e-03,  7.7220e-02, -4.1523e-06,  2.2893e-05,
          1.5818e-03, -1.7325e+00, -2.5952e+00]], device='cuda:2'))])
end of epoch 39: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.5267e-05,  4.0627e-02,  3.1346e-02,  6.2931e-06,  4.9256e-05,
          6.8814e-05, -2.2983e-03,  5.0277e-02, -2.1278e-05,  5.1857e-05,
         -1.1962e-03, -9.6800e-01, -2.3661e+00]], device='cuda:2'))])
end of epoch 40: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.2472e-05, -2.3563e-05,  3.7530e-05, -3.0328e-05, -2.2064e-04,
          6.4679e-05,  4.7039e-06, -4.9347e-06, -6.3028e-05, -2.6588e-04,
          3.6147e-04,  3.3734e-04, -1.8027e+00]], device='cuda:2'))])
end of epoch 41: val_loss 0.0003262604124827817, val_acc 1.0
trigger times: 1
end of epoch 42: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.4194e-03,  1.2936e-01,  4.6961e-02, -3.0002e-01, -8.9366e-05,
         -5.6084e-02,  4.1870e-03,  3.4821e-02, -1.9486e-05, -5.7228e-05,
         -5.1466e-04, -1.0838e+00, -2.0042e+00]], device='cuda:2'))])
end of epoch 43: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0979e-05,  2.1421e-05,  2.6171e-05, -1.2426e-01, -2.2901e-04,
          8.1569e-05,  2.3955e-07,  5.7283e-04, -5.5060e-05, -1.1451e-04,
          8.2072e-04, -1.5172e-04, -1.7161e+00]], device='cuda:2'))])
end of epoch 44: val_loss 3.039835707596694e-08, val_acc 1.0
trigger times: 1
end of epoch 45: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 2
end of epoch 46: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.9393e-02, -5.5032e-02,  1.3498e-01, -1.6140e-01, -9.5902e-05,
          1.5356e-01, -1.7481e-02, -1.1375e-02,  5.5796e-05, -8.8412e-02,
         -2.5001e-03, -1.2816e+00, -2.3333e+00]], device='cuda:2'))])
end of epoch 47: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0273e-04, -2.6519e-05, -2.9995e-05, -3.6197e-05,  3.9324e-05,
         -1.1629e-04, -6.0215e-04, -1.2991e-05, -2.9201e-04,  6.6508e-06,
          1.6383e-05,  9.6751e-05, -1.9593e+00]], device='cuda:2'))])
end of epoch 48: val_loss 1.847743508420763e-08, val_acc 1.0
trigger times: 1
end of epoch 49: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.5786e-02,  2.1370e-01,  1.9537e-01, -1.3068e-01,  8.7170e-02,
         -3.1067e-01,  1.3930e-03,  4.6698e-02, -1.7768e-05, -3.3340e-05,
          1.5840e-03, -1.2570e+00, -2.0635e+00]], device='cuda:2'))])
end of epoch 50: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0046e-05,  6.9100e-02,  5.4130e-02, -1.5821e-02, -1.9363e-05,
          5.1893e-05, -4.5947e-07,  1.3662e-02, -5.1984e-05, -1.0777e-04,
         -1.1940e-03, -2.3222e-01, -1.8224e+00]], device='cuda:2'))])
end of epoch 51: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 52: val_loss 3.5762775496550603e-09, val_acc 1.0
trigger times: 2
end of epoch 53: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.7878e-02,  1.6373e-02,  1.3011e-01, -1.4158e-01, -4.8968e-05,
         -4.5826e-05, -1.5220e-03,  9.3728e-07,  1.3934e-04,  3.6938e-05,
         -5.1246e-04, -8.4461e-01, -1.8844e+00]], device='cuda:2'))])
end of epoch 54: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.5989e-05, -3.3708e-05, -3.8575e-05, -3.1606e-05,  1.2128e-04,
         -1.0191e-04, -1.3352e-06,  2.4510e-06,  3.7782e-04, -1.4460e-04,
          8.2292e-04,  7.3882e-05, -1.5084e+00]], device='cuda:2'))])
end of epoch 55: val_loss 1.6336188156920174e-05, val_acc 1.0
trigger times: 1
end of epoch 56: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5612e-01,  4.4214e-01,  1.9090e-01, -2.1323e-02,  3.2495e-01,
         -2.5525e-01, -7.2232e-03,  5.2272e-02, -1.7003e-02, -4.1378e-05,
          3.8070e-03, -1.8393e+00, -2.6536e+00]], device='cuda:2'))])
end of epoch 57: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.9850e-02,  2.8921e-01,  5.0978e-02,  3.0125e-06, -2.6602e-05,
         -4.3752e-05,  1.3262e-06,  3.1190e-02,  5.0042e-05, -1.0942e-04,
         -2.4979e-03, -1.0409e+00, -2.4590e+00]], device='cuda:2'))])
end of epoch 58: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.8449e-05,  3.8015e-05, -3.5383e-05,  6.9424e-06, -7.5628e-05,
         -1.1025e-04, -2.6617e-06,  2.1694e-05,  1.2180e-04, -2.7414e-04,
          1.8583e-05, -3.5006e-06, -1.9804e+00]], device='cuda:2'))])
end of epoch 59: val_loss 5.829303763604799e-07, val_acc 1.0
trigger times: 1
end of epoch 60: val_loss 1.013277938000101e-08, val_acc 1.0
trigger times: 2
end of epoch 61: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-9.7146e-06,  1.5326e-02,  2.8244e-01, -2.3652e-01,  3.2350e-05,
         -1.2092e-04,  4.2059e-03,  7.9013e-02, -1.4828e-04,  7.3644e-05,
         -1.1918e-03, -8.2128e-01, -2.0639e+00]], device='cuda:2'))])
end of epoch 62: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9740e-05,  1.2232e-05, -1.2317e-04, -6.5943e-06,  1.1652e-04,
         -2.9519e-04, -1.5595e-07,  5.4309e-06, -3.9201e-04,  2.0057e-04,
          3.6587e-04,  1.5634e-04, -1.5446e+00]], device='cuda:2'))])
end of epoch 63: val_loss 0.06318557149363144, val_acc 0.99
trigger times: 1
end of epoch 64: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.6711e-05, -3.3220e-06,  3.3104e-01, -1.1332e-01,  3.0383e-01,
         -3.7809e-05, -5.1002e-03,  1.2059e-02, -3.8368e-05, -4.6515e-06,
         -5.1026e-04, -6.1035e-01, -1.6221e+00]], device='cuda:2'))])
end of epoch 65: val_loss 1.7881391656260348e-09, val_acc 1.0
trigger times: 1
end of epoch 66: val_loss 3.597543588604424e-05, val_acc 1.0
trigger times: 2
end of epoch 67: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.7504e-01,  2.7106e-01,  1.1677e-01, -3.5219e-01,  3.5749e-01,
          4.0027e-05,  4.1365e-02,  8.2043e-02, -2.5244e-04, -2.1251e-01,
          3.8092e-03, -1.7739e+00, -2.5129e+00]], device='cuda:2'))])
end of epoch 68: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.2794e-03,  1.3707e-01, -5.1138e-06, -2.6241e-01,  4.4389e-05,
          1.4710e-06,  3.3390e-02,  5.7743e-02, -9.9104e-05,  8.1042e-06,
         -2.4957e-03, -1.0589e+00, -2.2869e+00]], device='cuda:2'))])
end of epoch 69: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 70: val_loss 0.00028857197950941325, val_acc 1.0
trigger times: 2
end of epoch 71: val_loss 1.668927325226832e-08, val_acc 1.0
trigger times: 3
end of epoch 72: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 4
end of epoch 73: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.2526e-05, -9.9422e-05, -1.8254e-05,  1.0894e-05, -5.3096e-05,
          4.0193e-04,  2.3814e-06,  8.8112e-03,  2.9411e-04, -7.1081e-05,
          3.6807e-04,  1.6588e-04, -1.6917e+00]], device='cuda:2'))])
end of epoch 74: val_loss 0.00019870158299141848, val_acc 1.0
trigger times: 1
end of epoch 75: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.7051e-02,  2.8699e-01,  2.6497e-01, -5.9126e-06, -4.7819e-05,
          5.5092e-05, -1.5728e-02, -3.1055e-02,  8.6662e-05, -1.3377e-01,
         -5.0806e-04, -1.2278e+00, -2.1550e+00]], device='cuda:2'))])
end of epoch 76: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3095e-05,  9.5389e-02,  8.7309e-02, -1.1980e-05,  1.8039e-06,
          1.3464e-04, -3.9231e-03, -6.5118e-06,  2.1078e-04,  1.8871e-04,
          8.2732e-04, -8.7895e-02, -1.8825e+00]], device='cuda:2'))])
end of epoch 77: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 1
end of epoch 78: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.7743e-04,  2.3337e-03, -7.8112e-02, -1.9669e-02,  2.4241e-01,
          3.2223e-02,  1.1202e-02,  6.8249e-02,  8.3176e-05, -5.9210e-01,
          3.8114e-03, -1.2702e+00, -1.9842e+00]], device='cuda:2'))])
end of epoch 79: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.6277e-05, -1.0429e-05, -1.4056e-05,  3.7564e-06,  1.0327e-05,
          3.2160e-05,  1.3930e-03,  2.4232e-02,  2.1477e-04, -4.4254e-05,
         -2.4935e-03, -5.2188e-01, -1.7874e+00]], device='cuda:2'))])
end of epoch 80: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.0102e-05, -4.3961e-05, -3.2423e-05, -1.8126e-05,  6.0706e-05,
          3.2553e-04,  2.3586e-06, -1.0281e-05,  5.3509e-04, -4.4821e-04,
          2.2984e-05, -2.6499e-04, -1.3031e+00]], device='cuda:2'))])
end of epoch 81: val_loss 4.559547596727498e-07, val_acc 1.0
trigger times: 1
end of epoch 82: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 2
end of epoch 83: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 3
end of epoch 84: val_loss 1.0073173857705342e-07, val_acc 1.0
trigger times: 4
end of epoch 85: val_loss 1.4901138456480112e-08, val_acc 1.0
trigger times: 5
end of epoch 86: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.2574e-05,  2.5442e-01,  2.6962e-01, -2.0575e-01,  9.5851e-05,
         -7.3137e-05,  2.9177e-02,  9.1131e-02,  6.4319e-05,  2.4286e-05,
         -5.0586e-04, -1.1693e+00, -2.4581e+00]], device='cuda:2'))])
end of epoch 87: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.9421e-06, -2.2655e-05,  2.6324e-02, -1.8235e-02,  3.2795e-04,
         -2.6147e-04,  1.1466e-02,  5.1046e-02, -3.0986e-05,  1.1432e-04,
          8.2952e-04, -1.1969e-05, -2.0639e+00]], device='cuda:2'))])
end of epoch 88: val_loss 7.1525563782870446e-09, val_acc 1.0
trigger times: 1
end of epoch 89: val_loss 0.030532718041213228, val_acc 0.995
trigger times: 2
end of epoch 90: val_loss 5.215174405748257e-07, val_acc 1.0
trigger times: 3
end of epoch 91: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 4
end of epoch 92: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.2118e-05,  1.5088e-04,  1.9348e-05, -3.4573e-05, -9.4618e-04,
          3.1859e-06, -3.6946e-06,  4.3710e-05,  5.1267e-05,  1.6458e-04,
         -4.6618e-06,  5.8673e-05, -1.7282e+00]], device='cuda:2'))])
end of epoch 93: val_loss 0.00023484642594667093, val_acc 1.0
trigger times: 1
end of epoch 94: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0399e-02,  2.4804e-01,  1.5573e-01, -2.1906e-02,  6.6269e-02,
         -3.2021e-02,  2.4255e-02,  9.4712e-02,  1.7632e-05,  6.2566e-05,
         -1.1852e-03, -1.2582e+00, -2.1869e+00]], device='cuda:2'))])
end of epoch 95: val_loss 4.172324139517514e-09, val_acc 1.0
trigger times: 1
end of epoch 96: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.9879e-05,  3.3627e-05, -7.1141e-05, -5.4062e-05, -2.2815e-05,
          2.1116e-04,  4.7071e-06, -8.6228e-06, -3.4472e-05, -3.8346e-04,
         -3.0186e-04, -1.1126e-04, -1.2612e+00]], device='cuda:2'))])
end of epoch 97: val_loss 4.054632503535771e-05, val_acc 1.0
trigger times: 1
end of epoch 98: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.2797e-06,  4.3472e-02,  1.8677e-01, -1.6847e-01, -1.0158e-04,
         -1.4316e-01,  1.0998e-02, -1.2879e-02,  1.1440e-05, -4.9461e-06,
          8.3172e-04, -9.1455e-01, -1.9208e+00]], device='cuda:2'))])
end of epoch 99: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6335e-05, -8.3769e-06, -1.8325e-06,  1.4897e-06,  3.2846e-05,
         -1.3807e-04,  2.3635e-06,  7.4185e-06, -1.3596e-04, -6.2993e-05,
         -2.3457e-03,  2.3479e-04, -1.5165e+00]], device='cuda:2'))])
Finished training.
0 -64.38146372139454 -54.98547503240923
1 -66.89357435703278 -50.492268601198035
2 -59.92769366502762 -50.03933801517046
3 -61.66294130682945 -49.75347184620696
4 -61.030520260334015 -49.72654640753777
5 -64.8118037879467 -46.98011874490918
6 -61.394774824380875 -45.7351542845057
7 -58.95765345916152 -45.670579884154705
8 -60.713507890701294 -44.99030608142343
9 -57.054116770625114 -44.14602409201361
10 -57.68504664301872 -43.81326882122305
11 -58.436571445316076 -43.18878399086166
12 -59.692764922976494 -42.29180714825394
13 -56.48152784258127 -42.00401746161006
14 -62.38413891196251 -41.6910044370425
15 -57.57753345370293 -41.68588229294918
16 -58.17140930891037 -41.281777102712205
17 -55.7122408747673 -40.44278203413966
18 -57.50900536775589 -40.34838365523108
19 -55.116860300302505 -39.599701153458774
20 -55.16500783711672 -39.57586365327889
21 -52.53359255194664 -39.31972693233231
22 -50.101134687662125 -39.024610555047154
23 -52.64988470077515 -38.45534493538269
24 -52.59317719936371 -38.41270390343083
25 -54.24130855500698 -38.35634328077039
26 -50.92294754087925 -37.79713616772368
27 -48.25316500663757 -37.741528994987384
28 -57.2925728559494 -37.66475323879293
29 -51.937954023480415 -37.513139380385574
30 -57.80044749379158 -37.1809993033689
31 -52.81559357047081 -37.100703136010694
32 -51.68086093664169 -37.00630588930485
33 -53.869610607624054 -36.821916772458344
34 -53.18749335408211 -36.48799015296732
35 -49.87568259239197 -36.20965269874363
36 -51.36282246932387 -36.19207561676116
37 -54.15098676085472 -36.114459029559086
38 -50.66158011555672 -35.78149902167743
39 -47.78855008631945 -35.394503873250635
40 -52.70395378768444 -35.26282499693737
41 -51.08985163271427 -35.24303541418371
42 -53.319557167589664 -35.209705244501436
43 -51.53059795498848 -35.0654408505187
44 -49.62759489938617 -34.80241747531743
45 -49.59778590500355 -34.64469044638467
46 -46.95528405532241 -33.84284985953318
47 -44.54818311333656 -32.70706485357069
48 -44.144055277109146 -31.969099402548657
49 -45.26514285802841 -31.7109134007892
50 -47.39221262931824 -31.64414355845032
51 -46.465940311551094 -31.392382758954444
52 -49.24317529797554 -31.223196019713853
53 -43.84539443254471 -31.12953085092458
54 -45.6218758225441 -29.39157139549552
55 -46.74217876791954 -29.340125609942326
56 -38.03114095330238 -29.106189988903285
57 -42.19362134486437 -27.41102349748205
58 -42.974175579845905 -27.343722362182305
59 -43.5742072686553 -27.196681629483837
60 -41.93499691784382 -27.07399028854534
61 -38.187134981155396 -26.7047217556024
62 -40.923846274614334 -26.244794902859052
63 -41.37176722288132 -25.548365085275513
64 -37.525128081440926 -25.45878528601009
65 -39.90652436763048 -24.879106999799365
66 -39.0026855096221 -24.828695359328833
67 -40.3244189620018 -24.592745144504722
68 -38.883035972714424 -23.978745577896312
69 -37.447308383882046 -23.57262108435893
70 -35.45107824355364 -23.44970807952351
71 -37.29733492434025 -22.745309160183492
72 -35.71405676752329 -22.60679894414887
73 -35.911539264023304 -22.19891031871716
74 -35.49446968920529 -20.656863763892378
75 -32.447965923696756 -20.444472560731253
76 -32.396833926439285 -20.19699010077007
77 -35.83749159425497 -20.13839114930498
78 -34.13888733834028 -19.63760343800059
79 -34.574418194592 -19.515598718228343
80 -32.604018304497004 -18.92838809611677
81 -32.38611386716366 -17.994774057192853
82 -29.83783581107855 -17.55742370467821
83 -28.678369142115116 -16.823073927842348
84 -27.493043826892972 -14.855082803515382
85 -26.596436955034733 -14.531424598833084
86 -25.256019979715347 -14.442420089224363
87 -27.361867241561413 -13.596012850960644
88 -20.18600389547646 -12.68135972540495
89 -26.172610368579626 -12.66418205637357
90 -24.811680667102337 -12.30017947419658
91 -23.27116570621729 -12.151904772081672
92 -23.088498068973422 -11.788852141676486
93 -22.211370561271906 -10.869989101210326
94 -22.691012870986015 -10.327681503524177
95 -18.991274043917656 -9.8572159761571
96 -18.428677214775234 -8.330116995310416
97 -18.89939733222127 -8.133195842510668
98 -20.10806331411004 -8.108197691178031
99 -13.832594890147448 -7.57539849177145
100 -13.267675798386335 -7.362443126623615
101 -12.806239983998239 -7.108327355338034
102 -13.615984805393964 -6.959063561385431
103 -13.306355107575655 -6.776946485018116
104 -12.304881019052118 -6.7220638398623045
105 -13.6421467512846 -6.719970621583102
106 -18.971837393939495 -6.535447341844848
107 -15.675395034253597 -6.51820418055673
108 -15.258210748434067 -5.615796733870542
109 -12.999563715420663 -5.34720210027791
110 -12.649664947763085 -5.078485007852753
111 -13.034208612516522 -5.027957977402961
112 -11.8644303297624 -4.827572916892203
113 -12.379784037359059 -4.63049541560991
114 -11.992417879402637 -4.230832004686763
115 -12.313598454929888 -4.031048624093466
116 -11.332721155136824 -3.3844671463622564
117 -11.587159687653184 -3.3322555012187633
118 -12.018795602023602 -2.6416623314910934
119 -9.776183106936514 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -19.4608337  -18.9283881  -18.73129823 -17.99477406 -17.5574237
 -17.2877303  -16.99919682 -16.82307393 -16.73124049 -16.67001724
 -16.58659093 -16.32507254 -15.89510862 -15.83529718 -15.71653035
 -15.65454388 -15.29900699 -15.2719524  -15.13026299 -15.09811232
 -15.05822392 -14.89671643 -14.8550828  -14.594561   -14.5314246
 -14.44242009 -14.23740337 -14.04266231 -14.00356944 -13.99901576
 -13.96863183 -13.94553084 -13.86308449 -13.82168222 -13.67248689
 -13.608892   -13.59601285 -13.5769007  -13.53717325 -13.48391162
 -13.35053773 -13.24746818 -13.17814269 -13.17193674 -13.13997741
 -12.98804567 -12.94572355 -12.93760188 -12.89533945 -12.70915032
 -12.68787728 -12.68476663 -12.68135973 -12.66418206 -12.58492544
 -12.5568266  -12.54337072 -12.48846703 -12.37013582 -12.34671509
 -12.30017947 -12.24340388 -12.19488362 -12.17081576 -12.15190477
 -12.10286355 -11.97628117 -11.9718797  -11.94387787 -11.80244434
 -11.78885214 -11.78173063 -11.7724129  -11.70702865 -11.675454
 -11.67485775 -11.6440414  -11.54576322 -11.48214076 -11.27863965
 -11.10389205 -11.07648117 -11.07460556 -10.98789819 -10.96193206
 -10.89368728 -10.88447672 -10.8699891  -10.84529073 -10.83771365
 -10.7626531  -10.72252648 -10.4627546  -10.4515603  -10.37561746
 -10.3276815  -10.32701661 -10.17869462 -10.15370275 -10.06917835
 -10.05434057  -9.98600954  -9.97495295  -9.93667404  -9.89890309
  -9.87343312  -9.85721598  -9.84325222  -9.70803505  -9.63006997
  -9.47919626  -9.43633846  -9.41778779  -9.40824698  -9.35990043
  -9.34459136  -9.34125766  -9.34040597  -9.31853289  -9.25632952
  -9.22954286  -9.22723903  -9.13624814  -9.09796099  -9.09284414
  -8.93076101  -8.77833829  -8.77596385  -8.75445976  -8.49185067
  -8.330117    -8.30246843  -8.19902254  -8.13319584  -8.10819769
  -7.57539849  -7.51737497  -7.36244313  -7.10832736  -6.95906356
  -6.77694649  -6.72206384  -6.71997062  -6.68501373  -6.53544734
  -6.51820418  -6.43176687  -5.61579673  -5.3472021   -5.2737561
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.0003825570600411155, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4889e-02,  3.5268e-02,  1.9303e-02, -1.4821e-04,  2.6407e-05,
         -6.1761e-05, -5.8142e-03, -2.0598e-02,  3.2866e-05, -1.7966e-01,
         -2.3655e-03, -3.5095e-01, -4.4264e-01]], device='cuda:0'))])
end of epoch 1: val_loss 1.2092593737670432, val_acc 0.85
trigger times: 1
end of epoch 2: val_loss 2.6785304587448876e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 9.5747e-02,  1.7774e-01,  1.2371e-01, -1.4255e-04, -1.4436e-04,
          6.8491e-01, -4.1289e-02, -6.2715e-04, -5.0206e-05, -5.9898e-01,
         -2.5089e-03, -8.6239e-01, -1.4867e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.015157444896558374, val_acc 0.995
trigger times: 1
end of epoch 4: val_loss 0.0046694102595028755, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 2.5808588476188278e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0981e-03, -2.1183e-01,  5.1441e-02,  8.5534e-02, -1.6679e-01,
          1.7628e-01, -1.7540e-02, -1.2912e-03,  1.3652e-01, -2.8611e-01,
          1.5752e-03, -8.2007e-01, -1.7378e+00]], device='cuda:0'))])
end of epoch 6: val_loss 1.9992857705730672e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.0542e-01,  7.5349e-02,  3.3081e-01, -7.6218e-02, -8.6527e-07,
          5.1192e-01, -1.2896e-01,  1.7562e-02,  2.2189e-01, -7.5662e-01,
         -1.2028e-03, -1.4944e+00, -2.3865e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.00011301405797070174, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1410e-02, -2.8507e-02,  2.0561e-01,  5.8780e-02, -6.8909e-02,
          6.0846e-01, -4.4799e-02, -5.9005e-02,  1.3761e-01, -6.0362e-01,
         -3.1946e-04, -1.2906e+00, -2.3370e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.0002855416565540736, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 0.0014128030291145777, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 9.250216741293116e-05, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 5.5319680828667115e-05, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 0.00035590871546418155, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5788e-01,  4.1148e-02,  1.0971e-01, -2.0113e-05,  2.1280e-05,
          3.2895e-01, -3.0534e-02, -6.1440e-02, -8.1460e-05, -6.2761e-01,
          9.7824e-06, -1.4421e+00, -2.1529e+00]], device='cuda:0'))])
end of epoch 15: val_loss 0.0003192918982091442, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 4.571564368305303e-07, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.0006208898260990026, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 0.00017503164381690083, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 0.00024165963311574502, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 5.662425333952115e-08, val_acc 1.0
trigger times: 6
end of epoch 21: val_loss 1.5115708862936116e-05, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 7.794759267909513e-06, val_acc 1.0
trigger times: 8
end of epoch 23: val_loss 0.0019853612728427806, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 5.9604624880194025e-09, val_acc 1.0
trigger times: 10
Early stopping.
0 -116.85383605957031 -54.98547503240923
1 -120.39596539735794 -50.03933801517046
2 -123.39210152626038 -49.72654640753777
3 -100.89473843574524 -45.7351542845057
4 -98.20416396856308 -44.99030608142343
5 -115.02265429496765 -43.81326882122305
6 -101.41527646780014 -42.29180714825394
7 -109.54577112197876 -41.6910044370425
8 -102.30660319328308 -41.281777102712205
9 -97.97457909584045 -40.34838365523108
10 -91.62927857041359 -39.57586365327889
11 -89.7365728020668 -39.024610555047154
12 -90.30798789858818 -38.41270390343083
13 -98.4778180718422 -37.79713616772368
14 -95.00629562139511 -37.66475323879293
15 -98.41511297225952 -37.1809993033689
16 -88.23387932777405 -37.00630588930485
17 -95.65285676717758 -36.48799015296732
18 -83.36256712675095 -36.19207561676116
19 -88.12236422300339 -35.78149902167743
20 -103.29606133699417 -35.26282499693737
21 -88.88258810341358 -35.209705244501436
22 -90.54582598805428 -34.80241747531743
23 -86.53827041387558 -33.84284985953318
24 -88.06753206253052 -31.969099402548657
25 -72.72550988197327 -31.64414355845032
26 -79.40339668095112 -31.223196019713853
27 -85.85200646519661 -29.39157139549552
28 -81.16177761554718 -29.106189988903285
29 -68.66296819597483 -27.343722362182305
30 -76.20499867200851 -27.07399028854534
31 -71.58898383378983 -26.244794902859052
32 -72.29794871807098 -25.45878528601009
33 -66.57512376457453 -24.828695359328833
34 -75.79953813552856 -23.978745577896312
35 -72.90352973341942 -23.44970807952351
36 -65.36741161346436 -22.60679894414887
37 -65.86993116140366 -20.656863763892378
38 -56.225352227687836 -20.19699010077007
39 -60.93725860118866 -19.63760343800059
40 -56.726980686187744 -19.4608336980355
41 -54.41492956876755 -18.731298228582755
42 -51.711600095033646 -17.55742370467821
43 -51.74036544561386 -16.999196821113756
44 -46.58012330532074 -16.731240494206556
45 -50.67189186811447 -16.586590930336246
46 -46.65055572986603 -15.895108619308635
47 -46.439916491508484 -15.716530350593484
48 -46.05340528488159 -15.29900699234506
49 -45.62136244773865 -15.130262992826164
50 -44.445443749427795 -15.058223915588862
51 -40.84174643456936 -14.855082803515382
52 -47.75735379755497 -14.531424598833084
53 -36.88482251763344 -14.237403369666614
54 -45.57340729236603 -14.003569443724066
55 -42.86542522907257 -13.968631831022643
56 -41.46377441287041 -13.86308448731226
57 -43.23348271846771 -13.672486891155406
58 -38.803526163101196 -13.596012850960644
59 -32.36478979885578 -13.537173252032721
60 -43.917285203933716 -13.247468178452921
61 -40.60959792137146 -13.171936739318621
62 -29.031368166208267 -12.988045670217158
63 -34.26201927661896 -12.937601880303525
64 -27.929981984198093 -12.709150323057631
65 -36.19985815882683 -12.684766625629942
66 -44.33222192898393 -12.66418205637357
67 -23.34380564838648 -12.556826596461217
68 -39.519451677799225 -12.48846702822902
69 -29.879954017698765 -12.346715089027569
70 -22.416703768074512 -12.243403879196388
71 -37.68549716472626 -12.170815759622592
72 -30.180374890565872 -12.10286355263865
73 -22.875153623521328 -11.971879704988003
74 -30.126460313796997 -11.802444341088956
75 -29.41306585073471 -11.781730626483084
76 -32.60236921906471 -11.707028646158562
77 -35.15712344646454 -11.674857754603973
78 -27.264559112489223 -11.545763222571745
79 -21.59661216288805 -11.278639647398466
80 -29.45557478070259 -11.076481171996228
81 -33.76016306877136 -10.987898185149746
82 -38.17670273780823 -10.8936872753186
83 -33.34498035162687 -10.869989101210326
84 -26.48294347524643 -10.837713645707614
85 -34.040948033332825 -10.722526479931577
86 -29.455189168453217 -10.451560303375969
87 -27.849521532654762 -10.327681503524177
88 -30.392394363880157 -10.178694623110099
89 -29.998223930597305 -10.069178353474605
90 -19.117493238300085 -9.986009544966164
91 -18.169113591313362 -9.936674043396035
92 -29.21354354918003 -9.873433124041318
93 -20.463779501616955 -9.843252216045357
94 -24.98754146695137 -9.630069973226872
95 -24.92671473324299 -9.436338457972072
96 -21.67119738832116 -9.408246981646435
97 -18.898758947849274 -9.344591359124774
98 -38.62658616900444 -9.340405972508886
99 -21.915207862854004 -9.256329515217804
100 -17.18546301871538 -9.227239032064794
101 -23.748482435941696 -9.097960985817739
102 -19.00170885398984 -8.930761014077454
103 -16.34228817000985 -8.775963851048022
104 -16.040233798325062 -8.491850672526503
105 -27.330239981412888 -8.302468427151881
106 -27.005538303405046 -8.133195842510668
107 -28.200257889926434 -7.57539849177145
108 -26.031165719032288 -7.362443126623615
109 -19.264673613011837 -6.959063561385431
110 -23.80974289216101 -6.7220638398623045
111 -31.61197218298912 -6.685013730616453
112 -23.229005947709084 -6.51820418055673
113 -21.441532775759697 -5.615796733870542
114 -21.45617178082466 -5.273756100604971
115 -14.544543534517288 -5.027957977402961
116 -14.782319273799658 -4.63049541560991
117 -22.34475262835622 -4.031048624093466
118 -19.498657185584307 -3.3322555012187633
119 -13.162416795268655 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.8137634  -24.59274514 -24.03455963
 -23.97874558 -23.58332829 -23.57262108 -23.51029134 -23.44970808
 -23.39348065 -23.29229417 -23.03804936 -22.74530916 -22.60679894
 -22.19891032 -21.89787894 -21.83639166 -21.63652554 -21.56970954
 -21.50084218 -21.49276864 -20.65686376 -20.53025744 -20.44447256
 -20.43388583 -20.1969901  -20.13839115 -19.87520089 -19.7534442
 -19.6785601  -19.63760344 -19.57149014 -19.53099124 -19.51559872
 -19.4608337  -19.07829818 -18.96981233 -18.9283881  -18.770776
 -18.73129823 -18.63654911 -18.61182059 -18.4225203  -18.22567089
 -18.11123204 -18.09567148 -18.08894342 -18.08309075 -17.99477406
 -17.89591085 -17.85249559 -17.5574237  -17.5238387  -17.51466278
 -17.46955573 -17.30806862 -17.2877303  -17.10132819 -16.99919682
 -16.8614333  -16.84834102 -16.82307393 -16.73172811 -16.73124049
 -16.67001724 -16.64690105 -16.60283379 -16.58659093 -16.5431465
 -16.53401037 -16.49751746 -16.41235951 -16.32507254 -16.28251882
 -16.14612078 -16.12129103 -16.0872847  -16.08190245 -15.89510862
 -15.83529718 -15.81694256 -15.71653035 -15.65454388 -15.6145203
 -15.59459196 -15.49574917 -15.29900699 -15.2719524  -15.21158976
 -15.13026299 -15.09811232 -15.05822392 -14.89671643 -14.8550828
 -14.82695649 -14.77312633 -14.72489417 -14.6650308  -14.594561
 -14.58561812 -14.5314246  -14.44242009 -14.38609612 -14.32701492
 -14.23740337 -14.20878741 -14.17704171 -14.04266231 -14.00356944
 -13.99901576 -13.96863183 -13.94553084 -13.86308449 -13.82168222
 -13.75008674 -13.74313797 -13.67248689 -13.66965006 -13.608892
 -13.59601285 -13.5769007  -13.53717325 -13.50094998 -13.48391162
 -13.35053773 -13.24746818 -13.17814269 -13.17193674 -13.13997741
 -13.12635144 -12.98804567 -12.97427783 -12.94572355 -12.93760188
 -12.89533945 -12.74539386 -12.70915032 -12.68787728 -12.68476663
 -12.68135973 -12.66418206 -12.61270979 -12.58492544 -12.5568266
 -12.54337072 -12.51755483 -12.48846703 -12.45469195 -12.37013582
 -12.34671509 -12.30017947 -12.27979948 -12.24340388 -12.19488362
 -12.17081576 -12.15190477 -12.10286355 -12.02314402 -12.00833909
 -11.97628117 -11.9718797  -11.94387787 -11.80244434 -11.78885214
 -11.78173063 -11.7724129  -11.74297951 -11.70702865 -11.675454
 -11.67485775 -11.66062457 -11.6440414  -11.62446268 -11.54576322
 -11.48214076 -11.46504032 -11.4025404  -11.27863965 -11.19424126
 -11.18187983 -11.10389205 -11.07648117 -11.07460556 -11.0538427
 -10.98789819 -10.96193206 -10.89368728 -10.88447672 -10.8699891
 -10.84529073 -10.83771365 -10.77809419 -10.7626531  -10.72252648
 -10.62949326 -10.59437825 -10.54436717 -10.4627546  -10.4515603
 -10.42240761 -10.37561746 -10.3276815  -10.32701661 -10.31802362
 -10.30784685 -10.18766589 -10.17869462 -10.15370275 -10.06917835
 -10.05434057 -10.01626687  -9.98600954  -9.97495295  -9.93667404
  -9.89890309  -9.87343312  -9.85721598  -9.84325222  -9.83271252
  -9.70803505  -9.63006997  -9.47919626  -9.45479372  -9.45428377
  -9.43633846  -9.41778779  -9.40824698  -9.35990043  -9.34459136
  -9.34125766  -9.34040597  -9.31853289  -9.25632952  -9.22954286
  -9.22723903  -9.13624814  -9.09796099  -9.09284414  -9.08951284
  -8.93076101  -8.77833829  -8.77596385  -8.76739334  -8.75445976
  -8.54199036  -8.49185067  -8.330117    -8.30246843  -8.19902254
  -8.13319584  -8.10819769  -7.57539849  -7.51737497  -7.36244313
  -7.10832736  -6.95906356  -6.77694649  -6.72206384  -6.71997062
  -6.68501373  -6.53544734  -6.51820418  -6.43176687  -5.61579673
  -5.3472021   -5.2737561   -5.07848501  -5.02795798  -4.82757292
  -4.63049542  -4.43501791  -4.230832    -4.03122369  -4.03104862
  -3.44219496  -3.38446715  -3.37710268  -3.3322555   -3.08244978
  -2.92137389  -2.8803343   -2.77499084  -2.68532863  -2.65837176
  -2.64166233  -2.59195578  -2.49631954  -2.40363433  -2.33561719
  -2.28316262  -2.27877616  -1.91361965  -1.47859569  -1.42185534]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.013372756732360784, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0022,  0.0444,  0.1687,  0.0288,  0.2578,  0.1329,  0.0044, -0.0305,
         -0.1370, -0.4327, -0.0024, -0.8054, -1.3213]], device='cuda:0'))])
end of epoch 1: val_loss 0.0036738956146120925, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.6726e-02, -3.1390e-02,  9.2177e-02, -5.4193e-02,  9.7298e-03,
          4.8133e-02, -2.9727e-04, -2.3618e-02,  3.9980e-02, -1.9076e-01,
          3.7959e-03, -4.8390e-01, -1.1942e+00]], device='cuda:0'))])
end of epoch 2: val_loss 6.26294081682488e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.0044e-01,  9.9516e-02,  4.4333e-01,  8.0989e-02, -1.4341e-01,
          6.7152e-01, -5.2822e-03, -5.6631e-02,  1.8076e-05, -7.8334e-01,
         -2.5089e-03, -1.7551e+00, -3.2952e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.37845737457269707, val_acc 0.985
trigger times: 1
end of epoch 4: val_loss 0.010554603111941105, val_acc 0.995
trigger times: 2
end of epoch 5: val_loss 0.06348234300210606, val_acc 0.99
trigger times: 3
end of epoch 6: val_loss 0.0011130138137368916, val_acc 1.0
trigger times: 4
end of epoch 7: val_loss 0.05393061506794737, val_acc 0.995
trigger times: 5
end of epoch 8: val_loss 0.031109737751464764, val_acc 0.995
trigger times: 6
end of epoch 9: val_loss 0.11399486541748047, val_acc 0.995
trigger times: 7
end of epoch 10: val_loss 0.3721725092199631, val_acc 0.985
trigger times: 8
end of epoch 11: val_loss 0.35530900753685274, val_acc 0.985
trigger times: 9
end of epoch 12: val_loss 0.02613869920780303, val_acc 0.995
trigger times: 10
Early stopping.
0 -305.0773458480835 -54.98547503240923
1 -280.59184765815735 -49.75347184620696
2 -281.8842658996582 -45.7351542845057
3 -272.6098817586899 -44.14602409201361
4 -237.18674504756927 -42.29180714825394
5 -252.1449515223503 -41.68588229294918
6 -271.1068637371063 -40.34838365523108
7 -224.83856451511383 -39.31972693233231
8 -218.51874482631683 -38.41270390343083
9 -226.88564729690552 -37.741528994987384
10 -261.34286522865295 -37.1809993033689
11 -248.80314481258392 -36.821916772458344
12 -206.67858397960663 -36.19207561676116
13 -237.52975100278854 -35.394503873250635
14 -217.37084698677063 -35.209705244501436
15 -235.1909937262535 -34.64469044638467
16 -213.78231412172318 -31.969099402548657
17 -199.3015199303627 -31.392382758954444
18 -201.84421974420547 -29.39157139549552
19 -183.050430893898 -27.41102349748205
20 -173.8329882323742 -27.07399028854534
21 -168.60985630750656 -25.548365085275513
22 -164.27731001377106 -24.828695359328833
23 -196.46046769618988 -24.03455963170103
24 -168.02628979086876 -23.57262108435893
25 -192.41998052597046 -23.393480651117404
26 -186.44069248437881 -22.745309160183492
27 -183.9330495595932 -21.897878940604915
28 -182.2204048037529 -21.56970953716178
29 -187.48336884379387 -20.656863763892378
30 -153.02253913879395 -20.19699010077007
31 -149.44201970100403 -19.753444196511282
32 -159.68327486515045 -19.57149013584634
33 -91.83705389499664 -19.4608336980355
34 -131.60689587891102 -18.92838809611677
35 -151.25376915931702 -18.636549110679173
36 -154.5886242389679 -18.225670892175096
37 -151.13281798362732 -18.08894342148713
38 -126.83718860149384 -17.89591085036888
39 -115.37684255838394 -17.523838696040855
40 -128.79459810256958 -17.308068620764864
41 -81.29725003242493 -16.999196821113756
42 -134.03125296533108 -16.823073927842348
43 -81.1863785982132 -16.67001723842762
44 -85.46203070878983 -16.586590930336246
45 -126.62124842405319 -16.497517458997606
46 -101.60308188199997 -16.28251881875062
47 -98.71634230017662 -16.087284697115294
48 -75.69680696725845 -15.835297177460307
49 -80.85715335607529 -15.65454387704455
50 -97.45129960775375 -15.495749174847994
51 -142.7394037246704 -15.211589760607882
52 -77.27741318941116 -15.058223915588862
53 -122.52686321735382 -14.826956494110142
54 -94.70971496403217 -14.665030798936618
55 -109.77451001107693 -14.531424598833084
56 -94.08226209878922 -14.32701491735508
57 -89.24198537319899 -14.177041714508036
58 -72.99939048290253 -13.999015758512774
59 -71.86215114593506 -13.86308448731226
60 -87.6838110089302 -13.743137965876109
61 -69.65083575248718 -13.608891999152753
62 -64.47217971086502 -13.537173252032721
63 -64.29403334856033 -13.350537733785846
64 -71.2082856297493 -13.171936739318621
65 -68.41573294997215 -12.988045670217158
66 -78.8263521194458 -12.937601880303525
67 -60.444880433380604 -12.709150323057631
68 -106.79978883266449 -12.68135972540495
69 -64.66238987445831 -12.584925435004964
70 -107.70479279756546 -12.517554833463981
71 -62.10844197869301 -12.370135820080268
72 -87.39849907159805 -12.279799480157958
73 -71.77653175592422 -12.170815759622592
74 -88.98385626077652 -12.023144019180897
75 -62.255505815148354 -11.971879704988003
76 -83.73209947347641 -11.788852141676486
77 -83.43915468454361 -11.742979507066295
78 -69.94242441654205 -11.674857754603973
79 -87.29106783866882 -11.62446267798645
80 -87.30117729306221 -11.46504031635131
81 -85.82967692613602 -11.194241256345855
82 -60.19674098491669 -11.076481171996228
83 -64.70996770262718 -10.987898185149746
84 -56.18981984257698 -10.884476715792978
85 -56.365600764751434 -10.837713645707614
86 -69.48935168981552 -10.722526479931577
87 -78.56199750304222 -10.544367172248164
88 -86.96561550348997 -10.422407609032872
89 -63.73117271065712 -10.3270166115354
90 -57.813624039292336 -10.178694623110099
91 -61.70603561401367 -10.054340569736782
92 -54.32578840851784 -9.974952948632556
93 -60.075579822063446 -9.873433124041318
94 -85.1671000123024 -9.832712523589095
95 -54.16633114218712 -9.479196255133107
96 -57.52682116627693 -9.436338457972072
97 -53.11679740995169 -9.359900431081572
98 -73.16483044624329 -9.340405972508886
99 -53.05972622334957 -9.229542857842185
100 -49.09593150019646 -9.097960985817739
101 -49.508745953440666 -8.930761014077454
102 -86.51834374666214 -8.767393335512658
103 -48.791885793209076 -8.491850672526503
104 -51.98554249107838 -8.199022537534278
105 -68.48990624397993 -7.57539849177145
106 -72.55785752087831 -7.108327355338034
107 -60.683444276452065 -6.7220638398623045
108 -61.515633061528206 -6.535447341844848
109 -44.36139187216759 -5.615796733870542
110 -39.52089749276638 -5.078485007852753
111 -35.9452691078186 -4.63049541560991
112 -20.591672375798225 -4.0312236943449005
113 -27.006093233823776 -3.3844671463622564
114 -6.3713239803910255 -3.0824497779388045
115 -6.683886133134365 -2.7749908442579825
116 -31.49270038306713 -2.6416623314910934
117 -4.716927535831928 -2.4036343341762803
118 -6.1776556223630905 -2.278776163773821
119 -4.340755216777325 -1.4218553446486095
train accuracy: 1.0
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.8137634  -24.59274514 -24.03455963
 -23.97874558 -23.58332829 -23.57262108 -23.51029134 -23.44970808
 -23.39348065 -23.29229417 -23.03804936 -22.74530916 -22.60679894
 -22.19891032 -21.89787894 -21.83639166 -21.63652554 -21.56970954
 -21.50084218 -21.49276864 -20.65686376 -20.53025744 -20.44447256
 -20.43388583 -20.1969901  -20.13839115 -19.87520089 -19.7534442
 -19.6785601  -19.63760344 -19.57149014 -19.53099124 -19.51559872
 -19.4608337  -19.07829818 -18.96981233 -18.9283881  -18.770776
 -18.73129823 -18.63654911 -18.61182059 -18.4225203  -18.22567089
 -18.11123204 -18.09567148 -18.08894342 -18.08309075 -17.99477406
 -17.89591085 -17.85249559 -17.5574237  -17.5238387  -17.51466278
 -17.46955573 -17.30806862 -17.2877303  -17.10132819 -16.99919682
 -16.8614333  -16.84834102 -16.82307393 -16.73172811 -16.73124049
 -16.67001724 -16.64690105 -16.60283379 -16.58659093 -16.5431465
 -16.53401037 -16.49751746 -16.41235951 -16.32507254 -16.28251882
 -16.14612078 -16.12129103 -16.0872847  -16.08190245 -15.89510862
 -15.83529718 -15.81694256 -15.71653035 -15.65454388 -15.6145203
 -15.59459196 -15.49574917 -15.29900699 -15.2719524  -15.21158976
 -15.1860156  -15.16807292 -15.13026299 -15.09811232 -15.05822392
 -14.89671643 -14.8550828  -14.82856339 -14.82695649 -14.77312633
 -14.72489417 -14.6650308  -14.594561   -14.58561812 -14.5643771
 -14.5314246  -14.44242009 -14.38609612 -14.32701492 -14.24090388
 -14.23740337 -14.20878741 -14.17704171 -14.09737943 -14.04266231
 -14.03131293 -14.00356944 -14.00035481 -13.99901576 -13.99628479
 -13.98996721 -13.96863183 -13.94553084 -13.86308449 -13.8463564
 -13.82168222 -13.75008674 -13.74313797 -13.699817   -13.67248689
 -13.66965006 -13.63042046 -13.608892   -13.59601285 -13.58645341
 -13.5769007  -13.53717325 -13.50094998 -13.48391162 -13.35053773
 -13.28128196 -13.24746818 -13.17814269 -13.17193674 -13.13997741
 -13.12635144 -12.98804567 -12.97427783 -12.95400937 -12.94572355
 -12.93760188 -12.90919127 -12.89533945 -12.80363367 -12.7828239
 -12.74539386 -12.74448225 -12.70915032 -12.69049603 -12.68787728
 -12.68476663 -12.68135973 -12.66429794 -12.66418206 -12.6451144
 -12.61270979 -12.61011515 -12.58492544 -12.56825805 -12.5568266
 -12.55286515 -12.54337072 -12.52233257 -12.51755483 -12.48846703
 -12.45469195 -12.37013582 -12.36398782 -12.34671509 -12.30017947
 -12.27979948 -12.2534233  -12.24340388 -12.19488362 -12.17081576
 -12.15677563 -12.15190477 -12.10286355 -12.10232118 -12.02314402
 -12.00833909 -11.99766664 -11.97628117 -11.9718797  -11.94387787
 -11.80244434 -11.78885214 -11.78173063 -11.7724129  -11.74297951
 -11.70702865 -11.675454   -11.67485775 -11.66062457 -11.6440414
 -11.62446268 -11.58806794 -11.54576322 -11.48214076 -11.46504032
 -11.4025404  -11.36127947 -11.27863965 -11.20128427 -11.19424126
 -11.18187983 -11.15847697 -11.10389205 -11.07648117 -11.07460556
 -11.0538427  -10.99315133 -10.98789819 -10.96193206 -10.89368728
 -10.88447672 -10.8699891  -10.84529073 -10.83771365 -10.77809419
 -10.7626531  -10.73791243 -10.72252648 -10.62949326 -10.59437825
 -10.54436717 -10.46573704 -10.4627546  -10.4515603  -10.42240761
 -10.37561746 -10.3276815  -10.32701661 -10.31802362 -10.30784685
 -10.30481943 -10.29512946 -10.18766589 -10.17869462 -10.15370275
 -10.06917835 -10.05434057 -10.01626687 -10.01310085  -9.98600954
  -9.97495295  -9.93667404  -9.91846284  -9.90408056  -9.89890309
  -9.87343312  -9.86486813  -9.85721598  -9.8558125   -9.84325222
  -9.83271252  -9.76843159  -9.76204726  -9.70803505  -9.66915345
  -9.64819376  -9.63006997  -9.55526557  -9.52537989  -9.49549595
  -9.47919626  -9.45479372  -9.45428377  -9.43633846  -9.43051249
  -9.41778779  -9.40824698  -9.38226493  -9.35990043  -9.35124737
  -9.34459136  -9.34125766  -9.34040597  -9.31853289  -9.25632952
  -9.22954286  -9.22723903  -9.21865006  -9.17110142  -9.17049064
  -9.13624814  -9.09796099  -9.09284414  -9.08951284  -9.05654097
  -8.93076101  -8.8813358   -8.77833829  -8.77596385  -8.76739334
  -8.75445976  -8.63326004  -8.54199036  -8.49185067  -8.41487591
  -8.39777202  -8.330117    -8.30246843  -8.25657238  -8.19902254
  -8.16719519  -8.1394985   -8.13319584  -8.10968433  -8.10819769
  -7.88019905  -7.87095111  -7.85017431  -7.81815231  -7.78975702
  -7.57539849  -7.51737497  -7.36244313  -7.25578116  -7.20737556
  -7.1947267   -7.16145545  -7.10832736  -6.95906356  -6.90501541
  -6.77694649  -6.72206384  -6.71997062  -6.68501373  -6.55345137
  -6.53712004  -6.53544734  -6.51820418  -6.47177645  -6.43779084
  -6.43176687  -6.19461946  -5.88865356  -5.83461367  -5.78972364
  -5.69554755  -5.61579673  -5.3472021   -5.28999092  -5.27678673
  -5.2737561   -5.14856683  -5.09596379  -5.07848501  -5.0618994
  -5.02795798  -4.97387894  -4.95945946  -4.95123327  -4.8998206
  -4.86745311  -4.82757292  -4.63049542  -4.43501791  -4.32181279
  -4.26347708  -4.25898597  -4.230832    -4.21187786  -4.06284009
  -4.03122369  -4.03104862  -3.96201618  -3.95965653  -3.91566507
  -3.69868448  -3.69483696  -3.65483768  -3.5764627   -3.44219496
  -3.40184505  -3.38446715  -3.37710268  -3.37306409  -3.3322555
  -3.26750952  -3.08244978  -3.05618999  -3.03563761  -2.92137389
  -2.8803343   -2.82233026  -2.77499084  -2.77171746  -2.76670186
  -2.75763358  -2.70856633  -2.68532863  -2.65837176  -2.64166233
  -2.59195578  -2.49631954  -2.40363433  -2.33561719  -2.28316262
  -2.27877616  -2.25831239  -1.91361965  -1.47859569  -1.42185534]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:1
end of epoch 0: val_loss 0.02247045872117319, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.4876e-01,  1.7294e-01,  9.6735e-02, -3.7661e-02,  2.9807e-01,
         -1.0050e-02, -1.9958e-04,  1.0104e-02, -8.7213e-02, -3.6093e-01,
         -2.3655e-03, -1.4814e+00, -2.0394e+00]], device='cuda:1'))])
end of epoch 1: val_loss 5.453592309123678e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6115e-02,  3.3714e-02, -1.0909e-05, -3.1028e-02, -1.5062e-05,
          4.9359e-05, -1.5502e-03, -1.1688e-03,  5.7508e-05, -2.2012e-01,
          3.7959e-03, -1.4929e+00, -2.0100e+00]], device='cuda:1'))])
end of epoch 2: val_loss 0.000264533786344181, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.5815928585451341, val_acc 0.975
trigger times: 2
end of epoch 4: val_loss 3.4939951729029416e-06, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 6.589837887496231e-06, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 0.9523849612474337, val_acc 0.97
trigger times: 5
end of epoch 7: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.3481e-01,  5.2594e-06,  9.7120e-02, -3.3356e-01,  4.6027e-03,
          6.1683e-02,  1.7114e-02, -2.0190e-02,  4.1592e-05,  5.4545e-06,
          3.5488e-04, -2.5223e+00, -4.2262e+00]], device='cuda:1'))])
end of epoch 8: val_loss 7.05669226590544e-07, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 1.2604793300852179e-06, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.06271338165664474, val_acc 0.995
trigger times: 3
end of epoch 11: val_loss 8.761806384427473e-08, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.0623e-02, -9.8069e-03, -2.9954e-06, -2.6583e-01,  1.7612e-04,
         -1.6265e-04,  2.9759e-02, -2.6706e-06, -5.2706e-06,  4.1533e-05,
          3.7982e-03, -1.9095e+00, -3.4632e+00]], device='cuda:1'))])
end of epoch 13: val_loss 4.05891150876414e-07, val_acc 1.0
trigger times: 1
end of epoch 14: val_loss 0.00021688280627131463, val_acc 1.0
trigger times: 2
end of epoch 15: val_loss 2.0174779926236397e-05, val_acc 1.0
trigger times: 3
end of epoch 16: val_loss 1.2218914939410296e-07, val_acc 1.0
trigger times: 4
end of epoch 17: val_loss 2.7064676392534183e-05, val_acc 1.0
trigger times: 5
end of epoch 18: val_loss 0.013247859241015476, val_acc 0.995
trigger times: 6
end of epoch 19: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 7
end of epoch 20: val_loss 0.12282359860837459, val_acc 0.99
trigger times: 8
end of epoch 21: val_loss 3.00994397548493e-07, val_acc 1.0
trigger times: 9
end of epoch 22: val_loss 0.0031021505297582053, val_acc 1.0
trigger times: 10
Early stopping.
0 -170.7751760482788 -54.98547503240923
1 -153.32876551151276 -49.72654640753777
2 -162.6694723367691 -44.99030608142343
3 -130.2258259654045 -42.29180714825394
4 -145.12671142816544 -41.281777102712205
5 -134.4325499534607 -39.57586365327889
6 -130.18578553199768 -38.41270390343083
7 -136.6152812242508 -37.66475323879293
8 -122.13131728768349 -37.00630588930485
9 -124.08473148941994 -36.19207561676116
10 -126.86296451091766 -35.26282499693737
11 -121.53528040647507 -34.80241747531743
12 -123.09349298477173 -31.969099402548657
13 -119.2066747546196 -31.223196019713853
14 -105.89085388183594 -29.106189988903285
15 -97.47723571956158 -27.07399028854534
16 -100.86690330505371 -25.45878528601009
17 -93.97405300289392 -24.592745144504722
18 -91.49041360616684 -23.57262108435893
19 -129.45140480995178 -23.292294170915508
20 -129.5860242843628 -21.897878940604915
21 -123.05687189102173 -21.500842181580236
22 -88.86428253352642 -20.444472560731253
23 -114.70534491539001 -19.875200887783702
24 -114.44298708438873 -19.57149013584634
25 -113.75937271118164 -19.078298175956636
26 -40.159332901239395 -18.731298228582755
27 -112.70386159420013 -18.225670892175096
28 -114.65247356891632 -18.083090752342976
29 -71.3543539494276 -17.55742370467821
30 -97.37512826919556 -17.308068620764864
31 -83.33583319187164 -16.86143329904797
32 -34.96555119752884 -16.731240494206556
33 -38.363153010606766 -16.586590930336246
34 -100.88759744167328 -16.412359507841103
35 -98.42279863357544 -16.12129102540531
36 -37.28966614603996 -15.835297177460307
37 -75.47553706169128 -15.614520300205465
38 -37.155246406793594 -15.27195239970661
39 -35.454174906015396 -15.130262992826164
40 -64.50334555655718 -14.855082803515382
41 -72.79099065065384 -14.724894174552386
42 -37.373646289110184 -14.564377098474427
43 -74.6191029548645 -14.32701491735508
44 -70.87037020921707 -14.177041714508036
45 -35.058902472257614 -14.003569443724066
46 -37.84468287229538 -13.989967205775274
47 -33.33715355396271 -13.846356397801603
48 -36.81691426038742 -13.699817000119465
49 -32.41686649620533 -13.608891999152753
50 -32.791599079966545 -13.537173252032721
51 -37.982668191194534 -13.281281956834897
52 -33.239698976278305 -13.139977413001635
53 -38.729463666677475 -12.954009369904936
54 -29.973045125603676 -12.895339452657973
55 -36.16370749473572 -12.744482250997669
56 -31.3539080619812 -12.684766625629942
57 -36.23737168312073 -12.645114399678981
58 -34.4827518761158 -12.568258054812697
59 -34.16198045015335 -12.52233257362037
60 -35.023315116763115 -12.363987818034827
61 -36.660627126693726 -12.253423300965668
62 -37.274994015693665 -12.156775630468069
63 -69.75017428398132 -12.023144019180897
64 -33.20349456742406 -11.971879704988003
65 -30.395884051918983 -11.781730626483084
66 -32.18946224451065 -11.67545400252302
67 -70.05457103252411 -11.62446267798645
68 -68.88731789588928 -11.46504031635131
69 -33.37793347239494 -11.201284272779965
70 -30.03462314605713 -11.103892054948908
71 -32.7022452801466 -10.993151334549694
72 -26.790800377726555 -10.884476715792978
73 -77.59856247901917 -10.778094188592915
74 -71.19102245569229 -10.62949325534916
75 -28.384178891777992 -10.462754599027946
76 -53.020513862371445 -10.327681503524177
77 -31.204731538891792 -10.304819426032076
78 -27.778454639017582 -10.153702747757642
79 -32.93291065096855 -10.01310085281142
80 -32.23891831934452 -9.918462835369475
81 -29.36658190190792 -9.864868128756767
82 -68.12146127223969 -9.832712523589095
83 -30.792157888412476 -9.6691534486793
84 -30.658235922455788 -9.525379889860924
85 -66.24771231412888 -9.454283769375778
86 -25.460621187463403 -9.408246981646435
87 -31.17909201979637 -9.344591359124774
88 -27.28627859055996 -9.256329515217804
89 -30.173822790384293 -9.171101420775566
90 -29.10195155441761 -9.092844144225124
91 -30.19149473309517 -8.88133579627069
92 -28.968468487262726 -8.754459760935882
93 -27.27759374678135 -8.414875912908172
94 -30.308550506830215 -8.256572378740486
95 -44.9648989289999 -8.133195842510668
96 -30.082793906331062 -7.870951112588292
97 -36.51789438724518 -7.57539849177145
98 -28.98264428973198 -7.20737555633513
99 -33.116012362763286 -6.959063561385431
100 -30.681168377399445 -6.685013730616453
101 -37.13712813705206 -6.51820418055673
102 -27.282771289348602 -6.194619463988254
103 -27.01445372402668 -5.695547546561518
104 -27.437449984252453 -5.276786731851282
105 -29.70644997805357 -5.078485007852753
106 -24.776658356189728 -4.959459456437704
107 -31.25184355676174 -4.827572916892203
108 -23.08766906708479 -4.263477079931017
109 -22.80610851943493 -4.062840086602227
110 -23.13099356368184 -3.9596565321552384
111 -19.504515819251537 -3.6548376781765324
112 -26.128523215651512 -3.3844671463622564
113 -19.796082574874163 -3.267509520292097
114 -12.11586176045239 -2.9213738924472454
115 -21.710837030783296 -2.7717174635872235
116 -12.619896531105042 -2.685328629207377
117 -13.694648142904043 -2.4963195363892825
118 -11.409987218677998 -2.278776163773821
119 -10.94696936942637 -1.4218553446486095
train accuracy: 0.9983333333333333
validation accuracy: 1.0
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.8137634  -24.59274514 -24.03455963
 -23.97874558 -23.58332829 -23.57262108 -23.51029134 -23.44970808
 -23.39348065 -23.29229417 -23.03804936 -22.74530916 -22.60679894
 -22.19891032 -21.89787894 -21.83639166 -21.63652554 -21.56970954
 -21.50084218 -21.49276864 -20.65686376 -20.53025744 -20.44447256
 -20.43388583 -20.1969901  -20.13839115 -19.87520089 -19.7534442
 -19.6785601  -19.63760344 -19.57149014 -19.53099124 -19.51559872
 -19.4608337  -19.07829818 -18.96981233 -18.9283881  -18.770776
 -18.73129823 -18.63654911 -18.61182059 -18.4225203  -18.22567089
 -18.11123204 -18.09567148 -18.08894342 -18.08309075 -17.99477406
 -17.89591085 -17.85249559 -17.5574237  -17.5238387  -17.51466278
 -17.46955573 -17.30806862 -17.2877303  -17.10132819 -16.99919682
 -16.8614333  -16.84834102 -16.82307393 -16.73172811 -16.73124049
 -16.67001724 -16.64690105 -16.60283379 -16.58659093 -16.5431465
 -16.53401037 -16.49751746 -16.41235951 -16.32507254 -16.28251882
 -16.14612078 -16.12129103 -16.0872847  -16.08190245 -15.89510862
 -15.83529718 -15.81694256 -15.71653035 -15.65454388 -15.6145203
 -15.59459196 -15.49574917 -15.29900699 -15.2719524  -15.21158976
 -15.1860156  -15.16807292 -15.13026299 -15.09811232 -15.0806922
 -15.05822392 -14.89671643 -14.8550828  -14.82856339 -14.82695649
 -14.77312633 -14.72989964 -14.72489417 -14.6650308  -14.66284809
 -14.594561   -14.58561812 -14.5643771  -14.5314246  -14.44242009
 -14.38609612 -14.32701492 -14.24090388 -14.23740337 -14.21164262
 -14.20878741 -14.17704171 -14.09737943 -14.04266231 -14.03131293
 -14.00356944 -14.00035481 -13.99901576 -13.99628479 -13.98996721
 -13.96863183 -13.94553084 -13.86308449 -13.8463564  -13.82168222
 -13.75008674 -13.74313797 -13.699817   -13.67248689 -13.66965006
 -13.63042046 -13.608892   -13.59601285 -13.58803378 -13.58645341
 -13.5769007  -13.53717325 -13.51008468 -13.50094998 -13.48391162
 -13.35053773 -13.3127374  -13.28128196 -13.24746818 -13.23232486
 -13.20399316 -13.17814269 -13.17193674 -13.16092153 -13.13997741
 -13.12635144 -12.98804567 -12.97427783 -12.95400937 -12.95080968
 -12.94572355 -12.93760188 -12.90919127 -12.89533945 -12.86054854
 -12.85305787 -12.82640011 -12.80363367 -12.7828239  -12.74595291
 -12.74539386 -12.74448225 -12.7294417  -12.71725845 -12.70915032
 -12.69995766 -12.69049603 -12.68787728 -12.68476663 -12.68135973
 -12.66429794 -12.66418206 -12.64582593 -12.6451144  -12.61270979
 -12.61011515 -12.58492544 -12.57295794 -12.56825805 -12.56650197
 -12.5568266  -12.55286515 -12.54337072 -12.52233257 -12.51755483
 -12.49079212 -12.48904281 -12.48846703 -12.47519008 -12.45469195
 -12.4524432  -12.42922733 -12.37013582 -12.36398782 -12.34671509
 -12.30017947 -12.27979948 -12.25692808 -12.2534233  -12.24340388
 -12.19488362 -12.17081576 -12.15677563 -12.15190477 -12.10286355
 -12.10232118 -12.02314402 -12.00833909 -11.99766664 -11.97628117
 -11.9718797  -11.94387787 -11.91835728 -11.80244434 -11.78885214
 -11.78173063 -11.7724129  -11.75742969 -11.74297951 -11.70702865
 -11.69421946 -11.675454   -11.67485775 -11.66062457 -11.6440414
 -11.62446268 -11.58806794 -11.54576322 -11.48214076 -11.46504032
 -11.44310782 -11.4025404  -11.36127947 -11.31909132 -11.27863965
 -11.25703347 -11.20128427 -11.19424126 -11.18187983 -11.15847697
 -11.13382735 -11.10389205 -11.07648117 -11.07460556 -11.0538427
 -10.99315133 -10.98789819 -10.96193206 -10.89368728 -10.88447672
 -10.8699891  -10.85680187 -10.84529073 -10.83873062 -10.83771365
 -10.77809419 -10.7626531  -10.73791243 -10.72252648 -10.62949326
 -10.59437825 -10.54436717 -10.46573704 -10.4627546  -10.4515603
 -10.42240761 -10.37561746 -10.3276815  -10.32701661 -10.31802362
 -10.3154955  -10.30784685 -10.30481943 -10.29512946 -10.18766589
 -10.17869462 -10.15370275 -10.06917835 -10.05434057 -10.03697072
 -10.01626687 -10.01310085  -9.98600954  -9.97495295  -9.95068151
  -9.93667404  -9.91846284  -9.90408056  -9.89890309  -9.87343312
  -9.86486813  -9.85721598  -9.8558125   -9.84325222  -9.83271252
  -9.76843159  -9.76204726  -9.75506843  -9.71743121  -9.71426961
  -9.70803505  -9.66915345  -9.65923309  -9.64819376  -9.63006997
  -9.62799639  -9.61879836  -9.61625125  -9.55526557  -9.52537989
  -9.49549595  -9.47919626  -9.45479372  -9.45428377  -9.43633846
  -9.43051249  -9.41778779  -9.4142218   -9.40824698  -9.38226493
  -9.35990043  -9.35124737  -9.34459136  -9.34125766  -9.34040597
  -9.31853289  -9.25632952  -9.22954286  -9.22723903  -9.21865006
  -9.19782333  -9.17110142  -9.17049064  -9.13624814  -9.09796099
  -9.09284414  -9.08951284  -9.05654097  -8.97528392  -8.93076101
  -8.8813358   -8.77833829  -8.77596385  -8.76739334  -8.75445976
  -8.63326004  -8.54543523  -8.54199036  -8.49185067  -8.46177143
  -8.41487591  -8.39777202  -8.36790114  -8.33269028  -8.330117
  -8.30246843  -8.25657238  -8.19902254  -8.18713392  -8.17367941
  -8.16719519  -8.1394985   -8.13319584  -8.10968433  -8.10819769
  -7.89248975  -7.88019905  -7.87095111  -7.85017431  -7.81815231
  -7.78975702  -7.78314717  -7.77876881  -7.57539849  -7.51737497
  -7.36244313  -7.25578116  -7.20737556  -7.1947267   -7.16145545
  -7.10832736  -6.97281961  -6.95906356  -6.90501541  -6.8484965
  -6.8070956   -6.79536648  -6.77694649  -6.72206384  -6.71997062
  -6.68501373  -6.55345137  -6.54958497  -6.53712004  -6.53544734
  -6.51820418  -6.47177645  -6.43779084  -6.43176687  -6.40225018
  -6.36310578  -6.25773889  -6.19461946  -6.14243817  -5.96797387
  -5.88865356  -5.85009487  -5.83461367  -5.83403547  -5.79683102
  -5.78972364  -5.69554755  -5.61579673  -5.57908147  -5.53804057
  -5.36403467  -5.3472021   -5.32388913  -5.31794635  -5.28999092
  -5.27678673  -5.2737561   -5.24904671  -5.14856683  -5.13495061
  -5.09925409  -5.09596379  -5.07848501  -5.0618994   -5.02795798
  -4.99788354  -4.97387894  -4.95945946  -4.95123327  -4.91015168
  -4.8998206   -4.87889839  -4.86745311  -4.82757292  -4.82553736
  -4.76856788  -4.7559406   -4.7538339   -4.70632397  -4.63049542
  -4.58315757  -4.43501791  -4.40686836  -4.32181279  -4.27739805
  -4.27154892  -4.26347708  -4.25898597  -4.23630338  -4.230832
  -4.21187786  -4.17589412  -4.16076412  -4.14876031  -4.14587489
  -4.10038572  -4.09640849  -4.06284009  -4.05043258  -4.03122369
  -4.03104862  -3.96201618  -3.95965653  -3.93713814  -3.91566507
  -3.80212364  -3.80034614  -3.77940601  -3.74473335  -3.69868448
  -3.69483696  -3.65483768  -3.62050437  -3.5764627   -3.53148579
  -3.47337705  -3.4650898   -3.45287217  -3.44219496  -3.42063795
  -3.40184505  -3.38446715  -3.37710268  -3.37306409  -3.3322555
  -3.32416002  -3.3226389   -3.26750952  -3.26292098  -3.13346185
  -3.08244978  -3.05618999  -3.03563761  -2.98804766  -2.92137389
  -2.90148268  -2.8803343   -2.84460557  -2.82233026  -2.79634537
  -2.77499084  -2.77171746  -2.76670186  -2.75763358  -2.70856633
  -2.68532863  -2.65837176  -2.64166233  -2.63886455  -2.59195578
  -2.49631954  -2.40363433  -2.33561719  -2.28316262  -2.27877616
  -2.25831239  -2.22883534  -1.91361965  -1.47859569  -1.42185534]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:3
end of epoch 0: val_loss 0.42887166115276615, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.2823,  0.3017,  0.1987, -0.3131,  0.3370,  0.1008,  0.0655, -0.2044,
         -0.1732, -0.4041, -0.0024, -1.5407, -1.8651]], device='cuda:3'))])
end of epoch 1: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.2728e-01,  2.2547e-01,  4.2316e-06, -3.4128e-03,  2.9402e-01,
         -8.5256e-02, -1.6746e-03, -3.9930e-02, -2.5695e-01, -2.0189e-01,
          3.7959e-03, -2.0974e+00, -2.3960e+00]], device='cuda:3'))])
end of epoch 2: val_loss 4.82796960099563e-08, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.7246509695052862, val_acc 0.98
trigger times: 2
end of epoch 4: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.8800e-01,  3.8530e-01,  2.1887e-01, -5.1118e-02,  3.5488e-01,
          8.3736e-02, -7.0750e-03, -7.6030e-02, -1.6365e-01, -4.2808e-01,
         -2.2278e-05, -2.6788e+00, -3.8334e+00]], device='cuda:3'))])
end of epoch 5: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.6530e-01,  2.9757e-01,  1.3087e-01,  1.5500e-06, -5.5191e-05,
          1.3464e-04, -3.5101e-03, -4.9021e-02,  9.0804e-06, -7.7667e-05,
          1.5752e-03, -2.1726e+00, -3.5337e+00]], device='cuda:3'))])
end of epoch 6: val_loss 2.4624765224636745e-05, val_acc 1.0
trigger times: 1
end of epoch 7: val_loss 8.523428729034777e-08, val_acc 1.0
trigger times: 2
end of epoch 8: val_loss 1.0314667597377536e-05, val_acc 1.0
trigger times: 3
end of epoch 9: val_loss 0.13926782608032226, val_acc 0.995
trigger times: 4
end of epoch 10: val_loss 0.00011709602549672126, val_acc 1.0
trigger times: 5
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.7729e-01,  3.4864e-02, -4.5185e-02,  2.4463e-05,  4.5354e-05,
         -6.2880e-05,  2.4441e-02, -2.5757e-02,  1.6247e-05, -9.7355e-05,
         -2.3633e-03, -2.7419e+00, -4.9383e+00]], device='cuda:3'))])
end of epoch 12: val_loss 1.3868466967181804e-06, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 0.09504131317138671, val_acc 0.995
trigger times: 2
end of epoch 14: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 0.2609865931730108, val_acc 0.995
trigger times: 5
end of epoch 17: val_loss 8.714495206731954e-06, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.5899e-01,  1.0893e-01, -6.0882e-02, -9.2074e-02,  2.3067e-01,
         -3.9205e-03, -8.5098e-03, -3.6589e-02,  1.0382e-01, -2.6713e-01,
          3.5707e-04, -2.2754e+00, -4.6397e+00]], device='cuda:3'))])
end of epoch 19: val_loss 7.152552790046229e-09, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 0.11792103230953216, val_acc 0.995
trigger times: 2
end of epoch 21: val_loss 1.3063632649945588e-06, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 4.29152189340698e-08, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 5.997581425237541e-05, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 8.379908376809908e-07, val_acc 1.0
trigger times: 6
end of epoch 25: val_loss 2.9206189537944736e-08, val_acc 1.0
trigger times: 7
end of epoch 26: val_loss 6.583123416632475e-06, val_acc 1.0
trigger times: 8
end of epoch 27: val_loss 0.01073144674125615, val_acc 0.995
trigger times: 9
end of epoch 28: val_loss 0.02188993215560913, val_acc 0.995
trigger times: 10
Early stopping.
0 -227.1174311041832 -54.98547503240923
1 -209.38200759887695 -46.98011874490918
2 -200.06786388158798 -43.81326882122305
3 -197.72464656829834 -41.68588229294918
4 -186.3613652586937 -39.57586365327889
5 -197.99998092651367 -38.35634328077039
6 -212.78054213523865 -37.1809993033689
7 -142.27838718891144 -36.20965269874363
8 -198.172689974308 -35.26282499693737
9 -154.9968091249466 -34.64469044638467
10 -188.24964690208435 -31.64414355845032
11 -165.09720766544342 -29.340125609942326
12 -122.2733473032713 -27.07399028854534
13 -154.72257588803768 -24.879106999799365
14 -114.77115523815155 -23.978745577896312
15 -131.24860990047455 -23.292294170915508
16 -130.4156882762909 -21.897878940604915
17 -119.6576635837555 -21.492768640761124
18 -116.90632811188698 -20.19699010077007
19 -99.71244858205318 -19.63760343800059
20 -112.82745510339737 -19.078298175956636
21 -108.6831647157669 -18.636549110679173
22 -90.97264301776886 -18.095671482292943
23 -113.88075625896454 -17.852495586126633
24 -95.29994076490402 -17.308068620764864
25 -100.83897441625595 -16.8483410171778
26 -80.2814348936081 -16.646901048274707
27 -92.74625417590141 -16.497517458997606
28 -95.70366176962852 -16.12129102540531
29 -98.47656953334808 -15.816942556405305
30 -73.45594435930252 -15.495749174847994
31 -44.39361768960953 -15.168072916773488
32 -54.25205338001251 -14.896716433558865
33 -40.902573734521866 -14.729899636535844
34 -66.91410839557648 -14.585618122294337
35 -69.08957380056381 -14.32701491735508
36 -64.88064640760422 -14.177041714508036
37 -40.00606241822243 -14.000354805003274
38 -58.41912657022476 -13.945530841132388
39 -62.656502932310104 -13.743137965876109
40 -60.90345084667206 -13.608891999152753
41 -58.86458119750023 -13.537173252032721
42 -32.686502531170845 -13.312737400732429
43 -59.10087311267853 -13.178142688546147
44 -63.66115194559097 -12.988045670217158
45 -30.0207841694355 -12.909191272911288
46 -36.991163074970245 -12.803633669844256
47 -30.665297582745552 -12.729441698565406
48 -57.88778102397919 -12.687877277875348
49 -28.41446840763092 -12.645825928947477
50 -31.258239693939686 -12.572957935914882
51 -59.01504930853844 -12.543370724726458
52 -59.333605110645294 -12.48846702822902
53 -54.10819861292839 -12.370135820080268
54 -28.807090684771538 -12.25692808033158
55 -37.0735135525465 -12.156775630468069
56 -63.40740993618965 -12.008339088916161
57 -30.671435460448265 -11.91835728096409
58 -31.045706696808338 -11.75742969084085
59 -63.256224036216736 -11.674857754603973
60 -47.023455917835236 -11.545763222571745
61 -31.607347935438156 -11.361279470756807
62 -60.957194834947586 -11.194241256345855
63 -49.290429174900055 -11.076481171996228
64 -50.70031514763832 -10.961932060164017
65 -47.869441866874695 -10.845290734893993
66 -26.413788735866547 -10.737912430026146
67 -24.731558442115784 -10.4657370393539
68 -57.61371871829033 -10.327681503524177
69 -25.442256331443787 -10.304819426032076
70 -54.27204981446266 -10.069178353474605
71 -47.437298107892275 -9.986009544966164
72 -28.468059301376343 -9.904080562743319
73 -27.461822777986526 -9.855812500503099
74 -24.010741662234068 -9.75506843121589
75 -22.590314149856567 -9.648193762469901
76 -23.773746222257614 -9.55526557442281
77 -57.413796454668045 -9.454283769375778
78 -29.443985655903816 -9.408246981646435
79 -49.810698330402374 -9.341257661417394
80 -33.05991876125336 -9.227239032064794
81 -40.45135010778904 -9.136248135880153
82 -22.470609702169895 -8.975283916352673
83 -60.529646933078766 -8.767393335512658
84 -41.42786104977131 -8.491850672526503
85 -23.799425065517426 -8.332690283769862
86 -19.21281398832798 -8.187133918860544
87 -19.98836925625801 -8.10968433468653
88 -20.21103322505951 -7.850174313545857
89 -64.4461030960083 -7.57539849177145
90 -20.417626589536667 -7.194726704480378
91 -21.571752905845642 -6.905015411380009
92 -60.97064770758152 -6.7220638398623045
93 -10.305604867637157 -6.53712003577137
94 -45.26792639493942 -6.4317668719919645
95 -11.520765960216522 -6.142438172453475
96 -6.1436290591955185 -5.834035470069331
97 -12.354682669043541 -5.579081467736179
98 -4.824237018823624 -5.3179463541110685
99 -16.134292364120483 -5.148566829059203
100 -15.101123601198196 -5.06189940007427
101 -7.284262478351593 -4.951233272574713
102 -10.273438781499863 -4.827572916892203
103 -0.38036054372787476 -4.706323965920678
104 -5.999061077833176 -4.321812794749389
105 -10.987142264842987 -4.230832004686763
106 -2.7110742181539536 -4.145874891902638
107 8.26534291356802 -4.0312236943449005
108 -6.040138334035873 -3.915665072019217
109 -2.6965630650520325 -3.6986844800226515
110 5.025362610816956 -3.5314857883367363
111 -0.9540799036622047 -3.4206379509163436
112 -4.310994237661362 -3.3322555012187633
113 5.481789827346802 -3.1334618524345976
114 12.106922812759876 -2.9213738924472454
115 -3.001313328742981 -2.7963453736667327
116 3.9164317548274994 -2.708566332803348
117 6.5966452434659 -2.591955777061163
118 13.464757844805717 -2.278776163773821
119 14.647908076643944 -1.4218553446486095
train accuracy: 0.9994444444444445
validation accuracy: 0.995
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.8137634  -24.59274514 -24.03455963
 -23.97874558 -23.58332829 -23.57262108 -23.51029134 -23.44970808
 -23.39348065 -23.29229417 -23.03804936 -22.74530916 -22.60679894
 -22.19891032 -21.89787894 -21.83639166 -21.63652554 -21.56970954
 -21.50084218 -21.49276864 -20.65686376 -20.53025744 -20.44447256
 -20.43388583 -20.1969901  -20.13839115 -19.87520089 -19.7534442
 -19.6785601  -19.63760344 -19.57149014 -19.53099124 -19.51559872
 -19.4608337  -19.07829818 -18.96981233 -18.9283881  -18.770776
 -18.73129823 -18.63654911 -18.61182059 -18.4225203  -18.22567089
 -18.11123204 -18.09567148 -18.08894342 -18.08309075 -17.99477406
 -17.89591085 -17.85249559 -17.5574237  -17.5238387  -17.51466278
 -17.46955573 -17.30806862 -17.2877303  -17.10132819 -16.99919682
 -16.8614333  -16.84834102 -16.82307393 -16.79759203 -16.73172811
 -16.73124049 -16.67001724 -16.64690105 -16.60283379 -16.58659093
 -16.54498335 -16.5431465  -16.53401037 -16.49751746 -16.41235951
 -16.32507254 -16.28251882 -16.14612078 -16.12129103 -16.0872847
 -16.08335146 -16.08190245 -15.89510862 -15.88276488 -15.83529718
 -15.81694256 -15.7508656  -15.71653035 -15.67916687 -15.65454388
 -15.6145203  -15.59459196 -15.49574917 -15.29900699 -15.2719524
 -15.25069848 -15.21158976 -15.1860156  -15.16807292 -15.13026299
 -15.09811232 -15.0806922  -15.05822392 -14.97048199 -14.89671643
 -14.87860216 -14.8550828  -14.82856339 -14.82695649 -14.77312633
 -14.75122142 -14.72989964 -14.72489417 -14.6650308  -14.66284809
 -14.594561   -14.58561812 -14.5643771  -14.5314246  -14.50694118
 -14.44242009 -14.42936181 -14.42731755 -14.41131129 -14.38609612
 -14.32701492 -14.24090388 -14.23740337 -14.21164262 -14.20878741
 -14.20007142 -14.17704171 -14.09737943 -14.04266231 -14.03131293
 -14.00356944 -14.00035481 -13.99901576 -13.99628479 -13.98996721
 -13.96863183 -13.96459126 -13.94553084 -13.93312545 -13.86308449
 -13.8463564  -13.82168222 -13.75008674 -13.74547384 -13.74313797
 -13.699817   -13.67248689 -13.66965006 -13.63042046 -13.608892
 -13.59601285 -13.58803378 -13.58645341 -13.5769007  -13.53717325
 -13.51008468 -13.50094998 -13.48391162 -13.35053773 -13.3127374
 -13.28128196 -13.24746818 -13.23232486 -13.23104856 -13.22263323
 -13.20399316 -13.17814269 -13.17193674 -13.16092153 -13.13997741
 -13.12635144 -13.11486873 -12.98804567 -12.97427783 -12.95400937
 -12.95080968 -12.94572355 -12.93760188 -12.90919127 -12.89533945
 -12.8639276  -12.86054854 -12.85305787 -12.82640011 -12.80363367
 -12.7828239  -12.74595291 -12.74539386 -12.74448225 -12.73031723
 -12.7294417  -12.71725845 -12.71523584 -12.70915032 -12.69995766
 -12.69049603 -12.68787728 -12.68476663 -12.68135973 -12.66429794
 -12.66418206 -12.64582593 -12.6451144  -12.61270979 -12.61011515
 -12.58492544 -12.57295794 -12.56825805 -12.56650197 -12.5568266
 -12.55286515 -12.54337072 -12.52233257 -12.51755483 -12.49079212
 -12.48904281 -12.48846703 -12.48769446 -12.47519008 -12.45904543
 -12.45469195 -12.4524432  -12.42922733 -12.39594033 -12.37013582
 -12.36398782 -12.34671509 -12.30017947 -12.27979948 -12.25692808
 -12.2534233  -12.24340388 -12.19488362 -12.17081576 -12.15677563
 -12.15190477 -12.14267564 -12.10286355 -12.10232118 -12.02314402
 -12.00833909 -11.99766664 -11.97628117 -11.9718797  -11.94387787
 -11.91835728 -11.80244434 -11.78885214 -11.78248093 -11.78173063
 -11.7724129  -11.75742969 -11.74297951 -11.72702366 -11.70702865
 -11.69421946 -11.675454   -11.67485775 -11.66062457 -11.6440414
 -11.62993592 -11.62446268 -11.58806794 -11.5787919  -11.54576322
 -11.48866655 -11.48214076 -11.46504032 -11.44310782 -11.41887877
 -11.4025404  -11.39997317 -11.36127947 -11.31909132 -11.29723637
 -11.27863965 -11.25703347 -11.21090372 -11.20128427 -11.19424126
 -11.18187983 -11.15847697 -11.13382735 -11.12705605 -11.11803954
 -11.10389205 -11.09147706 -11.07648117 -11.07460556 -11.0538427
 -11.02187747 -10.99315133 -10.98789819 -10.96193206 -10.89368728
 -10.88447672 -10.8699891  -10.85680187 -10.84802596 -10.84529073
 -10.83873062 -10.83771365 -10.77809419 -10.7626531  -10.73791243
 -10.72252648 -10.62949326 -10.59437825 -10.54436717 -10.52044494
 -10.46573704 -10.4627546  -10.4515603  -10.42240761 -10.38248731
 -10.37561746 -10.3276815  -10.32701661 -10.31802362 -10.3154955
 -10.30784685 -10.30481943 -10.29512946 -10.19352401 -10.18766589
 -10.17890996 -10.17869462 -10.15370275 -10.06917835 -10.05434057
 -10.03697072 -10.01626687 -10.01310085  -9.98600954  -9.97495295
  -9.95068151  -9.93667404  -9.91846284  -9.90408056  -9.89890309
  -9.8901569   -9.87405689  -9.87343312  -9.86486813  -9.85721598
  -9.8558125   -9.84325222  -9.83271252  -9.79582895  -9.76843159
  -9.76204726  -9.75558394  -9.75506843  -9.71743121  -9.71426961
  -9.70803505  -9.66915345  -9.65923309  -9.64819376  -9.63006997
  -9.62799639  -9.61879836  -9.61625125  -9.55538452  -9.55526557
  -9.52537989  -9.49549595  -9.47919626  -9.4559665   -9.45479372
  -9.45428377  -9.44335942  -9.43633846  -9.43051249  -9.41778779
  -9.4142218   -9.40824698  -9.38226493  -9.35990043  -9.35124737
  -9.34459136  -9.34125766  -9.34040597  -9.31853289  -9.25632952
  -9.22954286  -9.22723903  -9.21865006  -9.19782333  -9.17110142
  -9.17049064  -9.13624814  -9.09796099  -9.09284414  -9.08951284
  -9.07359538  -9.05654097  -8.97528392  -8.93969488  -8.93076101
  -8.89825158  -8.8813358   -8.87834374  -8.77833829  -8.77596385
  -8.76739334  -8.75445976  -8.63326004  -8.5730172   -8.54543523
  -8.54199036  -8.49185067  -8.46177143  -8.41487591  -8.39777202
  -8.36790114  -8.33269028  -8.330117    -8.30246843  -8.25657238
  -8.24866536  -8.24655143  -8.19902254  -8.18713392  -8.17367941
  -8.16719519  -8.1394985   -8.13319584  -8.10968433  -8.10819769
  -8.0873198   -8.06488     -8.01611684  -7.89248975  -7.88019905
  -7.87095111  -7.85017431  -7.84349299  -7.83237732  -7.81815231
  -7.78975702  -7.78314717  -7.77876881  -7.7777532   -7.57539849
  -7.51737497  -7.36244313  -7.30879343  -7.2623837   -7.25578116
  -7.20737556  -7.1947267   -7.19292523  -7.16145545  -7.10914258
  -7.10832736  -6.97281961  -6.95906356  -6.9573908   -6.90501541
  -6.8484965   -6.8070956   -6.79536648  -6.78785233  -6.77694649
  -6.77384223  -6.72206384  -6.71997062  -6.68501373  -6.55345137
  -6.54958497  -6.53712004  -6.53544734  -6.51820418  -6.47177645
  -6.43779084  -6.43176687  -6.40225018  -6.37070013  -6.36310578
  -6.28316537  -6.25773889  -6.19461946  -6.14243817  -6.07201508
  -6.07176839  -6.02303716  -5.96797387  -5.95768104  -5.89029518
  -5.88865356  -5.85009487  -5.83461367  -5.83403547  -5.81935692
  -5.8139306   -5.79683102  -5.78972364  -5.77751783  -5.77395566
  -5.69554755  -5.61579673  -5.57908147  -5.57644001  -5.53804057
  -5.53634803  -5.36403467  -5.35596372  -5.3472021   -5.32388913
  -5.31794635  -5.28999092  -5.27678673  -5.2737561   -5.24904671
  -5.14856683  -5.13495061  -5.09925409  -5.09596379  -5.07848501
  -5.0618994   -5.040842    -5.02795798  -4.99788354  -4.97387894
  -4.96479309  -4.95945946  -4.95123327  -4.93784197  -4.91015168
  -4.8998206   -4.87889839  -4.86745311  -4.83265593  -4.82757292
  -4.82553736  -4.81323632  -4.79237002  -4.79101402  -4.76856788
  -4.7559406   -4.7538339   -4.70632397  -4.66258269  -4.65936342
  -4.63630822  -4.63049542  -4.62257339  -4.58315757  -4.50082182
  -4.43501791  -4.40686836  -4.38335814  -4.36987183  -4.32181279
  -4.27739805  -4.27154892  -4.26347708  -4.25898597  -4.2466408
  -4.23630338  -4.230832    -4.21187786  -4.20527009  -4.19174347
  -4.17589412  -4.16076412  -4.14876031  -4.14587489  -4.12629893
  -4.10038572  -4.09640849  -4.06284009  -4.05043258  -4.03122369
  -4.03104862  -4.00359414  -3.98683223  -3.96201618  -3.95965653
  -3.93713814  -3.93097812  -3.91566507  -3.87275881  -3.80863086
  -3.80212364  -3.80034614  -3.77940601  -3.74473335  -3.71617553
  -3.69868448  -3.69483696  -3.65483768  -3.62050437  -3.60479147
  -3.5764627   -3.53357853  -3.53148579  -3.47337705  -3.4650898
  -3.45287217  -3.44219496  -3.43795558  -3.42063795  -3.41930287
  -3.40184505  -3.38446715  -3.37710268  -3.37306409  -3.3322555
  -3.32416002  -3.3226389   -3.26750952  -3.26292098  -3.24052444
  -3.19490855  -3.13346185  -3.08244978  -3.05618999  -3.03563761
  -2.98804766  -2.92137389  -2.90148268  -2.8803343   -2.87795652
  -2.84460557  -2.82233026  -2.79634537  -2.77499084  -2.77171746
  -2.76670186  -2.75763358  -2.70856633  -2.68532863  -2.65837176
  -2.64166233  -2.63886455  -2.59195578  -2.49631954  -2.40363433
  -2.37558934  -2.33561719  -2.28316262  -2.27877616  -2.25831239
  -2.22883534  -1.91361965  -1.77293632  -1.47859569  -1.42185534]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 6.203528958508286e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 8.9996e-02,  1.5822e-01,  4.8568e-02, -1.3218e-01,  1.8757e-01,
         -2.5869e-01,  5.1859e-03, -3.5342e-04, -2.2486e-05, -9.4245e-06,
         -2.3655e-03, -7.1282e-01, -1.0780e+00]], device='cuda:0'))])
end of epoch 1: val_loss 8.37328295892803e-05, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 8.165809958882164e-08, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.9807e-01,  4.5720e-01,  2.1705e-01, -2.0482e-01,  4.1617e-01,
         -3.5499e-01,  8.9019e-03,  5.0771e-02,  9.1229e-06,  1.1232e-03,
         -2.5089e-03, -2.2581e+00, -2.5412e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.5168e-01,  2.1752e-01,  1.2599e-01, -2.2294e-01, -1.1008e-04,
          4.8470e-05,  2.4064e-02,  2.3666e-02,  7.4724e-05,  3.0211e-06,
          7.5657e-06, -2.2070e+00, -2.3070e+00]], device='cuda:0'))])
end of epoch 4: val_loss 1.6540765328976192e-05, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.0011385124176740645, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 5.668084481769142e-07, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 1.0680794485651291e-06, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.0015e-01,  5.1281e-01, -2.1589e-02, -2.9717e-01,  6.9131e-01,
         -1.1128e-01, -3.4735e-03, -5.1859e-02, -4.6735e-01, -5.2152e-02,
         -3.1946e-04, -2.5390e+00, -3.8052e+00]], device='cuda:0'))])
end of epoch 9: val_loss 0.02580516346970935, val_acc 0.995
trigger times: 1
end of epoch 10: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 5.4657e-02,  3.6708e-01,  2.5672e-02, -2.2179e-01,  3.1450e-03,
         -5.1981e-02,  8.6953e-03, -6.3165e-02, -5.9055e-03,  3.0875e-02,
          8.1412e-04, -2.6045e+00, -3.8547e+00]], device='cuda:0'))])
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.6686e-05,  2.0769e-01,  1.0306e-01, -1.7267e-01,  3.5231e-05,
         -2.1080e-04,  2.5012e-04, -1.1259e-02,  1.5074e-05,  7.2271e-05,
         -2.3633e-03, -2.5218e+00, -3.1753e+00]], device='cuda:0'))])
end of epoch 12: val_loss 3.849512692930545e-06, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 0.013509895205495113, val_acc 0.995
trigger times: 2
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.4890e-01,  3.6018e-01,  1.0390e-01, -4.1558e-01,  5.4230e-01,
         -9.7874e-03,  1.9799e-02,  8.5403e-02, -2.1544e-01,  4.2020e-05,
          9.7824e-06, -2.5226e+00, -2.9568e+00]], device='cuda:0'))])
end of epoch 15: val_loss 2.2053696646651133e-08, val_acc 1.0
trigger times: 1
end of epoch 16: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 0.0008641126689725809, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 0.00012474210001528263, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 2.7732335712116196e-06, val_acc 1.0
trigger times: 5
end of epoch 20: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.8918e-01,  4.6327e-01,  3.0944e-02, -3.1272e-01,  1.3043e-05,
         -1.7839e-01,  2.3361e-02,  8.6303e-03, -1.6152e-05,  4.3207e-05,
         -5.1906e-04, -2.8050e+00, -3.1339e+00]], device='cuda:0'))])
end of epoch 21: val_loss 1.7431589161631677e-06, val_acc 1.0
trigger times: 1
end of epoch 22: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3184e-01,  2.4013e-02,  1.8476e-01, -1.7389e-01, -2.2358e-05,
          6.9924e-05, -4.7880e-03,  6.5215e-03,  2.9093e-05, -1.3402e-06,
         -2.3611e-03, -2.7429e+00, -3.3356e+00]], device='cuda:0'))])
end of epoch 23: val_loss 3.96039000367665e-06, val_acc 1.0
trigger times: 1
end of epoch 24: val_loss 0.16079209208487782, val_acc 0.995
trigger times: 2
end of epoch 25: val_loss 2.0861582470388386e-08, val_acc 1.0
trigger times: 3
end of epoch 26: val_loss 9.2206778934667e-06, val_acc 1.0
trigger times: 4
end of epoch 27: val_loss 0.00035594031875689323, val_acc 1.0
trigger times: 5
end of epoch 28: val_loss 1.8445983496206964e-06, val_acc 1.0
trigger times: 6
end of epoch 29: val_loss 0.01974564529024043, val_acc 0.995
trigger times: 7
end of epoch 30: val_loss 0.011814935761765354, val_acc 0.995
trigger times: 8
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-2.0688e-05,  5.9054e-01,  2.5633e-01, -6.0746e-01,  2.5282e-01,
         -9.4059e-06,  3.9205e-02, -1.4411e-02, -2.2829e-01,  3.3153e-05,
         -5.1686e-04, -3.6362e+00, -5.3771e+00]], device='cuda:0'))])
end of epoch 32: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9719e-05,  4.1984e-01,  7.1916e-02, -4.8159e-01,  3.6437e-05,
         -6.3481e-05,  3.4222e-02,  5.5791e-06,  6.1637e-05,  1.1808e-04,
          8.1852e-04, -2.8192e+00, -5.0924e+00]], device='cuda:0'))])
end of epoch 33: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.5262e-01,  3.0190e-01, -7.2572e-02, -4.2407e-01,  4.6755e-01,
         -2.2860e-05,  1.0941e-02,  1.2065e-01, -7.1675e-02, -2.6298e-01,
         -2.3589e-03, -3.0729e+00, -5.3020e+00]], device='cuda:0'))])
end of epoch 34: val_loss 6.020036295240062e-08, val_acc 1.0
trigger times: 1
end of epoch 35: val_loss 0.00011007784544595722, val_acc 1.0
trigger times: 2
end of epoch 36: val_loss 0.0016748407855629566, val_acc 1.0
trigger times: 3
end of epoch 37: val_loss 1.6510213754372671e-07, val_acc 1.0
trigger times: 4
end of epoch 38: val_loss 9.723976836539805e-06, val_acc 1.0
trigger times: 5
end of epoch 39: val_loss 0.00025561157246180245, val_acc 1.0
trigger times: 6
end of epoch 40: val_loss 1.2445448664806235e-05, val_acc 1.0
trigger times: 7
end of epoch 41: val_loss 5.062423434729624e-06, val_acc 1.0
trigger times: 8
end of epoch 42: val_loss 3.635751272668131e-07, val_acc 1.0
trigger times: 9
end of epoch 43: val_loss 9.935968967056396e-06, val_acc 1.0
trigger times: 10
Early stopping.
0 -176.95164346694946 -54.98547503240923
1 -146.11302465200424 -45.7351542845057
2 -132.2390268445015 -42.29180714825394
3 -147.384317278862 -40.34838365523108
4 -136.92717105150223 -38.41270390343083
5 -137.3104522228241 -37.1809993033689
6 -101.58580715954304 -36.19207561676116
7 -108.59552812576294 -35.209705244501436
8 -125.84875971078873 -31.969099402548657
9 -111.08828291296959 -29.39157139549552
10 -93.4881030768156 -27.07399028854534
11 -71.07625326514244 -24.828695359328833
12 -94.67957037687302 -23.510291336001472
13 -100.86368557810783 -22.60679894414887
14 -85.21375179290771 -21.500842181580236
15 -77.82158979773521 -20.19699010077007
16 -79.42986324429512 -19.57149013584634
17 -67.32397145032883 -18.92838809611677
18 -77.09036153554916 -18.225670892175096
19 -68.25466661155224 -17.89591085036888
20 -68.50537744164467 -17.308068620764864
21 -87.58882623910904 -16.823073927842348
22 -67.81560088694096 -16.602833792628125
23 -68.13938410580158 -16.412359507841103
24 -23.097675289958715 -16.083351462420467
25 -20.964497230947018 -15.75086560322757
26 -56.94867977499962 -15.495749174847994
27 -30.886153653264046 -15.168072916773488
28 -49.61641734838486 -14.896716433558865
29 -22.62278688699007 -14.751221424712117
30 -52.84705590456724 -14.585618122294337
31 -17.298306200653315 -14.427317552179096
32 -30.620069667696953 -14.211642624044586
33 -26.652521952986717 -14.031312926503816
34 -55.60240197181702 -13.968631831022643
35 -57.666155487298965 -13.821682224843855
36 -26.192589789628983 -13.630420459048132
37 -56.11799891293049 -13.537173252032721
38 -24.30318108201027 -13.281281956834897
39 -55.22422915697098 -13.178142688546147
40 -58.56033410131931 -12.988045670217158
41 -23.76569025963545 -12.909191272911288
42 -23.86981850862503 -12.803633669844256
43 -26.79413317516446 -12.729441698565406
44 -51.446645736694336 -12.687877277875348
45 -21.781653650105 -12.645114399678981
46 -28.019206404685974 -12.566501971525874
47 -25.38170626759529 -12.490792120526482
48 -50.674849808216095 -12.454691952932533
49 -49.438509061932564 -12.346715089027569
50 -48.140377193689346 -12.19488362234604
51 -21.529698465019464 -12.10232117722664
52 -53.441619515419006 -11.943877868597472
53 -50.67509366571903 -11.772412895153552
54 -53.41380989551544 -11.67545400252302
55 -18.747389752417803 -11.588067938913353
56 -21.552691616117954 -11.443107821490939
57 -14.07386426627636 -11.297236369029804
58 -47.58221186697483 -11.181879825095224
59 -11.448994919657707 -11.091477057650106
60 -48.833441615104675 -10.961932060164017
61 -41.496173083782196 -10.845290734893993
62 -49.522607922554016 -10.722526479931577
63 -41.6237667798996 -10.462754599027946
64 -47.35644614696503 -10.3270166115354
65 -8.503971554338932 -10.193524005681162
66 -48.086281299591064 -10.054340569736782
67 -22.997978672385216 -9.95068150625345
68 -7.887730240821838 -9.874056892692396
69 -46.35367166996002 -9.832712523589095
70 -20.313793256878853 -9.717431214247757
71 -38.54987970367074 -9.630069973226872
72 -17.89040431380272 -9.525379889860924
73 -10.523360200226307 -9.443359418712095
74 -20.03167574852705 -9.382264926532343
75 -34.75005924701691 -9.318532890899364
76 -15.333072669804096 -9.171101420775566
77 -10.921943739056587 -9.073595376301892
78 -14.119432464241982 -8.88133579627069
79 -14.532302714884281 -8.633260039592288
80 -11.694191552698612 -8.414875912908172
81 -14.097965650260448 -8.256572378740486
82 -11.158486157655716 -8.167195187750787
83 -11.211611956357956 -8.064880001085562
84 -2.65220295637846 -7.8323773171952045
85 -52.54720622301102 -7.57539849177145
86 -8.937730014324188 -7.20737555633513
87 -18.280831277370453 -6.972819611644815
88 -14.22807652503252 -6.795366478714693
89 -44.375552862882614 -6.685013730616453
90 -8.804059617221355 -6.471776450097328
91 -6.772136598825455 -6.283165369919435
92 -2.1869700849056244 -6.023037160014419
93 -10.932923659682274 -5.834613669482052
94 -4.380925431847572 -5.777517827855116
95 -8.624505281448364 -5.538040568579479
96 3.393094003200531 -5.3179463541110685
97 -0.9152368903160095 -5.1349506111888745
98 -20.388696998357773 -5.027957977402961
99 -2.735450841486454 -4.937841966583841
100 -11.50528647005558 -4.827572916892203
101 -4.030082166194916 -4.755940596884328
102 -11.77222391963005 -4.63049541560991
103 4.159417033195496 -4.383358143484506
104 -2.0259909629821777 -4.258985972609648
105 5.697021000087261 -4.191743471790762
106 3.732940748333931 -4.100385716141807
107 0.9100155234336853 -4.00359414309241
108 5.0605329647660255 -3.8727588070098076
109 -7.466056905686855 -3.7161755336345528
110 5.333276137709618 -3.576462699036133
111 12.318833366036415 -3.442194963060387
112 11.284437492489815 -3.3771026804886444
113 9.648145318031311 -3.2629209834239195
114 5.466230928897858 -3.0356376109218366
115 7.377795100212097 -2.844605571933377
116 4.7834358513355255 -2.7576335768102926
117 12.161056205630302 -2.591955777061163
118 13.309695333242416 -2.278776163773821
119 15.823965340852737 -1.4218553446486095
train accuracy: 0.9977777777777778
validation accuracy: 1.0

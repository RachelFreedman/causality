demos: (120, 50, 13)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 3.589944860660665e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0780, -0.0350,  0.1117, -0.1538,  0.1327, -0.0231, -0.0069, -0.0102,
         -0.1358,  0.0012, -0.0024, -0.5116, -0.8672]], device='cuda:0'))])
end of epoch 1: val_loss 1.424545539308042e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.0830e-04,  8.1953e-02, -1.1929e-04,  7.5627e-06,  1.4401e-04,
         -5.2143e-04,  6.1245e-06, -2.6455e-05,  2.6338e-04,  5.6024e-06,
          3.7957e-03,  1.6881e-04, -9.9757e-01]], device='cuda:0'))])
end of epoch 2: val_loss 4.768369308294495e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-4.6100e-05,  8.4292e-02, -2.5585e-05, -1.1957e-01, -8.2464e-05,
          4.5896e-04, -1.3864e-05, -1.5041e-03, -3.5320e-04, -2.5455e-05,
         -2.5094e-03, -4.2483e-01, -1.5663e+00]], device='cuda:0'))])
end of epoch 3: val_loss 0.1412055620520907, val_acc 0.98
trigger times: 1
end of epoch 4: val_loss 0.21014957792799888, val_acc 0.975
trigger times: 2
end of epoch 5: val_loss 1.3828226173018265e-07, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 1.7881390590446245e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3496e-05,  1.3777e-01,  5.2063e-02, -4.8925e-02, -8.6611e-05,
          2.8003e-04, -6.1336e-06,  4.0987e-02,  1.7655e-04,  7.2642e-05,
         -1.2038e-03,  9.8599e-04, -1.8077e+00]], device='cuda:0'))])
end of epoch 7: val_loss 0.003137198952097151, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 1.080611166166534e-06, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 5.3644034956334964e-08, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.11455795225408742, val_acc 0.975
trigger times: 4
end of epoch 11: val_loss 1.6689289061844192e-08, val_acc 1.0
trigger times: 5
end of epoch 12: val_loss 2.8610197730927212e-08, val_acc 1.0
trigger times: 6
end of epoch 13: val_loss 0.22799648301787095, val_acc 0.985
trigger times: 7
end of epoch 14: val_loss 3.516669405456696e-08, val_acc 1.0
trigger times: 8
end of epoch 15: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.1730e-02,  2.4713e-02,  1.0409e-01,  1.5033e-05,  2.4665e-04,
         -4.2389e-04,  1.6526e-05,  1.2614e-05, -4.6345e-04,  1.3666e-04,
         -2.2169e-05, -1.9431e-04, -1.3759e+00]], device='cuda:0'))])
end of epoch 16: val_loss 3.5272806757866706e-05, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.8399e-04,  2.0975e-01, -5.4404e-06, -7.3521e-04, -1.8618e-05,
         -5.8732e-05,  1.5043e-03,  1.1221e-05,  1.8230e-04, -7.9352e-04,
         -1.2030e-03, -6.7513e-01, -1.6373e+00]], device='cuda:0'))])
end of epoch 18: val_loss 0.0015317345844005103, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 1.2636141111244115e-07, val_acc 1.0
trigger times: 2
end of epoch 20: val_loss 1.1086377522673274e-07, val_acc 1.0
trigger times: 3
end of epoch 21: val_loss 0.00023865734683170103, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 3.5762777983450177e-09, val_acc 1.0
trigger times: 5
end of epoch 23: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 6
end of epoch 24: val_loss 0.001909919391698267, val_acc 1.0
trigger times: 7
end of epoch 25: val_loss 1.5199122916698115e-07, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 3.170874631663878e-07, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 4.1127179457589594e-08, val_acc 1.0
trigger times: 10
Early stopping.
0 -42.16444788873196 -54.98547503240923
1 -42.351147532463074 -50.492268601198035
2 -38.159093618392944 -50.03933801517046
3 -39.626890018582344 -49.75347184620696
4 -39.75526650249958 -49.72654640753777
5 -41.15368466079235 -46.98011874490918
6 -40.176707580685616 -45.7351542845057
7 -38.44592133909464 -45.670579884154705
8 -39.64548248052597 -44.99030608142343
9 -37.36511458456516 -44.14602409201361
10 -36.39944639801979 -43.81326882122305
11 -36.825407799333334 -43.18878399086166
12 -37.830825820565224 -42.29180714825394
13 -36.50796016305685 -42.00401746161006
14 -39.49563421308994 -41.6910044370425
15 -37.15476915240288 -41.68588229294918
16 -36.4916528314352 -41.281777102712205
17 -36.231355518102646 -40.44278203413966
18 -37.3939382173121 -40.34838365523108
19 -35.93521748483181 -39.599701153458774
20 -34.700759429484606 -39.57586365327889
21 -33.12202379107475 -39.31972693233231
22 -31.994488686323166 -39.024610555047154
23 -33.56433141231537 -38.45534493538269
24 -33.51681886613369 -38.41270390343083
25 -35.32084280252457 -38.35634328077039
26 -33.13445932790637 -37.79713616772368
27 -31.403103798627853 -37.741528994987384
28 -36.20104409754276 -37.66475323879293
29 -33.51488131284714 -37.513139380385574
30 -37.72221265733242 -37.1809993033689
31 -33.29623011127114 -37.100703136010694
32 -32.97666845470667 -37.00630588930485
33 -34.88268322497606 -36.821916772458344
34 -33.568964660167694 -36.48799015296732
35 -32.17609108239412 -36.20965269874363
36 -32.07892815209925 -36.19207561676116
37 -33.89377249777317 -36.114459029559086
38 -32.02592505514622 -35.78149902167743
39 -30.866427078843117 -35.394503873250635
40 -34.59395977854729 -35.26282499693737
41 -33.36415410041809 -35.24303541418371
42 -33.69661245495081 -35.209705244501436
43 -33.87856301665306 -35.0654408505187
44 -31.176192812621593 -34.80241747531743
45 -31.565989680588245 -34.64469044638467
46 -29.492406513541937 -33.84284985953318
47 -29.33456799387932 -32.70706485357069
48 -29.091425582766533 -31.969099402548657
49 -28.34435048699379 -31.7109134007892
50 -30.94373259693384 -31.64414355845032
51 -29.389460489153862 -31.392382758954444
52 -30.7681579105556 -31.223196019713853
53 -29.108584225177765 -31.12953085092458
54 -28.511656507849693 -29.39157139549552
55 -31.000878766179085 -29.340125609942326
56 -25.137442015111446 -29.106189988903285
57 -26.411065630614758 -27.41102349748205
58 -26.69283203780651 -27.343722362182305
59 -27.339780185371637 -27.196681629483837
60 -26.22746709175408 -27.07399028854534
61 -25.147682040929794 -26.7047217556024
62 -25.67764177173376 -26.244794902859052
63 -25.839768804609776 -25.548365085275513
64 -24.611444555222988 -25.45878528601009
65 -26.08113945648074 -24.879106999799365
66 -24.157332111150026 -24.828695359328833
67 -25.269262347370386 -24.592745144504722
68 -24.560260102152824 -23.978745577896312
69 -25.033601015806198 -23.57262108435893
70 -22.202800795435905 -23.44970807952351
71 -24.38969476521015 -22.745309160183492
72 -23.65197605639696 -22.60679894414887
73 -23.336455158889294 -22.19891031871716
74 -23.395487968809903 -20.656863763892378
75 -21.247569922357798 -20.444472560731253
76 -21.423180878162384 -20.19699010077007
77 -22.422690326347947 -20.13839114930498
78 -21.909019734710455 -19.63760343800059
79 -21.650806665420532 -19.515598718228343
80 -20.131434004753828 -18.92838809611677
81 -21.698801949620247 -17.994774057192853
82 -18.67229374870658 -17.55742370467821
83 -19.27940444648266 -16.823073927842348
84 -17.010632841847837 -14.855082803515382
85 -16.156264808028936 -14.531424598833084
86 -16.911104943603277 -14.442420089224363
87 -16.724131762981415 -13.596012850960644
88 -13.79563918709755 -12.68135972540495
89 -15.849819757044315 -12.66418205637357
90 -15.148108198307455 -12.30017947419658
91 -15.930325224995613 -12.151904772081672
92 -14.29579334333539 -11.788852141676486
93 -15.24366918951273 -10.869989101210326
94 -13.718260569497943 -10.327681503524177
95 -11.438798702321947 -9.8572159761571
96 -10.904737876728177 -8.330116995310416
97 -11.256493125110865 -8.133195842510668
98 -12.00943442620337 -8.108197691178031
99 -9.727858792990446 -7.57539849177145
100 -9.130270034074783 -7.362443126623615
101 -9.05290837958455 -7.108327355338034
102 -7.924334350973368 -6.959063561385431
103 -9.358130630105734 -6.776946485018116
104 -8.831931620836258 -6.7220638398623045
105 -9.680417489260435 -6.719970621583102
106 -11.275995291769505 -6.535447341844848
107 -9.558383956551552 -6.51820418055673
108 -9.288838238455355 -5.615796733870542
109 -7.396447213366628 -5.34720210027791
110 -7.187927450984716 -5.078485007852753
111 -7.573502562940121 -5.027957977402961
112 -6.733647225424647 -4.827572916892203
113 -7.0317692793905735 -4.63049541560991
114 -7.056078039109707 -4.230832004686763
115 -6.94681016728282 -4.031048624093466
116 -6.422663230448961 -3.3844671463622564
117 -6.4980675065889955 -3.3322555012187633
118 -6.704847149550915 -2.6416623314910934
119 -5.445763822644949 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

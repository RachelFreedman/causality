demos: (120, 50, 2)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=2, out_features=1, bias=False)
)
Total number of parameters: 2
Number of trainable paramters: 2
device: cuda:0
end of epoch 0: val_loss 0.04375537414704013, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0029, -0.7312]], device='cuda:0'))])
end of epoch 1: val_loss 0.036039154027839686, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0101, -0.7649]], device='cuda:0'))])
end of epoch 2: val_loss 0.03802489936140773, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.050866515673624234, val_acc 0.99
trigger times: 2
end of epoch 4: val_loss 0.031916414231227465, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0015, -0.8516]], device='cuda:0'))])
end of epoch 5: val_loss 0.03817282033071024, val_acc 1.0
trigger times: 1
end of epoch 6: val_loss 0.033041004266633534, val_acc 1.0
trigger times: 2
end of epoch 7: val_loss 0.029336996119309334, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0142, -0.8332]], device='cuda:0'))])
end of epoch 8: val_loss 0.033480230069180834, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 0.033452966414479306, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.05008033646583499, val_acc 0.99
trigger times: 3
end of epoch 11: val_loss 0.0324372738778311, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 0.04935977558208833, val_acc 0.99
trigger times: 5
end of epoch 13: val_loss 0.03932902609374651, val_acc 0.995
trigger times: 6
end of epoch 14: val_loss 0.038485396795349514, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.03728909034241951, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.027702174611304146, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0207, -0.8471]], device='cuda:0'))])
end of epoch 17: val_loss 0.027850211287068306, val_acc 1.0
trigger times: 1
end of epoch 18: val_loss 0.03546111476676742, val_acc 1.0
trigger times: 2
end of epoch 19: val_loss 0.03331232120852292, val_acc 1.0
trigger times: 3
end of epoch 20: val_loss 0.046074971393009034, val_acc 0.99
trigger times: 4
end of epoch 21: val_loss 0.03367294285984826, val_acc 1.0
trigger times: 5
end of epoch 22: val_loss 0.020622381469459015, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0238, -0.9578]], device='cuda:0'))])
end of epoch 23: val_loss 0.039578742171761404, val_acc 1.0
trigger times: 1
end of epoch 24: val_loss 0.03181055649960399, val_acc 1.0
trigger times: 2
end of epoch 25: val_loss 0.052248082180494745, val_acc 0.99
trigger times: 3
end of epoch 26: val_loss 0.041390185345153443, val_acc 1.0
trigger times: 4
end of epoch 27: val_loss 0.06657761552820375, val_acc 0.975
trigger times: 5
end of epoch 28: val_loss 0.03898883077581559, val_acc 1.0
trigger times: 6
end of epoch 29: val_loss 0.031571152753203935, val_acc 1.0
trigger times: 7
end of epoch 30: val_loss 0.036925141986775996, val_acc 1.0
trigger times: 8
end of epoch 31: val_loss 0.04090275001344708, val_acc 0.99
trigger times: 9
end of epoch 32: val_loss 0.03214800845436912, val_acc 1.0
trigger times: 10
Early stopping.
0 -11.295912466710433 -50.00222059884506
1 -8.345843970775604 -48.823739140882175
2 -8.814025173895061 -47.15605336419176
3 -7.838276460766792 -46.19619104961985
4 -11.506477251648903 -45.594227093057754
5 -9.274595126509666 -45.36842966452394
6 -12.756728634238243 -45.19068756322445
7 -9.300484970211983 -44.16490839583478
8 -6.613209694623947 -44.078303125872196
9 -9.989249650388956 -43.993555290419714
10 -7.008560003712773 -43.86306534422809
11 -9.734134260565042 -43.84874807044028
12 -8.611344005912542 -43.84199129025074
13 -6.3517737574875355 -43.806385671938365
14 -6.495707601308823 -43.80581985978556
15 -7.166768565773964 -42.7594767358323
16 -6.003189593553543 -42.66316468983175
17 -8.40659570042044 -42.33177224591743
18 -9.020026184618473 -41.774963389485094
19 -7.971307802945375 -41.410068073767725
20 -7.49531064927578 -41.17786296442943
21 -6.701550638303161 -40.723520424948155
22 -9.435451053082943 -40.527189756101116
23 -8.053387485444546 -40.49595848244517
24 -9.625428102910519 -40.429389880911344
25 -8.275065403431654 -40.05653450521898
26 -7.482334794476628 -39.59232357792555
27 -10.533381275832653 -39.54162101198148
28 -8.594571240246296 -39.195237471709476
29 -9.55200707167387 -39.172389579378766
30 -7.960548656061292 -38.51095963496708
31 -7.043237264151685 -38.447265769744824
32 -9.483487199991941 -38.392107037026264
33 -5.858034131117165 -38.00745349944469
34 -6.562663696706295 -37.46482488602393
35 -9.303140673786402 -37.10988160586883
36 -7.593940317630768 -34.27116723637227
37 -5.214168375125155 -34.14139118114101
38 -5.679992917692289 -33.263072731706835
39 -6.2470924239605665 -33.13344797200536
40 -5.17280138283968 -33.07825234291984
41 -8.428177863359451 -33.0321314765637
42 -10.022425478324294 -32.44934973065406
43 -5.037625581026077 -32.4007978120153
44 -6.992942772805691 -32.40063925734975
45 -4.143813781440258 -30.734403792103194
46 -4.237448528409004 -30.57151371770873
47 -6.12107415497303 -30.131236504472803
48 -5.328735422343016 -29.99326722619033
49 -5.320389207452536 -29.66908258985071
50 -9.63875743933022 -29.297233511513635
51 -5.080720683559775 -29.288890423975797
52 -4.3914266638457775 -29.145878352769948
53 -4.642013200933434 -28.49601894351319
54 -5.99678660184145 -28.492023661124072
55 -6.715551126748323 -28.315961465855167
56 -5.141954485327005 -27.121110566589827
57 -3.866655223071575 -26.064532595535336
58 -4.708362963981926 -25.520524278341334
59 -4.696487313020043 -25.27101421179229
60 -5.147322654724121 -25.066644278800943
61 -6.1590538918972015 -24.925849381327673
62 -5.685174662619829 -24.188105669766596
63 -4.632421324029565 -23.48479966198816
64 -2.9590576554437575 -23.153943559703283
65 -3.9938442159909755 -22.954730295117237
66 -5.009892579168081 -22.74124885266394
67 -5.079018695279956 -22.739273544503753
68 -3.244402226060629 -22.264945050603636
69 -5.624549334635958 -22.15569724300287
70 -4.183071665465832 -21.055920928583344
71 -3.2976899426430464 -20.543356562348553
72 -4.216362798586488 -20.33499633836848
73 -4.377837213687599 -20.18157658281111
74 -4.263885252017644 -19.58144410477429
75 -4.472331914352253 -19.377225745334304
76 -3.6380648305639625 -19.243135617403095
77 -3.407184550538659 -19.060620225371707
78 -5.330251429229975 -18.964124524696246
79 -4.341375380754471 -18.448962308005108
80 -5.438215674832463 -17.740722019993825
81 -2.9086560923606157 -16.85889369985028
82 -3.26042727312597 -16.3381194095591
83 -3.773075334262103 -14.535892564189266
84 -3.228097092360258 -14.443670567499144
85 -2.798815549176652 -14.200413010108107
86 -3.6142040360718966 -13.936976181618805
87 -3.4276975337415934 -13.862253042167257
88 -4.501265838742256 -13.483098530680483
89 -3.1979081929894164 -13.455892754889845
90 -3.242385094985366 -13.355868275096913
91 -3.9556145039387047 -12.278515244993585
92 -2.328606816008687 -12.227387460046547
93 -2.708553608506918 -12.020717825467683
94 -4.160080675035715 -11.910094799877324
95 -3.2661572117358446 -11.400284019256157
96 -3.633623331959825 -11.134618158086587
97 -2.8748061433434486 -10.859166921158222
98 -2.6516424044966698 -9.595137958067907
99 -3.0445948117412627 -9.289921608799773
100 -3.590833359863609 -8.230877068641124
101 -3.991409083013423 -7.882363241796725
102 -4.288199516478926 -7.6478984168416355
103 -3.896373364608735 -7.459623237418707
104 -3.9819739079102874 -7.124357312750265
105 -3.613733851350844 -7.05379065585803
106 -3.6867937785573304 -6.853091098326624
107 -3.7794968830421567 -6.6211384471641495
108 -3.899008633568883 -6.494555224953677
109 -3.3037677318789065 -6.117354180737655
110 -3.9469984248280525 -6.087055095509873
111 -3.772494211792946 -5.43500831968483
112 -2.27514044882264 -5.105291741614599
113 -2.686759877949953 -4.628649413275992
114 -2.00871359105804 -4.471031187897325
115 -3.0435064304620028 -4.455504779070034
116 -3.0935176741331816 -4.2805498188182405
117 -3.0170558243989944 -3.7944735717969627
118 -1.9444818682968616 -2.9512438456190186
119 -1.8579057697206736 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

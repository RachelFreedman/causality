demos: (120, 50, 12)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.024033410660075027, val_acc 0.985
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.6021e-03, -2.3557e-02,  4.3607e-02,  8.5825e-03, -7.2072e-03,
         -3.5012e-04, -8.0917e-03,  1.0450e-02, -2.9844e-05, -6.1799e-05,
          1.3672e-03, -2.0817e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.002253144068590096, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.9989e-04, -3.0657e-02,  2.4368e-02, -4.3839e-02, -3.2301e-01,
          8.0151e-03,  4.5115e-03,  2.4161e-02,  1.1969e-01, -5.3602e-02,
          1.3667e-03, -2.8225e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.017127711697677482, val_acc 0.995
trigger times: 1
end of epoch 3: val_loss 0.36490846341255606, val_acc 0.935
trigger times: 2
end of epoch 4: val_loss 0.05701023979108278, val_acc 0.985
trigger times: 3
end of epoch 5: val_loss 1.0854146577055992, val_acc 0.87
trigger times: 4
end of epoch 6: val_loss 0.03459299040046691, val_acc 0.995
trigger times: 5
end of epoch 7: val_loss 0.00867087773731182, val_acc 0.995
trigger times: 6
end of epoch 8: val_loss 0.018114480582358112, val_acc 0.995
trigger times: 7
end of epoch 9: val_loss 0.0031354313214886176, val_acc 1.0
trigger times: 8
end of epoch 10: val_loss 0.05124099860301023, val_acc 0.99
trigger times: 9
end of epoch 11: val_loss 0.012552071959469337, val_acc 0.995
trigger times: 10
Early stopping.
0 -51.37847298383713 -50.00222059884506
1 -34.184408724308014 -48.823739140882175
2 -28.922559157013893 -47.15605336419176
3 -30.511154115200043 -46.19619104961985
4 -46.827541530132294 -45.594227093057754
5 -38.38509461283684 -45.36842966452394
6 -48.11682930588722 -45.19068756322445
7 -32.38299244642258 -44.16490839583478
8 -21.38455430418253 -44.078303125872196
9 -29.790860325098038 -43.993555290419714
10 -22.329376742243767 -43.86306534422809
11 -42.86968421190977 -43.84874807044028
12 -41.65148621797562 -43.84199129025074
13 -21.47321955859661 -43.806385671938365
14 -20.1534969098866 -43.80581985978556
15 -33.58382999897003 -42.7594767358323
16 -18.141505658626556 -42.66316468983175
17 -43.889470517635345 -42.33177224591743
18 -32.82440546154976 -41.774963389485094
19 -38.13933569192886 -41.410068073767725
20 -46.30857381224632 -41.17786296442943
21 -32.179094821214676 -40.723520424948155
22 -31.852773070335388 -40.527189756101116
23 -33.430575489997864 -40.49595848244517
24 -36.42220789194107 -40.429389880911344
25 -41.29070681333542 -40.05653450521898
26 -34.93329566717148 -39.59232357792555
27 -43.947017550468445 -39.54162101198148
28 -33.49733425676823 -39.195237471709476
29 -34.20233657956123 -39.172389579378766
30 -34.21941830217838 -38.51095963496708
31 -26.586239080876112 -38.447265769744824
32 -37.35913921892643 -38.392107037026264
33 -29.02492442727089 -38.00745349944469
34 -21.373377308249474 -37.46482488602393
35 -35.58351840078831 -37.10988160586883
36 -34.615572571754456 -34.27116723637227
37 -26.278640665113926 -34.14139118114101
38 -20.66741282120347 -33.263072731706835
39 -30.840076642110944 -33.13344797200536
40 -26.26308199763298 -33.07825234291984
41 -30.13038519024849 -33.0321314765637
42 -45.09785358607769 -32.44934973065406
43 -20.970253229141235 -32.4007978120153
44 -28.24179643392563 -32.40063925734975
45 -13.290515571832657 -30.734403792103194
46 -18.17925711721182 -30.57151371770873
47 -26.878684639930725 -30.131236504472803
48 -26.196277871727943 -29.99326722619033
49 -25.398718938231468 -29.66908258985071
50 -34.86665067821741 -29.297233511513635
51 -24.185954183340073 -29.288890423975797
52 -13.753200858831406 -29.145878352769948
53 -19.44363782554865 -28.49601894351319
54 -23.68806105852127 -28.492023661124072
55 -31.46252915263176 -28.315961465855167
56 -16.84274335950613 -27.121110566589827
57 -11.9231833294034 -26.064532595535336
58 -23.299550607800484 -25.520524278341334
59 -21.241471510380507 -25.27101421179229
60 -15.478583835065365 -25.066644278800943
61 -19.935077339410782 -24.925849381327673
62 -23.829545959830284 -24.188105669766596
63 -17.05685119330883 -23.48479966198816
64 -9.652609005570412 -23.153943559703283
65 -16.657487891614437 -22.954730295117237
66 -13.805832646787167 -22.74124885266394
67 -13.75203494913876 -22.739273544503753
68 -8.678034223616123 -22.264945050603636
69 -21.26970698684454 -22.15569724300287
70 -16.40982860326767 -21.055920928583344
71 -10.292609117925167 -20.543356562348553
72 -12.689943425357342 -20.33499633836848
73 -13.016592174768448 -20.18157658281111
74 -17.4469737932086 -19.58144410477429
75 -16.959615977481008 -19.377225745334304
76 -15.948716837912798 -19.243135617403095
77 -10.457430101931095 -19.060620225371707
78 -19.145611479878426 -18.964124524696246
79 -20.109891396015882 -18.448962308005108
80 -16.43992120027542 -17.740722019993825
81 -8.471454184502363 -16.85889369985028
82 -10.084207143634558 -16.3381194095591
83 -10.315932959318161 -14.535892564189266
84 -9.729046277701855 -14.443670567499144
85 -11.405699474737048 -14.200413010108107
86 -18.62919683754444 -13.936976181618805
87 -16.741215974092484 -13.862253042167257
88 -11.301672957837582 -13.483098530680483
89 -11.899156793951988 -13.455892754889845
90 -8.509065933525562 -13.355868275096913
91 -9.4672387316823 -12.278515244993585
92 -4.557096440345049 -12.227387460046547
93 -6.413323324173689 -12.020717825467683
94 -11.22701682895422 -11.910094799877324
95 -13.123848780989647 -11.400284019256157
96 -8.566367998719215 -11.134618158086587
97 -11.755104899406433 -10.859166921158222
98 -6.665042018517852 -9.595137958067907
99 -10.321496024727821 -9.289921608799773
100 -14.618811678141356 -8.230877068641124
101 -12.976174421608448 -7.882363241796725
102 -12.462358996272087 -7.6478984168416355
103 -12.52244695276022 -7.459623237418707
104 -11.130008332431316 -7.124357312750265
105 -20.558204412460327 -7.05379065585803
106 -9.31231789290905 -6.853091098326624
107 -8.631665386259556 -6.6211384471641495
108 -20.839129477739334 -6.494555224953677
109 -8.08652176707983 -6.117354180737655
110 -16.254764288663864 -6.087055095509873
111 -9.10300262644887 -5.43500831968483
112 -6.715739402920008 -5.105291741614599
113 -8.165615115314722 -4.628649413275992
114 -6.186558570712805 -4.471031187897325
115 -4.921687752008438 -4.455504779070034
116 -10.552377544343472 -4.2805498188182405
117 -10.38174770027399 -3.7944735717969627
118 -2.7388546876609325 -2.9512438456190186
119 -2.967007360421121 -2.541618164765197
train accuracy: 0.9905555555555555
validation accuracy: 0.995

demos: (120, 50, 12)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:2
end of epoch 0: val_loss 0.1313501036095286, val_acc 0.95
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.00645532135382382, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0011627333463775358, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.000365065755530054, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 2.8031626312596814e-06, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 5: val_loss 1.960989072102848e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 6: val_loss 1.7464027322944277e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 7: val_loss 1.138447711923618e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 8: val_loss 4.5299509245921856e-08, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.006449746024809428, val_acc 1.0
trigger times: 1
end of epoch 11: val_loss 0.0001740193474989482, val_acc 1.0
trigger times: 2
end of epoch 12: val_loss 2.0861620413370475e-08, val_acc 1.0
trigger times: 3
end of epoch 13: val_loss 1.908224103090106e-05, val_acc 1.0
trigger times: 4
end of epoch 14: val_loss 2.3841852225814363e-09, val_acc 1.0
trigger times: 5
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.3890685674987284, val_acc 0.965
trigger times: 1
end of epoch 17: val_loss 6.896225002606115e-07, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.0002615052013697294, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 0.0007340871513713409, val_acc 1.0
trigger times: 2
end of epoch 21: val_loss 0.0001462805796857225, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 9.443935913537871e-06, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 1.0376198956763006e-06, val_acc 1.0
trigger times: 5
end of epoch 24: val_loss 4.15347754250206e-05, val_acc 1.0
trigger times: 6
end of epoch 25: val_loss 2.9802313861182485e-09, val_acc 1.0
trigger times: 7
end of epoch 26: val_loss 2.737462722915751e-05, val_acc 1.0
trigger times: 8
end of epoch 27: val_loss 1.9716987710083344e-06, val_acc 1.0
trigger times: 9
end of epoch 28: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.010155012990351224, val_acc 0.995
trigger times: 1
end of epoch 30: val_loss 6.139274752570145e-08, val_acc 1.0
trigger times: 2
end of epoch 31: val_loss 4.678254760719369e-06, val_acc 1.0
trigger times: 3
end of epoch 32: val_loss 2.4109746816236566e-06, val_acc 1.0
trigger times: 4
end of epoch 33: val_loss 1.9729106465860013e-07, val_acc 1.0
trigger times: 5
end of epoch 34: val_loss 0.020526923475546147, val_acc 0.995
trigger times: 6
end of epoch 35: val_loss 2.0959532276246673, val_acc 0.885
trigger times: 7
end of epoch 36: val_loss 0.00021207962382376876, val_acc 1.0
trigger times: 8
end of epoch 37: val_loss 3.814695794801537e-08, val_acc 1.0
trigger times: 9
end of epoch 38: val_loss 1.5676001616782286e-07, val_acc 1.0
trigger times: 10
Early stopping.
0 -38.0510390996933 -54.98547503240923
1 -39.525990322232246 -50.492268601198035
2 -35.41549529135227 -50.03933801517046
3 -36.44099875539541 -49.75347184620696
4 -36.05372713506222 -49.72654640753777
5 -38.290036007761955 -46.98011874490918
6 -36.28061343729496 -45.7351542845057
7 -34.83387489616871 -45.670579884154705
8 -35.87532112002373 -44.99030608142343
9 -33.71958723664284 -44.14602409201361
10 -34.083298444747925 -43.81326882122305
11 -34.531279392540455 -43.18878399086166
12 -35.2736769169569 -42.29180714825394
13 -33.378512904047966 -42.00401746161006
14 -36.854740872979164 -41.6910044370425
15 -34.01617673039436 -41.68588229294918
16 -34.36759042739868 -41.281777102712205
17 -32.91382175683975 -40.44278203413966
18 -33.97680462524295 -40.34838365523108
19 -32.566783517599106 -39.599701153458774
20 -32.59766182303429 -39.57586365327889
21 -31.044335216283798 -39.31972693233231
22 -29.609809905290604 -39.024610555047154
23 -31.10775601863861 -38.45534493538269
24 -31.074215285480022 -38.41270390343083
25 -32.052971594035625 -38.35634328077039
26 -30.090630255639553 -37.79713616772368
27 -28.518931806087494 -37.741528994987384
28 -33.848421439528465 -37.66475323879293
29 -30.694270208477974 -37.513139380385574
30 -34.14860926568508 -37.1809993033689
31 -31.20424921810627 -37.100703136010694
32 -30.536725252866745 -37.00630588930485
33 -31.831467017531395 -36.821916772458344
34 -31.42369568347931 -36.48799015296732
35 -29.46935848891735 -36.20965269874363
36 -30.351702408865094 -36.19207561676116
37 -31.99746882915497 -36.114459029559086
38 -29.93397431075573 -35.78149902167743
39 -28.236419331282377 -35.394503873250635
40 -31.137795239686966 -35.26282499693737
41 -30.189056530594826 -35.24303541418371
42 -31.49832232296467 -35.209705244501436
43 -30.449491009116173 -35.0654408505187
44 -29.317706253379583 -34.80241747531743
45 -29.30024517327547 -34.64469044638467
46 -27.74200944043696 -33.84284985953318
47 -26.324006125330925 -32.70706485357069
48 -26.08218316733837 -31.969099402548657
49 -26.74059510976076 -31.7109134007892
50 -27.99864101409912 -31.64414355845032
51 -27.44979241490364 -31.392382758954444
52 -29.089544221758842 -31.223196019713853
53 -25.902379274368286 -31.12953085092458
54 -26.961149737238884 -29.39157139549552
55 -27.61503042280674 -29.340125609942326
56 -22.480737805366516 -29.106189988903285
57 -24.932177860289812 -27.41102349748205
58 -25.392078537493944 -27.343722362182305
59 -25.742658611387014 -27.196681629483837
60 -24.783433586359024 -27.07399028854534
61 -22.567133828997612 -26.7047217556024
62 -24.18047370016575 -26.244794902859052
63 -24.439202681183815 -25.548365085275513
64 -22.175035253167152 -25.45878528601009
65 -23.574093706905842 -24.879106999799365
66 -23.043602652847767 -24.828695359328833
67 -23.82670684531331 -24.592745144504722
68 -22.974357075989246 -23.978745577896312
69 -22.124978739768267 -23.57262108435893
70 -20.949692718684673 -23.44970807952351
71 -22.0380648560822 -22.745309160183492
72 -21.103675421327353 -22.60679894414887
73 -21.219983723014593 -22.19891031871716
74 -20.97370023932308 -20.656863763892378
75 -19.173818234354258 -20.444472560731253
76 -19.13759863935411 -20.19699010077007
77 -21.170219752937555 -20.13839114930498
78 -20.166813611984253 -19.63760343800059
79 -20.429081477224827 -19.515598718228343
80 -19.26260200329125 -18.92838809611677
81 -19.136405259370804 -17.994774057192853
82 -17.63043025135994 -17.55742370467821
83 -16.944513792172074 -16.823073927842348
84 -16.244628715328872 -14.855082803515382
85 -15.716896614059806 -14.531424598833084
86 -14.924080561846495 -14.442420089224363
87 -16.164858609437943 -13.596012850960644
88 -11.925523138605058 -12.68135972540495
89 -15.459925198927522 -12.66418205637357
90 -14.657475982792675 -12.30017947419658
91 -13.748954266309738 -12.151904772081672
92 -13.64333225414157 -11.788852141676486
93 -13.123620193451643 -10.869989101210326
94 -13.405687619931996 -10.327681503524177
95 -11.221164733171463 -9.8572159761571
96 -10.889493562979624 -8.330116995310416
97 -11.163201160728931 -8.133195842510668
98 -11.877376358956099 -8.108197691178031
99 -8.174294024705887 -7.57539849177145
100 -7.841214036568999 -7.362443126623615
101 -7.565573333296925 -7.108327355338034
102 -8.049139859620482 -6.959063561385431
103 -7.86310968734324 -6.776946485018116
104 -7.270549830747768 -6.7220638398623045
105 -8.061296809464693 -6.719970621583102
106 -11.206655826419592 -6.535447341844848
107 -9.262840745970607 -6.51820418055673
108 -9.014909325167537 -5.615796733870542
109 -7.683553085662425 -5.34720210027791
110 -7.475804964080453 -5.078485007852753
111 -7.702537960372865 -5.027957977402961
112 -7.011300469748676 -4.827572916892203
113 -7.3144759479910135 -4.63049541560991
114 -7.084494147449732 -4.230832004686763
115 -7.274662780575454 -4.031048624093466
116 -6.6937683410942554 -3.3844671463622564
117 -6.843885317444801 -3.3322555012187633
118 -7.097979625687003 -2.6416623314910934
119 -5.771663877181709 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

demos: (120, 50, 11)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=11, out_features=1, bias=False)
)
Total number of parameters: 11
Number of trainable paramters: 11
device: cuda:0
end of epoch 0: val_loss 0.4692084844996957, val_acc 0.92
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.1814e-02,  7.8942e-02,  1.3539e-01, -4.0922e-02, -1.4644e-01,
         -7.5806e-03, -2.0562e-02, -3.7620e-02,  3.2355e-04, -4.9433e-02,
         -2.0699e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.010579051909412022, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-6.1929e-02, -2.2056e-02,  1.6708e-02,  2.5311e-02, -9.7188e-02,
         -4.6451e-04,  3.9476e-03, -1.6444e-02,  3.1333e-04, -6.1873e-05,
         -2.3116e+00]], device='cuda:0'))])
end of epoch 2: val_loss 0.16702893918644932, val_acc 0.95
trigger times: 1
end of epoch 3: val_loss 0.0006193450875601414, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.0961e-03,  3.6146e-02,  1.6521e-02, -9.4564e-02, -3.0590e-01,
          1.0489e-02,  1.2865e-03,  2.9744e-02, -3.4364e-05, -2.0981e-01,
         -2.8982e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.026284626142355164, val_acc 0.99
trigger times: 1
end of epoch 5: val_loss 0.06065246003123107, val_acc 0.99
trigger times: 2
end of epoch 6: val_loss 0.033063625494588855, val_acc 0.99
trigger times: 3
end of epoch 7: val_loss 0.025223505278720887, val_acc 0.995
trigger times: 4
end of epoch 8: val_loss 0.033375856992891674, val_acc 0.985
trigger times: 5
end of epoch 9: val_loss 0.00850940938671208, val_acc 0.995
trigger times: 6
end of epoch 10: val_loss 0.1641387661577759, val_acc 0.97
trigger times: 7
end of epoch 11: val_loss 0.07265329398394182, val_acc 0.97
trigger times: 8
end of epoch 12: val_loss 2.1096162262124154, val_acc 0.81
trigger times: 9
end of epoch 13: val_loss 0.003215495032114575, val_acc 1.0
trigger times: 10
Early stopping.
0 -36.61191862076521 -50.00222059884506
1 -27.25304952263832 -48.823739140882175
2 -28.436973374336958 -47.15605336419176
3 -25.507243126630783 -46.19619104961985
4 -37.14156770706177 -45.594227093057754
5 -30.029033988714218 -45.36842966452394
6 -40.81713789701462 -45.19068756322445
7 -29.880353420972824 -44.16490839583478
8 -21.50353166460991 -44.078303125872196
9 -32.47965511679649 -43.993555290419714
10 -22.547209322452545 -43.86306534422809
11 -31.486598156392574 -43.84874807044028
12 -28.10722416639328 -43.84199129025074
13 -20.57698754966259 -43.806385671938365
14 -20.6050144135952 -43.80581985978556
15 -23.17886185646057 -42.7594767358323
16 -18.986602380871773 -42.66316468983175
17 -27.29394295066595 -42.33177224591743
18 -28.92189610004425 -41.774963389485094
19 -26.13912197947502 -41.410068073767725
20 -24.54452109336853 -41.17786296442943
21 -21.671454563736916 -40.723520424948155
22 -30.5491883456707 -40.527189756101116
23 -25.93713355064392 -40.49595848244517
24 -30.775767624378204 -40.429389880911344
25 -26.924474492669106 -40.05653450521898
26 -24.480780825018883 -39.59232357792555
27 -34.696067452430725 -39.54162101198148
28 -28.21946808323264 -39.195237471709476
29 -30.856895327568054 -39.172389579378766
30 -25.324576273560524 -38.51095963496708
31 -23.40740966424346 -38.447265769744824
32 -31.271610528230667 -38.392107037026264
33 -18.618319638073444 -38.00745349944469
34 -20.417307168245316 -37.46482488602393
35 -30.418704733252525 -37.10988160586883
36 -24.114402890205383 -34.27116723637227
37 -16.85110952332616 -34.14139118114101
38 -18.877932948991656 -33.263072731706835
39 -20.08491848036647 -33.13344797200536
40 -16.846813961863518 -33.07825234291984
41 -26.780035376548767 -33.0321314765637
42 -33.060809284448624 -32.44934973065406
43 -15.715164944529533 -32.4007978120153
44 -23.110408008098602 -32.40063925734975
45 -12.852607443928719 -30.734403792103194
46 -13.019900269806385 -30.57151371770873
47 -20.048012413084507 -30.131236504472803
48 -17.8858462870121 -29.99326722619033
49 -16.9602063074708 -29.66908258985071
50 -30.638267889618874 -29.297233511513635
51 -16.175527319312096 -29.288890423975797
52 -14.10874080657959 -29.145878352769948
53 -15.813537126407027 -28.49601894351319
54 -19.368897899985313 -28.492023661124072
55 -21.64670745562762 -28.315961465855167
56 -15.676736876368523 -27.121110566589827
57 -12.163983337581158 -26.064532595535336
58 -15.442949276417494 -25.520524278341334
59 -15.837677270174026 -25.27101421179229
60 -16.473226442933083 -25.066644278800943
61 -19.48571489751339 -24.925849381327673
62 -17.58752203732729 -24.188105669766596
63 -15.698432840406895 -23.48479966198816
64 -9.891779032535851 -23.153943559703283
65 -13.54183765500784 -22.954730295117237
66 -15.306632250547409 -22.74124885266394
67 -15.924890764057636 -22.739273544503753
68 -9.817443162202835 -22.264945050603636
69 -18.64906688220799 -22.15569724300287
70 -12.523565709590912 -21.055920928583344
71 -10.595729637891054 -20.543356562348553
72 -13.336355417966843 -20.33499633836848
73 -14.157835394144058 -20.18157658281111
74 -14.511685214936733 -19.58144410477429
75 -14.864706980064511 -19.377225745334304
76 -11.210735365748405 -19.243135617403095
77 -11.005022749304771 -19.060620225371707
78 -16.37246724963188 -18.964124524696246
79 -14.238489337265491 -18.448962308005108
80 -17.572189450263977 -17.740722019993825
81 -8.319234352558851 -16.85889369985028
82 -10.535617921501398 -16.3381194095591
83 -12.26740162819624 -14.535892564189266
84 -10.32029564678669 -14.443670567499144
85 -9.52265801280737 -14.200413010108107
86 -12.010182999074459 -13.936976181618805
87 -10.354716449975967 -13.862253042167257
88 -13.853924352675676 -13.483098530680483
89 -10.597970362752676 -13.455892754889845
90 -9.771580282598734 -13.355868275096913
91 -12.698852956295013 -12.278515244993585
92 -6.907619062811136 -12.227387460046547
93 -8.119357317686081 -12.020717825467683
94 -13.024993970990181 -11.910094799877324
95 -9.583653952926397 -11.400284019256157
96 -11.619674717076123 -11.134618158086587
97 -8.251505244523287 -10.859166921158222
98 -7.543691750615835 -9.595137958067907
99 -10.046636959537864 -9.289921608799773
100 -12.472903739660978 -8.230877068641124
101 -13.319421270862222 -7.882363241796725
102 -14.053045317530632 -7.6478984168416355
103 -12.972287418320775 -7.459623237418707
104 -13.017790883779526 -7.124357312750265
105 -11.720506601035595 -7.05379065585803
106 -11.881211971864104 -6.853091098326624
107 -12.070678126066923 -6.6211384471641495
108 -12.497294064611197 -6.494555224953677
109 -10.728899423032999 -6.117354180737655
110 -11.854384377598763 -6.087055095509873
111 -11.15415135025978 -5.43500831968483
112 -7.4691388458013535 -5.105291741614599
113 -8.527112741023302 -4.628649413275992
114 -6.616058239713311 -4.471031187897325
115 -8.931207530200481 -4.455504779070034
116 -8.881764218211174 -4.2805498188182405
117 -8.629812128841877 -3.7944735717969627
118 -5.585183434188366 -2.9512438456190186
119 -5.169131759554148 -2.541618164765197
train accuracy: 1.0
validation accuracy: 1.0

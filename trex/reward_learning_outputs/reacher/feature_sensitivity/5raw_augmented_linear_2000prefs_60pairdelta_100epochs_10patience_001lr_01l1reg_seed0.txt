demos: (120, 50, 6)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=6, out_features=1, bias=False)
)
Total number of parameters: 6
Number of trainable paramters: 6
device: cuda:0
end of epoch 0: val_loss 0.04038916331328892, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0398, -0.0014, -0.0051,  0.0171, -0.0046, -0.9398]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.036283153230856444, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0365,  0.0212,  0.0069,  0.0092,  0.0041, -0.8181]],
       device='cuda:0'))])
end of epoch 2: val_loss 0.02000918895430175, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0324,  0.0016, -0.0073,  0.0015, -0.0390, -1.0197]],
       device='cuda:0'))])
end of epoch 3: val_loss 0.02164931306371727, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.02520792560204427, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.024106232704382363, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 0.005960782406864063, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0231,  0.0083,  0.0196,  0.0046, -0.1710, -1.2781]],
       device='cuda:0'))])
end of epoch 7: val_loss 0.023196300985812286, val_acc 1.0
trigger times: 1
end of epoch 8: val_loss 0.022734081088940455, val_acc 1.0
trigger times: 2
end of epoch 9: val_loss 0.019691611811631447, val_acc 1.0
trigger times: 3
end of epoch 10: val_loss 0.013817988581532745, val_acc 1.0
trigger times: 4
end of epoch 11: val_loss 0.035634616893776186, val_acc 0.995
trigger times: 5
end of epoch 12: val_loss 0.027358718160825895, val_acc 0.99
trigger times: 6
end of epoch 13: val_loss 0.009457262465783458, val_acc 1.0
trigger times: 7
end of epoch 14: val_loss 0.028601851479082504, val_acc 0.995
trigger times: 8
end of epoch 15: val_loss 0.021788468976968574, val_acc 0.995
trigger times: 9
end of epoch 16: val_loss 0.012668275955052408, val_acc 1.0
trigger times: 10
Early stopping.
0 -15.1465880330652 -50.00222059884506
1 -11.865312732756138 -48.823739140882175
2 -11.841817449778318 -47.15605336419176
3 -11.329873278737068 -46.19619104961985
4 -16.162963703274727 -45.594227093057754
5 -13.01727332174778 -45.36842966452394
6 -17.112497061491013 -45.19068756322445
7 -12.176976069808006 -44.16490839583478
8 -9.689068796113133 -44.078303125872196
9 -14.33927620202303 -43.993555290419714
10 -9.579482264816761 -43.86306534422809
11 -13.181838274002075 -43.84874807044028
12 -11.60149423032999 -43.84199129025074
13 -8.452832009643316 -43.806385671938365
14 -8.475801752880216 -43.80581985978556
15 -9.497595489025116 -42.7594767358323
16 -7.771836381405592 -42.66316468983175
17 -11.864260647445917 -42.33177224591743
18 -11.462655752897263 -41.774963389485094
19 -11.606767892837524 -41.410068073767725
20 -10.502299010753632 -41.17786296442943
21 -9.272833513095975 -40.723520424948155
22 -13.362938344478607 -40.527189756101116
23 -11.974248014390469 -40.49595848244517
24 -13.138400927186012 -40.429389880911344
25 -12.026161074638367 -40.05653450521898
26 -10.544086857698858 -39.59232357792555
27 -15.624185726046562 -39.54162101198148
28 -12.053662244230509 -39.195237471709476
29 -13.131493240594864 -39.172389579378766
30 -9.872134502977133 -38.51095963496708
31 -9.785107729956508 -38.447265769744824
32 -12.934673817828298 -38.392107037026264
33 -7.83407848700881 -38.00745349944469
34 -7.6470409370958805 -37.46482488602393
35 -12.907023429870605 -37.10988160586883
36 -9.867904707789421 -34.27116723637227
37 -7.127440417185426 -34.14139118114101
38 -8.483699582517147 -33.263072731706835
39 -8.689822688698769 -33.13344797200536
40 -7.213508054614067 -33.07825234291984
41 -10.341323763132095 -33.0321314765637
42 -14.682468686252832 -32.44934973065406
43 -7.0288829281926155 -32.4007978120153
44 -10.496714927256107 -32.40063925734975
45 -4.406361725181341 -30.734403792103194
46 -5.417166003957391 -30.57151371770873
47 -9.02165512740612 -30.131236504472803
48 -7.437143683433533 -29.99326722619033
49 -7.7405019253492355 -29.66908258985071
50 -12.05128826200962 -29.297233511513635
51 -6.899503916501999 -29.288890423975797
52 -6.365682058036327 -29.145878352769948
53 -6.652700104285032 -28.49601894351319
54 -8.342923864722252 -28.492023661124072
55 -9.003953905776143 -28.315961465855167
56 -5.819286692887545 -27.121110566589827
57 -4.356248904019594 -26.064532595535336
58 -7.263960715383291 -25.520524278341334
59 -6.810384668409824 -25.27101421179229
60 -7.554815225303173 -25.066644278800943
61 -8.396764755249023 -24.925849381327673
62 -6.78040256164968 -24.188105669766596
63 -6.903135277330875 -23.48479966198816
64 -3.5129271559417248 -23.153943559703283
65 -6.219439687207341 -22.954730295117237
66 -5.995141372084618 -22.74124885266394
67 -6.144658074714243 -22.739273544503753
68 -3.497123245149851 -22.264945050603636
69 -8.06977991014719 -22.15569724300287
70 -3.8414913564920425 -21.055920928583344
71 -4.523033417761326 -20.543356562348553
72 -5.907053641974926 -20.33499633836848
73 -5.312204251997173 -20.18157658281111
74 -6.461194101721048 -19.58144410477429
75 -5.75520137604326 -19.377225745334304
76 -3.9115004166960716 -19.243135617403095
77 -4.322660965844989 -19.060620225371707
78 -6.1429599560797215 -18.964124524696246
79 -6.858979078009725 -18.448962308005108
80 -7.700689513236284 -17.740722019993825
81 -2.356997641734779 -16.85889369985028
82 -4.983723718672991 -16.3381194095591
83 -4.608965741004795 -14.535892564189266
84 -4.353839619085193 -14.443670567499144
85 -4.058162311092019 -14.200413010108107
86 -6.088153138756752 -13.936976181618805
87 -3.125764513388276 -13.862253042167257
88 -5.030075507238507 -13.483098530680483
89 -4.848818181082606 -13.455892754889845
90 -3.791079707443714 -13.355868275096913
91 -5.110976309515536 -12.278515244993585
92 -2.1284178644418716 -12.227387460046547
93 -2.636569544672966 -12.020717825467683
94 -5.876589328050613 -11.910094799877324
95 -3.4108816999942064 -11.400284019256157
96 -4.1336637968197465 -11.134618158086587
97 -2.2716555905062705 -10.859166921158222
98 -2.2677096240222454 -9.595137958067907
99 -4.608149107545614 -9.289921608799773
100 -5.092565608210862 -8.230877068641124
101 -5.364898334257305 -7.882363241796725
102 -6.064390117302537 -7.6478984168416355
103 -5.0830706236884 -7.459623237418707
104 -5.78936106339097 -7.124357312750265
105 -4.683800641447306 -7.05379065585803
106 -4.420650375075638 -6.853091098326624
107 -4.981975924223661 -6.6211384471641495
108 -5.055620415136218 -6.494555224953677
109 -3.752744193188846 -6.117354180737655
110 -4.653533354401588 -6.087055095509873
111 -4.16235838457942 -5.43500831968483
112 -3.838277155533433 -5.105291741614599
113 -3.2595483595505357 -4.628649413275992
114 -2.6655446202494204 -4.471031187897325
115 -2.781971079763025 -4.455504779070034
116 -2.8548001926392317 -4.2805498188182405
117 -2.5821003280580044 -3.7944735717969627
118 -1.1085536377504468 -2.9512438456190186
119 -0.9404106298461556 -2.541618164765197
train accuracy: 0.9961111111111111
validation accuracy: 1.0

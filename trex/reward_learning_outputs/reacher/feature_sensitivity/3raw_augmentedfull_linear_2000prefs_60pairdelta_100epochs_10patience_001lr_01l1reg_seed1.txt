demos: (120, 50, 5)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=5, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 5
Number of trainable paramters: 5
device: cuda:3
end of epoch 0: val_loss 0.007813145146355964, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.004299138095484522, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 6.19547815443866e-05, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.007450661268667318, val_acc 1.0
trigger times: 1
end of epoch 4: val_loss 0.006032211336278123, val_acc 1.0
trigger times: 2
end of epoch 5: val_loss 0.003060324154794216, val_acc 1.0
trigger times: 3
end of epoch 6: val_loss 0.008898768124117851, val_acc 1.0
trigger times: 4
end of epoch 7: val_loss 0.0035328530340120777, val_acc 1.0
trigger times: 5
end of epoch 8: val_loss 0.00824668412518804, val_acc 1.0
trigger times: 6
end of epoch 9: val_loss 9.20242305922514e-05, val_acc 1.0
trigger times: 7
end of epoch 10: val_loss 0.0072839274897705765, val_acc 1.0
trigger times: 8
end of epoch 11: val_loss 0.005714686387655092, val_acc 1.0
trigger times: 9
end of epoch 12: val_loss 0.009382368143415079, val_acc 1.0
trigger times: 10
Early stopping.
0 -9.260200688615441 -54.98547503240923
1 -9.505979180335999 -50.492268601198035
2 -8.432227125391364 -50.03933801517046
3 -8.83075812831521 -49.75347184620696
4 -8.463009472936392 -49.72654640753777
5 -9.307283725589514 -46.98011874490918
6 -8.715348858386278 -45.7351542845057
7 -8.234730064868927 -45.670579884154705
8 -8.631583254784346 -44.99030608142343
9 -8.06905822083354 -44.14602409201361
10 -8.353296771645546 -43.81326882122305
11 -8.125612751115113 -43.18878399086166
12 -8.443473137915134 -42.29180714825394
13 -8.216295659542084 -42.00401746161006
14 -8.818012870848179 -41.6910044370425
15 -8.03441840223968 -41.68588229294918
16 -8.068860676139593 -41.281777102712205
17 -7.722789820283651 -40.44278203413966
18 -8.118990011513233 -40.34838365523108
19 -7.904805172234774 -39.599701153458774
20 -7.841627879999578 -39.57586365327889
21 -7.457132078707218 -39.31972693233231
22 -7.050683772191405 -39.024610555047154
23 -7.592297883704305 -38.45534493538269
24 -7.376982456073165 -38.41270390343083
25 -7.694815689697862 -38.35634328077039
26 -7.239213773049414 -37.79713616772368
27 -6.8397537264972925 -37.741528994987384
28 -7.9298910573124886 -37.66475323879293
29 -7.40065479464829 -37.513139380385574
30 -8.173112113028765 -37.1809993033689
31 -7.543437092565 -37.100703136010694
32 -7.303670471534133 -37.00630588930485
33 -7.514100395143032 -36.821916772458344
34 -7.697958678007126 -36.48799015296732
35 -7.050572579726577 -36.20965269874363
36 -7.14670348749496 -36.19207561676116
37 -7.652693904936314 -36.114459029559086
38 -7.220315873622894 -35.78149902167743
39 -6.787564741913229 -35.394503873250635
40 -7.580906730145216 -35.26282499693737
41 -7.336655505001545 -35.24303541418371
42 -7.460315341129899 -35.209705244501436
43 -7.061963979154825 -35.0654408505187
44 -6.917869996279478 -34.80241747531743
45 -7.180362245067954 -34.64469044638467
46 -6.632509974762797 -33.84284985953318
47 -6.198798994533718 -32.70706485357069
48 -6.156714640557766 -31.969099402548657
49 -6.211540814489126 -31.7109134007892
50 -6.473773447796702 -31.64414355845032
51 -6.538359289988875 -31.392382758954444
52 -6.885882122907788 -31.223196019713853
53 -6.126471376977861 -31.12953085092458
54 -6.700167026370764 -29.39157139549552
55 -6.4427928905934095 -29.340125609942326
56 -5.644441932439804 -29.106189988903285
57 -6.025168586522341 -27.41102349748205
58 -5.883868578355759 -27.343722362182305
59 -5.924489272292703 -27.196681629483837
60 -6.04503577016294 -27.07399028854534
61 -5.401819132268429 -26.7047217556024
62 -5.537240285426378 -26.244794902859052
63 -5.626238415017724 -25.548365085275513
64 -5.428961360827088 -25.45878528601009
65 -5.482089954428375 -24.879106999799365
66 -5.571375949308276 -24.828695359328833
67 -5.725331734865904 -24.592745144504722
68 -5.694927469827235 -23.978745577896312
69 -5.025947321206331 -23.57262108435893
70 -5.291713567450643 -23.44970807952351
71 -5.538338556885719 -22.745309160183492
72 -5.091617539990693 -22.60679894414887
73 -4.854611873161048 -22.19891031871716
74 -5.306876202113926 -20.656863763892378
75 -4.728744828142226 -20.444472560731253
76 -4.2935200380161405 -20.19699010077007
77 -5.036364532075822 -20.13839114930498
78 -4.868786787614226 -19.63760343800059
79 -4.885602008551359 -19.515598718228343
80 -4.385201344732195 -18.92838809611677
81 -4.314135339576751 -17.994774057192853
82 -4.072820578701794 -17.55742370467821
83 -3.8645612576510757 -16.823073927842348
84 -3.705386016285047 -14.855082803515382
85 -3.8657125383615494 -14.531424598833084
86 -3.6265413127839565 -14.442420089224363
87 -3.627986769657582 -13.596012850960644
88 -2.594098238972947 -12.68135972540495
89 -3.672670779749751 -12.66418205637357
90 -3.242305633146316 -12.30017947419658
91 -2.9980969694443047 -12.151904772081672
92 -3.2333889678120613 -11.788852141676486
93 -2.83374978415668 -10.869989101210326
94 -2.9349750669207424 -10.327681503524177
95 -2.867130219936371 -9.8572159761571
96 -2.369275360368192 -8.330116995310416
97 -2.4526167805306613 -8.133195842510668
98 -2.565154660027474 -8.108197691178031
99 -1.7180770603008568 -7.57539849177145
100 -1.6317731915041804 -7.362443126623615
101 -1.5622907998040318 -7.108327355338034
102 -1.7686463478021324 -6.959063561385431
103 -2.064783213660121 -6.776946485018116
104 -1.462521796580404 -6.7220638398623045
105 -2.149720643647015 -6.719970621583102
106 -2.40311831375584 -6.535447341844848
107 -2.0325972409918904 -6.51820418055673
108 -1.9348121634684503 -5.615796733870542
109 -1.8995396019890904 -5.34720210027791
110 -1.5396819268353283 -5.078485007852753
111 -1.5972189132589847 -5.027957977402961
112 -1.8301037391647696 -4.827572916892203
113 -1.4712792022619396 -4.63049541560991
114 -1.8575978139415383 -4.230832004686763
115 -1.7102972459979355 -4.031048624093466
116 -1.3307691323570907 -3.3844671463622564
117 -1.557605444919318 -3.3322555012187633
118 -1.4187028212472796 -2.6416623314910934
119 -1.2227817010134459 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

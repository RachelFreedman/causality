Using reward based purely on privileged features...
demos: (120, 50, 12)
demo_rewards: (120,)
[-14.85791774 -13.76094503 -13.66535661 -13.42155585 -12.79752535
 -12.27695738 -12.1291282  -11.72643822 -11.57570342 -11.4560068
 -11.42866947 -11.22058305 -11.19541909 -10.99446173 -10.98210088
 -10.8571586  -10.63711923 -10.58125298 -10.57318568 -10.3663351
 -10.20167178 -10.11221097  -9.98959279  -9.63633162  -9.6011403
  -9.57875874  -9.31410649  -9.30546471  -9.22470903  -9.10966233
  -8.97686902  -8.63508006  -8.59927311  -8.30647364  -8.13122117
  -8.05033441  -8.01118418  -7.69056555  -7.64306483  -7.4153496
  -7.36787644  -7.36495659  -7.29057777  -7.17760929  -7.17153839
  -6.98241694  -6.93519891  -6.76900782  -6.64725897  -6.52709835
  -6.52152749  -6.41487851  -6.41037209  -6.36940409  -6.23328883
  -6.09453464  -5.99716727  -5.95074116  -5.93230188  -5.89948025
  -5.83692643  -5.80246552  -5.54279998  -5.53970939  -5.44046594
  -5.42333419  -5.41391022  -5.37107565  -5.32325185  -5.27134888
  -5.26824041  -5.1794402   -5.11163938  -5.10276504  -5.00834401
  -4.94983944  -4.7295443   -4.70425209  -4.65421193  -4.61602401
  -4.61436506  -4.57205556  -4.51428412  -4.39516432  -4.38817518
  -4.37430519  -4.35568938  -4.31449272  -4.26200003  -4.22302916
  -4.1333606   -4.08321706  -4.02633649  -3.92623516  -3.90981626
  -3.89942827  -3.89032645  -3.8872099   -3.88489119  -3.75465691
  -3.74416519  -3.71317916  -3.34717703  -3.29453532  -3.12907393
  -3.08291211  -3.04161692  -2.85755847  -2.81526423  -2.62322513
  -2.54733891  -2.52538476  -2.44036713  -2.34252113  -2.26615554
  -2.25698833  -2.08748768  -2.00991353  -1.53424024  -1.27345687]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.00013145170433990217, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.2156e-01,  1.4868e-04,  1.9005e-02, -9.4240e-02,  1.1901e-03,
         -7.5749e-05, -2.9672e-03,  3.5466e-03, -7.5715e-04, -1.3399e-04,
          1.3672e-03, -1.9799e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.0035521779379005737, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.35372612151991345, val_acc 0.945
trigger times: 2
end of epoch 3: val_loss 0.5154587234258259, val_acc 0.915
trigger times: 3
end of epoch 4: val_loss 0.002602371179526699, val_acc 1.0
trigger times: 4
end of epoch 5: val_loss 0.00119842624832188, val_acc 1.0
trigger times: 5
end of epoch 6: val_loss 1.7167583745028027e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.3049e-02, -2.4208e-05,  7.9476e-05,  6.3339e-06,  1.7148e-04,
          1.0337e-05,  3.4843e-06,  1.0732e-06, -2.8392e-04, -3.3239e-06,
          1.3644e-03, -1.9860e+00]], device='cuda:0'))])
end of epoch 7: val_loss 1.3588592405717747e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1798e-01, -2.5723e-05,  1.3591e-02, -9.4122e-02, -3.5209e-05,
         -4.7917e-05,  2.3455e-06,  1.3075e-02, -1.6617e-04,  2.3277e-05,
          1.3639e-03, -2.3440e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.003678454570431313, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 4.767489737652397e-05, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.0004330713403364683, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 0.008026327525325457, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 4.035102028595361e-05, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.00014461691913354003, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.0005016701648898092, val_acc 1.0
trigger times: 7
end of epoch 15: val_loss 0.0013865539826826989, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.004026916769180389, val_acc 1.0
trigger times: 9
end of epoch 17: val_loss 5.714024928771266e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-8.3678e-05, -6.6951e-05,  3.5537e-02, -4.8312e-02,  1.4643e-04,
          3.4208e-06, -1.5331e-06,  9.4481e-03,  2.9327e-04, -1.5801e-07,
          1.3593e-03, -2.1643e+00]], device='cuda:0'))])
end of epoch 18: val_loss 7.821233827819185e-05, val_acc 1.0
trigger times: 1
end of epoch 19: val_loss 0.12728460119884044, val_acc 0.97
trigger times: 2
end of epoch 20: val_loss 0.02719142525554947, val_acc 0.985
trigger times: 3
end of epoch 21: val_loss 0.0055559604326845145, val_acc 1.0
trigger times: 4
end of epoch 22: val_loss 0.5258177207906968, val_acc 0.84
trigger times: 5
end of epoch 23: val_loss 0.3458845241365669, val_acc 0.92
trigger times: 6
end of epoch 24: val_loss 1.1689196549650571, val_acc 0.85
trigger times: 7
end of epoch 25: val_loss 0.00013456584621049928, val_acc 1.0
trigger times: 8
end of epoch 26: val_loss 7.325659321253397e-06, val_acc 1.0
trigger times: 9
end of epoch 27: val_loss 9.34231487580206e-06, val_acc 1.0
trigger times: 10
Early stopping.
0 -23.276075452566147 -14.857917737169885
1 -28.63816311955452 -13.760945032136553
2 -30.91511894762516 -13.665356611697247
3 -36.953725188970566 -13.421555850023143
4 -33.94655451178551 -12.79752535131424
5 -31.86767239868641 -12.276957383174551
6 -31.13283509016037 -12.129128199666564
7 -21.27834552526474 -11.726438221774067
8 -27.655095487833023 -11.575703421057804
9 -21.361385762691498 -11.45600679926644
10 -23.342089787125587 -11.428669465994592
11 -28.186613351106644 -11.220583049581975
12 -24.785189419984818 -11.195419085153274
13 -17.946651577949524 -10.994461725751476
14 -25.175156593322754 -10.982100883253427
15 -26.44997973740101 -10.857158604065402
16 -22.60342165827751 -10.637119226889629
17 -18.10745170712471 -10.581252977120647
18 -21.323768466711044 -10.57318568221164
19 -22.04304938018322 -10.366335101082012
20 -18.958547927439213 -10.201671783331733
21 -22.08883009850979 -10.112210969210379
22 -24.92039104551077 -9.989592788267442
23 -17.208265885710716 -9.636331624790365
24 -18.595090925693512 -9.601140304879115
25 -24.08288472890854 -9.578758736670897
26 -17.992302238941193 -9.314106491939148
27 -15.214709624648094 -9.305464709502179
28 -28.039158552885056 -9.224709027836628
29 -14.609334141016006 -9.109662326462507
30 -18.205147728323936 -8.9768690196357
31 -20.61458019912243 -8.635080059050962
32 -20.408415764570236 -8.599273111315565
33 -17.6265784278512 -8.306473643465596
34 -16.65027478337288 -8.131221165134493
35 -24.651683259755373 -8.050334414267054
36 -24.21418745070696 -8.011184177045752
37 -15.70382396876812 -7.690565550918285
38 -17.416325755417347 -7.643064825505027
39 -14.193101566284895 -7.415349598732486
40 -17.348122112452984 -7.367876442206603
41 -16.05840033106506 -7.364956588829062
42 -17.635345360264182 -7.290577772068284
43 -14.25676916539669 -7.177609292647348
44 -25.68524770438671 -7.171538386651344
45 -10.423648677766323 -6.9824169445409625
46 -11.03375169634819 -6.9351989118904225
47 -12.70605093985796 -6.7690078175369015
48 -8.961260966956615 -6.647258965381362
49 -8.822344448417425 -6.52709834517458
50 -19.08167127519846 -6.521527485301631
51 -18.14162665605545 -6.414878510216887
52 -20.256388649344444 -6.410372091550173
53 -18.55803330987692 -6.3694040946839525
54 -11.144652619957924 -6.233288826291319
55 -10.951336950063705 -6.094534642524499
56 -11.120126768946648 -5.997167272764618
57 -15.843980144709349 -5.950741155645245
58 -11.944916866719723 -5.93230188156937
59 -13.611746221780777 -5.899480249325287
60 -11.648674568161368 -5.836926428931782
61 -8.477340936660767 -5.80246551779649
62 -5.334000341594219 -5.542799983276438
63 -16.85513649880886 -5.539709393523854
64 -9.757865026593208 -5.440465935277435
65 -9.561316773295403 -5.423334187907614
66 -8.750983454287052 -5.4139102154677
67 -8.409392714500427 -5.371075648457043
68 -18.13481441140175 -5.323251845363989
69 -11.059355311095715 -5.271348878324376
70 -8.212954772636294 -5.268240406327687
71 -10.786903634667397 -5.179440204599235
72 -10.672640010714531 -5.111639376062079
73 -5.764975517988205 -5.102765035513635
74 -5.188421614468098 -5.008344009609225
75 -7.096132185310125 -4.949839435408552
76 -7.482239753007889 -4.729544301417749
77 -8.803160220384598 -4.704252094305983
78 -5.515571127645671 -4.654211925967799
79 -2.774985520169139 -4.616024012787141
80 -7.505931593477726 -4.614365057638494
81 -12.556740444153547 -4.572055555427089
82 -6.676081329584122 -4.514284121276057
83 -12.5846634991467 -4.395164317798481
84 -6.461960982531309 -4.388175179034372
85 -4.354989407584071 -4.3743051941816775
86 -0.2503447085618973 -4.355689380234473
87 -7.946981869637966 -4.3144927222222025
88 -2.2672424763441086 -4.2620000260882644
89 -4.90106092998758 -4.2230291643481825
90 -5.6939003095030785 -4.133360599164207
91 -0.4399586692452431 -4.083217057852922
92 -5.8133733700960875 -4.026336493873877
93 -6.926030369475484 -3.9262351613387976
94 -7.59927589353174 -3.9098162592153867
95 -5.407591596245766 -3.8994282747402877
96 0.40580450743436813 -3.8903264513768816
97 -4.392621651291847 -3.8872099001190463
98 -1.6279590353369713 -3.8848911864576006
99 -8.267190992832184 -3.7546569093221462
100 -2.1852845326066017 -3.744165186620453
101 -6.410400331020355 -3.713179158085272
102 0.9517755061388016 -3.3471770256080786
103 -4.782858695834875 -3.294535322484194
104 1.1468377262353897 -3.129073931650425
105 1.243653193116188 -3.082912105420874
106 -0.2945338450372219 -3.041616924495897
107 -4.225945543497801 -2.8575584743648856
108 -2.2551336586475372 -2.815264229940162
109 0.7838245779275894 -2.62322513096209
110 0.06707096472382545 -2.547338912767599
111 4.4961129650473595 -2.5253847629391624
112 1.8234452456235886 -2.4403671323608287
113 2.492172986268997 -2.342521127844057
114 0.7868692968040705 -2.266155535426123
115 4.578034467995167 -2.256988325062527
116 5.429845951497555 -2.0874876766280326
117 5.635139260441065 -2.0099135282821634
118 6.61645470559597 -1.5342402374776334
119 6.790901556611061 -1.2734568732176126
train accuracy: 1.0
validation accuracy: 1.0

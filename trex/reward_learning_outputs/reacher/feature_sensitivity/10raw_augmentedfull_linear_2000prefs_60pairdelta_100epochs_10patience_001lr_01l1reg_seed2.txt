demos: (120, 50, 12)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:2
end of epoch 0: val_loss 0.0164161354206837, val_acc 0.995
trigger times: 0
saving model weights...
end of epoch 1: val_loss 3.0039939687753756e-07, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0036375557402789126, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 4: val_loss 9.357857489789013e-08, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 0.6940234073674161, val_acc 0.91
trigger times: 2
end of epoch 6: val_loss 1.7464128742261665e-07, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 1.1920928244535389e-09, val_acc 1.0
trigger times: 4
end of epoch 8: val_loss 0.2909366957587405, val_acc 0.975
trigger times: 5
end of epoch 9: val_loss 8.152323076409119e-06, val_acc 1.0
trigger times: 6
end of epoch 10: val_loss 0.010367521033167577, val_acc 1.0
trigger times: 7
end of epoch 11: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
end of epoch 12: val_loss 1.1818564335897008e-06, val_acc 1.0
trigger times: 1
end of epoch 13: val_loss 2.980231634808206e-09, val_acc 1.0
trigger times: 2
end of epoch 14: val_loss 0.0007884631301886813, val_acc 1.0
trigger times: 3
end of epoch 15: val_loss 1.3941372459314038e-06, val_acc 1.0
trigger times: 4
end of epoch 16: val_loss 5.364416608699685e-09, val_acc 1.0
trigger times: 5
end of epoch 17: val_loss 3.6119246114907356e-07, val_acc 1.0
trigger times: 6
end of epoch 18: val_loss 7.1525522571391775e-09, val_acc 1.0
trigger times: 7
end of epoch 19: val_loss 8.952582500337098, val_acc 0.65
trigger times: 8
end of epoch 20: val_loss 2.5033942279151234e-08, val_acc 1.0
trigger times: 9
end of epoch 21: val_loss 1.1920927533992654e-09, val_acc 1.0
trigger times: 10
Early stopping.
0 -64.53273710608482 -54.98547503240923
1 -68.24306628108025 -50.492268601198035
2 -62.37580978870392 -50.03933801517046
3 -54.78666330873966 -49.75347184620696
4 -61.67562639713287 -49.72654640753777
5 -45.30001026391983 -46.98011874490918
6 -63.521313816308975 -45.7351542845057
7 -57.27507523447275 -45.670579884154705
8 -53.662726521492004 -44.99030608142343
9 -67.56823000311852 -44.14602409201361
10 -50.91691605746746 -43.81326882122305
11 -59.308644473552704 -43.18878399086166
12 -67.06083232164383 -42.29180714825394
13 -55.046379156410694 -42.00401746161006
14 -63.034443974494934 -41.6910044370425
15 -55.82230067253113 -41.68588229294918
16 -49.666139394044876 -41.281777102712205
17 -55.56321904063225 -40.44278203413966
18 -58.98166073858738 -40.34838365523108
19 -46.09615206718445 -39.599701153458774
20 -51.8942644149065 -39.57586365327889
21 -57.827427357435226 -39.31972693233231
22 -50.272855788469315 -39.024610555047154
23 -48.106098368763924 -38.45534493538269
24 -49.916637033224106 -38.41270390343083
25 -49.10621503740549 -38.35634328077039
26 -55.73747059702873 -37.79713616772368
27 -52.35152181982994 -37.741528994987384
28 -51.72585169970989 -37.66475323879293
29 -43.79532086849213 -37.513139380385574
30 -63.470381289720535 -37.1809993033689
31 -44.29605820029974 -37.100703136010694
32 -53.612483114004135 -37.00630588930485
33 -50.792181953787804 -36.821916772458344
34 -41.320398434996605 -36.48799015296732
35 -50.5793753862381 -36.20965269874363
36 -47.94359916448593 -36.19207561676116
37 -50.68068325519562 -36.114459029559086
38 -52.40737873315811 -35.78149902167743
39 -41.60840092971921 -35.394503873250635
40 -56.73949587345123 -35.26282499693737
41 -44.713744297623634 -35.24303541418371
42 -50.5014810860157 -35.209705244501436
43 -52.12378744781017 -35.0654408505187
44 -44.670045003294945 -34.80241747531743
45 -45.32033370435238 -34.64469044638467
46 -48.77401739358902 -33.84284985953318
47 -47.41510111093521 -32.70706485357069
48 -39.05146914720535 -31.969099402548657
49 -40.57228150218725 -31.7109134007892
50 -46.94343440234661 -31.64414355845032
51 -43.083689749240875 -31.392382758954444
52 -43.07355835288763 -31.223196019713853
53 -44.35515634715557 -31.12953085092458
54 -39.974132627248764 -29.39157139549552
55 -48.52608358860016 -29.340125609942326
56 -39.08861968666315 -29.106189988903285
57 -43.012079894542694 -27.41102349748205
58 -37.09595688432455 -27.343722362182305
59 -37.06006722897291 -27.196681629483837
60 -42.3693009801209 -27.07399028854534
61 -41.12235152721405 -26.7047217556024
62 -36.67045854777098 -26.244794902859052
63 -36.030175134539604 -25.548365085275513
64 -35.37309416383505 -25.45878528601009
65 -40.04677093029022 -24.879106999799365
66 -34.645690873265266 -24.828695359328833
67 -39.55920282006264 -24.592745144504722
68 -33.997221533209085 -23.978745577896312
69 -39.66310581564903 -23.57262108435893
70 -32.25366957485676 -23.44970807952351
71 -36.57910378277302 -22.745309160183492
72 -41.35212504863739 -22.60679894414887
73 -35.620974361896515 -22.19891031871716
74 -35.27022651024163 -20.656863763892378
75 -30.78608151525259 -20.444472560731253
76 -31.859873913228512 -20.19699010077007
77 -31.614064686000347 -20.13839114930498
78 -33.46991228312254 -19.63760343800059
79 -35.18308998644352 -19.515598718228343
80 -27.718250632286072 -18.92838809611677
81 -33.26956244558096 -17.994774057192853
82 -27.13021600805223 -17.55742370467821
83 -30.90108449012041 -16.823073927842348
84 -23.568898379802704 -14.855082803515382
85 -22.395279409363866 -14.531424598833084
86 -23.428829289972782 -14.442420089224363
87 -22.394264720380306 -13.596012850960644
88 -21.204732928425074 -12.68135972540495
89 -21.201903089880943 -12.66418205637357
90 -20.139734618365765 -12.30017947419658
91 -25.1247381567955 -12.151904772081672
92 -22.57305521517992 -11.788852141676486
93 -23.701115556061268 -10.869989101210326
94 -17.605460308492184 -10.327681503524177
95 -16.24610209837556 -9.8572159761571
96 -14.013022854924202 -8.330116995310416
97 -13.885535806417465 -8.133195842510668
98 -14.836686100810766 -8.108197691178031
99 -16.59044409915805 -7.57539849177145
100 -15.060975842177868 -7.362443126623615
101 -15.262195082381368 -7.108327355338034
102 -10.765381963923573 -6.959063561385431
103 -14.361831158399582 -6.776946485018116
104 -14.882124286144972 -6.7220638398623045
105 -15.563449703156948 -6.719970621583102
106 -14.003072515130043 -6.535447341844848
107 -12.810138557106256 -6.51820418055673
108 -11.938021544367075 -5.615796733870542
109 -9.136275932192802 -5.34720210027791
110 -8.147961024194956 -5.078485007852753
111 -9.328126359730959 -5.027957977402961
112 -8.856146210804582 -4.827572916892203
113 -7.705336529761553 -4.63049541560991
114 -9.962076063267887 -4.230832004686763
115 -8.236224062740803 -4.031048624093466
116 -6.667314052581787 -3.3844671463622564
117 -7.586511917412281 -3.3322555012187633
118 -6.697520099580288 -2.6416623314910934
119 -5.976842250674963 -1.9136196540088464
train accuracy: 1.0
validation accuracy: 1.0

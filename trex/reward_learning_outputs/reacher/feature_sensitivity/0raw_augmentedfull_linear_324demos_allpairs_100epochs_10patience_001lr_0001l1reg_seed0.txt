demos: (360, 50, 2)
demo_rewards: (360,)
sorted_train_rewards: [-52.04485265 -51.15845655 -50.95730303 -48.93380144 -48.88659405
 -48.81199543 -47.73632204 -47.63792498 -47.09890202 -46.40659067
 -45.58205756 -45.55641869 -45.3781584  -45.1889403  -45.10455724
 -45.09649042 -45.06090831 -44.90717655 -44.66710015 -44.58646443
 -44.40276741 -44.29353296 -44.26132764 -44.205686   -44.18844659
 -44.18649198 -43.97817017 -43.90835803 -43.90233576 -43.67337234
 -43.63795834 -43.57867127 -43.41725811 -43.07917355 -42.78812793
 -42.73280043 -42.71498917 -42.37660312 -42.35156982 -42.2741431
 -42.27178642 -42.06976011 -42.02441567 -41.9208162  -41.85021898
 -41.83003384 -41.71300333 -41.60676811 -41.5158946  -41.4288974
 -41.35473131 -41.27204701 -41.26835888 -41.17517134 -41.09778195
 -41.05637645 -41.03450743 -40.98849948 -40.88277159 -40.7682006
 -40.57815481 -40.52517704 -40.51972295 -40.4735601  -40.44146654
 -40.36907268 -40.27883122 -40.20765046 -40.10250365 -40.09758852
 -39.39946011 -39.38715059 -39.27347855 -39.23883604 -39.23607073
 -39.21869941 -39.21327141 -39.20948267 -39.10415592 -39.02946046
 -39.01949066 -39.00315068 -38.70707769 -38.45325451 -38.40734868
 -38.32682998 -38.09085041 -37.84470936 -37.52355928 -37.4434499
 -37.12776422 -36.83830962 -36.66844362 -36.58641248 -36.39170138
 -36.34182169 -36.30418157 -36.28270385 -36.11080763 -36.03273585
 -35.76965183 -35.72835706 -35.45254062 -35.34226992 -34.87824674
 -34.56405996 -34.50306279 -34.47986837 -34.42748575 -34.13144584
 -34.0228191  -33.83098719 -33.80045069 -33.61206111 -33.59175617
 -33.55287238 -33.49044026 -33.40995872 -33.3865897  -33.37339807
 -33.27546848 -32.98496162 -32.81082139 -32.78886158 -32.74101924
 -32.7313686  -32.46498395 -32.03564157 -31.90682883 -31.83556376
 -31.79116041 -31.50474657 -30.99431461 -30.93631085 -30.90992683
 -30.86825618 -30.8534927  -30.80257739 -30.68244687 -30.48486935
 -29.90514552 -29.87882928 -29.8114822  -29.5623783  -29.48688888
 -29.47008372 -29.4431262  -29.16728751 -29.08632404 -28.75963456
 -28.72161062 -28.26842147 -28.13673546 -28.13429517 -28.10052984
 -28.08369991 -28.02777081 -27.84489659 -27.61843292 -27.55718389
 -27.06419421 -27.0029758  -26.76388365 -26.60775678 -26.41157825
 -26.14307162 -26.10745646 -25.91146004 -25.38206609 -25.33112486
 -25.30351849 -25.18999305 -25.13806635 -24.2272877  -24.05919843
 -23.72346546 -23.61704597 -23.57894664 -23.49865824 -23.3885205
 -23.23362913 -22.8922421  -22.65599334 -22.64864934 -22.55715908
 -22.47349563 -22.28075152 -22.20139031 -22.12605556 -22.02464667
 -21.90842064 -21.73813504 -21.51677505 -21.48630258 -21.28560677
 -20.49946861 -20.14066085 -20.06168433 -19.95070877 -19.94914683
 -19.89507051 -19.78533441 -19.75469854 -19.7198622  -19.71871673
 -19.69991772 -19.51898865 -19.35763998 -19.06566814 -19.02836343
 -18.79967717 -18.79931946 -18.69683752 -18.69338489 -18.59323781
 -18.23945715 -18.12685193 -18.0241144  -17.92817501 -17.84544435
 -17.74148009 -17.68079099 -17.40776893 -17.26560969 -17.20010184
 -16.7595022  -16.72635288 -16.29122883 -16.26549553 -16.090455
 -16.05119129 -16.04208483 -15.66572915 -15.35726758 -15.29501063
 -15.23723291 -15.00526075 -14.96920977 -14.96583859 -14.77403271
 -14.39781398 -14.11669085 -14.111545   -13.79702407 -13.72364022
 -13.60070757 -13.54966089 -13.4867368  -13.39595816 -13.26567075
 -13.24583955 -13.16316874 -13.03199125 -13.02215529 -12.69460232
 -12.57904671 -12.34665554 -12.30014847 -12.05112503 -12.04804099
 -11.43360312 -11.20504127 -10.93302499 -10.58831597 -10.51085023
 -10.28383593 -10.20667265  -9.80901492  -9.4678784   -8.96235737
  -8.40276084  -8.36555609  -8.32638751  -8.0269637   -7.83599816
  -7.68743448  -7.57539849  -7.5455064   -7.54460172  -7.36244313
  -7.34546388  -7.1893336   -7.15421432  -7.10832736  -6.95906356
  -6.92008425  -6.72206384  -6.71997062  -6.64795748  -6.51820418
  -6.47884361  -6.05448903  -5.85405865  -5.64485143  -5.61579673
  -5.39544196  -5.38326081  -5.3472021   -5.25793019  -5.24856721
  -5.07848501  -5.06486011  -4.90228293  -4.82757292  -4.63049542
  -4.37983153  -4.35856953  -4.230832    -4.0660223   -4.03104862
  -4.00401798  -3.97870856  -3.65032555  -3.38446715  -3.3322555
  -3.329867    -3.29356852  -3.07904644  -2.88591659  -2.83192847
  -2.67390706  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-47.07471735 -42.35130146 -39.48434976 -37.45493938 -37.19212668
 -33.51725369 -33.34018377 -33.26935494 -32.8995352  -29.49907263
 -28.44248532 -27.53369262 -26.09478846 -25.01985189 -22.71273286
 -21.98062894 -19.41242087 -18.14667207 -17.06094997 -16.98261229
 -16.67622467 -14.66804697 -11.49544096 -11.39697889 -10.65880318
 -10.25505313  -7.70107293  -7.67259169  -7.3740849   -6.77694649
  -6.74096238  -6.29894775  -5.94629658  -5.89467275  -5.02795798
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 52326
num train_labels 52326
num val_obs 630
num val_labels 630
ModuleList(
  (0): Linear(in_features=2, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 2
Number of trainable paramters: 2
device: cuda:1
end of epoch 0: val_loss 0.05654325879038643, val_acc 0.973015873015873
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.043112871451032896, val_acc 0.9761904761904762
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.043797316009903296, val_acc 0.9761904761904762
trigger times: 1
end of epoch 3: val_loss 0.04349636119707196, val_acc 0.9761904761904762
trigger times: 2
end of epoch 4: val_loss 0.05527479225463657, val_acc 0.973015873015873
trigger times: 3
end of epoch 5: val_loss 0.0494537917241337, val_acc 0.9746031746031746
trigger times: 4
end of epoch 6: val_loss 0.04662649925398138, val_acc 0.9761904761904762
trigger times: 5
end of epoch 7: val_loss 0.06283587307129362, val_acc 0.9698412698412698
trigger times: 6
end of epoch 8: val_loss 0.05514298727784036, val_acc 0.973015873015873
trigger times: 7
end of epoch 9: val_loss 0.0653692759930058, val_acc 0.9682539682539683
trigger times: 8
end of epoch 10: val_loss 0.05147124237011981, val_acc 0.973015873015873
trigger times: 9
end of epoch 11: val_loss 0.033325030293699584, val_acc 0.9873015873015873
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.04196697630553048, val_acc 0.9777777777777777
trigger times: 1
end of epoch 13: val_loss 0.047178401862012824, val_acc 0.9761904761904762
trigger times: 2
end of epoch 14: val_loss 0.06637178334497261, val_acc 0.9698412698412698
trigger times: 3
end of epoch 15: val_loss 0.057643219704081525, val_acc 0.9746031746031746
trigger times: 4
end of epoch 16: val_loss 0.0355691259083598, val_acc 0.9857142857142858
trigger times: 5
end of epoch 17: val_loss 0.05480154820684958, val_acc 0.973015873015873
trigger times: 6
end of epoch 18: val_loss 0.04840907439447177, val_acc 0.9761904761904762
trigger times: 7
end of epoch 19: val_loss 0.04512477203347708, val_acc 0.9761904761904762
trigger times: 8
end of epoch 20: val_loss 0.0419484965435339, val_acc 0.9777777777777777
trigger times: 9
end of epoch 21: val_loss 0.051116175469970535, val_acc 0.973015873015873
trigger times: 10
Early stopping.
0 -96.14457166194916 -47.074717348941945
1 -90.1701326072216 -42.351301463417975
2 -85.41842925548553 -39.48434976267673
3 -81.38506057858467 -37.45493938259741
4 -82.01258587837219 -37.19212667661568
5 -76.25484865903854 -33.517253689504955
6 -75.47161328792572 -33.34018376832633
7 -74.94242510199547 -33.269354941899884
8 -75.50047010183334 -32.899535198020125
9 -69.15670964121819 -29.499072630706173
10 -64.2076154500246 -28.44248531658195
11 -65.0727368593216 -27.53369262320323
12 -63.21860912442207 -26.094788464864997
13 -58.28118294477463 -25.019851894257886
14 -56.761149883270264 -22.71273285863043
15 -58.37091492116451 -21.98062894154843
16 -51.37120375037193 -19.412420866484695
17 -49.16578805446625 -18.14667206650413
18 -46.011421993374825 -17.060949974564405
19 -47.018369130790234 -16.982612286746377
20 -48.52914257347584 -16.676224674693263
21 -41.47287290915847 -14.668046967545768
22 -33.710053503513336 -11.495440962126883
23 -36.065524376928806 -11.396978889687901
24 -36.21494534611702 -10.658803176241303
25 -32.06081007421017 -10.255053134746943
26 -29.44534818828106 -7.70107292515296
27 -26.689213268458843 -7.67259168928286
28 -25.76285284385085 -7.374084902530061
29 -24.87008012831211 -6.776946485018116
30 -27.32742539793253 -6.740962377474064
31 -22.873572938144207 -6.298947752982548
32 -24.18514993786812 -5.946296581927391
33 -22.902601309120655 -5.894672754152052
34 -21.90569159388542 -5.027957977402961
35 -16.875728622078896 -2.49009826539426
train accuracy: 0.9823988074762069
validation accuracy: 0.973015873015873

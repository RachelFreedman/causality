demos: (120, 50, 12)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
Normalizing input features...
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 36
Number of trainable paramters: 36
device: cuda:0
end of epoch 0: val_loss 0.4445679889898747, val_acc 0.795
trigger times: 0
saving model weights...
Weights: OrderedDict([('layer_norm.weight', tensor([ 8.2009e-04, -2.3344e-04,  2.5354e-01,  1.0234e-01,  1.7514e-03,
         1.2212e-04, -1.1180e-04,  2.2306e-04, -3.1828e-04,  1.4721e-03,
         1.5401e-03,  2.2314e-04], device='cuda:0')), ('layer_norm.bias', tensor([ 0.0017,  0.0051, -0.0050,  0.0001, -0.0029, -0.0009,  0.0009,  0.0050,
         0.0002, -0.0002,  0.0002, -0.0048], device='cuda:0')), ('fcs.0.weight', tensor([[ 1.6407e-03,  1.0000e-04,  1.6523e-01, -5.8303e-02, -1.4408e-02,
         -5.7579e-05,  7.0797e-06, -2.5703e-06,  4.1085e-05, -1.2492e-05,
         -5.2713e-05, -5.9411e-05]], device='cuda:0'))])
end of epoch 1: val_loss 0.4436834187246859, val_acc 0.78
trigger times: 0
saving model weights...
Weights: OrderedDict([('layer_norm.weight', tensor([-1.6575e-04, -8.6622e-05,  2.0356e-01,  1.2388e-01,  4.2540e-04,
         4.5826e-04,  1.3681e-04,  1.2118e-04,  2.2247e-03, -2.4749e-04,
        -1.0103e-03,  1.3805e-03], device='cuda:0')), ('layer_norm.bias', tensor([ 0.0017,  0.0051, -0.0049,  0.0001, -0.0004, -0.0009,  0.0009,  0.0050,
         0.0002,  0.0005, -0.0005, -0.0048], device='cuda:0')), ('fcs.0.weight', tensor([[-1.7058e-04,  1.1809e-04,  1.7864e-01, -8.7491e-02, -2.1013e-04,
          8.8628e-05,  5.5573e-05, -1.3652e-05, -1.2434e-05, -1.0646e-04,
         -1.9049e-04, -1.4718e-04]], device='cuda:0'))])
end of epoch 2: val_loss 0.5024522352963686, val_acc 0.785
trigger times: 1
end of epoch 3: val_loss 0.47354564170353114, val_acc 0.785
trigger times: 2
end of epoch 4: val_loss 0.4600714802742004, val_acc 0.82
trigger times: 3
end of epoch 5: val_loss 0.437190533708781, val_acc 0.835
trigger times: 0
saving model weights...
Weights: OrderedDict([('layer_norm.weight', tensor([ 1.4276e-01, -4.6975e-05,  2.0210e-01,  7.7057e-03,  6.9139e-04,
         9.4898e-04, -2.5451e-04, -4.4218e-04, -3.1158e-03,  7.6374e-05,
         2.5971e-03,  1.4842e-03], device='cuda:0')), ('layer_norm.bias', tensor([ 0.0018,  0.0051, -0.0048,  0.0014,  0.0009, -0.0009,  0.0009,  0.0050,
         0.0002,  0.0015, -0.0015, -0.0048], device='cuda:0')), ('fcs.0.weight', tensor([[ 1.4230e-01, -3.0459e-03,  2.0181e-01,  1.5607e-03,  2.7573e-03,
          1.6803e-03, -1.1758e-04,  2.0245e-03,  6.9275e-05, -1.1734e-03,
         -1.8830e-03, -1.6796e-03]], device='cuda:0'))])
end of epoch 6: val_loss 0.4672046974860132, val_acc 0.785
trigger times: 1
end of epoch 7: val_loss 0.4524697128683329, val_acc 0.785
trigger times: 2
end of epoch 8: val_loss 0.47047806611284615, val_acc 0.785
trigger times: 3
end of epoch 9: val_loss 0.47397300351411104, val_acc 0.785
trigger times: 4
end of epoch 10: val_loss 0.5309633256494999, val_acc 0.83
trigger times: 5
end of epoch 11: val_loss 0.46397153350524606, val_acc 0.785
trigger times: 6
end of epoch 12: val_loss 0.4629411764070392, val_acc 0.785
trigger times: 7
end of epoch 13: val_loss 0.45131246972829103, val_acc 0.8
trigger times: 8
end of epoch 14: val_loss 0.45274028134299443, val_acc 0.785
trigger times: 9
end of epoch 15: val_loss 0.4633290537819266, val_acc 0.785
trigger times: 10
Early stopping.
0 -0.3033056008571293 -50.00222059884506
1 -0.18842373741790652 -48.823739140882175
2 -0.36185921103606233 -47.15605336419176
3 -0.16720837575849146 -46.19619104961985
4 0.19744051355519332 -45.594227093057754
5 -0.1060479472798761 -45.36842966452394
6 0.03937594490707852 -45.19068756322445
7 0.5223546710330993 -44.16490839583478
8 0.2982573961489834 -44.078303125872196
9 0.396883110050112 -43.993555290419714
10 0.40374415784026496 -43.86306534422809
11 -0.7141812707995996 -43.84874807044028
12 -0.3912314212502679 -43.84199129025074
13 0.29568186003598385 -43.806385671938365
14 0.19167121918872 -43.80581985978556
15 -0.07366198382806033 -42.7594767358323
16 0.18221643986180425 -42.66316468983175
17 -0.8532548907969613 -42.33177224591743
18 0.4524156488914741 -41.774963389485094
19 -0.4611361870774999 -41.410068073767725
20 -0.5660295053276059 -41.17786296442943
21 -0.7924529481679201 -40.723520424948155
22 0.05438631499418989 -40.527189756101116
23 -0.2505899648385821 -40.49595848244517
24 0.07824679906480014 -40.429389880911344
25 -0.28325611024411046 -40.05653450521898
26 -0.016225861203565728 -39.59232357792555
27 -0.04804557247553021 -39.54162101198148
28 0.14276135192631045 -39.195237471709476
29 0.6301436384383123 -39.172389579378766
30 -0.7781551367370412 -38.51095963496708
31 0.01960814607446082 -38.447265769744824
32 -0.08730918960645795 -38.392107037026264
33 -0.20622306004952407 -38.00745349944469
34 0.2533181827166118 -37.46482488602393
35 0.318884078762494 -37.10988160586883
36 -0.8107517049356829 -34.27116723637227
37 -0.31827896600589156 -34.14139118114101
38 0.4710689722560346 -33.263072731706835
39 -0.26523335560341366 -33.13344797200536
40 -0.8953039716143394 -33.07825234291984
41 -0.013081416404020274 -33.0321314765637
42 -0.8356139603420161 -32.44934973065406
43 -1.2616569305537269 -32.4007978120153
44 0.6729574032360688 -32.40063925734975
45 0.19831926041661063 -30.734403792103194
46 -0.03806077491026372 -30.57151371770873
47 0.013402547891018912 -30.131236504472803
48 -0.6211099372994795 -29.99326722619033
49 0.16094735654769465 -29.66908258985071
50 -0.09571849228814244 -29.297233511513635
51 -0.7113322966033593 -29.288890423975797
52 0.890544158173725 -29.145878352769948
53 0.09906992713513318 -28.49601894351319
54 0.57054483005777 -28.492023661124072
55 -0.8348466340685263 -28.315961465855167
56 0.2103357994928956 -27.121110566589827
57 1.0300620854832232 -26.064532595535336
58 -1.0549083920195699 -25.520524278341334
59 -0.668083027798275 -25.27101421179229
60 1.1362060985993594 -25.066644278800943
61 -0.1576975577045232 -24.925849381327673
62 -0.32219230802729726 -24.188105669766596
63 0.39199101678968873 -23.48479966198816
64 1.119596811127849 -23.153943559703283
65 -0.22527386170986574 -22.954730295117237
66 0.24829353264067322 -22.74124885266394
67 0.8047148019977612 -22.739273544503753
68 1.3011091764783487 -22.264945050603636
69 0.3384236857527867 -22.15569724300287
70 -0.8958318738878006 -21.055920928583344
71 1.4501852517714724 -20.543356562348553
72 1.3346115100139286 -20.33499633836848
73 0.7074940740130842 -20.18157658281111
74 0.4602051555993967 -19.58144410477429
75 0.4522839657001896 -19.377225745334304
76 -1.441364262602292 -19.243135617403095
77 0.9527039644308388 -19.060620225371707
78 0.04383884486742318 -18.964124524696246
79 -0.6497787719854387 -18.448962308005108
80 1.2535142340057064 -17.740722019993825
81 0.44176612138107885 -16.85889369985028
82 1.3009589285356924 -16.3381194095591
83 1.6690202724494156 -14.535892564189266
84 1.4193143986631185 -14.443670567499144
85 1.0889787985943258 -14.200413010108107
86 -0.8597009432851337 -13.936976181618805
87 -1.7557756317110034 -13.862253042167257
88 1.010791717650136 -13.483098530680483
89 1.6596653297892772 -13.455892754889845
90 1.2375139353098348 -13.355868275096913
91 1.986254611532786 -12.278515244993585
92 1.380980016583635 -12.227387460046547
93 1.5056356156710535 -12.020717825467683
94 2.427511772773869 -11.910094799877324
95 -0.803113607049454 -11.400284019256157
96 1.8761949404724874 -11.134618158086587
97 -0.8081977927358821 -10.859166921158222
98 0.9164239086094312 -9.595137958067907
99 2.1244107182719745 -9.289921608799773
100 0.19928578386316076 -8.230877068641124
101 1.793158454587683 -7.882363241796725
102 1.8620264648634475 -7.6478984168416355
103 1.7165674542629858 -7.459623237418707
104 1.8759849065972958 -7.124357312750265
105 -2.47535751137184 -7.05379065585803
106 2.077664757380262 -6.853091098326624
107 2.28772524476517 -6.6211384471641495
108 -2.3874138545361347 -6.494555224953677
109 2.3813048866577446 -6.117354180737655
110 -1.1847403406864032 -6.087055095509873
111 0.13352573523297906 -5.43500831968483
112 2.2040105184496497 -5.105291741614599
113 1.91456588357687 -4.628649413275992
114 2.2932350477785803 -4.471031187897325
115 1.0633425517007709 -4.455504779070034
116 -0.04508651931246277 -4.2805498188182405
117 -0.05006781592965126 -3.7944735717969627
118 2.16991820250405 -2.9512438456190186
119 1.7762293913401663 -2.541618164765197
train accuracy: 0.7727777777777778
validation accuracy: 0.785

demos: (360, 50, 13)
demo_rewards: (360,)
sorted_train_rewards: [-51.15845655 -50.95730303 -48.93380144 -48.88659405 -48.81199543
 -47.73632204 -47.63792498 -46.40659067 -45.58205756 -45.55641869
 -45.3781584  -45.1889403  -45.10455724 -45.09649042 -45.06090831
 -44.90717655 -44.40276741 -44.29353296 -44.26132764 -44.205686
 -44.18844659 -44.18649198 -43.97817017 -43.90835803 -43.90233576
 -43.67337234 -43.63795834 -43.57867127 -43.41725811 -43.07917355
 -42.78812793 -42.71498917 -42.37660312 -42.35156982 -42.35130146
 -42.27178642 -42.06976011 -42.02441567 -41.9208162  -41.85021898
 -41.83003384 -41.71300333 -41.60676811 -41.5158946  -41.4288974
 -41.35473131 -41.27204701 -41.26835888 -41.17517134 -41.09778195
 -41.05637645 -41.03450743 -40.98849948 -40.88277159 -40.7682006
 -40.57815481 -40.52517704 -40.51972295 -40.4735601  -40.44146654
 -40.36907268 -40.27883122 -40.20765046 -40.10250365 -39.48434976
 -39.39946011 -39.38715059 -39.27347855 -39.23883604 -39.23607073
 -39.21869941 -39.21327141 -39.20948267 -39.10415592 -39.02946046
 -39.01949066 -39.00315068 -38.70707769 -38.45325451 -38.40734868
 -38.32682998 -38.09085041 -37.84470936 -37.52355928 -37.45493938
 -37.4434499  -37.19212668 -37.12776422 -36.83830962 -36.66844362
 -36.58641248 -36.39170138 -36.34182169 -36.30418157 -36.28270385
 -36.11080763 -36.03273585 -35.76965183 -35.72835706 -35.45254062
 -35.34226992 -34.87824674 -34.50306279 -34.47986837 -34.42748575
 -34.13144584 -34.0228191  -33.83098719 -33.80045069 -33.61206111
 -33.59175617 -33.51725369 -33.49044026 -33.40995872 -33.3865897
 -33.37339807 -33.34018377 -33.27546848 -33.26935494 -32.98496162
 -32.8995352  -32.81082139 -32.78886158 -32.74101924 -32.7313686
 -32.03564157 -31.90682883 -31.83556376 -31.79116041 -31.50474657
 -30.99431461 -30.93631085 -30.90992683 -30.8534927  -30.80257739
 -30.68244687 -29.90514552 -29.87882928 -29.8114822  -29.5623783
 -29.49907263 -29.4431262  -29.16728751 -29.08632404 -28.75963456
 -28.72161062 -28.44248532 -28.26842147 -28.13673546 -28.10052984
 -28.08369991 -28.02777081 -27.84489659 -27.61843292 -27.55718389
 -27.53369262 -27.06419421 -27.0029758  -26.60775678 -26.41157825
 -26.14307162 -26.10745646 -26.09478846 -25.91146004 -25.38206609
 -25.33112486 -25.30351849 -25.18999305 -25.13806635 -25.01985189
 -24.2272877  -24.05919843 -23.72346546 -23.57894664 -23.49865824
 -23.3885205  -23.23362913 -22.8922421  -22.71273286 -22.65599334
 -22.55715908 -22.47349563 -22.28075152 -22.20139031 -22.12605556
 -22.02464667 -21.98062894 -21.90842064 -21.73813504 -21.51677505
 -21.48630258 -21.28560677 -20.49946861 -20.14066085 -20.06168433
 -19.95070877 -19.94914683 -19.89507051 -19.78533441 -19.75469854
 -19.7198622  -19.71871673 -19.69991772 -19.51898865 -19.41242087
 -19.35763998 -18.79967717 -18.79931946 -18.69683752 -18.69338489
 -18.59323781 -18.23945715 -18.14667207 -18.0241144  -17.92817501
 -17.68079099 -17.40776893 -17.26560969 -17.20010184 -17.06094997
 -16.98261229 -16.7595022  -16.72635288 -16.67622467 -16.26549553
 -16.090455   -16.05119129 -15.35726758 -15.29501063 -15.23723291
 -15.00526075 -14.96920977 -14.96583859 -14.77403271 -14.66804697
 -14.39781398 -14.11669085 -13.79702407 -13.72364022 -13.60070757
 -13.54966089 -13.4867368  -13.39595816 -13.26567075 -13.24583955
 -13.16316874 -13.03199125 -12.69460232 -12.57904671 -12.34665554
 -12.30014847 -12.05112503 -12.04804099 -11.49544096 -11.43360312
 -11.39697889 -11.20504127 -10.93302499 -10.65880318 -10.58831597
 -10.51085023 -10.28383593 -10.20667265  -9.80901492  -9.4678784
  -8.96235737  -8.40276084  -8.32638751  -8.0269637   -7.83599816
  -7.70107293  -7.68743448  -7.67259169  -7.57539849  -7.54460172
  -7.3740849   -7.36244313  -7.34546388  -7.1893336   -7.15421432
  -7.10832736  -6.95906356  -6.92008425  -6.77694649  -6.74096238
  -6.72206384  -6.71997062  -6.64795748  -6.51820418  -6.47884361
  -6.29894775  -6.05448903  -5.94629658  -5.89467275  -5.85405865
  -5.64485143  -5.61579673  -5.39544196  -5.38326081  -5.3472021
  -5.25793019  -5.24856721  -5.07848501  -5.06486011  -5.02795798
  -4.90228293  -4.82757292  -4.63049542  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.00401798  -3.97870856
  -3.65032555  -3.3322555   -3.329867    -3.29356852  -2.88591659
  -2.83192847  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-52.04485265 -47.09890202 -47.07471735 -44.66710015 -44.58646443
 -42.73280043 -42.2741431  -40.09758852 -34.56405996 -33.55287238
 -32.46498395 -30.86825618 -30.48486935 -29.48688888 -29.47008372
 -28.13429517 -26.76388365 -23.61704597 -22.64864934 -19.06566814
 -19.02836343 -18.12685193 -17.84544435 -17.74148009 -16.29122883
 -16.04208483 -15.66572915 -14.111545   -13.02215529 -10.25505313
  -8.36555609  -7.5455064   -3.38446715  -3.07904644  -2.67390706
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 7140
num train_labels 7140
num val_obs 630
num val_labels 630
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:2
end of epoch 0: val_loss 0.22965153442177838, val_acc 0.9682539682539683
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.7940996703290292, val_acc 0.9603174603174603
trigger times: 1
end of epoch 2: val_loss 0.1799926344105879, val_acc 0.9825396825396825
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.9169294896117403, val_acc 0.9682539682539683
trigger times: 1
end of epoch 4: val_loss 0.22661093919671726, val_acc 0.9793650793650793
trigger times: 2
end of epoch 5: val_loss 0.11754155472646473, val_acc 0.9888888888888889
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.09133787787973664, val_acc 0.9888888888888889
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.2588706322933758, val_acc 0.9825396825396825
trigger times: 1
end of epoch 8: val_loss 0.08382896989155378, val_acc 0.9904761904761905
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.11161066696811991, val_acc 0.9873015873015873
trigger times: 1
end of epoch 10: val_loss 0.08708942096612583, val_acc 0.9920634920634921
trigger times: 2
end of epoch 11: val_loss 0.2843793976718794, val_acc 0.9825396825396825
trigger times: 3
end of epoch 12: val_loss 0.1532520229444387, val_acc 0.9873015873015873
trigger times: 4
end of epoch 13: val_loss 0.26728594876857426, val_acc 0.9841269841269841
trigger times: 5
end of epoch 14: val_loss 0.23102951864418736, val_acc 0.9857142857142858
trigger times: 6
end of epoch 15: val_loss 0.28036117165191077, val_acc 0.9841269841269841
trigger times: 7
end of epoch 16: val_loss 0.2598487919840547, val_acc 0.9825396825396825
trigger times: 8
end of epoch 17: val_loss 0.28241558377631787, val_acc 0.9857142857142858
trigger times: 9
end of epoch 18: val_loss 0.24385123201001252, val_acc 0.9793650793650793
trigger times: 10
Early stopping.
0 -1113.2869491577148 -52.04485264929628
1 -1048.5516862869263 -47.09890202042859
2 -997.492753982544 -47.074717348941945
3 -966.9206857681274 -44.66710014641005
4 -977.4404006004333 -44.586464426850064
5 -947.2846913337708 -42.732800431694656
6 -915.8459467887878 -42.2741431026605
7 -908.8671865463257 -40.09758852204594
8 -830.4641804695129 -34.564059960585915
9 -761.9071559906006 -33.552872381361496
10 -759.0023417472839 -32.46498394828641
11 -725.5014138221741 -30.868256178181344
12 -761.4672288894653 -30.484869352703917
13 -743.3045125007629 -29.486888875399583
14 -731.5809683799744 -29.47008371885125
15 -736.8910503387451 -28.134295168749553
16 -657.2523462772369 -26.763883647694865
17 -619.2487847805023 -23.617045967193892
18 -622.7819278240204 -22.64864934371813
19 -527.4200936555862 -19.06566813675252
20 -565.1084008216858 -19.02836343405756
21 -534.0993757247925 -18.126851928592817
22 -509.59938859939575 -17.845444345119244
23 -486.35141706466675 -17.741480087754162
24 -470.34752321243286 -16.291228826654716
25 -458.6023813486099 -16.04208483259092
26 -458.7597396373749 -15.665729145237293
27 -449.48575007915497 -14.111544997910872
28 -410.7057526707649 -13.022155288614591
29 -361.01106095314026 -10.255053134746943
30 -316.20583319664 -8.36555609326107
31 -250.8264101743698 -7.545506402407465
32 -188.38309207558632 -3.3844671463622564
33 -200.66803067922592 -3.079046443285274
34 -190.25344395637512 -2.673907055233777
35 -187.6054703593254 -2.49009826539426
train accuracy: 0.9771708683473389
validation accuracy: 0.9793650793650793

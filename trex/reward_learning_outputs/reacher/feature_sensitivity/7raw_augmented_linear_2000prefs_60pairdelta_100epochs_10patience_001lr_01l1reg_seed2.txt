demos: (120, 50, 8)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=8, out_features=1, bias=False)
)
Total number of parameters: 8
Number of trainable paramters: 8
device: cuda:0
end of epoch 0: val_loss 0.01050262538802741, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.4982e-02,  5.4328e-03,  2.5331e-02, -5.4841e-03,  8.9433e-05,
         -1.1343e-03, -2.4451e-04, -1.3518e+00]], device='cuda:0'))])
end of epoch 1: val_loss 0.04738031201995074, val_acc 0.975
trigger times: 1
end of epoch 2: val_loss 0.01644656624739728, val_acc 0.995
trigger times: 2
end of epoch 3: val_loss 0.004024474595025964, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-3.0200e-04, -7.1288e-02,  4.9148e-02,  1.1915e-04, -1.0355e-01,
         -1.2920e-04, -1.4886e-03, -2.2680e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.022709639494818957, val_acc 0.99
trigger times: 1
end of epoch 5: val_loss 0.06891299840551912, val_acc 0.98
trigger times: 2
end of epoch 6: val_loss 0.010705665522214716, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 0.0008385359517594892, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 6.2134e-06,  8.1323e-05,  5.5961e-02,  3.5416e-04, -3.1270e-01,
         -2.0829e-06,  3.3961e-05, -2.7187e+00]], device='cuda:0'))])
end of epoch 8: val_loss 0.01116360343592344, val_acc 0.99
trigger times: 1
end of epoch 9: val_loss 0.0038957753829721754, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 0.03944013408632678, val_acc 0.98
trigger times: 3
end of epoch 11: val_loss 0.07063173409052861, val_acc 0.98
trigger times: 4
end of epoch 12: val_loss 0.004750755086942497, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 0.0172544561416802, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 2.125346343747055, val_acc 0.79
trigger times: 7
end of epoch 15: val_loss 0.27059362698993716, val_acc 0.93
trigger times: 8
end of epoch 16: val_loss 0.009901811671887124, val_acc 0.995
trigger times: 9
end of epoch 17: val_loss 0.004001273331136623, val_acc 1.0
trigger times: 10
Early stopping.
0 -28.848301615566015 -50.00222059884506
1 -23.075141072273254 -48.823739140882175
2 -22.29539460130036 -47.15605336419176
3 -21.567790806293488 -46.19619104961985
4 -29.417379766702652 -45.594227093057754
5 -25.567265436053276 -45.36842966452394
6 -31.152703434228897 -45.19068756322445
7 -21.866922095417976 -44.16490839583478
8 -18.004427820444107 -44.078303125872196
9 -25.68084616959095 -43.993555290419714
10 -15.91373435407877 -43.86306534422809
11 -26.125897154211998 -43.84874807044028
12 -22.556804440915585 -43.84199129025074
13 -14.455941841006279 -43.806385671938365
14 -14.538901720196009 -43.80581985978556
15 -17.600631281733513 -42.7594767358323
16 -14.872330889105797 -42.66316468983175
17 -22.854735791683197 -42.33177224591743
18 -21.117882534861565 -41.774963389485094
19 -21.97355143725872 -41.410068073767725
20 -21.003186866641045 -41.17786296442943
21 -18.16465798765421 -40.723520424948155
22 -24.110014721751213 -40.527189756101116
23 -22.774730756878853 -40.49595848244517
24 -24.57722395658493 -40.429389880911344
25 -22.57092845439911 -40.05653450521898
26 -20.001723494380713 -39.59232357792555
27 -29.865281254053116 -39.54162101198148
28 -23.143623791635036 -39.195237471709476
29 -22.45808009803295 -39.172389579378766
30 -20.528868198394775 -38.51095963496708
31 -18.60343237966299 -38.447265769744824
32 -24.86164117604494 -38.392107037026264
33 -14.9777193069458 -38.00745349944469
34 -13.947254501283169 -37.46482488602393
35 -22.557049557566643 -37.10988160586883
36 -20.785538017749786 -34.27116723637227
37 -13.262512546032667 -34.14139118114101
38 -13.745874604210258 -33.263072731706835
39 -17.06533864326775 -33.13344797200536
40 -15.880963459610939 -33.07825234291984
41 -18.743276357650757 -33.0321314765637
42 -27.40485018864274 -32.44934973065406
43 -14.282569333910942 -32.4007978120153
44 -18.121563836932182 -32.40063925734975
45 -7.649087332189083 -30.734403792103194
46 -11.053940128535032 -30.57151371770873
47 -16.08211500197649 -30.131236504472803
48 -15.756360560655594 -29.99326722619033
49 -14.006197549402714 -29.66908258985071
50 -21.971629679203033 -29.297233511513635
51 -13.053840637207031 -29.288890423975797
52 -10.19074010848999 -29.145878352769948
53 -12.968133058398962 -28.49601894351319
54 -14.400086104869843 -28.492023661124072
55 -19.251118890941143 -28.315961465855167
56 -10.94635022431612 -27.121110566589827
57 -7.287965416908264 -26.064532595535336
58 -13.459975704550743 -25.520524278341334
59 -13.739073101431131 -25.27101421179229
60 -11.80146374553442 -25.066644278800943
61 -15.098240688443184 -24.925849381327673
62 -14.318529635667801 -24.188105669766596
63 -12.506064802408218 -23.48479966198816
64 -5.338557694107294 -23.153943559703283
65 -11.905667636543512 -22.954730295117237
66 -9.943508449941874 -22.74124885266394
67 -9.721728743985295 -22.739273544503753
68 -5.605422616004944 -22.264945050603636
69 -13.459199564531446 -22.15569724300287
70 -9.126633055508137 -21.055920928583344
71 -6.907077867537737 -20.543356562348553
72 -9.258020367473364 -20.33499633836848
73 -8.28889598697424 -20.18157658281111
74 -10.572926498949528 -19.58144410477429
75 -9.710309859365225 -19.377225745334304
76 -9.438991725444794 -19.243135617403095
77 -6.787119783461094 -19.060620225371707
78 -11.663621254265308 -18.964124524696246
79 -12.20233703404665 -18.448962308005108
80 -11.85098922252655 -17.740722019993825
81 -4.920878889039159 -16.85889369985028
82 -7.475135643035173 -16.3381194095591
83 -6.717562526464462 -14.535892564189266
84 -6.618930712342262 -14.443670567499144
85 -6.4845072235912085 -14.200413010108107
86 -10.973130784928799 -13.936976181618805
87 -8.727369084954262 -13.862253042167257
88 -7.957126654684544 -13.483098530680483
89 -7.561880676075816 -13.455892754889845
90 -6.063191005960107 -13.355868275096913
91 -7.136747134849429 -12.278515244993585
92 -3.010262394323945 -12.227387460046547
93 -4.114397667348385 -12.020717825467683
94 -8.837862603366375 -11.910094799877324
95 -8.015473540872335 -11.400284019256157
96 -5.779474455863237 -11.134618158086587
97 -6.236926216632128 -10.859166921158222
98 -4.183816580101848 -9.595137958067907
99 -6.940591653808951 -9.289921608799773
100 -9.822438372299075 -8.230877068641124
101 -8.155004851520061 -7.882363241796725
102 -8.90475245192647 -7.6478984168416355
103 -7.732673265039921 -7.459623237418707
104 -8.237750217318535 -7.124357312750265
105 -11.639743454754353 -7.05379065585803
106 -6.236167408525944 -6.853091098326624
107 -6.782380133867264 -6.6211384471641495
108 -12.138380110263824 -6.494555224953677
109 -5.155211631208658 -6.117354180737655
110 -10.204252809286118 -6.087055095509873
111 -6.8979857712984085 -5.43500831968483
112 -5.143320295959711 -5.105291741614599
113 -4.9090394377708435 -4.628649413275992
114 -3.6456557288765907 -4.471031187897325
115 -3.9122654385864735 -4.455504779070034
116 -6.357712458819151 -4.2805498188182405
117 -5.970138195902109 -3.7944735717969627
118 -1.3838860839605331 -2.9512438456190186
119 -1.3911128249019384 -2.541618164765197
train accuracy: 0.9988888888888889
validation accuracy: 1.0

demos: (360, 50, 3)
demo_rewards: (360,)
sorted_train_rewards: [-51.15845655 -50.95730303 -48.93380144 -48.88659405 -48.81199543
 -47.73632204 -47.63792498 -46.40659067 -45.58205756 -45.55641869
 -45.3781584  -45.1889403  -45.10455724 -45.09649042 -45.06090831
 -44.90717655 -44.40276741 -44.29353296 -44.26132764 -44.205686
 -44.18844659 -44.18649198 -43.97817017 -43.90835803 -43.90233576
 -43.67337234 -43.63795834 -43.57867127 -43.41725811 -43.07917355
 -42.78812793 -42.71498917 -42.37660312 -42.35156982 -42.35130146
 -42.27178642 -42.06976011 -42.02441567 -41.9208162  -41.85021898
 -41.83003384 -41.71300333 -41.60676811 -41.5158946  -41.4288974
 -41.35473131 -41.27204701 -41.26835888 -41.17517134 -41.09778195
 -41.05637645 -41.03450743 -40.98849948 -40.88277159 -40.7682006
 -40.57815481 -40.52517704 -40.51972295 -40.4735601  -40.44146654
 -40.36907268 -40.27883122 -40.20765046 -40.10250365 -39.48434976
 -39.39946011 -39.38715059 -39.27347855 -39.23883604 -39.23607073
 -39.21869941 -39.21327141 -39.20948267 -39.10415592 -39.02946046
 -39.01949066 -39.00315068 -38.70707769 -38.45325451 -38.40734868
 -38.32682998 -38.09085041 -37.84470936 -37.52355928 -37.45493938
 -37.4434499  -37.19212668 -37.12776422 -36.83830962 -36.66844362
 -36.58641248 -36.39170138 -36.34182169 -36.30418157 -36.28270385
 -36.11080763 -36.03273585 -35.76965183 -35.72835706 -35.45254062
 -35.34226992 -34.87824674 -34.50306279 -34.47986837 -34.42748575
 -34.13144584 -34.0228191  -33.83098719 -33.80045069 -33.61206111
 -33.59175617 -33.51725369 -33.49044026 -33.40995872 -33.3865897
 -33.37339807 -33.34018377 -33.27546848 -33.26935494 -32.98496162
 -32.8995352  -32.81082139 -32.78886158 -32.74101924 -32.7313686
 -32.03564157 -31.90682883 -31.83556376 -31.79116041 -31.50474657
 -30.99431461 -30.93631085 -30.90992683 -30.8534927  -30.80257739
 -30.68244687 -29.90514552 -29.87882928 -29.8114822  -29.5623783
 -29.49907263 -29.4431262  -29.16728751 -29.08632404 -28.75963456
 -28.72161062 -28.44248532 -28.26842147 -28.13673546 -28.10052984
 -28.08369991 -28.02777081 -27.84489659 -27.61843292 -27.55718389
 -27.53369262 -27.06419421 -27.0029758  -26.60775678 -26.41157825
 -26.14307162 -26.10745646 -26.09478846 -25.91146004 -25.38206609
 -25.33112486 -25.30351849 -25.18999305 -25.13806635 -25.01985189
 -24.2272877  -24.05919843 -23.72346546 -23.57894664 -23.49865824
 -23.3885205  -23.23362913 -22.8922421  -22.71273286 -22.65599334
 -22.55715908 -22.47349563 -22.28075152 -22.20139031 -22.12605556
 -22.02464667 -21.98062894 -21.90842064 -21.73813504 -21.51677505
 -21.48630258 -21.28560677 -20.49946861 -20.14066085 -20.06168433
 -19.95070877 -19.94914683 -19.89507051 -19.78533441 -19.75469854
 -19.7198622  -19.71871673 -19.69991772 -19.51898865 -19.41242087
 -19.35763998 -18.79967717 -18.79931946 -18.69683752 -18.69338489
 -18.59323781 -18.23945715 -18.14667207 -18.0241144  -17.92817501
 -17.68079099 -17.40776893 -17.26560969 -17.20010184 -17.06094997
 -16.98261229 -16.7595022  -16.72635288 -16.67622467 -16.26549553
 -16.090455   -16.05119129 -15.35726758 -15.29501063 -15.23723291
 -15.00526075 -14.96920977 -14.96583859 -14.77403271 -14.66804697
 -14.39781398 -14.11669085 -13.79702407 -13.72364022 -13.60070757
 -13.54966089 -13.4867368  -13.39595816 -13.26567075 -13.24583955
 -13.16316874 -13.03199125 -12.69460232 -12.57904671 -12.34665554
 -12.30014847 -12.05112503 -12.04804099 -11.49544096 -11.43360312
 -11.39697889 -11.20504127 -10.93302499 -10.65880318 -10.58831597
 -10.51085023 -10.28383593 -10.20667265  -9.80901492  -9.4678784
  -8.96235737  -8.40276084  -8.32638751  -8.0269637   -7.83599816
  -7.70107293  -7.68743448  -7.67259169  -7.57539849  -7.54460172
  -7.3740849   -7.36244313  -7.34546388  -7.1893336   -7.15421432
  -7.10832736  -6.95906356  -6.92008425  -6.77694649  -6.74096238
  -6.72206384  -6.71997062  -6.64795748  -6.51820418  -6.47884361
  -6.29894775  -6.05448903  -5.94629658  -5.89467275  -5.85405865
  -5.64485143  -5.61579673  -5.39544196  -5.38326081  -5.3472021
  -5.25793019  -5.24856721  -5.07848501  -5.06486011  -5.02795798
  -4.90228293  -4.82757292  -4.63049542  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.00401798  -3.97870856
  -3.65032555  -3.3322555   -3.329867    -3.29356852  -2.88591659
  -2.83192847  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-52.04485265 -47.09890202 -47.07471735 -44.66710015 -44.58646443
 -42.73280043 -42.2741431  -40.09758852 -34.56405996 -33.55287238
 -32.46498395 -30.86825618 -30.48486935 -29.48688888 -29.47008372
 -28.13429517 -26.76388365 -23.61704597 -22.64864934 -19.06566814
 -19.02836343 -18.12685193 -17.84544435 -17.74148009 -16.29122883
 -16.04208483 -15.66572915 -14.111545   -13.02215529 -10.25505313
  -8.36555609  -7.5455064   -3.38446715  -3.07904644  -2.67390706
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 7140
num train_labels 7140
num val_obs 630
num val_labels 630
ModuleList(
  (0): Linear(in_features=3, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 3
Number of trainable paramters: 3
device: cuda:0
end of epoch 0: val_loss 0.07736699051190024, val_acc 0.9714285714285714
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.027178465568473885, val_acc 0.9904761904761905
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.025640819540130148, val_acc 0.9920634920634921
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.03735780203301054, val_acc 0.9841269841269841
trigger times: 1
end of epoch 4: val_loss 0.038669570944998864, val_acc 0.9857142857142858
trigger times: 2
end of epoch 5: val_loss 0.02581901861873808, val_acc 0.9888888888888889
trigger times: 3
end of epoch 6: val_loss 0.027018967883443965, val_acc 0.9904761904761905
trigger times: 4
end of epoch 7: val_loss 0.026031407880509515, val_acc 0.9936507936507937
trigger times: 5
end of epoch 8: val_loss 0.027287884416416053, val_acc 0.9904761904761905
trigger times: 6
end of epoch 9: val_loss 0.045492185729472674, val_acc 0.9841269841269841
trigger times: 7
end of epoch 10: val_loss 0.026018407241920817, val_acc 0.9888888888888889
trigger times: 8
end of epoch 11: val_loss 0.02891952944545478, val_acc 0.9904761904761905
trigger times: 9
end of epoch 12: val_loss 0.03410963172211164, val_acc 0.9857142857142858
trigger times: 10
Early stopping.
0 -139.9108298420906 -52.04485264929628
1 -131.95362615585327 -47.09890202042859
2 -128.4417474269867 -47.074717348941945
3 -127.27130538225174 -44.66710014641005
4 -123.76635086536407 -44.586464426850064
5 -123.02746623754501 -42.732800431694656
6 -119.1042437851429 -42.2741431026605
7 -116.01682260632515 -40.09758852204594
8 -105.31458455324173 -34.564059960585915
9 -96.44890132546425 -33.552872381361496
10 -98.11956632137299 -32.46498394828641
11 -95.13612484931946 -30.868256178181344
12 -95.68681219220161 -30.484869352703917
13 -93.05295813083649 -29.486888875399583
14 -95.13778096437454 -29.47008371885125
15 -92.90148252248764 -28.134295168749553
16 -85.76964086294174 -26.763883647694865
17 -79.28042006492615 -23.617045967193892
18 -76.62505781650543 -22.64864934371813
19 -66.78152345120907 -19.06566813675252
20 -72.0555949807167 -19.02836343405756
21 -66.40176233649254 -18.126851928592817
22 -63.63208381831646 -17.845444345119244
23 -61.69026246666908 -17.741480087754162
24 -59.78678733110428 -16.291228826654716
25 -56.15506283938885 -16.04208483259092
26 -57.29499599337578 -15.665729145237293
27 -57.31713369488716 -14.111544997910872
28 -49.279017362743616 -13.022155288614591
29 -41.709383457899094 -10.255053134746943
30 -36.633045587688684 -8.36555609326107
31 -34.42773222923279 -7.545506402407465
32 -23.0380425080657 -3.3844671463622564
33 -23.069633075618185 -3.079046443285274
34 -21.878594748675823 -2.673907055233777
35 -21.397064220160246 -2.49009826539426
train accuracy: 0.9851540616246499
validation accuracy: 0.9857142857142858

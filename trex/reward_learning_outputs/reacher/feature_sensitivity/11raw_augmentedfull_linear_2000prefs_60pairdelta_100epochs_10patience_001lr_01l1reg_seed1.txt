demos: (120, 50, 13)
demo_rewards: (120,)
[-54.98547503 -50.4922686  -50.03933802 -49.75347185 -49.72654641
 -46.98011874 -45.73515428 -45.67057988 -44.99030608 -44.14602409
 -43.81326882 -43.18878399 -42.29180715 -42.00401746 -41.69100444
 -41.68588229 -41.2817771  -40.44278203 -40.34838366 -39.59970115
 -39.57586365 -39.31972693 -39.02461056 -38.45534494 -38.4127039
 -38.35634328 -37.79713617 -37.74152899 -37.66475324 -37.51313938
 -37.1809993  -37.10070314 -37.00630589 -36.82191677 -36.48799015
 -36.2096527  -36.19207562 -36.11445903 -35.78149902 -35.39450387
 -35.262825   -35.24303541 -35.20970524 -35.06544085 -34.80241748
 -34.64469045 -33.84284986 -32.70706485 -31.9690994  -31.7109134
 -31.64414356 -31.39238276 -31.22319602 -31.12953085 -29.3915714
 -29.34012561 -29.10618999 -27.4110235  -27.34372236 -27.19668163
 -27.07399029 -26.70472176 -26.2447949  -25.54836509 -25.45878529
 -24.879107   -24.82869536 -24.59274514 -23.97874558 -23.57262108
 -23.44970808 -22.74530916 -22.60679894 -22.19891032 -20.65686376
 -20.44447256 -20.1969901  -20.13839115 -19.63760344 -19.51559872
 -18.9283881  -17.99477406 -17.5574237  -16.82307393 -14.8550828
 -14.5314246  -14.44242009 -13.59601285 -12.68135973 -12.66418206
 -12.30017947 -12.15190477 -11.78885214 -10.8699891  -10.3276815
  -9.85721598  -8.330117    -8.13319584  -8.10819769  -7.57539849
  -7.36244313  -7.10832736  -6.95906356  -6.77694649  -6.72206384
  -6.71997062  -6.53544734  -6.51820418  -5.61579673  -5.3472021
  -5.07848501  -5.02795798  -4.82757292  -4.63049542  -4.230832
  -4.03104862  -3.38446715  -3.3322555   -2.64166233  -1.91361965]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=13, out_features=1, bias=False)
)
Total number of parameters: 13
Number of trainable paramters: 13
device: cuda:0
end of epoch 0: val_loss 0.44462163331394594, val_acc 0.965
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0771, -0.0496,  0.2209, -0.3463,  0.3971, -0.3634,  0.0542,  0.0631,
         -0.4816,  0.2442, -0.0014, -0.8518, -1.2506]], device='cuda:0'))])
end of epoch 1: val_loss 7.475779893997725e-05, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 2.4248e-05, -9.5402e-05, -9.9170e-05,  3.2641e-05,  2.8766e-04,
          4.1588e-04, -1.6212e-04,  1.1454e-05,  7.0173e-05, -3.5848e-05,
         -1.4324e-03, -1.0105e-04, -4.7305e-01]], device='cuda:0'))])
end of epoch 2: val_loss 1.1205655631840727e-07, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.7651e-05,  8.4376e-06, -9.1673e-05, -1.2960e-04, -6.0545e-04,
         -1.4227e-04,  1.1686e-05, -3.8254e-05,  2.8198e-04, -1.3546e-04,
         -1.4325e-03,  1.4930e-04, -8.8416e-01]], device='cuda:0'))])
end of epoch 3: val_loss 2.980231883498163e-09, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 3.7906e-05,  6.9047e-05, -1.0782e-04,  8.5427e-05,  4.0859e-04,
         -2.0023e-04,  5.0882e-03, -2.2948e-05,  5.1578e-04,  1.4137e-04,
         -1.4327e-03, -9.4392e-05, -1.1501e+00]], device='cuda:0'))])
end of epoch 4: val_loss 0.00017963530488131595, val_acc 1.0
trigger times: 1
end of epoch 5: val_loss 4.2528049848407076e-05, val_acc 1.0
trigger times: 2
end of epoch 6: val_loss 6.046029674138254e-06, val_acc 1.0
trigger times: 3
end of epoch 7: val_loss 5.960464122267694e-10, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-5.9755e-05,  9.9824e-07,  7.7446e-05, -1.1678e-02,  2.5748e-04,
          9.6353e-04, -3.8881e-06,  1.1520e-02, -3.1731e-04,  1.9178e-04,
         -1.4333e-03, -1.2907e-03, -1.1858e+00]], device='cuda:0'))])
end of epoch 8: val_loss 9.15550281661126e-06, val_acc 1.0
trigger times: 1
end of epoch 9: val_loss 3.0746923485054364e-05, val_acc 1.0
trigger times: 2
end of epoch 10: val_loss 1.0132787515715335e-08, val_acc 1.0
trigger times: 3
end of epoch 11: val_loss 2.2053690997836384e-08, val_acc 1.0
trigger times: 4
end of epoch 12: val_loss 7.2992185596376655e-06, val_acc 1.0
trigger times: 5
end of epoch 13: val_loss 5.699155838527759e-06, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.8625e-05,  1.0189e-05,  1.2959e-01, -9.8144e-03,  6.2948e-04,
          5.3083e-05,  6.2543e-03, -4.2107e-03, -7.9597e-05,  5.5945e-04,
         -1.4347e-03,  2.9287e-04, -1.9037e+00]], device='cuda:0'))])
end of epoch 15: val_loss 1.352704674316533, val_acc 0.91
trigger times: 1
end of epoch 16: val_loss 0.0002191561341965098, val_acc 1.0
trigger times: 2
end of epoch 17: val_loss 7.569728309064771e-07, val_acc 1.0
trigger times: 3
end of epoch 18: val_loss 4.2438200519967495e-07, val_acc 1.0
trigger times: 4
end of epoch 19: val_loss 0.6556406947427827, val_acc 0.94
trigger times: 5
end of epoch 20: val_loss 0.0001136900354593351, val_acc 1.0
trigger times: 6
end of epoch 21: val_loss 1.9669505064712213e-08, val_acc 1.0
trigger times: 7
end of epoch 22: val_loss 4.184072892243762e-07, val_acc 1.0
trigger times: 8
end of epoch 23: val_loss 0.0038748343518818728, val_acc 1.0
trigger times: 9
end of epoch 24: val_loss 0.0898099064093628, val_acc 0.99
trigger times: 10
Early stopping.
0 -81.81373007595539 -54.98547503240923
1 -111.4743641614914 -50.492268601198035
2 -104.22941175103188 -50.03933801517046
3 -79.34809070825577 -49.75347184620696
4 -91.2552419602871 -49.72654640753777
5 -66.90404132008553 -46.98011874490918
6 -75.06159719824791 -45.7351542845057
7 -67.70293028652668 -45.670579884154705
8 -60.336304157972336 -44.99030608142343
9 -80.29488104581833 -44.14602409201361
10 -65.6200205385685 -43.81326882122305
11 -72.27986198663712 -43.18878399086166
12 -109.93109661340714 -42.29180714825394
13 -52.89966011047363 -42.00401746161006
14 -105.56449449062347 -41.6910044370425
15 -80.46983048319817 -41.68588229294918
16 -67.67384044826031 -41.281777102712205
17 -68.60982048511505 -40.44278203413966
18 -57.72618518769741 -40.34838365523108
19 -56.46180859208107 -39.599701153458774
20 -66.28395161032677 -39.57586365327889
21 -104.63016557693481 -39.31972693233231
22 -66.41907787322998 -39.024610555047154
23 -83.07748812437057 -38.45534493538269
24 -77.13273352384567 -38.41270390343083
25 -58.42158156633377 -38.35634328077039
26 -92.54957629740238 -37.79713616772368
27 -60.90548665821552 -37.741528994987384
28 -78.97917565703392 -37.66475323879293
29 -33.21535128355026 -37.513139380385574
30 -72.47889637947083 -37.1809993033689
31 -67.54231315851212 -37.100703136010694
32 -85.2199766933918 -37.00630588930485
33 -46.98160508275032 -36.821916772458344
34 -63.64355719089508 -36.48799015296732
35 -80.49574077129364 -36.20965269874363
36 -76.58464735746384 -36.19207561676116
37 -74.00654408335686 -36.114459029559086
38 -84.08507889509201 -35.78149902167743
39 -31.11069344729185 -35.394503873250635
40 -72.29964339733124 -35.26282499693737
41 -36.54117551445961 -35.24303541418371
42 -72.2753877043724 -35.209705244501436
43 -58.85601383447647 -35.0654408505187
44 -63.812134116888046 -34.80241747531743
45 -45.182401210069656 -34.64469044638467
46 -87.32836112380028 -33.84284985953318
47 -54.125366404652596 -32.70706485357069
48 -43.18609058856964 -31.969099402548657
49 -56.44929313659668 -31.7109134007892
50 -43.32174450159073 -31.64414355845032
51 -58.28324484825134 -31.392382758954444
52 -55.97042255103588 -31.223196019713853
53 -50.047403648495674 -31.12953085092458
54 -72.11816155910492 -29.39157139549552
55 -45.067880526185036 -29.340125609942326
56 -41.83596557378769 -29.106189988903285
57 -64.88360926508904 -27.41102349748205
58 -59.12001124024391 -27.343722362182305
59 -54.15645559132099 -27.196681629483837
60 -77.07586645334959 -27.07399028854534
61 -48.67473229765892 -26.7047217556024
62 -60.71542254090309 -26.244794902859052
63 -55.68538607656956 -25.548365085275513
64 -26.813243746757507 -25.45878528601009
65 -33.999066948890686 -24.879106999799365
66 -57.25895815342665 -24.828695359328833
67 -76.81781789660454 -24.592745144504722
68 -68.58228969573975 -23.978745577896312
69 -36.79482625424862 -23.57262108435893
70 -31.343601141124964 -23.44970807952351
71 -30.86340433359146 -22.745309160183492
72 -51.86804100871086 -22.60679894414887
73 -40.0474928021431 -22.19891031871716
74 -29.949890300631523 -20.656863763892378
75 -23.56237254291773 -20.444472560731253
76 -21.14281652867794 -20.19699010077007
77 -45.046292655169964 -20.13839114930498
78 -25.677881203591824 -19.63760343800059
79 -72.99793538451195 -19.515598718228343
80 -35.52892879769206 -18.92838809611677
81 -28.08616143465042 -17.994774057192853
82 -55.639944300055504 -17.55742370467821
83 -24.391954571008682 -16.823073927842348
84 -48.13639598712325 -14.855082803515382
85 -42.82721130549908 -14.531424598833084
86 -10.825067952275276 -14.442420089224363
87 -39.41194702684879 -13.596012850960644
88 -14.616683259606361 -12.68135972540495
89 -30.5390934497118 -12.66418205637357
90 -26.96659564971924 -12.30017947419658
91 -17.801886945962906 -12.151904772081672
92 -56.30496982857585 -11.788852141676486
93 -13.103460356593132 -10.869989101210326
94 -25.558373518288136 -10.327681503524177
95 -18.666298925876617 -9.8572159761571
96 -22.842003010213375 -8.330116995310416
97 -19.325810492038727 -8.133195842510668
98 -17.337495487183332 -8.108197691178031
99 -7.2934414222836494 -7.57539849177145
100 -4.76387831568718 -7.362443126623615
101 -7.819143407046795 -7.108327355338034
102 -13.549857642501593 -6.959063561385431
103 -3.8520531952381134 -6.776946485018116
104 -5.153471736237407 -6.7220638398623045
105 -7.477453708648682 -6.719970621583102
106 -23.458914428949356 -6.535447341844848
107 -38.550547152757645 -6.51820418055673
108 -37.09557921439409 -5.615796733870542
109 -28.214066073298454 -5.34720210027791
110 -20.196144431829453 -5.078485007852753
111 -12.940890774130821 -5.027957977402961
112 -14.536841213703156 -4.827572916892203
113 -20.144301258027554 -4.63049541560991
114 -10.50436732172966 -4.230832004686763
115 -19.793303433805704 -4.031048624093466
116 -21.846183590590954 -3.3844671463622564
117 -13.967696122825146 -3.3322555012187633
118 -13.708987779915333 -2.6416623314910934
119 -5.610496297478676 -1.9136196540088464
train accuracy: 0.9861111111111112
validation accuracy: 0.99

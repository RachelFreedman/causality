demos: (120, 50, 4)
demo_rewards: (120,)
[-50.0022206  -48.82373914 -47.15605336 -46.19619105 -45.59422709
 -45.36842966 -45.19068756 -44.1649084  -44.07830313 -43.99355529
 -43.86306534 -43.84874807 -43.84199129 -43.80638567 -43.80581986
 -42.75947674 -42.66316469 -42.33177225 -41.77496339 -41.41006807
 -41.17786296 -40.72352042 -40.52718976 -40.49595848 -40.42938988
 -40.05653451 -39.59232358 -39.54162101 -39.19523747 -39.17238958
 -38.51095963 -38.44726577 -38.39210704 -38.0074535  -37.46482489
 -37.10988161 -34.27116724 -34.14139118 -33.26307273 -33.13344797
 -33.07825234 -33.03213148 -32.44934973 -32.40079781 -32.40063926
 -30.73440379 -30.57151372 -30.1312365  -29.99326723 -29.66908259
 -29.29723351 -29.28889042 -29.14587835 -28.49601894 -28.49202366
 -28.31596147 -27.12111057 -26.0645326  -25.52052428 -25.27101421
 -25.06664428 -24.92584938 -24.18810567 -23.48479966 -23.15394356
 -22.9547303  -22.74124885 -22.73927354 -22.26494505 -22.15569724
 -21.05592093 -20.54335656 -20.33499634 -20.18157658 -19.5814441
 -19.37722575 -19.24313562 -19.06062023 -18.96412452 -18.44896231
 -17.74072202 -16.8588937  -16.33811941 -14.53589256 -14.44367057
 -14.20041301 -13.93697618 -13.86225304 -13.48309853 -13.45589275
 -13.35586828 -12.27851524 -12.22738746 -12.02071783 -11.9100948
 -11.40028402 -11.13461816 -10.85916692  -9.59513796  -9.28992161
  -8.23087707  -7.88236324  -7.64789842  -7.45962324  -7.12435731
  -7.05379066  -6.8530911   -6.62113845  -6.49455522  -6.11735418
  -6.0870551   -5.43500832  -5.10529174  -4.62864941  -4.47103119
  -4.45550478  -4.28054982  -3.79447357  -2.95124385  -2.54161816]
maximum traj length 50
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=4, out_features=1, bias=False)
)
Total number of parameters: 4
Number of trainable paramters: 4
device: cuda:0
end of epoch 0: val_loss 0.040460158476971625, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0069, -0.0073, -0.0093, -0.8517]], device='cuda:0'))])
end of epoch 1: val_loss 0.045843550560921355, val_acc 0.99
trigger times: 1
end of epoch 2: val_loss 0.04610888948904176, val_acc 0.99
trigger times: 2
end of epoch 3: val_loss 0.03906231278337145, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 1.1261e-04, -2.1426e-02,  2.2315e-02, -7.7276e-01]], device='cuda:0'))])
end of epoch 4: val_loss 0.12056293258914592, val_acc 0.955
trigger times: 1
end of epoch 5: val_loss 0.2189917848263717, val_acc 0.885
trigger times: 2
end of epoch 6: val_loss 0.028764584944397598, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0050,  0.0023,  0.0279, -1.0355]], device='cuda:0'))])
end of epoch 7: val_loss 0.026108350285504116, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0081,  0.0123,  0.0025, -1.0452]], device='cuda:0'))])
end of epoch 8: val_loss 0.09801566545429978, val_acc 0.96
trigger times: 1
end of epoch 9: val_loss 0.09072879674825345, val_acc 0.975
trigger times: 2
end of epoch 10: val_loss 0.045630135042138135, val_acc 0.99
trigger times: 3
end of epoch 11: val_loss 0.036383683420569884, val_acc 0.995
trigger times: 4
end of epoch 12: val_loss 0.04645973302471305, val_acc 0.985
trigger times: 5
end of epoch 13: val_loss 0.032435828578154545, val_acc 1.0
trigger times: 6
end of epoch 14: val_loss 0.05967497129031472, val_acc 0.99
trigger times: 7
end of epoch 15: val_loss 0.04790272843232288, val_acc 1.0
trigger times: 8
end of epoch 16: val_loss 0.039668720134632164, val_acc 1.0
trigger times: 9
end of epoch 17: val_loss 0.08332507647845716, val_acc 0.975
trigger times: 10
Early stopping.
0 -11.437242032960057 -50.00222059884506
1 -9.82083174586296 -48.823739140882175
2 -10.028229770512553 -47.15605336419176
3 -8.956360101699829 -46.19619104961985
4 -12.059009067714214 -45.594227093057754
5 -10.887024223804474 -45.36842966452394
6 -13.35522674024105 -45.19068756322445
7 -8.871416121721268 -44.16490839583478
8 -7.958388950675726 -44.078303125872196
9 -10.64640114782378 -43.993555290419714
10 -6.125072779599577 -43.86306534422809
11 -11.244160749018192 -43.84874807044028
12 -8.31902683665976 -43.84199129025074
13 -5.160526596009731 -43.806385671938365
14 -6.2564361351542175 -43.80581985978556
15 -6.639097690582275 -42.7594767358323
16 -6.881909782998264 -42.66316468983175
17 -9.194250762462616 -42.33177224591743
18 -8.36390669643879 -41.774963389485094
19 -8.41768479347229 -41.410068073767725
20 -7.30189766548574 -41.17786296442943
21 -7.48260373622179 -40.723520424948155
22 -10.291809149086475 -40.527189756101116
23 -10.01175943017006 -40.49595848244517
24 -10.873061075806618 -40.429389880911344
25 -8.794971776893362 -40.05653450521898
26 -6.964800985530019 -39.59232357792555
27 -11.827279679477215 -39.54162101198148
28 -9.052452187985182 -39.195237471709476
29 -8.312608793377876 -39.172389579378766
30 -9.63785070180893 -38.51095963496708
31 -7.0994060398079455 -38.447265769744824
32 -9.732155258767307 -38.392107037026264
33 -6.212075058487244 -38.00745349944469
34 -6.4137159660458565 -37.46482488602393
35 -8.121964430902153 -37.10988160586883
36 -9.869225591421127 -34.27116723637227
37 -4.891701039750842 -34.14139118114101
38 -4.378783495631069 -33.263072731706835
39 -6.8237266056239605 -33.13344797200536
40 -7.256417416036129 -33.07825234291984
41 -7.759482070803642 -33.0321314765637
42 -10.818766824435443 -32.44934973065406
43 -7.912464156746864 -32.4007978120153
44 -5.962511397898197 -32.40063925734975
45 -2.7743858064641245 -30.734403792103194
46 -5.835767347365618 -30.57151371770873
47 -5.6565240528434515 -30.131236504472803
48 -6.2038532719016075 -29.99326722619033
49 -5.733546779956669 -29.66908258985071
50 -9.492561944120098 -29.297233511513635
51 -5.639149107038975 -29.288890423975797
52 -3.6300337712164037 -29.145878352769948
53 -4.513610664755106 -28.49601894351319
54 -5.160676399245858 -28.492023661124072
55 -8.95548628270626 -28.315961465855167
56 -5.584902126342058 -27.121110566589827
57 -2.2435823373962194 -26.064532595535336
58 -5.542903762310743 -25.520524278341334
59 -5.376337131485343 -25.27101421179229
60 -4.335378574207425 -25.066644278800943
61 -7.413819916546345 -24.925849381327673
62 -7.46886432915926 -24.188105669766596
63 -4.267830000055255 -23.48479966198816
64 -0.6106699556112289 -23.153943559703283
65 -4.5780769798439 -22.954730295117237
66 -5.274580962955952 -22.74124885266394
67 -4.119759418419562 -22.739273544503753
68 -2.181100756220985 -22.264945050603636
69 -4.444569782353938 -22.15569724300287
70 -5.212288543581963 -21.055920928583344
71 -1.7414280434604734 -20.543356562348553
72 -3.5007184594869614 -20.33499633836848
73 -2.464841630833689 -20.18157658281111
74 -2.8572888049529865 -19.58144410477429
75 -2.5222583669237792 -19.377225745334304
76 -5.382953591644764 -19.243135617403095
77 -1.5559077193029225 -19.060620225371707
78 -5.798159355297685 -18.964124524696246
79 -4.948454945348203 -18.448962308005108
80 -4.310870683693793 -17.740722019993825
81 -3.0241004023700953 -16.85889369985028
82 -2.3495032116770744 -16.3381194095591
83 -1.7501709008938633 -14.535892564189266
84 -1.624583378317766 -14.443670567499144
85 -0.8280050803441554 -14.200413010108107
86 -4.352545805799309 -13.936976181618805
87 -5.173617891967297 -13.862253042167257
88 -3.7783414986915886 -13.483098530680483
89 -1.6899180499603972 -13.455892754889845
90 -2.733242146554403 -13.355868275096913
91 -2.3380621115793474 -12.278515244993585
92 -0.8869078553980216 -12.227387460046547
93 -1.3714646369335242 -12.020717825467683
94 -3.5693380497395992 -11.910094799877324
95 -5.273070223629475 -11.400284019256157
96 -1.641215275041759 -11.134618158086587
97 -4.256768550723791 -10.859166921158222
98 -2.517975769005716 -9.595137958067907
99 -1.4728751866932726 -9.289921608799773
100 -3.0187182370573282 -8.230877068641124
101 -1.9922210969089065 -7.882363241796725
102 -2.742296307347715 -7.6478984168416355
103 -1.8446723809465766 -7.459623237418707
104 -2.5632280884310603 -7.124357312750265
105 -5.72890892624855 -7.05379065585803
106 -1.7772138500586152 -6.853091098326624
107 -2.3414310209918767 -6.6211384471641495
108 -6.131563223898411 -6.494555224953677
109 -1.0186957800760865 -6.117354180737655
110 -6.2930343225598335 -6.087055095509873
111 -4.408228039741516 -5.43500831968483
112 -1.0545083885081112 -5.105291741614599
113 -0.9426140517462045 -4.628649413275992
114 -0.007852325681596994 -4.471031187897325
115 -2.463128007017076 -4.455504779070034
116 -4.276999961584806 -4.2805498188182405
117 -3.9968227446079254 -3.7944735717969627
118 -0.24966119276359677 -2.9512438456190186
119 -0.5847276961576426 -2.541618164765197
train accuracy: 0.9827777777777778
validation accuracy: 0.975

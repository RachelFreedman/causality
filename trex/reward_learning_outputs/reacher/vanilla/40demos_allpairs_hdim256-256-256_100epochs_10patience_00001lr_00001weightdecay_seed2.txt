demos: (360, 50, 13)
demo_rewards: (360,)
sorted_train_rewards: [-54.46505855 -51.17814647 -49.6099766  -49.13001653 -49.03866708
 -49.0331838  -48.84818037 -48.58850771 -48.19509289 -47.96991522
 -47.53054283 -47.33487662 -47.14538324 -47.10889581 -46.92993068
 -46.92815321 -46.7141986  -46.51840523 -46.50762306 -46.08033935
 -45.96910073 -45.94004954 -45.34946654 -45.23627882 -45.23541431
 -44.99494244 -44.99184926 -44.7644634  -44.62310756 -44.53780078
 -44.32699505 -44.29565561 -43.84464734 -43.7920154  -43.22211069
 -43.2041764  -43.14444939 -42.63170505 -42.49903983 -42.18990615
 -42.17868583 -42.08941791 -41.97868948 -41.96459824 -41.91819405
 -41.89252759 -41.76630373 -41.75643755 -41.50687738 -41.35020296
 -41.32515547 -41.22179983 -41.18200046 -41.03992135 -41.03294083
 -40.99991982 -40.98448758 -40.86627436 -40.84937019 -40.63970267
 -40.33775477 -40.11576908 -39.97085477 -39.92712396 -39.89750906
 -39.72931271 -39.35357961 -39.14272956 -38.91826709 -38.52213956
 -38.29413029 -38.28199013 -38.28138827 -38.04661848 -37.94300503
 -37.88709355 -37.79694607 -37.70658771 -37.51412288 -37.37133772
 -37.3251261  -37.15321081 -37.11841822 -37.10344577 -36.99477768
 -36.69656462 -36.65639269 -36.38667894 -36.33815177 -36.28177521
 -36.03378032 -36.02351695 -35.97994531 -35.89835563 -35.77878287
 -35.74239985 -35.67327882 -35.43726783 -35.41556943 -35.28979253
 -35.19353216 -35.12234482 -35.07963015 -35.06137062 -35.02402835
 -35.01091702 -34.95588623 -34.91427059 -34.88495647 -34.86472602
 -34.63851695 -34.34266183 -33.86129846 -33.77802335 -33.7629591
 -33.680383   -33.4779641  -33.26579709 -33.04852912 -32.41611773
 -32.41053817 -32.02508724 -31.90686017 -31.71028623 -31.64335581
 -31.48876757 -31.44406811 -31.37649579 -31.35230126 -31.24419956
 -31.21642916 -31.15850305 -31.06744474 -30.95117262 -30.772676
 -30.76937686 -30.63466958 -30.59673213 -30.42034344 -30.36982897
 -30.36743577 -30.27374553 -30.10812976 -30.06299271 -29.98083048
 -29.97767593 -29.94526105 -29.22737048 -29.20595883 -28.99559546
 -28.95484843 -28.59191373 -28.5494411  -28.42026397 -28.31342715
 -28.1712167  -28.13409378 -27.79654204 -27.70726756 -27.60103341
 -27.27876998 -26.73963502 -26.7058308  -26.50715502 -26.39763335
 -26.36197542 -26.19987885 -25.99907479 -25.47507675 -25.31217996
 -25.29134047 -25.11681575 -24.96039421 -24.91350015 -24.71538539
 -24.58444108 -24.25434383 -24.12244562 -23.96980649 -23.76917367
 -23.71252839 -23.48246476 -23.05835519 -22.79553869 -22.61211306
 -22.55283027 -22.53572133 -22.14406511 -21.86139195 -21.84262733
 -21.72370465 -21.7167854  -21.64185366 -21.62657756 -21.62597106
 -20.76562386 -20.68104247 -20.66287704 -20.66020817 -20.5100558
 -20.35567458 -20.10309328 -19.94369578 -19.67432429 -19.50391949
 -19.30989567 -19.03153444 -19.02121155 -18.9341265  -18.87189699
 -18.77655165 -18.71737606 -18.4634721  -18.2465562  -18.08445937
 -17.94319772 -17.85124778 -17.82515236 -17.77364067 -17.67989719
 -17.48133145 -17.10217865 -17.06769934 -16.68041488 -16.325741
 -16.25152473 -16.24688067 -16.23288157 -16.01542464 -15.4984665
 -15.44345121 -15.26574292 -15.017961   -14.79738881 -14.47834852
 -14.39794675 -14.24892226 -14.15625316 -13.8953595  -13.73405689
 -13.43873671 -13.16397094 -12.97987294 -12.6606731  -12.5410027
 -12.53347897 -12.51335454 -12.37040308 -12.29992996 -12.26611853
 -12.16816754 -12.13592928 -11.97407887 -11.96626278 -11.75626016
 -11.47155848 -11.04421156 -10.859881   -10.78272714 -10.76409188
 -10.64954177 -10.49423816 -10.01645863  -9.92399886  -9.67992171
  -9.56750101  -9.4003034   -8.4761537   -8.40276084  -8.32638751
  -8.0269637   -7.70107293  -7.68743448  -7.67259169  -7.57539849
  -7.54460172  -7.3740849   -7.36244313  -7.34546388  -7.1893336
  -7.15421432  -7.10832736  -7.08431127  -6.95906356  -6.92008425
  -6.77694649  -6.72206384  -6.71997062  -6.64795748  -6.51820418
  -6.47884361  -6.29894775  -6.05448903  -5.89467275  -5.85405865
  -5.64485143  -5.61579673  -5.39544196  -5.38326081  -5.3472021
  -5.25793019  -5.24856721  -5.07848501  -5.06486011  -5.02795798
  -4.90228293  -4.82757292  -4.63049542  -4.37983153  -4.35856953
  -4.230832    -4.0660223   -4.03104862  -4.00401798  -3.97870856
  -3.65032555  -3.3322555   -3.329867    -3.29356852  -2.88591659
  -2.83192847  -2.64166233  -2.24005036  -1.91361965]
sorted_val_rewards: [-55.21445752 -46.46343149 -45.47189047 -41.86250044 -39.17708227
 -39.07321282 -38.77785603 -37.47672391 -36.58958472 -33.17157461
 -31.00231541 -30.64081715 -30.52049104 -29.66568464 -28.11939797
 -27.77638947 -27.50815322 -21.44290047 -18.65414818 -18.36358054
 -17.64064603 -17.27921081 -16.44066556 -16.35270212 -16.20186147
 -15.29527985 -13.88057881 -13.69675016 -11.9523875  -11.4307611
  -7.5455064   -7.52029309  -3.38446715  -3.07904644  -2.67390706
  -2.49009827]
maximum traj length 50
maximum traj length 50
num train_obs 780
num train_labels 780
num val_obs 630
num val_labels 630
num_distractorfeatures: 8
ModuleList(
  (0): Linear(in_features=13, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 135424
Number of trainable paramters: 135424
device: cuda:1
end of epoch 0: val_loss 0.3388955810172971, val_acc 0.8746031746031746
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.38451888782032634, val_acc 0.8587301587301587
trigger times: 1
end of epoch 2: val_loss 0.32610722958937444, val_acc 0.9
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.35272008244534675, val_acc 0.9
trigger times: 1
end of epoch 4: val_loss 0.30751019355634324, val_acc 0.8952380952380953
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.4308886022023627, val_acc 0.8666666666666667
trigger times: 1
end of epoch 6: val_loss 0.3769600877852093, val_acc 0.8777777777777778
trigger times: 2
end of epoch 7: val_loss 0.33331493014694275, val_acc 0.8936507936507937
trigger times: 3
end of epoch 8: val_loss 0.3615081552350138, val_acc 0.8984126984126984
trigger times: 4
end of epoch 9: val_loss 0.38803196805205, val_acc 0.8920634920634921
trigger times: 5
end of epoch 10: val_loss 0.3631727142611073, val_acc 0.8888888888888888
trigger times: 6
end of epoch 11: val_loss 0.5080677039384467, val_acc 0.8888888888888888
trigger times: 7
end of epoch 12: val_loss 0.4532906534893253, val_acc 0.8904761904761904
trigger times: 8
end of epoch 13: val_loss 0.5641534882485041, val_acc 0.8809523809523809
trigger times: 9
end of epoch 14: val_loss 0.5189099302486829, val_acc 0.8904761904761904
trigger times: 10
Early stopping.
0 -20.317319694906473 -55.21445752462833
1 -10.637886088341475 -46.463431492780636
2 -19.264111559838057 -45.47189046691946
3 -14.108408448286355 -41.86250043832566
4 -15.778775734361261 -39.17708227320146
5 -10.708347777253948 -39.07321282259262
6 -10.330402083694935 -38.77785603121339
7 -13.770729788462631 -37.47672390906202
8 -6.798363740555942 -36.58958472489279
9 -7.620002624578774 -33.17157461008636
10 -4.429150387819391 -31.002315411370166
11 -20.069087956100702 -30.640817151541906
12 -16.594390634447336 -30.520491036162525
13 -13.401519306004047 -29.66568463563045
14 -10.839866458438337 -28.11939796920964
15 -8.091905739158392 -27.776389469559156
16 -13.337710004299879 -27.50815321826314
17 -3.103660429827869 -21.442900472353543
18 -1.9703845200128853 -18.654148179968352
19 -4.954445235431194 -18.363580535271076
20 -6.264780916273594 -17.640646027280784
21 -1.8806063951924443 -17.27921080502319
22 -4.681558582000434 -16.440665556315388
23 5.501420052256435 -16.352702123112557
24 -0.8724474022164941 -16.201861466781004
25 0.5965028218924999 -15.295279851466828
26 5.2511285385116935 -13.880578810015049
27 5.851006286451593 -13.696750155340938
28 2.6695188377052546 -11.952387503909815
29 8.029311314225197 -11.430761099930217
30 9.612908232025802 -7.545506402407465
31 9.27848482131958 -7.5202930852533045
32 10.552809591405094 -3.3844671463622564
33 10.862731847912073 -3.079046443285274
34 13.242830034345388 -2.673907055233777
35 14.368137011304498 -2.49009826539426
train accuracy: 0.9551282051282052
validation accuracy: 0.8904761904761904
